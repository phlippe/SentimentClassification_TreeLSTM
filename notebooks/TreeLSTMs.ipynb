{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jILqpPLlE9r0"
   },
   "source": [
    "# Practical 2: Encoding Sentences with Neural Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8JXOZ5uhQ8Qq"
   },
   "source": [
    "In this practical we will train neural models to encode sentences, after which we can use our sentence representation for a downstream task such as sentiment classification. Rather than simply answering questions, like in the previous lab, **this time you are expected to write a four-page scientific report with your findings**. You will be judged by the quality of your report (see below). In this notebook, we will help you to develop models for your experiments. You do not have to hand in your answers, but please do include a link to a private Github gist with your code in your report as a footnote. \n",
    "\n",
    "### Data set\n",
    "We will make use of the [Stanford Sentiment Treebank](https://nlp.stanford.edu/sentiment/) (SST), which provides sentences, their tree structure, and (fine-grained) sentiment scores.\n",
    "This dataset is different from the one we used in the first practical. \n",
    "Before, a review consisted of several sentences, and we would have one sentiment score for it. Now, a review is a single sentence, for which we get a sentiment score. The special thing about our new data set is that we get a binary parse tree for each sentence, and a sentiment score has been assigned to each node in the tree. (We will look at an example below.)\n",
    "\n",
    "For the first part of this practical we will only make use of the tokens, but in the end we will also exploit the tree-structure that is provided!\n",
    "\n",
    "We will cover the following approaches:\n",
    "\n",
    "- Bag-of-words (BOW)\n",
    "- Continuous bag-of-words (CBOW)\n",
    "- Deep continuous bag-of-words (Deep CBOW)\n",
    "- LSTM\n",
    "- TreeLSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bSmmXQoC8ebA"
   },
   "source": [
    "### Important: Report \n",
    "\n",
    "The main purpose of this lab is for you to learn how to answer research questions by experimenting and then writing a scientific report.\n",
    "Your grade will be based on the quality of your report, not on the exercises in this notebook.\n",
    "You can find the requirements for the report at the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YbNKef3lymaj"
   },
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9jxTkpg59FlU"
   },
   "source": [
    "Let's first download the data set and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WZp53HmMP3F2"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "plt.style.use('default')\n",
    "\n",
    "\n",
    "CHECKPOINT_DIR = \"checkpoints/\"\n",
    "if not os.path.exists(CHECKPOINT_DIR):\n",
    "    os.makedirs(CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TovFkDTgE_d6"
   },
   "outputs": [],
   "source": [
    "# !wget http://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip\n",
    "# !unzip trainDevTestTrees_PTB.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0IpAphkBO5eW"
   },
   "outputs": [],
   "source": [
    "# this function reads in a textfile and fixes an issue with \\\\\n",
    "def filereader(path): \n",
    "  with open(path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "      yield line.strip().replace(\"\\\\\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ylkIopm0QJML"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3 (2 It) (4 (4 (2 's) (4 (3 (2 a) (4 (3 lovely) (2 film))) (3 (2 with) (4 (3 (3 lovely) (2 performances)) (2 (2 by) (2 (2 (2 Buy) (2 and)) (2 Accorsi))))))) (2 .)))\n"
     ]
    }
   ],
   "source": [
    "# Let's look at a data point.\n",
    "# If you look closely you will find that this is a flattened tree, \n",
    "# with sentiment scores at every node, and words as the leaves.\n",
    "s = next(filereader(\"trees/dev.txt\"))\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7_U7HTFwdrWt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              3                                                                     \n",
      "  ____________|____________________                                                  \n",
      " |                                 4                                                \n",
      " |        _________________________|______________________________________________   \n",
      " |       4                                                                        | \n",
      " |    ___|______________                                                          |  \n",
      " |   |                  4                                                         | \n",
      " |   |         _________|__________                                               |  \n",
      " |   |        |                    3                                              | \n",
      " |   |        |               _____|______________________                        |  \n",
      " |   |        |              |                            4                       | \n",
      " |   |        |              |            ________________|_______                |  \n",
      " |   |        |              |           |                        2               | \n",
      " |   |        |              |           |                 _______|___            |  \n",
      " |   |        3              |           |                |           2           | \n",
      " |   |    ____|_____         |           |                |        ___|_____      |  \n",
      " |   |   |          4        |           3                |       2         |     | \n",
      " |   |   |     _____|___     |      _____|_______         |    ___|___      |     |  \n",
      " 2   2   2    3         2    2     3             2        2   2       2     2     2 \n",
      " |   |   |    |         |    |     |             |        |   |       |     |     |  \n",
      " It  's  a  lovely     film with lovely     performances  by Buy     and Accorsi  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can use NLTK to print the tree structure more nicely\n",
    "from nltk import Tree\n",
    "from nltk.treeprettyprinter import TreePrettyPrinter\n",
    "tree = Tree.fromstring(s)\n",
    "print(TreePrettyPrinter(tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ekAWKsji9t93"
   },
   "source": [
    "You can see the sentiment annotations here **at every node**! 0 is very negative, 4 is very positive. For now, we will only use the sentiment score at the **root node**; this is the score for the complete sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DKynLm0xPKr2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', \"'s\", 'a', 'lovely', 'film', 'with', 'lovely', 'performances', 'by', 'Buy', 'and', 'Accorsi', '.']\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "# Let's first make a function that extracts the tokens (the leaves).\n",
    "\n",
    "def tokens_from_treestring(s):\n",
    "  \"\"\"extract the tokens from a sentiment tree\"\"\"\n",
    "  return re.sub(r\"\\([0-9] |\\)\", \"\", s).split()\n",
    " \n",
    "# let's try it on our example tree\n",
    "tokens = tokens_from_treestring(s)\n",
    "print(tokens)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B8vFkeqN-NLP"
   },
   "source": [
    "> *Warning: you could also parse a treestring using NLTK and ask it to return the leaves, but there seems to be an issue with NLTK not always correctly parsing the input, so do not rely on it.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Akr9K_Mv4dym"
   },
   "outputs": [],
   "source": [
    "# We will also need the following function, but you can ignore this for now.\n",
    "# It is explained later on.\n",
    "\n",
    "SHIFT = 0\n",
    "REDUCE = 1\n",
    "\n",
    "\n",
    "def transitions_from_treestring(s):\n",
    "  s = re.sub(\"\\([0-5] ([^)]+)\\)\", \"0\", s)\n",
    "  s = re.sub(\"\\)\", \" )\", s)\n",
    "  s = re.sub(\"\\([0-4] \", \"\", s)\n",
    "  s = re.sub(\"\\([0-4] \", \"\", s)\n",
    "  s = re.sub(\"\\)\", \"1\", s)\n",
    "  return list(map(int, s.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mNtPdlwPgRat"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trees/train.txt  8544\n",
      "trees/dev.txt    1101\n",
      "trees/test.txt   2210\n"
     ]
    }
   ],
   "source": [
    "# Now let's first see how large our data sets are.\n",
    "for path in (\"trees/train.txt\", \"trees/dev.txt\", \"trees/test.txt\"):\n",
    "  print(\"{:16s} {:4d}\".format(path, sum(1 for _ in filereader(path))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HexlSqTR_UrY"
   },
   "source": [
    "You can see that this data set is not very large! That's probably because it required so much manual annotation. However, it's large enough to train a neural network on.\n",
    "\n",
    "It will be useful to store each data example in an `Example` object,\n",
    "containing everything that we may need for each data point.\n",
    "It will contain the tokens, the tree, the top-level sentiment label, and \n",
    "the transitions (explained later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4I07Hb_-q8wg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 8544\n",
      "dev 1101\n",
      "test 2210\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "from nltk import Tree\n",
    "\n",
    "# A simple way to define a class is using namedtuple.\n",
    "Example = namedtuple(\"Example\", [\"tokens\", \"tree\", \"label\", \"transitions\", \"index\", \"loss\", \"transition_matrix\"])\n",
    "\n",
    "   \n",
    "def examplereader(path, lower=False):\n",
    "    \"\"\"Returns all examples in a file one by one.\"\"\"\n",
    "    index=0\n",
    "    for line in filereader(path):\n",
    "        line = line.lower() if lower else line\n",
    "        tokens = tokens_from_treestring(line)\n",
    "        tree = Tree.fromstring(line)  # use NLTK's Tree\n",
    "        label = int(line[1])\n",
    "        trans = transitions_from_treestring(line)\n",
    "        index += 1\n",
    "        yield Example(tokens=tokens, tree=tree, label=label, transitions=trans, index=index, loss=list(), transition_matrix=None)\n",
    "  \n",
    "\n",
    "# Let's load the data into memory.\n",
    "LOWER = False  # we will keep the original casing\n",
    "train_data = list(examplereader(\"trees/train.txt\", lower=LOWER))\n",
    "dev_data = list(examplereader(\"trees/dev.txt\", lower=LOWER))\n",
    "test_data = list(examplereader(\"trees/test.txt\", lower=LOWER))\n",
    "\n",
    "print(\"train\", len(train_data))\n",
    "print(\"dev\", len(dev_data))\n",
    "print(\"test\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6KM0bDyeVZtP"
   },
   "source": [
    "Let's check out an example object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J8mwcaZwxP1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First train example: Example(tokens=['It', \"'s\", 'a', 'lovely', 'film', 'with', 'lovely', 'performances', 'by', 'Buy', 'and', 'Accorsi', '.'], tree=Tree('3', [Tree('2', ['It']), Tree('4', [Tree('4', [Tree('2', [\"'s\"]), Tree('4', [Tree('3', [Tree('2', ['a']), Tree('4', [Tree('3', ['lovely']), Tree('2', ['film'])])]), Tree('3', [Tree('2', ['with']), Tree('4', [Tree('3', [Tree('3', ['lovely']), Tree('2', ['performances'])]), Tree('2', [Tree('2', ['by']), Tree('2', [Tree('2', [Tree('2', ['Buy']), Tree('2', ['and'])]), Tree('2', ['Accorsi'])])])])])])]), Tree('2', ['.'])])]), label=3, transitions=[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1], index=1, loss=[], transition_matrix=None)\n",
      "First train example tokens: ['It', \"'s\", 'a', 'lovely', 'film', 'with', 'lovely', 'performances', 'by', 'Buy', 'and', 'Accorsi', '.']\n",
      "First train example label: 3\n"
     ]
    }
   ],
   "source": [
    "example = dev_data[0]\n",
    "print(\"First train example:\", example)\n",
    "print(\"First train example tokens:\", example.tokens)\n",
    "print(\"First train example label:\",  example.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-WDSprDBVcr-"
   },
   "source": [
    "#### Vocabulary \n",
    "To work with this data we will need a vocabulary.\n",
    "The job of the vocabulary is to map each word to a unique ID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VvNgKx7usRSt"
   },
   "outputs": [],
   "source": [
    "# Here we first define a class that can map a word to an ID (w2i)\n",
    "# and back (i2w).\n",
    "\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "\n",
    "\n",
    "class OrderedCounter(Counter, OrderedDict):\n",
    "  \"\"\"Counter that remembers the order elements are first seen\"\"\"\n",
    "  def __repr__(self):\n",
    "    return '%s(%r)' % (self.__class__.__name__,\n",
    "                      OrderedDict(self))\n",
    "  def __reduce__(self):\n",
    "    return self.__class__, (OrderedDict(self),)\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "  \"\"\"A vocabulary, assigns IDs to tokens\"\"\"\n",
    "  \n",
    "  def __init__(self):\n",
    "    self.freqs = OrderedCounter()\n",
    "    self.w2i = {}\n",
    "    self.i2w = []\n",
    "\n",
    "  def count_token(self, t):\n",
    "    self.freqs[t] += 1\n",
    "    \n",
    "  def add_token(self, t):\n",
    "    self.w2i[t] = len(self.w2i)\n",
    "    self.i2w.append(t)    \n",
    "    \n",
    "  def build(self, min_freq=0):\n",
    "    self.add_token(\"<unk>\")  # reserve 0 for <unk> (unknown words)\n",
    "    self.add_token(\"<pad>\")  # reserve 1 for <pad> (discussed later)   \n",
    "    \n",
    "    tok_freq = list(self.freqs.items())\n",
    "    tok_freq.sort(key=lambda x: x[1], reverse=True)\n",
    "    for tok, freq in tok_freq:\n",
    "      if freq >= min_freq:\n",
    "        self.add_token(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kOvkH_llVsoW"
   },
   "source": [
    "Let's build the token vocabulary!\n",
    "\n",
    "When randomly initializing word vectors, we take the words in our training\n",
    "set and assign them unique IDs, and assign all other words to <unk>, \n",
    "because those we cannot learn a vector for based on our training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GwGQgQQBNUSq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 18280\n"
     ]
    }
   ],
   "source": [
    "# This process should be deterministic and should have the same result \n",
    "# if run multiple times on the same data set.\n",
    "\n",
    "v = Vocabulary()\n",
    "for data_set in (train_data,):\n",
    "  for ex in data_set:\n",
    "    for token in ex.tokens:\n",
    "      v.count_token(token)\n",
    "\n",
    "v.build()\n",
    "print(\"Vocabulary size:\", len(v.w2i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-UNIedPrPdCw"
   },
   "source": [
    "Let's have a closer look at the properties of our vocabulary. Having a good idea of what it is like can facilitate data analysis and debugging later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oJyuogmh0CA7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1973\n"
     ]
    }
   ],
   "source": [
    "# What is the ID for \"century?\"\n",
    "print(v.w2i[\"century\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O8OkPQ8Zv-rI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 0: <unk>\n",
      "Word 1: <pad>\n",
      "Word 2: .\n",
      "Word 3: ,\n",
      "Word 4: the\n",
      "Word 5: and\n",
      "Word 6: a\n",
      "Word 7: of\n",
      "Word 8: to\n",
      "Word 9: 's\n"
     ]
    }
   ],
   "source": [
    "# What are the first 10 words in the vocabulary?\n",
    "for i in range(10):\n",
    "    print(\"Word \" + str(i) + \": \" + v.i2w[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kmXwu02lOLWI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency top  1 (8024): .\n",
      "Frequency top  2 (7131): ,\n",
      "Frequency top  3 (6037): the\n",
      "Frequency top  4 (4431): and\n",
      "Frequency top  5 (4403): a\n",
      "Frequency top  6 (4386): of\n",
      "Frequency top  7 (2995): to\n",
      "Frequency top  8 (2544): 's\n",
      "Frequency top  9 (2536): is\n",
      "Frequency top 10 (1915): that\n"
     ]
    }
   ],
   "source": [
    "# What are the 10 most common words?\n",
    "common_indices_list = sorted([(i, v.freqs[v.i2w[i]]) for i in range(len(v.freqs))], key=lambda x: -x[1])\n",
    "for i in range(10):\n",
    "    print(\"Frequency top \" + str(i+1).rjust(2) + \" (\" + str(common_indices_list[i][1]) + \"): \" + v.i2w[common_indices_list[i][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "__NDPaCeOT_m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words with frequency 1: 9541\n"
     ]
    }
   ],
   "source": [
    "# And how many words are there with frequency 1?\n",
    "no_freq_1 = sum([v.freqs[v.i2w[i]]==1 for i in range(len(v.freqs))])\n",
    "print(\"Words with frequency 1: \" + str(no_freq_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xKHocugctZGM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disobedience\n",
      "aloof\n",
      "homicide\n",
      "curious\n",
      "Discursive\n",
      "heroine\n",
      "emphasis\n",
      "find\n",
      "explodes\n",
      "three-minute\n",
      "spooky\n",
      "asks\n",
      "off\n",
      "Benjamins\n",
      "devastating\n",
      "tackled\n",
      "incredible\n",
      "discovery\n",
      "Society\n",
      "situations\n"
     ]
    }
   ],
   "source": [
    "# Finally 20 random words from the vocabulary.\n",
    "# This is a simple way to get a feeling for the data.\n",
    "for i in range(20):\n",
    "    word_index = random.randint(0,len(v.freqs)-1)\n",
    "    print(v.i2w[word_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nGWaZahKV_dH"
   },
   "source": [
    "#### Sentiment label vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AmTC-rvQelpl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['very negative', 'negative', 'neutral', 'positive', 'very positive']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's map the sentiment labels 0-4 to a more readable form\n",
    "i2t = [\"very negative\", \"negative\", \"neutral\", \"positive\", \"very positive\"]\n",
    "i2t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D7UI26DP2dr2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('very negative', 0),\n",
       "             ('negative', 1),\n",
       "             ('neutral', 2),\n",
       "             ('positive', 3),\n",
       "             ('very positive', 4)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And let's also create the opposite mapping.\n",
    "# We won't use a Vocabulary for this (although we could), since the labels\n",
    "# are already numeric.\n",
    "t2i = OrderedDict({p : i for p, i in zip(i2t, range(len(i2t)))})\n",
    "t2i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y0067ax54-rd"
   },
   "source": [
    "## Installing PyTorch\n",
    "\n",
    "We are going to need PyTorch and Google Colab does not have it installed by default. Run the cell below to install it.\n",
    "\n",
    "*For installing PyTorch in your own computer, follow the instructions on [pytorch.org](pytorch.org) instead. This is for Google Colab only.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qKQMGtkR5KWr"
   },
   "outputs": [],
   "source": [
    "# http://pytorch.org/\n",
    "from os.path import exists\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BhiRqhTM5V4c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.1.post2'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this should result in the PyTorch version\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BYt8uTyGCKc7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyTorch can run on CPU or on Nvidia GPU (video card) using CUDA\n",
    "# This cell selects the GPU if one is available.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2d1VMOOYx1Bw"
   },
   "outputs": [],
   "source": [
    "# Seed manually to make runs reproducible\n",
    "# You need to set this again if you do multiple runs of the same model\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# When running on the CuDNN backend two further options must be set for reproducibility\n",
    "if torch.cuda.is_available():\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uWBTzkuE3CtZ"
   },
   "source": [
    "# BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TBAjYYySOA5W"
   },
   "source": [
    "Our first model is a super simple neural **bag-of-words (BOW) model**.\n",
    "Unlike the bag-of-words model that you used in the previous lab, here we associate each word with a vector of size 5, exactly our number of sentiment classes. \n",
    "\n",
    "To make a classification, we simply **sum** the vectors of the words in our sentence and a bias vector. Because we sum the vectors, we lose word order: that's why we call this a neural bag-of-words model.\n",
    "\n",
    "```\n",
    "this   [0.0, 0.1, 0.1, 0.1, 0.0]\n",
    "movie  [0.1, 0.1, 0.1, 0.2, 0.1]\n",
    "is     [0.0, 0.1, 0.0, 0.0, 0.0]\n",
    "stupid [0.9, 0.5, 0.1, 0.0, 0.0]\n",
    "\n",
    "bias   [0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "--------------------------------\n",
    "sum    [0.9, 0.8, 0.3, 0.3, 0.1]\n",
    "\n",
    "argmax: 0 (very negative)\n",
    "```\n",
    "\n",
    "Now, the **argmax** of this sum is our predicted label.\n",
    "\n",
    "We initialize all vectors *randomly* and train them using cross-entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rLtBAIQGynkB"
   },
   "source": [
    "#### Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QZfNklWf3tvs"
   },
   "outputs": [],
   "source": [
    "class BOW(nn.Module):\n",
    "  \"\"\"A simple bag-of-words model\"\"\"\n",
    "\n",
    "  def __init__(self, vocab_size, embedding_dim, vocab):\n",
    "    super(BOW, self).__init__()\n",
    "    self.vocab = vocab\n",
    "    \n",
    "    # this is a trained look-up table with word embeddings\n",
    "    self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "    \n",
    "    # this is a trained bias term\n",
    "    self.bias = nn.Parameter(torch.zeros(embedding_dim), requires_grad=True)        \n",
    "\n",
    "  def forward(self, inputs):\n",
    "    # this is the forward pass of the neural network\n",
    "    # given inputs, it computes the output\n",
    "\n",
    "    # this looks up the embeddings for each word ID in inputs\n",
    "    # the result is a sequence of word embeddings\n",
    "    embeds = self.embed(inputs)\n",
    "    \n",
    "    # the output is the sum across the time dimension (1)\n",
    "    # with the bias term added\n",
    "    logits = embeds.sum(1) + self.bias\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eKHvBnoBAr6z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW(\n",
      "  (embed): Embedding(18280, 5)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Let's create a model.\n",
    "vocab_size = len(v.w2i)\n",
    "n_classes = len(t2i)\n",
    "bow_model = BOW(vocab_size, n_classes, v)\n",
    "print(bow_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vfCx-HvMH1qQ"
   },
   "source": [
    "> **Hey, wait, where is the bias vector?**\n",
    "> PyTorch does not print Parameters, only Modules!\n",
    "\n",
    "> We can print it ourselves though, to check that it is there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fhvk5HenAroT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias                     [5]          requires_grad=True\n",
      "embed.weight             [18280, 5]   requires_grad=True\n",
      "\n",
      "Total parameters: 91405\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Here we print each parameter name, shape, and if it is trainable.\n",
    "def print_parameters(model):\n",
    "  total = 0\n",
    "  for name, p in model.named_parameters():\n",
    "    total += np.prod(p.shape)\n",
    "    print(\"{:24s} {:12s} requires_grad={}\".format(name, str(list(p.shape)), p.requires_grad))\n",
    "  print(\"\\nTotal parameters: {}\\n\".format(total))\n",
    "    \n",
    "\n",
    "print_parameters(bow_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WSAw292WxuP4"
   },
   "source": [
    "#### Preparing an example for input\n",
    "\n",
    "To feed sentences to our PyTorch model, we need to convert a sequence of tokens to a sequence of IDs. The `prepare_example` function below takes care of this for us. With these IDs we index the word embedding table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YWeGTC_OGReV"
   },
   "outputs": [],
   "source": [
    "def prepare_example(example, vocab):\n",
    "  \"\"\"\n",
    "  Map tokens to their IDs for 1 example\n",
    "  \"\"\"\n",
    "  \n",
    "  # vocab returns 0 if the word is not there\n",
    "  x = [vocab.w2i.get(t, 0) for t in example.tokens]\n",
    "  \n",
    "  x = torch.LongTensor([x])\n",
    "  x = x.to(device)\n",
    "  \n",
    "  y = torch.LongTensor([example.label])\n",
    "  y = y.to(device)\n",
    "  \n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oKNQjEc0yXnJ"
   },
   "source": [
    "#### Evaluation\n",
    "We will need one more thing: an evaluation metric.\n",
    "How many predictions do we get right? The accuracy will tell us.\n",
    "Make sure that you understand this code block.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yGmQLcVYKZsh"
   },
   "outputs": [],
   "source": [
    "\n",
    "def simple_evaluate(model, data, prep_fn=prepare_example, **kwargs):\n",
    "  \"\"\"Accuracy of a model on given data set.\"\"\"\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  model.eval()  # disable dropout (explained later)\n",
    "\n",
    "  for example in data:\n",
    "    \n",
    "    # convert the example input and label to PyTorch tensors\n",
    "    x, target = prep_fn(example, model.vocab)\n",
    "\n",
    "    # forward pass\n",
    "    # get the output from the neural network for input x\n",
    "    with torch.no_grad():\n",
    "      logits = model(x)\n",
    "    \n",
    "    # get out the prediction\n",
    "    prediction = logits.argmax(dim=-1)\n",
    "    \n",
    "    # add the number of correct predictions to the total correct\n",
    "    correct += (prediction == target).sum().item()\n",
    "    total += 1\n",
    "\n",
    "  return correct, total, correct / float(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dIk6OtSdzGRP"
   },
   "source": [
    "#### Example feed\n",
    "For stochastic gradient descent (SGD) we will need a random training example for every update.\n",
    "We implement this by shuffling the training data and returning examples one by one using `yield`.\n",
    "\n",
    "Shuffling is optional so that we get to use this to get validation and test examples, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dxDFOZLfCXvJ"
   },
   "outputs": [],
   "source": [
    "def get_examples(data, shuffle=True, **kwargs):\n",
    "  \"\"\"Shuffle data set and return 1 example at a time (until nothing left)\"\"\"\n",
    "  if shuffle:\n",
    "    print(\"Shuffling training data\")\n",
    "    random.shuffle(data)  # shuffle training data each epoch\n",
    "  for example in data:\n",
    "    yield example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g09SM8yb2cjx"
   },
   "source": [
    "#### Exercise: Training function\n",
    "\n",
    "Your task is now to complete the training loop below.\n",
    "Before you do so, please read the section about optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TVfUukVdM_1c"
   },
   "source": [
    "**Optimization**\n",
    "\n",
    "As mentioned in the \"Introduction to PyTorch\" notebook, one of the perks of using PyTorch are its automatic differentiation abilities. We will use these to train our BOW model. \n",
    "\n",
    "We train our model by feeding it an input and performing **forward** pass, which results in an output for which we obtain a **loss** with our loss function.\n",
    "After the gradients are calculated in the **backward** pass, we can take a step on the loss surface towards more optimal parameter settings (e.g. gradient descent). \n",
    "\n",
    "The package we will use to do this optimization is [torch.optim](https://pytorch.org/docs/stable/optim.html). Besides implementations of stochastic gradient descent (SGD), this package also implements the optimization algorithm Adam, which we'll be using in this practical. \n",
    "For the purposes of this assignment you do not need to know what Adam does besides that it uses gradient information to update our model parameters by calling: \n",
    "\n",
    "```\n",
    "optimizer.step()\n",
    "```\n",
    "Remember when we updated our parameters in the PyTorch tutorial in a loop?\n",
    "\n",
    "\n",
    "```python\n",
    "# update weights\n",
    "learning_rate = 0.5\n",
    "for f in net.parameters():\n",
    "    # for each parameter, take a small step in the opposite dir of the gradient\n",
    "    p.data = p.data - p.grad.data * learning_rate\n",
    "\n",
    "```\n",
    "The function call optimizer.step() does effectively the same thing.\n",
    "\n",
    "*(If you want to know more about optimization algorithms using gradient information [this blog](http://ruder.io/optimizing-gradient-descent/.) gives a nice intuitive overview.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ktFnKBux25lD"
   },
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, num_iterations=10000, \n",
    "                print_every=1000, eval_every=1000,\n",
    "                batch_fn=get_examples, \n",
    "                prep_fn=prepare_example,\n",
    "                eval_fn=simple_evaluate,\n",
    "                batch_size=1, eval_batch_size=None):\n",
    "    \"\"\"Train a model.\"\"\"  \n",
    "    iter_i = 0\n",
    "    train_loss = 0.\n",
    "    print_num = 0\n",
    "    start = time.time()\n",
    "    criterion = nn.CrossEntropyLoss() # loss function\n",
    "    best_eval = 0.\n",
    "    best_iter = 0\n",
    "\n",
    "    # store train loss and validation accuracy during training\n",
    "    # so we can plot them afterwards\n",
    "    losses = []\n",
    "    accuracies = []  \n",
    "\n",
    "    if eval_batch_size is None:\n",
    "        eval_batch_size = batch_size\n",
    "\n",
    "    while True:  # when we run out of examples, shuffle and continue\n",
    "        for batch in batch_fn(train_data, batch_size=batch_size):\n",
    "\n",
    "            # forward pass\n",
    "            model.train()\n",
    "            x, targets = prep_fn(batch, model.vocab)\n",
    "            logits = model(x)\n",
    "\n",
    "            B = targets.size(0)  # later we will use B examples per update\n",
    "\n",
    "            # compute cross-entropy loss (our criterion)\n",
    "            # note that the cross entropy loss function computes the softmax for us\n",
    "            loss = criterion(logits.view([B, -1]), targets.view(-1))\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # backward pass\n",
    "            # Tip: check the Introduction to PyTorch notebook.\n",
    "\n",
    "            # erase previous gradients\n",
    "            model.zero_grad()\n",
    "            \n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            # update weights - take a small step in the opposite dir of the gradient\n",
    "            optimizer.step()\n",
    "\n",
    "            print_num += 1\n",
    "            iter_i += 1\n",
    "\n",
    "            # print info\n",
    "            if iter_i % print_every == 0:\n",
    "                print(\"Iter %r: loss=%.4f, time=%.2fs\" % \n",
    "                (iter_i, train_loss, time.time()-start))\n",
    "                losses.append(train_loss)\n",
    "                print_num = 0        \n",
    "                train_loss = 0.\n",
    "\n",
    "            # evaluate\n",
    "            if iter_i % eval_every == 0:\n",
    "                _, _, accuracy = eval_fn(model, dev_data, batch_size=eval_batch_size,\n",
    "                         batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "                accuracies.append(accuracy)\n",
    "                print(\"iter %r: dev acc=%.4f\" % (iter_i, accuracy))       \n",
    "\n",
    "                # save best model parameters\n",
    "                if accuracy > best_eval:\n",
    "                    print(\"new highscore\")\n",
    "                    best_eval = accuracy\n",
    "                    best_iter = iter_i\n",
    "                    path = \"{}.pt\".format(model.__class__.__name__)\n",
    "                    ckpt = {\n",
    "                    \"state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"best_eval\": best_eval,\n",
    "                    \"best_iter\": best_iter\n",
    "                    }\n",
    "                    torch.save(ckpt, path)\n",
    "\n",
    "            # done training\n",
    "            if iter_i == num_iterations:\n",
    "                print(\"Done training\")\n",
    "\n",
    "                # evaluate on train, dev, and test with best model\n",
    "                print(\"Loading best model\")\n",
    "                path = \"{}.pt\".format(model.__class__.__name__)        \n",
    "                ckpt = torch.load(path)\n",
    "                model.load_state_dict(ckpt[\"state_dict\"])\n",
    "\n",
    "                _, _, train_acc = eval_fn(\n",
    "                model, train_data, batch_size=eval_batch_size, \n",
    "                batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "                _, _, dev_acc = eval_fn(\n",
    "                model, dev_data, batch_size=eval_batch_size,\n",
    "                batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "                _, _, test_acc = eval_fn(\n",
    "                model, test_data, batch_size=eval_batch_size, \n",
    "                batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "\n",
    "                print(\"best model iter {:d}: \"\n",
    "                \"train acc={:.4f}, dev acc={:.4f}, test acc={:.4f}\".format(\n",
    "                best_iter, train_acc, dev_acc, test_acc))\n",
    "\n",
    "                return losses, accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XEPsLvI-3D5b"
   },
   "source": [
    "### Training the BOW model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E9mB1_XhMPNN"
   },
   "source": [
    "# CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pWk78FvNMw4o"
   },
   "source": [
    "We now continue with a **continuous bag-of-words (CBOW)** model.\n",
    "It is similar to the BOW model, but now embeddings can have a dimension of *arbitrary size*. \n",
    "This means that we can make them bigger and learn more aspects of each word. We will still sum the vectors to get a sentence representation, but now the result is no longer conveniently the number of sentiment classes. \n",
    "\n",
    "*Note that this is not the same as word2vec CBOW.* Both models take a few words as input, sum and make a prediction. However, in word2vec the prediction is the word in the middle of the input words. Here, the prediction is a sentiment class (1 out of 5 possible numbers 0-4).\n",
    "\n",
    "So what can we do? We can *learn* a parameter matrix $W$ that turns our summed vector into the number of output classes, by matrix-multiplying it: $$Wx$$\n",
    "If the size of $W$ is `5 x d`, and $x$ is `d x 1` and , then the result will be `5x1`, and then we can again find our prediction using an argmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gIjrCPfCwsXI"
   },
   "source": [
    "## Exercise: write CBOW class & train it\n",
    "\n",
    "Write a class `CBOW` that:\n",
    "\n",
    "- has word embeddings with size 300\n",
    "- sums the word vectors for the input words (just like in `BOW`)\n",
    "- projects the resulting vector down to 5 units using a linear layer (including a bias term)\n",
    "\n",
    "Train your CBOW model and plot the validation accuracy and training loss over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PEV22aR2MP0Q"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "class CBOW(nn.Module):\n",
    "    \"\"\"A continuos bag-of-words model\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim, vocab):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.vocab = vocab\n",
    "\n",
    "        # this is a trained look-up table with word embeddings\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.applyW = nn.Linear(embedding_dim, output_dim)\n",
    "\n",
    "        # this is a trained bias term\n",
    "        self.bias = nn.Parameter(torch.zeros(output_dim), requires_grad=True)        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # this is the forward pass of the neural network\n",
    "        # given inputs, it computes the output\n",
    "\n",
    "        # this looks up the embeddings for each word ID in inputs\n",
    "        # the result is a sequence of word embeddings\n",
    "        embeds = self.embed(inputs)\n",
    "        trans_embeds = self.applyW(embeds)\n",
    "\n",
    "        # the output is the sum across the time dimension (1)\n",
    "        # with the bias term added\n",
    "        logits = trans_embeds.sum(1) + self.bias\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cbow_model = CBOW(len(v.w2i), 300, len(t2i), vocab=v)\n",
    "# print(cbow_model)\n",
    "\n",
    "# cbow_model = cbow_model.to(device)\n",
    "\n",
    "# optimizer = optim.Adam(cbow_model.parameters(), lr=0.0005)\n",
    "# cbow_losses, cbow_accuracies = train_model(\n",
    "#     cbow_model, optimizer, num_iterations=300000, \n",
    "#     print_every=10000, eval_every=10000, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(cbow_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(cbow_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zpFt_Fo2TdN0"
   },
   "source": [
    "# Deep CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iZanOMesTfEZ"
   },
   "source": [
    "To see if we can squeeze some more performance out of the CBOW model, we can make it deeper and non-linear: add more layers and tanh-activations.\n",
    "By using more parameters we can learn more aspects of the data, and by using more layers and non-linearities, we can try to learn a more complex function. \n",
    "This is not something that always works. If the input-output mapping of your data is simple, then a complicated function could easily overfit on your training set, which will lead to poor generalization. \n",
    "\n",
    "We will use E for the size of the word embeddings (use: 300) and D for the size of a hidden layer (use: 100).\n",
    "\n",
    "#### Exercise: write Deep CBOW class and train it\n",
    "\n",
    "Write a class `DeepCBOW`.\n",
    "\n",
    "In your code, make sure that your `output_layer` consists of the following:\n",
    "- A linear transformation from E units to D units.\n",
    "- A Tanh activation\n",
    "- A linear transformation from D units to D units\n",
    "- A Tanh activation\n",
    "- A linear transformation from D units to 5 units (our output classes).\n",
    "\n",
    "We recommend using [nn.Sequential](https://pytorch.org/docs/stable/nn.html?highlight=sequential#torch.nn.Sequential) to implement this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l8Z1igvpTrZq"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "class DeepCBOW(nn.Module):\n",
    "    \"\"\"A continuos bag-of-words model\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, E, D, output_dim, vocab):\n",
    "        super(DeepCBOW, self).__init__()\n",
    "        self.vocab = vocab\n",
    "\n",
    "        # this is a trained look-up table with word embeddings\n",
    "        self.embed = nn.Embedding(vocab_size, E)\n",
    "        self.output_layer = nn.Sequential(OrderedDict([\n",
    "          ('linear1', nn.Linear(E, D)),\n",
    "          ('tanh1', nn.Tanh()),\n",
    "          ('dropout1', nn.Dropout(0.1)),\n",
    "          ('linear2', nn.Linear(D,D)),\n",
    "          ('relu2', nn.Tanh()),\n",
    "          ('dropout2', nn.Dropout(0.1)),\n",
    "          ('linear3', nn.Linear(D,output_dim))\n",
    "        ]))\n",
    "\n",
    "        # this is a trained bias term\n",
    "        self.bias = nn.Parameter(torch.zeros(output_dim), requires_grad=True)        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # this is the forward pass of the neural network\n",
    "        # given inputs, it computes the output\n",
    "\n",
    "        # this looks up the embeddings for each word ID in inputs\n",
    "        # the result is a sequence of word embeddings\n",
    "        embeds = self.embed(inputs)\n",
    "        for layer in self.output_layer:\n",
    "            embeds = layer(embeds)\n",
    "\n",
    "        # the output is the sum across the time dimension (1)\n",
    "        # with the bias term added\n",
    "        logits = embeds.sum(1) + self.bias\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep_cbow_model = DeepCBOW(len(v.w2i), 300, 100, len(t2i), vocab=v)\n",
    "# print(deep_cbow_model)\n",
    "\n",
    "# deep_cbow_model = deep_cbow_model.to(device)\n",
    "\n",
    "# optimizer = optim.Adam(deep_cbow_model.parameters(), lr=0.002)\n",
    "# deep_cbow_losses, deep_cbow_accuracies = train_model(\n",
    "#     deep_cbow_model, optimizer, num_iterations=300000, \n",
    "#     print_every=1000, eval_every=1000, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(deep_cbow_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(deep_cbow_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MQZ5flHwiiHY"
   },
   "source": [
    "# Pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9NX35vecmHy6"
   },
   "source": [
    "The Stanford sentiment treebank is a very small data set, since it was manually annotated. This makes it difficult for the model to learn good word embeddings, to learn the \"meaning\" of the words in our vocabulary.\n",
    "\n",
    "Think for a moment about the fact that the only error signal that the network receives is from predicting the sentiment!\n",
    "\n",
    "To mitigate our data sparsity, we can download **pre-trained word embeddings**. \n",
    "You can choose which pre-trained word embeddings to use:\n",
    "\n",
    "- **Glove**. The \"original\" Stanford Sentiment classification [paper](http://aclweb.org/anthology/P/P15/P15-1150.pdf) used Glove embeddings, which are just another method (like *word2vec*) to get word embeddings from unannotated text. Glove is described in the following paper which you should cite if you use them:\n",
    "> Jeffrey Pennington, Richard Socher, and Christopher Manning. [\"Glove: Global vectors for word representation.\"](https://nlp.stanford.edu/pubs/glove.pdf) EMNLP 2014. \n",
    "\n",
    "- **word2vec**. This is the method that you learned about in class, described in:\n",
    "> Mikolov, Tomas, et al. [\"Distributed representations of words and phrases and their compositionality.\"](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) Advances in neural information processing systems. 2013.\n",
    "\n",
    "With these pre-trained word embeddings, we can initialize our word embedding lookup table and start form a point where similar words are already close to one another. \n",
    "\n",
    "You can choose to keep the word embeddings **fixed** or to train them further.\n",
    "We will keep them fixed for now.\n",
    "\n",
    "For the purposes of this lab, it is enough if you understand how word2vec works (whichever vectors you use). If you are interested you may also check out the Glove paper.\n",
    "\n",
    "You can either download the word2vec vectors, or the Glove vectors.\n",
    "If you want to compare your results to the Stanford paper later on, then you should use Glove. \n",
    "**At the end of this lab you have the option to compare which vectors give you the best performance. For now, simply choose one of them and continue with that.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lGYr02WWO993"
   },
   "outputs": [],
   "source": [
    "# This downloads the Glove 840B 300d embeddings.\n",
    "# The original file is at http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "# Since that file is 2GB, we provide you with a *filtered version*\n",
    "# which contains all the words you need for this data set.\n",
    "\n",
    "# You only need to do this once.\n",
    "# Please comment this cell out after downloading.\n",
    "\n",
    "# !wget https://gist.githubusercontent.com/bastings/b094de2813da58056a05e8e7950d4ad1/raw/3fbd3976199c2b88de2ae62afc0ecc6f15e6f7ce/glove.840B.300d.sst.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6NLsgFGiTjmI"
   },
   "outputs": [],
   "source": [
    "# This downloads the word2vec 300D Google News vectors \n",
    "# The file has been truncated to only contain words that appear in our data set.\n",
    "# You can find the original file here: https://code.google.com/archive/p/word2vec/\n",
    "\n",
    "# You only need to do this once.\n",
    "# Please comment this out after downloading.\n",
    "# !wget https://gist.githubusercontent.com/bastings/4d1c346c68969b95f2c34cfbc00ba0a0/raw/76b4fefc9ef635a79d0d8002522543bc53ca2683/googlenews.word2vec.300d.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GXBITzPRQUQb"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive (to save the downloaded files)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uFvzPuiKSCbl"
   },
   "outputs": [],
   "source": [
    "# Copy word vectors *to* Google Drive\n",
    "\n",
    "# You only need to do this once.\n",
    "# Please comment this out after running it. \n",
    "# !cp \"glove.840B.300d.sst.txt\" \"/gdrive/My Drive/\"\n",
    "# !cp \"googlenews.word2vec.300d.txt\" \"/gdrive/My Drive/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kUMH0bM6BuY9"
   },
   "outputs": [],
   "source": [
    "# If you copied the word vectors to your Drive before,\n",
    "# here is where you copy them back to the Colab notebook.\n",
    "\n",
    "# Copy Glove vectors *from* Google Drive\n",
    "# !cp \"/gdrive/My Drive/glove.840B.300d.sst.txt\" .\n",
    "# !cp \"/gdrive/My Drive/googlenews.word2vec.300d.txt\" ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MX2GJVHILM8n"
   },
   "source": [
    "At this point you have the pre-trained word embedding files, but what do they look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ChsChH14Ruxn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n"
     ]
    }
   ],
   "source": [
    "# Exercise: Print the first 4 lines of the files that you downloaded.\n",
    "# What do you see?\n",
    "with open(\"glove.840B.300d.sst.txt\", \"r\") as f:\n",
    "    glove_init_lines = f.readlines()\n",
    "    print(glove_init_lines[2].split(\" \")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WIVCkUkE_IjR"
   },
   "source": [
    "#### Exercise: New Vocabulary\n",
    "\n",
    "Since we now use pre-trained word embeddings, we need to create a new vocabulary. \n",
    "This is because of two reasons:\n",
    "\n",
    "1. Not all words in our training set are in the set of pre-trained word embeddings. We do not want words in our vocabulary that are not in that set.\n",
    "2. We get to look up the pre-trained word embedding for words in the validation and test set, even if we don't know them from training. \n",
    "\n",
    "Now, create a new vocabulary object `v` and load the pre-trained vectors into a `vectors`.\n",
    "\n",
    "The vocabulary `v` should consist of:\n",
    " - a  `<unk>` token at position 0,\n",
    " - a  `<pad>` token at position 1, \n",
    " - and then all words in the pre-trained vector set.\n",
    " \n",
    "\n",
    "After storing each vector in a list `vectors`, turn in into a numpy matrix like this:\n",
    "```python\n",
    " vectors = np.stack(vectors, axis=0)\n",
    "```\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ITyyCvDnCL4U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.082752   0.67204   -0.14987   -0.064983   0.056491   0.40228\n",
      "  0.0027747 -0.3311    -0.30691    2.0817     0.031819   0.013643\n",
      "  0.30265    0.0071297 -0.5819    -0.2774    -0.062254   1.1451\n",
      " -0.24232    0.1235    -0.12243    0.33152   -0.006162  -0.30541\n",
      " -0.13057   -0.054601   0.037083  -0.070552   0.5893    -0.30385\n",
      "  0.2898    -0.14653   -0.27052    0.37161    0.32031   -0.29125\n",
      "  0.0052483 -0.13212   -0.052736   0.087349  -0.26668   -0.16897\n",
      "  0.015162  -0.0083746 -0.14871    0.23413   -0.20719   -0.091386\n",
      "  0.40075   -0.17223    0.18145    0.37586   -0.28682    0.37289\n",
      " -0.16185    0.18008    0.3032    -0.13216    0.18352    0.095759\n",
      "  0.094916   0.008289   0.11761    0.34046    0.03677   -0.29077\n",
      "  0.058303  -0.027814   0.082941   0.1862    -0.031494   0.27985\n",
      " -0.074412  -0.13762   -0.21866    0.18138    0.040855  -0.113\n",
      "  0.24107    0.3657    -0.27525   -0.05684    0.34872    0.011884\n",
      "  0.14517   -0.71395    0.48497    0.14807    0.62287    0.20599\n",
      "  0.58379   -0.13438    0.40207    0.18311    0.28021   -0.42349\n",
      " -0.25626    0.17715   -0.54095    0.16596   -0.036058   0.08499\n",
      " -0.64989    0.075549  -0.28831    0.40626   -0.2802     0.094062\n",
      "  0.32406    0.28437   -0.26341    0.11553    0.071918  -0.47215\n",
      " -0.18366   -0.34709    0.29964   -0.66514    0.002516  -0.42333\n",
      "  0.27512    0.36012    0.16311    0.23964   -0.05923    0.3261\n",
      "  0.20559    0.038677  -0.045816   0.089764   0.43151   -0.15954\n",
      "  0.08532   -0.26572   -0.15001    0.084286  -0.16714   -0.43004\n",
      "  0.060807   0.13121   -0.24112    0.66554    0.4453    -0.18019\n",
      " -0.13919    0.56252    0.21457   -0.46443   -0.012211   0.029988\n",
      " -0.051094  -0.20135    0.80788    0.47377   -0.057647   0.46216\n",
      "  0.16084   -0.20954   -0.05452    0.15572   -0.13712    0.12972\n",
      " -0.011936  -0.003378  -0.13595   -0.080711   0.20065    0.054056\n",
      "  0.046816   0.059539   0.046265   0.17754   -0.31094    0.28119\n",
      " -0.24355    0.085252  -0.21011   -0.19472    0.0027297 -0.46341\n",
      "  0.14789   -0.31517   -0.065939   0.036106   0.42903   -0.33759\n",
      "  0.16432    0.32568   -0.050392  -0.054297   0.24074    0.41923\n",
      "  0.13012   -0.17167   -0.37808   -0.23089   -0.019477  -0.29291\n",
      " -0.30824    0.30297   -0.22659    0.081574  -0.18516   -0.21408\n",
      "  0.40616   -0.28974    0.074174  -0.17795    0.28595   -0.039626\n",
      " -0.2339    -0.36054   -0.067503  -0.091065   0.23438   -0.0041331\n",
      "  0.003232   0.0072134  0.008697   0.21614    0.049904   0.35582\n",
      "  0.13748    0.073361   0.14166    0.2412    -0.013322   0.15613\n",
      "  0.083381   0.088146  -0.019357   0.43795    0.083961   0.45309\n",
      " -0.50489   -0.10865   -0.2527    -0.18251    0.20441    0.13319\n",
      "  0.1294     0.050594  -0.15612   -0.39543    0.12538    0.24881\n",
      " -0.1927    -0.31847   -0.12719    0.4341     0.31177   -0.0040946\n",
      " -0.2094    -0.079961   0.1161    -0.050794   0.015266  -0.2803\n",
      " -0.12486    0.23587    0.2339    -0.14023    0.028462   0.56923\n",
      " -0.1649    -0.036429   0.010051  -0.17107   -0.042608   0.044965\n",
      " -0.4393    -0.26137    0.30088   -0.060772  -0.45312   -0.19076\n",
      " -0.20288    0.27694   -0.060888   0.11944    0.62206   -0.19343\n",
      "  0.47849   -0.30113    0.059389   0.074901   0.061068  -0.4662\n",
      "  0.40054   -0.19099   -0.14331    0.018267  -0.18643    0.20709\n",
      " -0.35598    0.05338   -0.050821  -0.1918    -0.37846   -0.06589  ]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "v = Vocabulary()\n",
    "vectors = list()\n",
    "vectors.append(np.zeros(300, dtype=np.float32)) # <unk>\n",
    "vectors.append(np.zeros(300, dtype=np.float32)) # <pad>\n",
    "for line in glove_init_lines:\n",
    "    v.count_token(line.split(\" \")[0])\n",
    "    vectors.append(np.array([float(x) for x in line.split(\" \")[1:]], dtype=np.float32))\n",
    "vectors = np.stack(vectors, axis=0)\n",
    "v.build()\n",
    "print(vectors[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xC-7mRyYNG9b"
   },
   "source": [
    "#### Exercise: words not in our pre-trained set\n",
    "\n",
    "How many words in the training, dev, and test set are also in your vector set?\n",
    "How many words are not there?\n",
    "\n",
    "Store the words that are not in the word vector set in the set below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K6MA3-wF_X5M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ill-wrought', 'pro-Serb', 'celeb-strewn', 'dog-paddle', 'long-on-the-shelf', 'Heremakono', '102-minute', 'Wollter', 'tryingly', 'sequel-for-the-sake', 'wild-and-woolly', 'audience-pleaser', 'Underachieves', 'in-jokey', 'out-outrage', 'Good-naturedly', 'none-too-original', 'sillified', 'ultra-loud', 'Whiffle-Ball', 'fustily', 'college-spawned', 'pseudo-witty', 'Frankenstein-monster', 'unhibited', 'as-it', 'rock-n-rolling', 'Kahlories', 'Driver-esque', 'coming-of-age/coming-out', 'Rintar', 'dullingly', 'sports-movie', 'anti-adult', 'Ourside', 'indie-heads', 'Co-writer/director', 'feardotcom', 'Phonce', 'flick-knife', 'life-at-arm', 'flex-a-thon', 'Oscar-size', 'teen-speak', 'headline-fresh', 'pseudo-bio', 'flck', 'underventilated', 'ricture', 'show-stoppingly', 'Nickelodeon-esque', 'hotsies', 'nonethnic', 'unsalvageability', 'Ki-Deok', 'Bjorkness', 'she-cute', 'like-themed', 'doing-it-for', 'Wewannour', 'democracie', 'actorish', 'voices-from-the-other-side', 'Seldhal', 'disaffected-indie-film', 'flatula', 'Sandlerian', 'message-mongering', 'unconned', 'step-printing', 'unlaughable', 'yarn-spinner', 'mush-hearted', 'gut-clutching', 'Kafka-inspired', 'By-the-numbers', 'too-frosty', 'gender-war', 'hippie-turned-yuppie', 'splendid-looking', 'genial-rogue', 'made-for-home-video', 'yahoo-ing', 'oh-so-Hollywood', 'achronological', 'barn-side', 'lower-wit', 'Marcken', 'pre-fils', 'goth-vampire', 'bibbidy-bobbidi-bland', 'hammily', 'unembarrassing', 'Piercingly', 'non-exploitive', 'gore-free', 'Qutting', 'travel-agency', 'monster-in-the', 'Star/producer', 'ego-destroying', 'dirgelike', 'grunge-pirate', 'doofus-on', 'wonder-what', 'unencouraging', 'Birot', 'post-Tarantino', 'cinemantic', 'O2-tank', 'Damon/Bourne', 'Jaglomized', 'Glizty', 'late-twenty-somethings', 'lascivious-minded', 'not-so-Divine', 'rape-payback', 'junk-calorie', 'natural-seeming', 'direct-to-void', 'Jean-Claud', 'derisions', 'quick-cuts', 'Director-writer', 'farewell-to-innocence', 'Otto-Sallies', 'Denlopp', 'kids-in-peril', 'police-oriented', 'quasi-Shakespearean', 'soul-stripping', 'cat-and-mouser', 'snow-and-stuntwork', 'hell-jaunt', 'prefeminist', 'anti-Kieslowski', 'too-spectacular', 'disappearing/reappearing', 'non-Britney', 'teen-exploitation', 'smile-button', 'Jeong-Hyang', 'Formuliac', 'beast-within', 'so-bad-they', 'Poke-mania', 'tech-geeks', 'gunfest', 'Georgian-Israeli', 'best-foreign-film', 'under-7', 'Insufferably', 'dictator-madman', '91-minute', 'girl-buddy', 'sex-in-the-city', 'smarter-than-thou', 'diciness', 'non-firsthand', 'surfacey', 'self-amused', 'raw-nerved', 'pro-wildlife', 'oft-brilliant', 'valley-girl', 'of-a-sequel', 'Singer/composer', 'neo-Hitchcockianism', 'lovable-loser', 'yawn-provoking', 'Choquart', 'Jae-eun', 'I-2-spoofing', 'qatsi', 'FearDotCom', 'kid-empowerment', 'pseudo-serious', 'stadium-seat', 'Self-congratulatory', 'and-miss', 'LePlouff', 'Nebrida', 'at-a-frat-party', 'life-embracing', 'movie-specific', 'out-depress', 'often-funny', 'Short-story', 'surehanded', 'landbound', 'spaniel-eyed', 'ballistic-pyrotechnic', 'ultra-cheesy', 'psychodramatics', 'groupie/scholar', 'half-dimensional', '112-minute', 'hour-and-a-half-long', 'dungpile', 'snazziness', 'not-quite-dead', 'eardrum-dicing', 'guilt-suffused', 'shock-you-into-laughter', 'Kosashvili', 'actress-producer', 'self-glorified', 'who-wrote-Shakespeare', 'cellophane-pop', 'e-graveyard', 'Watstein', 'Cosby-Seinfeld', 'disease-of-the-week', 'bare-midriff', 'action-and-popcorn', 'made-for-movie', 'Felinni', 'waydowntown', 'big-budget/all-star', 'cameo-packed', 'Juliet/West', 'ANTWONE', 'cold-fish', 'double-pistoled', 'affirmational', 'Nietzsche-referencing', 'blood-splattering', 'movie-movie', 'Minac', 'industrial-model', 'power-lunchers', 'art-conscious', 'flakeball', 'Fulford-Wierzbicki', 'near-hypnotic', 'daytime-drama', 'sub-formulaic', 'stagecrafts', 'guy-in-a-dress', 'thrill-kill', 'Bottom-rung', 'pathos-filled', 'MIBII', 'Phoned-in', 'marveilleux', 'food-spittingly', 'Mordantly', 'Stortelling', 'merchandised-to-the-max', '-RRB-', 'based-on-truth', 'Journalistically', 'Priggish', 'nonchallenging', 'remain-nameless', 'a-bornin', 'Achero', 'truncheoning', 'special-effects-laden', 'Brothers/Abrahams', 'wind-in-the-hair', 'female-bonding', 'pseudo-philosophic', 'kid-vid', 'reality-snubbing', 'imponderably', 'Asiaphiles', 'un-bear-able', 'Romething', 'no-surprise', 'renegade-cop', 'tolerable-to-adults', 'revigorates', 'truck-loving', 'bad-movie', 'therapy-dependent', 'Margolo', 'Banderas-Lucy', 'message-movie', 'whoopee-cushion', 'uncharismatically', 'character-who-shall', 'teen-driven', 'seventy-minute', 'Spy-vs', 'monster/science', 'egocentricities', 'rough-trade', 'tough-man', 'kid-movie', 'Unspools', 'gender-provoking', 'sitcom-worthy', 'big-fisted', 'I-heard-a-joke', 'musclefest', 'damaged-goods', 'hotter-two-years-ago', 'tub-thumpingly', 'war-movie', 'affectation-free', 'heart-rate-raising', 'dolphin-gasm', 'song-and-dance-man', 'image-mongering', 'badly-rendered', 'auto-critique', 'nerve-raked', 'Copmovieland', 'dudsville', 'Talkiness', 'show-don', 'scary-funny', 'acting-workshop', 'German-Expressionist', 'Univac-like', 'redneck-versus-blueblood', 'brain-deadening', 'so-five-minutes-ago', 'Annie-Mary', 'still-raw', 'TV-insider', 'video-shot', 'film-culture', 'unfakable', 'stunt-hungry', 'shoe-loving', 'jazz-playing', 'media-constructed', 'Efteriades', 'rubber-face', 'already-shallow', 'raunch-fests', 'Equlibrium', 'queasy-stomached', 'quasi-improvised', 'wise-cracker', 'buzz-obsessed', 'Warmed-over', 'non-Bondish', 'still-inestimable', 'kibbitzes', 'not-nearly', 'Recoing', 'Holofcenter', 'retro-refitting', 'Hopkins/Rock', 'pseudo-educational', 'cipherlike', 'girl-meets-girl', 'razor-sided', 'independent-community', 'stuporously', 'East-vs', 'Puportedly', 'dead-undead', 'clung-to', 'bottomlessly', 'give-me-an-Oscar', 'tuba-playing', 'sitcomishly', 'Sychowski', 'stomach-knotting', 'Feardotcom', 'out-bad-act', 'often-cute', 'toilet-humor', 'pseudo-rock-video', 'Komediant', 'the-week', 'kids-cute', 'water-camera', 'hit-hungry', 'Steinis', 'Brit-com', 'talking-animal', 'oh-those-wacky-Brits', 'super-stupid', 'cheatfully', 'animated-movie', 'stable-full', 'revenge-of-the-nerds', 'too-hot-for-TV', 'faux-urban', '-LRB-', 'murder-on-campus', 'best-sustained', 'under-inspired', 'out-to-change-the-world', 'semimusical', 'docu-Dogma', 'follow-your-dream', 'indieflick', 'gone-to-seed', 'talk-heavy', 'groan-to-guffaw', 'Imaxy', 'overcoming-obstacles', 'beloved-major', 'artsploitation', 'self-defeatingly', 'Unambitious', 'pasta-fagioli', 'Norrington-directed', 'Waters-like', 'style-free', 'Venice/Venice', 'old-fashioned-movie', 'pulpiness', 'brain-slappingly', 'preachy-keen', 'time-switching', 'anti-Harry', 'not-at-all-good', 'male-ridden', 'Cliff-Notes', 'demented-funny', 'Ga', 'teen-sleaze', 'every-joke-has', 'Djeinaba', 'big-bug', 'young-guns', 'Brothers-style', 'Janklowicz-Mann', 'Bazadona', 'adventues', 'telanovela', 'now-cliched', 'boundary-hopping', 'Ill-considered', 'Devolves', 'singer-turned', 'Cineasts', 'thinly-conceived', 'Wisegirls', 'Auteil', 'cheese-laced', 'unslick', 'Schneidermeister', 'pooper-scoopers', 'drama/character', 'direct-to-video/DVD', 'Re-Fried', 'materalism', 'underdramatized', 'videologue', 'trash-cinema', 'Less-than-compelling', '168-minute', 'sub-music', 'college-friends', 'cop-flick', 'foul-natured', 'ultra-provincial', 'Oscar-sweeping', 'slummer', 'Oprahfication', 'Well-meant', 'Talancn', 'skit-com', 'romantic/comedy', 'zinger-filled', 'fang-baring', 'stand-up-comedy', 'Director-chef', 'S1M0NE', 'Globetrotters-Generals', 'social/economic/urban', 'autocritique', 'wannabe-hip', 'Debrauwer', 'RunTelDat', 'Hitler-study', 'slasher-movie', 'Witch-style', 'modern-office', 'overly-familiar', 'Truckzilla', 'Chabrolian', 'underrehearsed', 'II-Birkenau', 'huge-screen', 'too-conscientious', 'screeching-metal', 'Idemoto', 'crummy-looking', 'techno-sex', 'shuck-and-jive', 'Vulakoro', 'well-lensed', 'run-of-the-filth', 'hidden-agenda', 'even-flowing', 'pop-induced', 'Brazil-like', 'uncinematic', 'grade-grubbers', 'just-above-average', 'jump-in-your-seat', 'Hollywood-itis', 'French-produced', 'fizzability', 'unemotive', 'Ear-splitting', 'beyond-lame', 'kiddie-oriented', 'church-wary', 'Hollywood-action', 'as-nasty', 'splatterfests', 'digital-effects-heavy', 'fun-for-fun', 'cor-blimey-luv-a-duck', 'slash-fest', 'Rashomon-for-dipsticks', 'from-television', 'Rocky-like', 'semi-amusing', 'clich-riddled', 'makeup-deep', 'Hjelje', 'media-soaked', 'Kubrick-meets-Spielberg', 'sub-sophomoric', 'banter-filled', 'semi-throwback', 'spook-a-rama', 'too-extreme-for-TV', 'eroti-comedy', 'moral-condundrum', 'drama/action', 'Japanimator', 'sub-Tarantino', 'Eckstraordinarily', 'girls-behaving-badly', 'artnering', 'Tiresomely', 'repellantly', 'better-focused', 'pro-Serbian', 'pantomimesque', 'out-shock', 'pee-related', 'Cool-J', 'barking-mad', 'matinee-style', 'boom-bam', 'the-loose', 'superficiale', 'disease-of', 'outgag', 'than-likely', 'moldy-oldie', 'video-cam', 'Gator-bashing', 'skyscraper-trapeze', 'movie-biz', 'action-thriller/dark', 'achival', 'ever-ruminating', 'genre-curling', 'Koshashvili', 'two-hour-and-fifteen-minute', 'unfussily', 'bore-athon', 'Potty-mouthed', 'stiletto-stomps', 'often-hilarious', 'reel/real', 'over-amorous', 'dust-caked', 'high-buffed', 'Zishe', 'gabbiest', 'Clarke-Williams', 'Pie-like', 'been-told-a', 'Je-Gyu', 'cable-sports', 'pop-cyber', 'identity-seeking', 'surface-effect', 'Punitively', 'Mushes', 'smashups', 'Silbersteins', 'close-to-solid', 'crash-and-bash', 'lip-non-synching', 'propriety-obsessed', 'dirty-joke', 'two-wrongs-make-a-right', 'bump-in', 'Bond-inspired', 'techno-tripe', 'pyro-correctly', 'the-cash', 'pokepie', 'kids-and-family-oriented', 'bio-doc', 'ennui-hobbled', 'barn-burningly', 'creepy-scary', 'bang-the-drum', 'thousand-times', 'Butterfingered', 'triple-crosses', '24-and-unders', 'wide-smiling', 'well-contructed', 'phoney-feeling', '103-minute', 'inside-show-biz', 'Live-style', 'Hellstenius', 'Age-inspired', 'neo-Augustinian', 'crime-land', 'happily-ever', 'Punch-and-Judy', 'trance-noir', 'time-it-is', 'lack-of-attention', 'prechewed', 'gangster/crime', 'road-and-buddy', 'tardier', 'bottom-of-the-bill', 'white-empowered', 'teen-gang', 'hyper-artificiality', 'handbag-clutching', 'Laissez-passer', 'Marine/legal', 'anti-erotic', 'gay-niche', 'deadeningly', 'hayseeds-vs', 'sleep-inducingly', 'Reeboir', 'tear-drenched', 'fast-edit', 'two-drink-minimum', 'clich-laden', 'Audacious-impossible', 'family-film', 'TV-cops', 'Channel-style', 'sense-of-humour', 'McBeal-style', 'Bettany/McDowell', 'surface-obsession', 'semi-surrealist', 'dark-as-pitch', 'goose-pimple', 'Dognini', 'sense-spinning', 'actorliness', 'super-dooper-adorability', 'title-bout', 'kinetically-charged', 'Hollywood-predictable', 'Verete', 'bio-drama', 'estrogen-free', 'soon-to-be-forgettable', 'warm-milk', 'Interminably', 'tries-so-hard-to-be-cool', 'feardotcom.com', 'Brother-Man', 'thinks-it-is', 'not-very-funny', '129-minute', 'Altman-esque', 'Pasach', 'spy-savvy', 'something-borrowed', 'less-compelling', 'clone-gag', 'Bruckheimeresque', 'Stevenon', 'video-game-based', 'trouble-in-the-ghetto', 'overstylized', 'humor-seeking', 'community-therapy', 'overmanipulative', 'zombie-land', 'shlockmeister', 'non-mystery', 'Lynch-like', 'Volletta', 'meets-John', 'spy-action', 'Seldahl', 'none-too-funny', 'Veret'}\n",
      "699\n"
     ]
    }
   ],
   "source": [
    "words_not_found = set()\n",
    "# YOUR CODE HERE\n",
    "for data_set in (train_data,):\n",
    "    for ex in data_set:\n",
    "        for token in ex.tokens:\n",
    "            if v.w2i.get(token, 0) == 0:\n",
    "                words_not_found.update([token])\n",
    "print(words_not_found)\n",
    "print(len(words_not_found))\n",
    "for word in words_not_found:\n",
    "    initial_word_vector = np.zeros(300, dtype=np.float32)\n",
    "    if \"-\" in word:\n",
    "        subwords = word.split(\"-\")\n",
    "        count_valid_words = 0\n",
    "        for s in subwords:\n",
    "            if v.w2i.get(token, 0) != 0:\n",
    "                initial_word_vector += vectors[v.w2i.get(token, 0)]\n",
    "                count_valid_words += 1\n",
    "        if count_valid_words != 0:\n",
    "            initial_word_vector /= count_valid_words\n",
    "        else:\n",
    "            initial_word_vector = np.random.uniform(-0.5, 0.5, size=(300,))\n",
    "    else:\n",
    "        initial_word_vector = np.random.uniform(-0.5, 0.5, size=(300,))\n",
    "        \n",
    "# Rebuild vocab\n",
    "v = Vocabulary()\n",
    "for line in glove_init_lines:\n",
    "    v.count_token(line.split(\" \")[0])\n",
    "for word in words_not_found:\n",
    "    v.count_token(word)\n",
    "v.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "words_not_found = set()\n",
    "# YOUR CODE HERE\n",
    "for data_set in (train_data,):\n",
    "    for ex in data_set:\n",
    "        for token in ex.tokens:\n",
    "            if v.w2i.get(token, 0) == 0:\n",
    "                words_not_found.update([token])\n",
    "print(words_not_found)\n",
    "print(len(words_not_found))\n",
    "for word in words_not_found:\n",
    "    initial_word_vector = np.zeros(300, dtype=np.float32)\n",
    "    if \"-\" in word:\n",
    "        subwords = word.split(\"-\")\n",
    "        count_valid_words = 0\n",
    "        for s in subwords:\n",
    "            if v.w2i.get(token, 0) != 0:\n",
    "                initial_word_vector += vectors[v.w2i.get(token, 0)]\n",
    "                count_valid_words += 1\n",
    "        if count_valid_words != 0:\n",
    "            initial_word_vector /= count_valid_words\n",
    "        else:\n",
    "            initial_word_vector = np.random.uniform(-0.5, 0.5, size=(300,))\n",
    "    else:\n",
    "        initial_word_vector = np.random.uniform(-0.5, 0.5, size=(300,))\n",
    "        \n",
    "# Rebuild vocab\n",
    "v = Vocabulary()\n",
    "for line in glove_init_lines:\n",
    "    v.count_token(line.split(\" \")[0])\n",
    "for word in words_not_found:\n",
    "    v.count_token(word)\n",
    "v.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BfEd38W0NnAI"
   },
   "source": [
    "#### Exercise: train Deep CBOW with (fixed) pre-trained embeddings\n",
    "\n",
    "Now train Deep CBOW again using the pre-trained word vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z_6ooqgEsB20"
   },
   "outputs": [],
   "source": [
    "# We define a dummy class so that we save the model to a different file.\n",
    "class PTDeepCBOW(DeepCBOW):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, vocab):\n",
    "        super(PTDeepCBOW, self).__init__(\n",
    "            vocab_size, embedding_dim, hidden_dim, output_dim, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JfIh4Ni6yuAh"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# pt_deep_cbow_model = PTDeepCBOW(len(v.w2i), 300, 100, 5, v)\n",
    "\n",
    "# copy pre-trained word vectors into embeddings table\n",
    "# pt_deep_cbow_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
    "\n",
    "# disable training the pre-trained embeddings\n",
    "# pt_deep_cbow_model.embed.weight.requires_grad = False\n",
    "\n",
    "# move model to specified device\n",
    "# pt_deep_cbow_model = pt_deep_cbow_model.to(device)\n",
    "\n",
    "# train the model\n",
    "# pt_deep_cbow_model = pt_deep_cbow_model.to(device)\n",
    "\n",
    "# optimizer = optim.Adam(pt_deep_cbow_model.parameters(), lr=0.0001)\n",
    "# pt_deep_cbow_losses, pt_deep_cbow_accuracies = train_model(\n",
    "#     pt_deep_cbow_model, optimizer, num_iterations=30000, \n",
    "#    print_every=1000, eval_every=1000, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ufujv3x31ufD"
   },
   "outputs": [],
   "source": [
    "# plot dev accuracies\n",
    "# plt.plot(pt_deep_cbow_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YTJtKBzd7Qjr"
   },
   "outputs": [],
   "source": [
    "# plot train loss\n",
    "# plt.plot(pt_deep_cbow_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yFu8xzCy9XDW"
   },
   "source": [
    "**It looks like we've hit what is possible with just using words.**\n",
    "Let's move on by incorporating word order!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g41yW4PL9jG0"
   },
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ODzXEH0MaGpa"
   },
   "source": [
    "It is time to get more serious. Even with pre-trained word embeddings and multiple layers, we seem to do pretty badly at sentiment classification here. \n",
    "The next step we can take is to introduce word order again, and to get a representation of the sentence as a whole, without independence assumptions.\n",
    "\n",
    "We will get this representation using an **Long Short-Term Memory** (LSTM). As an exercise, we will code our own LSTM cell, so that we get comfortable with its inner workings.\n",
    "Once we have an LSTM cell, we can call it repeatedly, updating its hidden state one word at a time:\n",
    "\n",
    "```python\n",
    "rnn = MyLSTMCell(input_size, hidden_size)\n",
    "\n",
    "hx = torch.zeros(1, hidden_size)  # initial state\n",
    "cx = torch.zeros(1, hidden_size)  # initial memory cell\n",
    "output = []                       # to save intermediate LSTM states\n",
    "\n",
    "# feed one word at a time\n",
    "for i in range(n_timesteps):\n",
    "  hx, cx = rnn(input[i], (hx, cx))\n",
    "  output.append(hx)\n",
    "```\n",
    "\n",
    "If you need some more help understanding LSTMs, then check out these resources:\n",
    "- Blog post (highly recommended): http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- Paper covering LSTM formulas in detail: https://arxiv.org/abs/1503.04069 \n",
    "\n",
    "#### Exercise: Finish the LSTM cell below. \n",
    "You will need to implement the LSTM formulas:\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "        i = \\sigma(W_{ii} x + b_{ii} + W_{hi} h + b_{hi}) \\\\\n",
    "        f = \\sigma(W_{if} x + b_{if} + W_{hf} h + b_{hf}) \\\\\n",
    "        g = \\tanh(W_{ig} x + b_{ig} + W_{hg} h + b_{hg}) \\\\\n",
    "        o = \\sigma(W_{io} x + b_{io} + W_{ho} h + b_{ho}) \\\\\n",
    "        c' = f * c + i * g \\\\\n",
    "        h' = o \\tanh(c') \\\\\n",
    "\\end{array}\n",
    " $$\n",
    "\n",
    "where $\\sigma$ is the sigmoid function.\n",
    "\n",
    "*Note that the LSTM formulas can differ slightly between different papers. We use the PyTorch LSTM formulation here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zJ9m5kLMd7-v"
   },
   "outputs": [],
   "source": [
    "class MyLSTMCell(nn.Module):\n",
    "    \"\"\"Our own LSTM cell\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        \"\"\"Creates the weights for this LSTM\"\"\"\n",
    "        super(MyLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "\n",
    "        self.tanh_act = nn.Tanh()\n",
    "        self.sigmoid_act = nn.Sigmoid()\n",
    "        # self.input_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        # self.forget_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        # self.candidate_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        # self.output_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.combined_gate = nn.Linear(input_size + hidden_size, 4 * hidden_size)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"This is PyTorch's default initialization method\"\"\"\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)  \n",
    "\n",
    "    def forward(self, input_, hx, mask=None):\n",
    "        \"\"\"\n",
    "        input is (batch, input_size)\n",
    "        hx is ((batch, hidden_size), (batch, hidden_size))\n",
    "        \"\"\"\n",
    "        prev_h, prev_c = hx\n",
    "\n",
    "        # project input and prev state\n",
    "        cat_input = torch.cat([input_,prev_h], dim=1)\n",
    "\n",
    "        # main LSTM computation    \n",
    "\n",
    "        # i = self.sigmoid_act(self.input_gate(cat_input))\n",
    "        # f = self.sigmoid_act(self.forget_gate(cat_input))\n",
    "        # g = self.tanh_act(self.candidate_gate(cat_input))\n",
    "        # o = self.sigmoid_act(self.output_gate(cat_input))\n",
    "        combined_output = self.combined_gate(cat_input)\n",
    "        i = self.sigmoid_act(combined_output[:,0 * self.hidden_size:1 * self.hidden_size])\n",
    "        f = self.sigmoid_act(combined_output[:,1 * self.hidden_size:2 * self.hidden_size])\n",
    "        g = self.tanh_act(combined_output[:,2 * self.hidden_size:3 * self.hidden_size])\n",
    "        o = self.sigmoid_act(combined_output[:,3 * self.hidden_size:4 * self.hidden_size])\n",
    "\n",
    "        c = f * prev_c + i * g\n",
    "        h = o * self.tanh_act(c)\n",
    "\n",
    "        return h, c\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"{}({:d}, {:d})\".format(\n",
    "            self.__class__.__name__, self.input_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4JM7xPhkQeE5"
   },
   "source": [
    "#### Optional: Efficient Matrix Multiplication\n",
    "\n",
    "It is more efficient to do a few big matrix multiplications than to do many smaller ones.\n",
    "\n",
    "It is possible to implement the above cell using just **two** linear layers.\n",
    "\n",
    "This is because the eight linear transformations from one forward pass through an LSTM cell can be done in just two:\n",
    "$$W_h h + b_h$$\n",
    "$$W_i x + b_i $$ \n",
    "\n",
    "with $h = $ `prev_h` and $x = $ `input_`.\n",
    "\n",
    "and where: \n",
    "\n",
    "$W_h =  \\begin{pmatrix}\n",
    "W_{hi}\\\\ \n",
    "W_{hf}\\\\ \n",
    "W_{hg}\\\\ \n",
    "W_{ho}\n",
    "\\end{pmatrix}$, $b_h = \\begin{pmatrix}\n",
    "b_{hi}\\\\ \n",
    "b_{hf}\\\\ \n",
    "b_{hg}\\\\ \n",
    "b_{ho}\n",
    "\\end{pmatrix}$,  $W_i = \\begin{pmatrix}\n",
    "W_{ii}\\\\ \n",
    "W_{if}\\\\ \n",
    "W_{ig}\\\\ \n",
    "W_{io}\n",
    "\\end{pmatrix}$ and $b_i = \\begin{pmatrix}\n",
    "b_{ii}\\\\ \n",
    "b_{if}\\\\ \n",
    "b_{ig}\\\\ \n",
    "b_{io}\n",
    "\\end{pmatrix}$.\n",
    "\n",
    "Convince yourself that, after chunking with [torch.chunk](https://pytorch.org/docs/stable/torch.html?highlight=chunk#torch.chunk), the output of those two linear transformations is equivalent to the output of the eight linear transformations in the LSTM cell calculations above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X9gA-UcqSBe0"
   },
   "source": [
    "#### LSTM Classifier\n",
    "\n",
    "Having an LSTM cell is not enough: we still need some code that calls it repeatedly, and then makes a prediction from the final hidden state. \n",
    "You will find that code below. Make sure that you understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3iuYZm5poEn5"
   },
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"Encodes sentence with an LSTM and projects final hidden state\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, vocab):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim, padding_idx=1)\n",
    "        self.rnn = MyLSTMCell(embedding_dim, hidden_dim)\n",
    "\n",
    "        self.output_layer = nn.Sequential(     \n",
    "            nn.Dropout(p=0.1),  # explained later\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        B = x.size(0)  # batch size (this is 1 for now, i.e. 1 single example)\n",
    "        T = x.size(1)  # time (the number of words in the sentence)\n",
    "        \n",
    "        input_ = self.embed(x)\n",
    "        \n",
    "\n",
    "        # here we create initial hidden states containing zeros\n",
    "        # we use a trick here so that, if input is on the GPU, then so are hx and cx\n",
    "        hx = input_.new_zeros(B, self.rnn.hidden_size)\n",
    "        cx = input_.new_zeros(B, self.rnn.hidden_size)\n",
    "\n",
    "        # process input sentences one word/timestep at a time\n",
    "        # input is batch-major, so the first word(s) is/are input_[:, 0]\n",
    "        outputs = []   \n",
    "        for i in range(T):\n",
    "            hx, cx = self.rnn(input_[:, i], (hx, cx))\n",
    "            outputs.append(hx)\n",
    "\n",
    "        # if we have a single example, our final LSTM state is the last hx\n",
    "        if B == 1:\n",
    "            final = hx\n",
    "        else:\n",
    "            #\n",
    "            # This part is explained in next section, ignore this else-block for now.\n",
    "            #\n",
    "            # we processed sentences with different lengths, so some of the sentences\n",
    "            # had already finished and we have been adding padding inputs to hx\n",
    "            # we select the final state based on the length of each sentence\n",
    "\n",
    "            # two lines below not needed if using LSTM form pytorch\n",
    "            outputs = torch.stack(outputs, dim=0)          # [T, B, D]\n",
    "            outputs = outputs.transpose(0, 1).contiguous()  # [B, T, D]\n",
    "\n",
    "            # to be super-sure we're not accidentally indexing the wrong state\n",
    "            # we zero out positions that are invalid\n",
    "            pad_positions = (x == 1).unsqueeze(-1)\n",
    "\n",
    "            outputs = outputs.contiguous()      \n",
    "            outputs = outputs.masked_fill_(pad_positions, 0.)\n",
    "\n",
    "            mask = (x != 1)  # true for valid positions [B, T]\n",
    "            lengths = mask.sum(dim=1)                  # [B, 1]\n",
    "\n",
    "            indexes = (lengths - 1) + torch.arange(B, device=x.device, dtype=x.dtype) * T\n",
    "            final = outputs.view(-1, self.hidden_dim)[indexes]  # [B, D]\n",
    "\n",
    "        # we use the last hidden state to classify the sentence\n",
    "        logits = self.output_layer(final)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FxFoVpvMPB6g"
   },
   "source": [
    "#### Dropout\n",
    "\n",
    "Besides not being able to learn meaningful word embeddings, there is another negative effect that can follow from data sparsity and a small data set: *overfitting*. This is a phenomenom that is very likely to occur when fitting strong and expressive models, like LSTMs, to small data. In practice, if your model overfits, this means that it will be very good at predicting (or: 'remembering') the sentiment of the training set, but unable to generalize to new, unseen data in the test set. This is undesirable and one technique to mitigate it is *dropout*. \n",
    "\n",
    "A dropout layer is defined by the following formula: $\\mathbf{d} \\in \\{0, 1\\}^n$, with $d_j \\sim \\text{Bernoulli}(p)$, and can be applied to for example a linear layer:\n",
    "\n",
    "$$\\text{tanh}(W(\\mathbf{h}\\odot \\mathbf{d}) + \\mathbf{b})$$\n",
    "\n",
    "\n",
    "These formulas simply mean that we *drop* certain parameters during training (by setting them to zero). Which parameters we drop is stochastically determined by a Bernoulli distribution and the probability of each parameter being dropped is set to $p = 0.5$ in our experiments (see the previous cell of code where we define our output layer). A dropout layer can be applied at many different places in our models. This technique helps against the undesirable effect where our model relies on single parameters for  prediction (e.g. if $h^{\\prime}_j$ is large, always predict positive). If we use dropout, the model needs to learn to rely on different parameters, which is desirable when generalizing to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XQjEjLt9z0XW"
   },
   "source": [
    "**Let's train our LSTM! ** Note that is will be a lot slower, because we need to do many more computations per sentence!\n",
    "\n",
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LgZoSPD4fsf_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlstm_model = LSTMClassifier(len(v.w2i), 300, 168, len(t2i), v)\\n\\n# copy pre-trained word vectors into embeddings table\\nwith torch.no_grad():\\n    lstm_model.embed.weight.data.copy_(torch.from_numpy(vectors))\\n    lstm_model.embed.weight.requires_grad = False\\n\\nprint(lstm_model)\\nprint_parameters(lstm_model)\\n\\nlstm_model = lstm_model.to(device)\\noptimizer = optim.Adam(lstm_model.parameters(), lr=3e-4)\\n\\nlstm_losses, lstm_accuracies = train_model(\\n    lstm_model, optimizer, num_iterations=25000, \\n    print_every=250, eval_every=1000)\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "lstm_model = LSTMClassifier(len(v.w2i), 300, 168, len(t2i), v)\n",
    "\n",
    "# copy pre-trained word vectors into embeddings table\n",
    "with torch.no_grad():\n",
    "    lstm_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
    "    lstm_model.embed.weight.requires_grad = False\n",
    "\n",
    "print(lstm_model)\n",
    "print_parameters(lstm_model)\n",
    "\n",
    "lstm_model = lstm_model.to(device)\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=3e-4)\n",
    "\n",
    "lstm_losses, lstm_accuracies = train_model(\n",
    "    lstm_model, optimizer, num_iterations=25000, \n",
    "    print_every=250, eval_every=1000)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2BKVnyg0Hq5E"
   },
   "outputs": [],
   "source": [
    "# plot validation accuracy\n",
    "# plt.plot(lstm_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZowTV0EBTb3z"
   },
   "outputs": [],
   "source": [
    "# plot training loss\n",
    "# plt.plot(lstm_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YEw6XHQY_AAQ"
   },
   "source": [
    "# Mini-batching\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FPf96wGzBTQJ"
   },
   "source": [
    "**Why is the LSTM so slow?** Despite our best efforts, we still need to make a lot of matrix multiplications per example (linear in the length of the example) just to get a single classification, and we can only process the 2nd word once we have computed the hidden state for the 1st word (sequential computation).\n",
    "\n",
    "GPUs are more efficient if we do a few big matrix multiplications, rather than lots of small ones. If we could process multiple examples at the same time, then we could exploit that. We still process the input sequentially, but now we can do so for multiple sentences at the same time.\n",
    "\n",
    "Up to now our \"minibatch\" consisted of a single example. This was for a reason: the sentences in our data sets have **different lengths**, and this makes it difficult to process them at the same time.\n",
    "\n",
    "Consider a batch of 2 sentences:\n",
    "\n",
    "```\n",
    "this movie is bad\n",
    "this movie is super cool !\n",
    "```\n",
    "\n",
    "Let's say the IDs for these sentences are:\n",
    "\n",
    "```\n",
    "2 3 4 5\n",
    "2 3 4 6 7 8\n",
    "```\n",
    "\n",
    "We cannot feed PyTorch an object with variable length rows! We need to turn this into a matrix.\n",
    "\n",
    "The solution is to add **padding values** to our mini-batch:\n",
    "\n",
    "```\n",
    "2 3 4 5 1 1\n",
    "2 3 4 6 7 8\n",
    "```\n",
    "\n",
    "Whenever a sentence is shorter than the longest sentence in a mini-batch, we just use a padding value (here: 1) to fill the matrix.\n",
    "\n",
    "In our computation, we should **ignore** the padding positions (e.g. mask them out). Paddings should not contribute to the loss.\n",
    "\n",
    "#### Mini-batch feed\n",
    "We will now code a `get_minibatch` function that will replace our `get_example` function, and returns a mini-batch of the requested size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IoAE2JBiXJ3P"
   },
   "outputs": [],
   "source": [
    "def get_minibatch(data, batch_size=25, shuffle=True):\n",
    "    \"\"\"Return minibatches, optional shuffling\"\"\"\n",
    "\n",
    "    if shuffle:\n",
    "        # print(\"Shuffling training data\")\n",
    "        random.shuffle(data)  # shuffle training data each epoch\n",
    "\n",
    "    batch = []\n",
    "\n",
    "    # yield minibatches\n",
    "    for example in data:\n",
    "        batch.append(example)\n",
    "\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "\n",
    "    # in case there is something left\n",
    "    if len(batch) > 0:\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DwZM-XYkT8Zx"
   },
   "source": [
    "#### Pad function\n",
    "We will need a function that adds padding 1s to a sequence of IDs so that\n",
    "it becomes as long as the longest sequencen in the minibatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sp0sK1ghw4Ft"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 1, 1]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pad(tokens, length, pad_value=1):\n",
    "    \"\"\"add padding 1s to a sequence to that it has the desired length\"\"\"\n",
    "    return tokens + [pad_value] * (length - len(tokens))\n",
    "\n",
    "# example\n",
    "tokens = [2, 3, 4]\n",
    "pad(tokens, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SL2iixMYUgfh"
   },
   "source": [
    "#### New prepare function\n",
    "\n",
    "We will also need a new function that turns a mini-batch into PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZID0cqozWks8"
   },
   "outputs": [],
   "source": [
    "def prepare_minibatch(mb, vocab):\n",
    "    \"\"\"\n",
    "    Minibatch is a list of examples.\n",
    "    This function converts words to IDs and returns\n",
    "    torch tensors to be used as input/targets.\n",
    "    \"\"\"\n",
    "    batch_size = len(mb)\n",
    "    maxlen = max([len(ex.tokens) for ex in mb])\n",
    "\n",
    "    # vocab returns 0 if the word is not there\n",
    "    x = [pad([vocab.w2i.get(t, 0) for t in ex.tokens], maxlen) for ex in mb]\n",
    "\n",
    "    x = torch.LongTensor(x)\n",
    "    x = x.to(device)\n",
    "\n",
    "    y = [ex.label for ex in mb]\n",
    "    y = torch.LongTensor(y)\n",
    "    y = y.to(device)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OwDAtCv1x2hB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Example(tokens=['The', 'Rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'Century', \"'s\", 'new', '``', 'Conan', \"''\", 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'Arnold', 'Schwarzenegger', ',', 'Jean-Claud', 'Van', 'Damme', 'or', 'Steven', 'Segal', '.'], tree=Tree('3', [Tree('2', [Tree('2', ['The']), Tree('2', ['Rock'])]), Tree('4', [Tree('3', [Tree('2', ['is']), Tree('4', [Tree('2', ['destined']), Tree('2', [Tree('2', [Tree('2', [Tree('2', [Tree('2', ['to']), Tree('2', [Tree('2', ['be']), Tree('2', [Tree('2', ['the']), Tree('2', [Tree('2', ['21st']), Tree('2', [Tree('2', [Tree('2', ['Century']), Tree('2', [\"'s\"])]), Tree('2', [Tree('3', ['new']), Tree('2', [Tree('2', ['``']), Tree('2', ['Conan'])])])])])])])]), Tree('2', [\"''\"])]), Tree('2', ['and'])]), Tree('3', [Tree('2', ['that']), Tree('3', [Tree('2', ['he']), Tree('3', [Tree('2', [\"'s\"]), Tree('3', [Tree('2', ['going']), Tree('3', [Tree('2', ['to']), Tree('4', [Tree('3', [Tree('2', ['make']), Tree('3', [Tree('3', [Tree('2', ['a']), Tree('3', ['splash'])]), Tree('2', [Tree('2', ['even']), Tree('3', ['greater'])])])]), Tree('2', [Tree('2', ['than']), Tree('2', [Tree('2', [Tree('2', [Tree('2', [Tree('1', [Tree('2', ['Arnold']), Tree('2', ['Schwarzenegger'])]), Tree('2', [','])]), Tree('2', [Tree('2', ['Jean-Claud']), Tree('2', [Tree('2', ['Van']), Tree('2', ['Damme'])])])]), Tree('2', ['or'])]), Tree('2', [Tree('2', ['Steven']), Tree('2', ['Segal'])])])])])])])])])])])])]), Tree('2', ['.'])])]), label=3, transitions=[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], index=1, loss=[], transition_matrix=None)\n",
      "Example(tokens=['The', 'gorgeously', 'elaborate', 'continuation', 'of', '``', 'The', 'Lord', 'of', 'the', 'Rings', \"''\", 'trilogy', 'is', 'so', 'huge', 'that', 'a', 'column', 'of', 'words', 'can', 'not', 'adequately', 'describe', 'co-writer/director', 'Peter', 'Jackson', \"'s\", 'expanded', 'vision', 'of', 'J.R.R.', 'Tolkien', \"'s\", 'Middle-earth', '.'], tree=Tree('4', [Tree('4', [Tree('4', [Tree('2', ['The']), Tree('4', [Tree('3', ['gorgeously']), Tree('3', [Tree('2', ['elaborate']), Tree('2', ['continuation'])])])]), Tree('2', [Tree('2', [Tree('2', ['of']), Tree('2', ['``'])]), Tree('2', [Tree('2', ['The']), Tree('2', [Tree('2', [Tree('2', ['Lord']), Tree('2', [Tree('2', ['of']), Tree('2', [Tree('2', ['the']), Tree('2', ['Rings'])])])]), Tree('2', [Tree('2', [\"''\"]), Tree('2', ['trilogy'])])])])])]), Tree('2', [Tree('3', [Tree('2', [Tree('2', ['is']), Tree('2', [Tree('2', ['so']), Tree('2', ['huge'])])]), Tree('2', [Tree('2', ['that']), Tree('3', [Tree('2', [Tree('2', [Tree('2', ['a']), Tree('2', ['column'])]), Tree('2', [Tree('2', ['of']), Tree('2', ['words'])])]), Tree('2', [Tree('2', [Tree('2', [Tree('2', ['can']), Tree('1', ['not'])]), Tree('3', ['adequately'])]), Tree('2', [Tree('2', ['describe']), Tree('2', [Tree('3', [Tree('2', [Tree('2', ['co-writer/director']), Tree('2', [Tree('2', ['Peter']), Tree('3', [Tree('2', ['Jackson']), Tree('2', [\"'s\"])])])]), Tree('3', [Tree('2', ['expanded']), Tree('2', ['vision'])])]), Tree('2', [Tree('2', ['of']), Tree('2', [Tree('2', [Tree('2', ['J.R.R.']), Tree('2', [Tree('2', ['Tolkien']), Tree('2', [\"'s\"])])]), Tree('2', ['Middle-earth'])])])])])])])])]), Tree('2', ['.'])])]), label=4, transitions=[0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], index=2, loss=[], transition_matrix=None)\n",
      "Example(tokens=['Singer/composer', 'Bryan', 'Adams', 'contributes', 'a', 'slew', 'of', 'songs', '--', 'a', 'few', 'potential', 'hits', ',', 'a', 'few', 'more', 'simply', 'intrusive', 'to', 'the', 'story', '--', 'but', 'the', 'whole', 'package', 'certainly', 'captures', 'the', 'intended', ',', 'er', ',', 'spirit', 'of', 'the', 'piece', '.'], tree=Tree('3', [Tree('3', [Tree('2', [Tree('2', [Tree('2', [Tree('2', [Tree('2', ['Singer/composer']), Tree('2', [Tree('2', ['Bryan']), Tree('2', ['Adams'])])]), Tree('2', [Tree('2', ['contributes']), Tree('2', [Tree('2', [Tree('2', ['a']), Tree('2', ['slew'])]), Tree('2', [Tree('2', ['of']), Tree('2', ['songs'])])])])]), Tree('2', [Tree('2', ['--']), Tree('2', [Tree('2', [Tree('2', [Tree('2', ['a']), Tree('2', [Tree('2', ['few']), Tree('3', ['potential'])])]), Tree('2', [Tree('2', [Tree('2', ['hits']), Tree('2', [','])]), Tree('2', [Tree('2', [Tree('2', ['a']), Tree('2', ['few'])]), Tree('1', [Tree('1', [Tree('2', ['more']), Tree('1', [Tree('2', ['simply']), Tree('2', ['intrusive'])])]), Tree('2', [Tree('2', ['to']), Tree('2', [Tree('2', ['the']), Tree('2', ['story'])])])])])])]), Tree('2', ['--'])])])]), Tree('2', ['but'])]), Tree('3', [Tree('4', [Tree('2', ['the']), Tree('3', [Tree('2', ['whole']), Tree('2', ['package'])])]), Tree('2', [Tree('3', ['certainly']), Tree('3', [Tree('2', ['captures']), Tree('2', [Tree('1', [Tree('2', ['the']), Tree('2', [Tree('2', [Tree('2', ['intended']), Tree('2', [Tree('2', [',']), Tree('2', [Tree('2', ['er']), Tree('2', [','])])])]), Tree('3', ['spirit'])])]), Tree('2', [Tree('2', ['of']), Tree('2', [Tree('2', ['the']), Tree('2', ['piece'])])])])])])])]), Tree('2', ['.'])]), label=3, transitions=[0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1], index=3, loss=[], transition_matrix=None)\n"
     ]
    }
   ],
   "source": [
    "# Let's test our new function.\n",
    "# This should give us 3 examples.\n",
    "mb = next(get_minibatch(train_data, batch_size=3, shuffle=False))\n",
    "print(len(mb))\n",
    "for ex in mb:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dg8zEK8zyUCH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x tensor([[   23,  1414,    11,  8222,     6,    27,     4,  2826,  3764,    21,\n",
      "            92,  5233,  8558, 15010,     5,    16,    53,    21,   183,     6,\n",
      "           107,     8,  7409,   148,  1552,    97,  6000, 10464,     2,     0,\n",
      "          2740, 15451,    33,  4008, 12925,     3,     1,     1,     1],\n",
      "        [   23, 15300,  6665,  8307,     7,  5233,    23,  1383,     7,     4,\n",
      "          4884, 15010,  8681,    11,    59,   933,    16,     8,  3045,     7,\n",
      "           566,    42,    36,  7800,  2886, 20302,  1433,  1977,    21,  4360,\n",
      "          2274,     7, 15256, 10099,    21, 15561,     3,     1,     1],\n",
      "        [    0,  5314,  3878,  7735,     8,  9726,     7,  1261,   158,     8,\n",
      "           219,  1060,  2187,     2,     8,   219,    50,   688, 11605,     6,\n",
      "             4,   414,   158,    43,     4,   494,  1568,  1214,  6985,     4,\n",
      "          2243,     2,  5986,     2,  2320,     7,     4,   982,     3]],\n",
      "       device='cuda:0')\n",
      "y tensor([3, 4, 3], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# We should find 1s at the end where padding is.\n",
    "x, y = prepare_minibatch(mb, v)\n",
    "print(\"x\", x)\n",
    "print(\"y\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xYBJEoSNUwI0"
   },
   "source": [
    "#### Evaluate (mini-batch version)\n",
    "\n",
    "We can now update our evaluation function to use minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eiZZpEghzqou"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data, \n",
    "             batch_fn=get_minibatch, prep_fn=prepare_minibatch,\n",
    "             batch_size=16):\n",
    "    \"\"\"Accuracy of a model on given data set (using minibatches)\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()  # disable dropout\n",
    "\n",
    "    for mb in batch_fn(data, batch_size=batch_size, shuffle=False):\n",
    "        x, targets = prep_fn(mb, model.vocab)\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)\n",
    "\n",
    "        predictions = logits.argmax(dim=-1).view(-1)\n",
    "\n",
    "        # add the number of correct predictions to the total correct\n",
    "        correct += (predictions == targets.view(-1)).sum().item()\n",
    "        total += targets.size(0)\n",
    "    print(\"Evaluation on \" + str(total) + \" elements. Correct: \" + str(correct)) \n",
    "\n",
    "    return correct, total, correct / float(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "23wAZomozh_2"
   },
   "source": [
    "# LSTM (Mini-batched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B-gkPU7jzBe2"
   },
   "source": [
    "With this, let's run the LSTM again but now using minibatches!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "226Xg9OPzFbA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlstm_model = LSTMClassifier(\\n    len(v.w2i), 300, 168, len(t2i), v)\\n\\n# copy pre-trained vectors into embeddings table\\nwith torch.no_grad():\\n    lstm_model.embed.weight.data.copy_(torch.from_numpy(vectors))\\n    lstm_model.embed.weight.requires_grad = False\\n\\nprint(lstm_model)\\nprint_parameters(lstm_model)  \\n  \\nlstm_model = lstm_model.to(device)\\n\\nbatch_size = 512\\noptimizer = optim.Adam(lstm_model.parameters(), lr=2e-4)\\n\\nlstm_losses, lstm_accuracies = train_model(\\n    lstm_model, optimizer, num_iterations=30000, \\n    print_every=250, eval_every=250,\\n    batch_size=batch_size,\\n    batch_fn=get_minibatch, \\n    prep_fn=prepare_minibatch,\\n    eval_fn=evaluate)\\n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "lstm_model = LSTMClassifier(\n",
    "    len(v.w2i), 300, 168, len(t2i), v)\n",
    "\n",
    "# copy pre-trained vectors into embeddings table\n",
    "with torch.no_grad():\n",
    "    lstm_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
    "    lstm_model.embed.weight.requires_grad = False\n",
    "\n",
    "print(lstm_model)\n",
    "print_parameters(lstm_model)  \n",
    "  \n",
    "lstm_model = lstm_model.to(device)\n",
    "\n",
    "batch_size = 512\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=2e-4)\n",
    "\n",
    "lstm_losses, lstm_accuracies = train_model(\n",
    "    lstm_model, optimizer, num_iterations=30000, \n",
    "    print_every=250, eval_every=250,\n",
    "    batch_size=batch_size,\n",
    "    batch_fn=get_minibatch, \n",
    "    prep_fn=prepare_minibatch,\n",
    "    eval_fn=evaluate)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ymj1rLDMvyhp"
   },
   "outputs": [],
   "source": [
    "# plot validation accuracy\n",
    "# plt.plot(lstm_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1je5S1RHVC5R"
   },
   "outputs": [],
   "source": [
    "# plot training loss\n",
    "# plt.plot(lstm_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q7WjcxXntMi5"
   },
   "source": [
    "# Tree LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jyj_UD6GtO5M"
   },
   "source": [
    "In the final part of this lab we will exploit the tree-structure of our data. \n",
    "Until now we only used the surface tokens, but remember that our data examples include trees with a sentiment score at every node.\n",
    "\n",
    "In particular, we will implement **N-ary Tree-LSTMs** which are described in:\n",
    "\n",
    "> Kai Sheng Tai, Richard Socher, and Christopher D. Manning. [Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks](http://aclweb.org/anthology/P/P15/P15-1150.pdf) ACL 2015.\n",
    "\n",
    "Since our trees are binary, N=2, and we can refer to these as *Binary Tree-LSTMs*.\n",
    "\n",
    "You should read this paper carefully and make sure that you understand the approach. You will also find our LSTM baseline there.\n",
    "Note however that Tree LSTMs were also invented around the same time by two other groups:\n",
    "\n",
    "> Phong Le and Willem Zuidema. [Compositional distributional semantics with long short term memory](http://anthology.aclweb.org/S/S15/S15-1002.pdf). *SEM 2015.\n",
    "\n",
    "> Xiaodan Zhu, Parinaz Sobihani,  and Hongyu Guo. [Long short-term memory over recursive structures](http://proceedings.mlr.press/v37/zhub15.pdf). ICML 2015.\n",
    "\n",
    "It is good scientific practice to cite all three papers in your report.\n",
    "\n",
    "If you study equations (9) to (14) in the paper, you will find that they are not all too different from the original LSTM that you already have.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1rDzvSos3JFp"
   },
   "source": [
    "## Computation\n",
    "\n",
    "Do you remember the `transitions_from_treestring` function all the way in the beginning of this lab? Every example contains a **transition sequence** made by this function. Let's look at it again:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5pg0Xumc3ZUS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              3                                                                     \n",
      "  ____________|____________________                                                  \n",
      " |                                 4                                                \n",
      " |        _________________________|______________________________________________   \n",
      " |       4                                                                        | \n",
      " |    ___|______________                                                          |  \n",
      " |   |                  4                                                         | \n",
      " |   |         _________|__________                                               |  \n",
      " |   |        |                    3                                              | \n",
      " |   |        |               _____|______________________                        |  \n",
      " |   |        |              |                            4                       | \n",
      " |   |        |              |            ________________|_______                |  \n",
      " |   |        |              |           |                        2               | \n",
      " |   |        |              |           |                 _______|___            |  \n",
      " |   |        3              |           |                |           2           | \n",
      " |   |    ____|_____         |           |                |        ___|_____      |  \n",
      " |   |   |          4        |           3                |       2         |     | \n",
      " |   |   |     _____|___     |      _____|_______         |    ___|___      |     |  \n",
      " 2   2   2    3         2    2     3             2        2   2       2     2     2 \n",
      " |   |   |    |         |    |     |             |        |   |       |     |     |  \n",
      " It  's  a  lovely     film with lovely     performances  by Buy     and Accorsi  . \n",
      "\n",
      "Transitions:\n",
      "[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "ex = next(examplereader(\"trees/dev.txt\"))\n",
    "print(TreePrettyPrinter(ex.tree))\n",
    "print(\"Transitions:\")\n",
    "print(ex.transitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ceBFe9fU4BI_"
   },
   "source": [
    "Note that the tree is **binary**. Every node has two children, except for pre-terminal nodes.\n",
    "\n",
    "A tree like this can be described by a sequence of **SHIFT (0)** and **REDUCE (1)** actions.\n",
    "\n",
    "We can use the transitions like this to construct the tree:\n",
    "- **reverse** the sentence (a list of tokens) and call this the **buffer**\n",
    "   - the first word is now on top (last in the list), and we would get it when calling pop() on the buffer\n",
    "- create an empty list and call it the **stack**\n",
    "- iterate through the transition sequence:\n",
    "  - if it says SHIFT(0), we pop a word from the buffer, and push it to the stack\n",
    "  - if it says REDUCE(1), we pop the **top two items** from the stack, and combine them (e.g. with a tree LSTM!), creating a new node that we push back on the stack\n",
    "  \n",
    "Convince yourself that going through the transition sequence above will result in the tree that you see.\n",
    "For example, we would start by putting the following words on the stack (by shifting 5 times, starting with `It`):\n",
    "\n",
    "```\n",
    "Top of the stack:\n",
    "-----------------\n",
    "film\n",
    "lovely\n",
    "a \n",
    "'s  \n",
    "It\n",
    "```\n",
    "Now we find a REDUCE in the transition sequence, so we get the top two words (film and lovely), and combine them, so our new stack becomes:\n",
    "```\n",
    "Top of the stack:\n",
    "-----------------\n",
    "lovely film\n",
    "a \n",
    "'s  \n",
    "It\n",
    "```\n",
    "\n",
    "We will use this approach when encoding sentences with our Tree LSTM.\n",
    "Now, our sentence is a (reversed) list of word embeddings.\n",
    "When we shift, we move a word embedding to the stack.\n",
    "When we reduce, we apply a Tree LSTM to the top two vectors, and the result is a single vector that we put back on the stack.\n",
    "After going through the whole transition sequence, we will have the root node on our stack! We can use that to classify the sentence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pDWKShm1AfmR"
   },
   "source": [
    "## Obtaining the transition sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fO7VKWVpAbWj"
   },
   "source": [
    "\n",
    "So what goes on in the `transitions_from_treestring` function?\n",
    "\n",
    "The idea ([explained in this blog post](https://devblogs.nvidia.com/recursive-neural-networks-pytorch/)) is that, if we had a tree, we could traverse through the tree, and every time that we find a node containing only a word, we output a SHIFT.\n",
    "Every time **after** we have finished visiting the children of a node, we output a REDUCE.\n",
    "(What is this tree traversal called?)\n",
    "\n",
    "However, our `transitions_from_treestring` function operates directly on the string representation. It works as follows.\n",
    "\n",
    "We start with the representation:\n",
    "\n",
    "```\n",
    "(3 (2 It) (4 (4 (2 's) (4 (3 (2 a) (4 (3 lovely) (2 film))) (3 (2 with) (4 (3 (3 lovely) (2 performances)) (2 (2 by) (2 (2 (2 Buy) (2 and)) (2 Accorsi))))))) (2 .)))\n",
    "```\n",
    "\n",
    "First we remove pre-terminal nodes (and add spaces before closing brackets):\n",
    "\n",
    "```\n",
    "(3 It (4 (4 's (4 (3 a (4 lovely film ) ) (3 with (4 (3 lovely performances ) (2 by (2 (2 Buy and )  Accorsi ) ) ) ) ) ) . ) )\n",
    "```\n",
    "\n",
    "Then we remove node labels:\n",
    "\n",
    "```\n",
    "( It ( ( 's ( ( a ( lovely film ) ) ( with ( ( lovely performances) ( by ( ( Buy and )  Accorsi ) ) ) ) ) ) . ) )\n",
    "```\n",
    "\n",
    "Then we remove opening brackets:\n",
    "\n",
    "```\n",
    "It 's a lovely film ) ) with lovely performances ) by Buy and ) Accorsi ) ) ) ) ) ) . ) )\n",
    "```\n",
    "\n",
    "Now we replace words by S (for SHIFT), and closing brackets by R (for REDUCE):\n",
    "\n",
    "```\n",
    "S S S S S R R S S S R S S S R S R R R R R R S R R\n",
    "0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 \n",
    "```\n",
    "\n",
    "Et voila. We just obtained the transition sequence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1y069gM4_v64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S S S S S R R S S S R S S S R S R R R R R R S R R\n",
      "0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1\n"
     ]
    }
   ],
   "source": [
    "# for comparison\n",
    "seq = ex.transitions\n",
    "s = \" \".join([\"S\" if t == 0 else \"R\" for t in seq])\n",
    "print(s)\n",
    "print(\" \".join(map(str, seq)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d-qOuKbDAiBn"
   },
   "source": [
    "## Coding the Tree LSTM\n",
    "\n",
    "The code below contains a Binary Tree LSTM cell.\n",
    "It is used in the TreeLSTM class below it, which in turn is used in the TreeLSTMClassifier.\n",
    "The job of the TreeLSTM class is to encode a complete sentence and return the root node.\n",
    "The job of the TreeLSTMCell is to return a new state when provided with two children (a reduce action). By repeatedly calling the TreeLSTMCell, the TreeLSTM will encode a sentence. This can be done for multiple sentences at the same time.\n",
    "\n",
    "\n",
    "#### Exercise \n",
    "Check the `forward` function and complete the Tree LSTM formulas.\n",
    "You can see that we defined a large linear layer for you, that projects the *concatenation* of the left and right child into the input gate, left forget gate, right forget gate, candidate, and output gate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J9b9mjMlN7Pb"
   },
   "outputs": [],
   "source": [
    "class TreeLSTMCell(nn.Module):\n",
    "    \"\"\"A Binary Tree LSTM cell\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        \"\"\"Creates the weights for this LSTM\"\"\"\n",
    "        super(TreeLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "\n",
    "        self.reduce_layer = nn.Linear(2 * hidden_size, 5 * hidden_size)\n",
    "        self.dropout_layer = nn.Dropout(p=0.25)\n",
    "        self.tanh_act = nn.Tanh()\n",
    "        self.sigmoid_act = nn.Sigmoid()\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"This is PyTorch's default initialization method\"\"\"\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)  \n",
    "\n",
    "    def forward(self, hx_l, hx_r, mask=None):\n",
    "        \"\"\"\n",
    "        hx_l is ((batch, hidden_size), (batch, hidden_size))\n",
    "        hx_r is ((batch, hidden_size), (batch, hidden_size))    \n",
    "        \"\"\"\n",
    "        prev_h_l, prev_c_l = hx_l  # left child\n",
    "        prev_h_r, prev_c_r = hx_r  # right child\n",
    "\n",
    "        B = prev_h_l.size(0)\n",
    "\n",
    "        # we concatenate the left and right children\n",
    "        # you can also project from them separately and then sum\n",
    "        children = torch.cat([prev_h_l, prev_h_r], dim=1)\n",
    "\n",
    "        # project the combined children into a 5D tensor for i,fl,fr,g,o\n",
    "        # this is done for speed, and you could also do it separately\n",
    "        proj = self.reduce_layer(children)  # shape: B x 5D\n",
    "\n",
    "        # each shape: B x D\n",
    "        i, f_l, f_r, g, o = torch.chunk(proj, 5, dim=-1)\n",
    "\n",
    "        # main Tree LSTM computation\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # You only need to complete the commented lines below.\n",
    "\n",
    "        # The shape of each of these is [batch_size, hidden_size]\n",
    "\n",
    "        i = self.sigmoid_act(i)\n",
    "        f_l = self.sigmoid_act(f_l)    \n",
    "        f_r = self.sigmoid_act(f_r)\n",
    "        g = self.tanh_act(g)\n",
    "        o = self.sigmoid_act(o)\n",
    "\n",
    "        c = i * g + f_l * prev_c_l + f_r * prev_c_r\n",
    "        h = o * self.tanh_act(c)\n",
    "\n",
    "        return h, c\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"{}({:d}, {:d})\".format(\n",
    "            self.__class__.__name__, self.input_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dj5dYSGh_643"
   },
   "source": [
    "## Explanation of the TreeLSTM class\n",
    "\n",
    "\n",
    "The code below contains the TreeLSTM class, which implements everything we need in order to encode a sentence from word embeddings. The calculations are the same as in the paper, implemented such that the class `TreeLSTMCell` above is as general as possible and only takes two children to reduce them into a parent. \n",
    "\n",
    "\n",
    "**Initialize $\\mathbf{h}$ and $\\mathbf{c}$ outside of the cell for the leaves**\n",
    "\n",
    "At the leaves of each tree the children nodes are **empty**, whereas in higher levels the nodes are binary tree nodes that *do* have a left and right child (but no input $x$). By initializing the leaf nodes outside of the cell class (`TreeLSTMCell`), we avoid if-else statements in the forward pass.\n",
    "\n",
    "The `TreeLSTM` class (among other things) pre-calculates an initial $h$ and $c$ for every word in the sentence. Since the initial left and right child are 0, the only calculations we need to do are based on $x$, and we can drop the forget gate calculation (`prev_c_l` and `prev_c_r` are zero). The calculations we do in order to initalize $h$ and $c$ are then:\n",
    "\n",
    "$$\n",
    "c_1 =  W^{(u)}x_1 \\\\\n",
    "o_1 = \\sigma (W^{(i)}x_1) \\\\\n",
    "h_1 = o_1 \\odot \\text{tanh}(c_1)$$\n",
    "*NB: note that these equations are chosen as initializations of $c$ and $h$, other initializations are possible and might work equally well.*\n",
    "\n",
    "**Sentence Representations**\n",
    "\n",
    "All our leaf nodes are now initialized, so we can start processing the sentence in its tree form. Each sentence is represented by a buffer (initially a list with a concatenation of $[h_1, c_1]$ for every word in the reversed sentence), a stack (initially an empty list) and a transition sequence. To encode our sentence, we construct the tree from its transition sequence as explained earlier. \n",
    "\n",
    "*A short example that constructs a tree:*\n",
    "\n",
    "We loop over the time dimension of the batched transition sequences (i.e. row by row), which contain values of 0's, 1's and 2's (representing SHIFT, REDUCE and padding respectively). If we have a batch of size 2 where the first example has a transition sequence given by [0, 0, 1, 0, 0, 0, 1] and the second by [0, 0, 1, 0, 0, 1], our transition batch will be given by the following two-dimensional numpy array:\n",
    "\n",
    "$$\n",
    "\\text{transitions} = \n",
    "\\begin{pmatrix}\n",
    "0 & 0\\\\ \n",
    "0 & 0\\\\ \n",
    "1 & 1\\\\ \n",
    "0 & 0\\\\ \n",
    "0 & 0\\\\ \n",
    "0 & 1\\\\ \n",
    "1 & 2\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "The inner loop (`for transition, buffer, stack in zip(t_batch, buffers, stacks)`) goes over each example in the batch and updates its buffer and stack. The nested loop for this example will then do roughy the following:\n",
    "\n",
    "```\n",
    "Time = 0:  t_batch = [0, 0], the inner loop performs 2 SHIFTs. \n",
    "\n",
    "Time = 1:  t_batch = [0, 0], \"..\"\n",
    "\n",
    "Time = 2:  t_batch = [1, 1], causing the inner loop to fill the list child_l and child_r for both examples in the batch. Now the statement if child_l will return True, triggering a REDUCE action to be performed by our Tree LSTM cell with a batch size of 2. \n",
    "\n",
    "Time = 3:  t_batch = [0, 0], \"..\".\n",
    "\n",
    "Time = 4:  t_batch = [0, 0], \"..\"\n",
    "\n",
    "Time = 5:  t_batch = [0, 1], one SHIFT will be done and another REDUCE action will be performed by our Tree LSTM, this time of batch size 1.  \n",
    "\n",
    "Time = 6:  t_batch = [1, 2], triggering another REDUCE action with batch size 1.\n",
    "```\n",
    "*NB: note that this was an artificial example for the purpose of demonstrating parts of the code, the transition sequences do not necessarily represent actual trees.*\n",
    "\n",
    "**Batching and Unbatching**\n",
    "\n",
    "Within the body of the outer loop over time, we use the functions for batching and unbatching. \n",
    "\n",
    "*Batching*\n",
    "\n",
    "Before passing two lists of children to the reduce layer (an instance of `TreeLSTMCell`), we batch the children as they are at this point a list of tensors of variable length based on how many REDUCE actions there are to perform at a certain time step across the batch (let's call the length `L`). To do an efficient forward pass we want to transform the list to a pair of tensors of shape `([L, D], [L, D])`, which the function `batch` achieves. \n",
    "\n",
    "*Unbatching*\n",
    "\n",
    "In the same line where we batched the children, we unbatch the output of the forward pass to become a list of states of length `L` again. We do this because we need to loop over each example's transition at the current time step and push the children that are reduced into a parent to the stack.\n",
    "\n",
    "*The batch and unbatch functions let us switch between the \"PyTorch world\" (Tensors) and the Python world (easy to manipulate lists).*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5PixvTd4AqsQ"
   },
   "outputs": [],
   "source": [
    "# Helper functions for batching and unbatching states\n",
    "# For speed we want to combine computations by batching, but \n",
    "# for processing logic we want to turn the output into lists again\n",
    "# to easily manipulate.\n",
    "\n",
    "def batch(states):\n",
    "    \"\"\"\n",
    "    Turns a list of states into a single tensor for fast processing. \n",
    "    This function also chunks (splits) each state into a (h, c) pair\"\"\"\n",
    "    return torch.cat(states, 0).chunk(2, 1)\n",
    "\n",
    "def unbatch(state):\n",
    "    \"\"\"\n",
    "    Turns a tensor back into a list of states.\n",
    "    First, (h, c) are merged into a single state.\n",
    "    Then the result is split into a list of sentences.\n",
    "    \"\"\"\n",
    "    return torch.split(torch.cat(state, 1), 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CynltDasaLPt"
   },
   "source": [
    "Take some time to understand the class below, having read the explanation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rQOqMXG4gX5G"
   },
   "outputs": [],
   "source": [
    "class TreeLSTM(nn.Module):\n",
    "    \"\"\"Encodes a sentence using a TreeLSTMCell\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        \"\"\"Creates the weights for this LSTM\"\"\"\n",
    "        super(TreeLSTM, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "        self.reduce = TreeLSTMCell(input_size, hidden_size)\n",
    "\n",
    "        # project word to initial c\n",
    "        self.proj_x = nn.Linear(input_size, hidden_size)\n",
    "        self.proj_x_gate = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        self.buffers_dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x, transitions):\n",
    "        \"\"\"\n",
    "        WARNING: assuming x is reversed!\n",
    "        :param x: word embeddings [B, T, E]\n",
    "        :param transitions: [2T-1, B]\n",
    "        :return: root states\n",
    "        \"\"\"\n",
    "\n",
    "        B = x.size(0)  # batch size\n",
    "        T = x.size(1)  # time\n",
    "\n",
    "        # compute an initial c and h for each word\n",
    "        # Note: this corresponds to input x in the Tai et al. Tree LSTM paper.\n",
    "        # We do not handle input x in the TreeLSTMCell itself.\n",
    "        buffers_c = self.proj_x(x)\n",
    "        buffers_h = buffers_c.tanh()\n",
    "        buffers_h_gate = self.proj_x_gate(x).sigmoid()\n",
    "        buffers_h = buffers_h_gate * buffers_h\n",
    "\n",
    "        # concatenate h and c for each word\n",
    "        buffers = torch.cat([buffers_h, buffers_c], dim=-1)\n",
    "\n",
    "        D = buffers.size(-1) // 2\n",
    "\n",
    "        # we turn buffers into a list of stacks (1 stack for each sentence)\n",
    "        # first we split buffers so that it is a list of sentences (length B)\n",
    "        # then we split each sentence to be a list of word vectors\n",
    "        buffers = buffers.split(1, dim=0)  # Bx[T, 2D]\n",
    "        buffers = [list(b.squeeze(0).split(1, dim=0)) for b in buffers]  # BxTx[2D]\n",
    "\n",
    "        # create B empty stacks\n",
    "        stacks = [[] for _ in buffers]\n",
    "\n",
    "        # t_batch holds 1 transition for each sentence\n",
    "        for t_batch in transitions:\n",
    "\n",
    "            child_l = []  # contains the left child for each sentence with reduce action\n",
    "            child_r = []  # contains the corresponding right child\n",
    "\n",
    "            # iterate over sentences in the batch\n",
    "            # each has a transition t, a buffer and a stack\n",
    "            for transition, buffer, stack in zip(t_batch, buffers, stacks):\n",
    "                if transition == SHIFT:\n",
    "                    stack.append(buffer.pop())\n",
    "                elif transition == REDUCE:\n",
    "                    assert len(stack) >= 2, \\\n",
    "                        \"Stack too small! Should not happen with valid transition sequences\"\n",
    "                    child_r.append(stack.pop())  # right child is on top\n",
    "                    child_l.append(stack.pop())\n",
    "\n",
    "            # if there are sentences with reduce transition, perform them batched\n",
    "            if child_l:\n",
    "                reduced = iter(unbatch(self.reduce(batch(child_l), batch(child_r))))\n",
    "                for transition, stack in zip(t_batch, stacks):\n",
    "                    if transition == REDUCE:\n",
    "                        stack.append(next(reduced))\n",
    "\n",
    "        final = [stack.pop().chunk(2, -1)[0] for stack in stacks]\n",
    "        final = torch.cat(final, dim=0)  # tensor [B, D]\n",
    "\n",
    "        return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s4EzbVzqaXkw"
   },
   "source": [
    "Just like the LSTM before, we will need an extra class that does the classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nLxpYRvtQKge"
   },
   "outputs": [],
   "source": [
    "class TreeLSTMClassifier(nn.Module):\n",
    "    \"\"\"Encodes sentence with a TreeLSTM and projects final hidden state\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, vocab):\n",
    "        super(TreeLSTMClassifier, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim, padding_idx=1, requires_grad=True)\n",
    "        self.treelstm = TreeLSTM(embedding_dim, hidden_dim)\n",
    "        self.output_layer = nn.Sequential(     \n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(hidden_dim, output_dim, bias=True)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # x is a pair here of words and transitions; we unpack it here.\n",
    "        # x is batch-major: [B, T], transitions is time major [2T-1, B]\n",
    "        x, transitions = x\n",
    "        emb = self.embed(x)\n",
    "\n",
    "        # we use the root/top state of the Tree LSTM to classify the sentence\n",
    "        root_states = self.treelstm(emb, transitions)\n",
    "\n",
    "        # we use the last hidden state to classify the sentence\n",
    "        logits = self.output_layer(root_states)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gh9RbhGwaiLg"
   },
   "source": [
    "## Special prepare function for Tree LSTM\n",
    "\n",
    "We need yet another prepare function. For our implementation our sentences to be *reversed*. We will do that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DiqH-_2xdm9H"
   },
   "outputs": [],
   "source": [
    "def prepare_treelstm_minibatch(mb, vocab):\n",
    "    \"\"\"\n",
    "    Returns sentences reversed (last word first)\n",
    "    Returns transitions together with the sentences.  \n",
    "    \"\"\"\n",
    "    batch_size = len(mb)\n",
    "    maxlen = max([len(ex.tokens) for ex in mb])\n",
    "\n",
    "    # vocab returns 0 if the word is not there\n",
    "    # NOTE: reversed sequence!\n",
    "    x = [pad([vocab.w2i.get(t, 0) for t in ex.tokens], maxlen)[::-1] for ex in mb]\n",
    "\n",
    "    x = torch.LongTensor(x)\n",
    "    x = x.to(device)\n",
    "\n",
    "    y = [ex.label for ex in mb]\n",
    "    y = torch.LongTensor(y)\n",
    "    y = y.to(device)\n",
    "\n",
    "    maxlen_t = max([len(ex.transitions) for ex in mb])\n",
    "    transitions = [pad(ex.transitions, maxlen_t, pad_value=2) for ex in mb]\n",
    "    transitions = np.array(transitions)\n",
    "    transitions = transitions.T  # time-major\n",
    "\n",
    "    return (x, transitions), y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IMUsrlL9ayVe"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IpOYUdg2D3v0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntree_model = TreeLSTMClassifier(\\n    len(v.w2i), 300, 150, len(t2i), v)\\n\\nwith torch.no_grad():\\n    tree_model.embed.weight.data.copy_(torch.from_numpy(vectors))\\n    tree_model.embed.weight.requires_grad = False\\n  \\ndef do_train(model):\\n\\n    print(model)\\n    print_parameters(model)\\n\\n    model = model.to(device)\\n\\n    optimizer = optim.Adam(model.parameters(), lr=2e-4)\\n\\n    return train_model(\\n        model, optimizer, num_iterations=30000, \\n        print_every=250, eval_every=250,\\n        prep_fn=prepare_treelstm_minibatch,\\n        eval_fn=evaluate,\\n        batch_fn=get_minibatch,\\n        batch_size=32, eval_batch_size=32)\\n  \\nresults = do_train(tree_model)\\n'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's train the Tree LSTM!\n",
    "\"\"\"\n",
    "tree_model = TreeLSTMClassifier(\n",
    "    len(v.w2i), 300, 150, len(t2i), v)\n",
    "\n",
    "with torch.no_grad():\n",
    "    tree_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
    "    tree_model.embed.weight.requires_grad = False\n",
    "  \n",
    "def do_train(model):\n",
    "\n",
    "    print(model)\n",
    "    print_parameters(model)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
    "\n",
    "    return train_model(\n",
    "        model, optimizer, num_iterations=30000, \n",
    "        print_every=250, eval_every=250,\n",
    "        prep_fn=prepare_treelstm_minibatch,\n",
    "        eval_fn=evaluate,\n",
    "        batch_fn=get_minibatch,\n",
    "        batch_size=32, eval_batch_size=32)\n",
    "  \n",
    "results = do_train(tree_model)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DHcHHaLtguUg"
   },
   "outputs": [],
   "source": [
    "# plot\n",
    "# plt.plot(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([2, 2, 2, 3, 2, 4, 3, 2, 3, 2, 3, 2, 2, 2, 2, 2, 2, 2, 4, 3, 4, 4, 2, 4, 3], [1, 3, 5, 6, 6, 5, 4, 5, 7, 7, 6, 7, 9, 9, 8, 8, 7, 6, 5, 4, 3, 2, 2, 1, 0])\n",
      "[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "def get_sublabel_of_tree(tree, depth=0):\n",
    "    label_list = []\n",
    "    depth_list = []\n",
    "    if isinstance(tree[0], Tree):\n",
    "        for subtree in tree:\n",
    "            sublabel_list, subdepth_list = get_sublabel_of_tree(subtree, depth=depth+1)\n",
    "            label_list += sublabel_list\n",
    "            depth_list += subdepth_list\n",
    "    label_list += [int(tree.label())]\n",
    "    depth_list += [depth]\n",
    "    return label_list, depth_list\n",
    "\n",
    "print(get_sublabel_of_tree(dev_data[0].tree))\n",
    "print(dev_data[0].transitions)\n",
    "\n",
    "DEFAULT_LOSS = 50 # High so that in the training progress we first try to explore all words/subtrees\n",
    "\n",
    "min_tree_height = 100\n",
    "\n",
    "for t in train_data:\n",
    "    if len(t.tokens)==5:\n",
    "        break\n",
    "\n",
    "def mean(l):\n",
    "    if len(l) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return sum(l) * 1.0 / len(l)\n",
    "        \n",
    "def augment_example(ex, iteration_num=-1, loss_to_prop=lambda x: x**2):\n",
    "    subtrees = subtrees_train_data[ex.index]\n",
    "    max_depth = max(subtrees.keys())\n",
    "    # Determine probabilities for deciding on the depth of the tree\n",
    "    probs = [4**(-i/(iteration_num/100.0+1)) for i in range(max_depth+1)]\n",
    "    probs = [int(i==0) for i in range(max_depth+1)]\n",
    "    norm_const = sum(probs)\n",
    "    probs = [x/norm_const for x in probs]\n",
    "    rand_depth = np.random.choice(range(max_depth+1), p=probs)\n",
    "    \n",
    "    # Determine probabilities for the exact element chosen for the specified depth\n",
    "    mean_losses = [sum(ex.loss)/len(ex.loss) if len(ex.loss) > 0 else DEFAULT_LOSS for ex in subtrees[rand_depth]]\n",
    "    sum_losses = sum([loss_to_prop(x) for x in mean_losses]) # We square the loss so that we even stronger focus on hard examples. Is a hyperparameter that needs to be adjusted!\n",
    "    probs = [loss_to_prop(x)/sum_losses for x in mean_losses]\n",
    "    rand_index = np.random.choice(range(len(subtrees[rand_depth])), p=probs)\n",
    "    return subtrees[rand_depth][rand_index]\n",
    "\n",
    "\n",
    "def get_minibatch(data, batch_size=25, shuffle=True, is_eval=True, iteration_num=-1):\n",
    "    \"\"\"Return minibatches, optional shuffling\"\"\"\n",
    "\n",
    "    indices = list(range(len(data)))\n",
    "    if not is_eval:\n",
    "        ex_prob = [(1 - mean(d.loss)) for d in data]\n",
    "        norm_const = sum(ex_prob)\n",
    "        if norm_const > 0:\n",
    "            ex_prob = [i/norm_const for i in ex_prob]\n",
    "            add_indices = list(np.random.choice(range(len(data)), p=ex_prob, size=int(len(data)/2)))\n",
    "            indices += add_indices\n",
    "        else:\n",
    "            print(\"Got loss information yet...\")\n",
    "        \n",
    "    if shuffle:\n",
    "        print(\"Shuffling training data\")\n",
    "        random.shuffle(indices)  # shuffle training data each epoch # data\n",
    "\n",
    "    batch = []\n",
    "\n",
    "    # yield minibatches\n",
    "    for ex_index in indices:\n",
    "        example = data[ex_index]\n",
    "        if False and not is_eval:\n",
    "            example = augment_example(example, iteration_num=iteration_num)\n",
    "        batch.append(example) # example\n",
    "\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "\n",
    "    # in case there is something left\n",
    "    if len(batch) > 0:\n",
    "        yield batch\n",
    "        \n",
    "\n",
    "def prepare_treelstm_minibatch(mb, vocab, is_eval=False):\n",
    "    \"\"\"\n",
    "    Returns sentences reversed (last word first)\n",
    "    Returns transitions together with the sentences.  \n",
    "    \"\"\"\n",
    "    batch_size = len(mb)\n",
    "    maxlen = max([len(ex.tokens) for ex in mb])\n",
    "\n",
    "    # vocab returns 0 if the word is not there\n",
    "    # NOTE: reversed sequence!\n",
    "    x = [pad([vocab.w2i.get(t, 0) for t in ex.tokens], maxlen)[::-1] for ex in mb]\n",
    "\n",
    "    x = torch.LongTensor(x)\n",
    "    x = x.to(device)\n",
    "\n",
    "    maxlen_t = max([len(ex.transitions) for ex in mb])\n",
    "    transitions = [pad(ex.transitions, maxlen_t, pad_value=2) for ex in mb]\n",
    "    transitions = np.array(transitions)\n",
    "    transitions = transitions.T  # time-major\n",
    "    \n",
    "    if is_eval:\n",
    "        transition_matrices = None\n",
    "    else:\n",
    "        \n",
    "        transition_matrices = [train_transition_matrices[ex.index] for ex in mb]\n",
    "        # print(\"Transition matrix before: \" + str(transition_matrices[0]))\n",
    "        transition_matrices = [np.pad(m, (0,maxlen_t - m.shape[0]), \"constant\", constant_values=(0,-1)) for m in transition_matrices]\n",
    "        transition_matrices = np.transpose(np.array(transition_matrices), (1,2,0))\n",
    "        transition_matrices = torch.FloatTensor(transition_matrices).to(device)\n",
    "        # print(\"Transition matrix after: \" + str(transition_matrices[:,:,0]))\n",
    "    \n",
    "    sublabels = [get_sublabel_of_tree(ex.tree) for ex in mb]\n",
    "\n",
    "    y = [pad(sublabels[i][0], maxlen_t, pad_value=-1) for i in range(len(mb))]\n",
    "    y = torch.LongTensor(np.array(y).T)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    label_depth = [pad(sublabels[i][1], maxlen_t, pad_value=0) for i in range(len(mb))]\n",
    "    label_depth = torch.LongTensor(np.array(label_depth).T)\n",
    "    label_depth = label_depth.to(device)\n",
    "\n",
    "    return (x, transitions), (y, label_depth, transition_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling training data\n"
     ]
    }
   ],
   "source": [
    "indices = list()\n",
    "for mb in get_minibatch(train_data, batch_size=32):\n",
    "    indices += [train_data.index(ex) for ex in mb]\n",
    "    \n",
    "for i in range(len(train_data)):\n",
    "    if i not in indices:\n",
    "        print(\"Did not find index \" + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth 12: 1\n",
      "Depth 7: 2\n",
      "Depth 3: 4\n",
      "Depth 0: 37\n",
      "Depth 2: 7\n",
      "Depth 1: 11\n",
      "Depth 6: 2\n",
      "Depth 5: 2\n",
      "Depth 4: 3\n",
      "Depth 11: 1\n",
      "Depth 10: 1\n",
      "Depth 9: 1\n",
      "Depth 8: 1\n",
      "                                                                                                                                         4                                                                                                                                                                \n",
      "                                       __________________________________________________________________________________________________|_____________________________________________________________                                                                                                    \n",
      "                                      |                                                                                                                                                                2                                                                                                  \n",
      "                                      |                                                                                                                                                    ____________|________________________________________________________________________________________________   \n",
      "                                      |                                                                                                                                                   3                                                                                                             | \n",
      "                                      |                                                                        ___________________________________________________________________________|____________                                                                                                 |  \n",
      "                                      |                                                                       |                                                                                        2                                                                                                | \n",
      "                                      |                                                                       |             ___________________________________________________________________________|___________                                                                                     |  \n",
      "                                      |                                                                       |            |                                                                                       3                                                                                    | \n",
      "                                      |                                                                       |            |                    ___________________________________________________________________|____________________                                                                |  \n",
      "                                      4                                                                       |            |                   |                                                                                        2                                                               | \n",
      "                 _____________________|________________                                                       |            |                   |                          ______________________________________________________________|_____                                                          |  \n",
      "                |                                      2                                                      |            |                   |                         |                                                                    2                                                         | \n",
      "                |                                   ___|_______                                               |            |                   |                         |                 ___________________________________________________|___________                                              |  \n",
      "                |                                  |           2                                              |            |                   |                         |                |                                                               2                                             | \n",
      "                |                                  |        ___|____________                                  |            |                   |                         |                |                                    ___________________________|__________________                           |  \n",
      "                |                                  |       |                2                                 |            |                   |                         |                |                                   3                                              2                          | \n",
      "                |                                  |       |             ___|_________________                |            |                   |                         |                |                              _____|______________________            ____________|_____                     |  \n",
      "                4                                  |       |            2                     |               |            |                   |                         |                |                             2                            |          |                  2                    | \n",
      "  ______________|______                            |       |    ________|___                  |               |            |                   |                         |                |             ________________|_____                       |          |             _____|___________         |  \n",
      " |                     4                           |       |   |            2                 |               2            |                   2                         2                |            |                      2                      |          |            2                 |        | \n",
      " |       ______________|______                     |       |   |     _______|___              |            ___|___         |         __________|_______               ___|______          |            |            __________|_____                 |          |     _______|_____            |        |  \n",
      " |      |                     3                    2       |   |    |           2             2           |       2        |        2                  2             2          |         |            |           |                3                3          |    |             2           |        | \n",
      " |      |               ______|_______          ___|___    |   |    |        ___|____      ___|_____      |    ___|___     |     ___|____           ___|____      ___|___       |         |            |           |           _____|___       ______|____      |    |        _____|___        |        |  \n",
      " 2      3              2              2        2       2   2   2    2       2        2    2         2     2   2       2    2    2        2         2        2    2       1      3         2            2           2          2         2     2           2     2    2       2         2       2        2 \n",
      " |      |              |              |        |       |   |   |    |       |        |    |         |     |   |       |    |    |        |         |        |    |       |      |         |            |           |          |         |     |           |     |    |       |         |       |        |  \n",
      "The gorgeously     elaborate     continuation  of      `` The Lord  of     the     Rings  ''     trilogy  is  so     huge that  a      column      of     words can     not adequately describe co-writer/direct Peter     Jackson      's expanded     vision  of J.R.R. Tolkien      's Middle-earth  . \n",
      "                                                                                                                                                                                                       or                                                                                                 \n",
      "\n",
      "[0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1]\n",
      "[0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get all subtrees...\n",
      "Order all subtrees...\n",
      "Make sure that every subtree is unique...\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def get_all_subtrees(tree, depth_dict=None):\n",
    "    if depth_dict is None:\n",
    "        depth_dict = dict()\n",
    "    my_depth = tree.height() - 2\n",
    "    subtree_ex = subtree_to_example(copy.deepcopy(tree))\n",
    "    if my_depth in depth_dict:\n",
    "        depth_dict[my_depth].append(subtree_ex)\n",
    "    else:\n",
    "        depth_dict[my_depth] = [subtree_ex]\n",
    "    if isinstance(tree[0], Tree):\n",
    "        for subtree in tree:\n",
    "            get_all_subtrees(subtree, depth_dict=depth_dict)\n",
    "    return depth_dict\n",
    "\n",
    "def tree_to_token_list(tree, token_list=None):\n",
    "    if token_list is None:\n",
    "        token_list = list()\n",
    "    if isinstance(tree[0], Tree):\n",
    "        for subtree in tree:\n",
    "            tree_to_token_list(subtree, token_list=token_list)\n",
    "    else:\n",
    "        token_list.append(tree[0])\n",
    "    return token_list\n",
    "\n",
    "def tree_to_transition_list(tree, transition_list=None):\n",
    "    if transition_list is None:\n",
    "        transition_list = list()\n",
    "    if isinstance(tree[0], Tree):\n",
    "        for subtree in tree:\n",
    "            tree_to_transition_list(subtree, transition_list=transition_list)\n",
    "        transition_list.append(1)\n",
    "    else:\n",
    "        transition_list.append(0)\n",
    "    return transition_list\n",
    "\n",
    "def subtree_to_example(tree):\n",
    "    return Example(tokens=tree_to_token_list(tree), tree=tree, label=int(tree.label()), transitions=tree_to_transition_list(tree), index=-1, loss=list(), transition_matrix=None)\n",
    "\n",
    "subtree_dict = get_all_subtrees(train_data[1].tree)\n",
    "for key, val in subtree_dict.items():\n",
    "    print(\"Depth \" + str(key) + \": \" + str(len(val)))\n",
    "    \n",
    "print(TreePrettyPrinter(train_data[1].tree))\n",
    "print(train_data[1].transitions)\n",
    "print(tree_to_transition_list(train_data[1].tree))\n",
    "\n",
    "subtrees_train_data = dict()\n",
    "train_data_by_depth = dict()\n",
    "p = Pool()\n",
    "print(\"Get all subtrees...\")\n",
    "subtrees_res = p.map(get_all_subtrees, [ex.tree for ex in train_data])\n",
    "p.close()\n",
    "print(\"Order all subtrees...\")\n",
    "for ex_ind, ex in enumerate(train_data):\n",
    "    subtrees_train_data[ex.index] = subtrees_res[ex_ind]\n",
    "print(\"Make sure that every subtree is unique...\") # This shares loss between examples that have the same tree/tokens and therefore the same input\n",
    "all_subtrees = dict()\n",
    "subtrees_token_lists = dict() # List of strings representing token list (used for checking if tree is already there...)\n",
    "subtrees_frequency = dict()\n",
    "for ex_ind, ex in enumerate(train_data):\n",
    "    for depth, subtrees in subtrees_train_data[ex.index].items():\n",
    "        if depth not in all_subtrees:\n",
    "            all_subtrees[depth] = list()\n",
    "        if depth not in subtrees_token_lists:\n",
    "            subtrees_token_lists[depth] = list()\n",
    "            subtrees_frequency[depth] = list()\n",
    "        \n",
    "        replacement_list = list()\n",
    "        for subtree_index, subtree in enumerate(subtrees):\n",
    "            token_str = \" \".join(subtree.tokens)\n",
    "            if token_str in subtrees_token_lists[depth]:\n",
    "                copy_ind = subtrees_token_lists[depth].index(token_str)\n",
    "                subtrees_frequency[depth][copy_ind] += 1\n",
    "                replacement_list.append((subtree_index, copy_ind))\n",
    "            else:\n",
    "                subtrees_token_lists[depth].append(token_str)\n",
    "                subtrees_frequency[depth].append(1)\n",
    "                all_subtrees[depth].append(subtree)\n",
    "        \n",
    "        for subtree_index, copy_ind in replacement_list:\n",
    "            old_tree = subtrees[subtree_index]\n",
    "            subtrees[subtree_index] = all_subtrees[depth][copy_ind]\n",
    "            del old_tree\n",
    "        \n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start creating transition matrices...\n",
      "Convert them into a dict...\n",
      "Finished\n",
      "(73, 73)\n"
     ]
    }
   ],
   "source": [
    "# Create transition matrices based on our examples\n",
    "\n",
    "TRANSITION_DECAY_FACTOR = 0.5\n",
    "\n",
    "def get_transition_matrix_of_example(ex):\n",
    "    transitions = ex.transitions\n",
    "    sublabels = get_sublabel_of_tree(ex.tree)\n",
    "    \n",
    "    no_trans = len(transitions)\n",
    "    transition_matrix = np.zeros((no_trans, no_trans), dtype=np.float32) - 1\n",
    "    \n",
    "    for node_index in range(no_trans)[::-1]:\n",
    "        zero_counter = 2 if transitions[node_index] == 1 else 0\n",
    "        transition_matrix[node_index][node_index] = 1\n",
    "        for sub_node in range(node_index)[::-1]:\n",
    "            if zero_counter <= 0:\n",
    "                break\n",
    "            if transitions[sub_node] == 1:\n",
    "                zero_counter += 1\n",
    "            elif transitions[sub_node] == 0:\n",
    "                zero_counter -= 1\n",
    "            height_diff = sublabels[1][sub_node] - sublabels[1][node_index] \n",
    "            transition_matrix[sub_node][node_index] = 0 # Node_index is a parent of sub_node\n",
    "            transition_matrix[node_index][sub_node] = TRANSITION_DECAY_FACTOR**height_diff # Sub_node is a child of node_index\n",
    "        # print(transition_matrix)\n",
    "    return transition_matrix\n",
    "\n",
    "if False:\n",
    "    test_example = dev_data[smallest_tree]# Example(tokens=['Does','this','work'], tree=Tree('2', [Tree('2', [Tree('2', ['Does']), Tree('2', ['this'])]), Tree('2', ['work'])]), label=2, transitions=[0,0,1,0,1], index=-1, loss=list(), transition_matrix=None)\n",
    "    print(TreePrettyPrinter(test_example.tree))\n",
    "    trans_matrix = get_transition_matrix_of_example(test_example)\n",
    "    print(trans_matrix)\n",
    "\n",
    "train_transition_matrices = dict()\n",
    "def create_transition_matrices():\n",
    "    global train_transition_matrices\n",
    "    p = Pool()\n",
    "    print(\"Start creating transition matrices...\")\n",
    "    list_transition_matrices = p.map(get_transition_matrix_of_example, train_data)\n",
    "    p.close()\n",
    "    print(\"Convert them into a dict...\")\n",
    "    train_transition_matrices = dict()\n",
    "    for d_index, d in enumerate(train_data):\n",
    "        train_transition_matrices[d.index] = list_transition_matrices[d_index]\n",
    "    train_transition_matrices[-1] = np.array([[-1]], dtype=np.float32) # Default transition matrix where every node is independent\n",
    "    print(\"Finished\")\n",
    "    print(train_transition_matrices[train_data[1].index].shape)\n",
    "\n",
    "create_transition_matrices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function\n",
    "for d_index, d in enumerate(train_data):\n",
    "    assert train_transition_matrices[d.index].shape[0] == len(d.transitions) and train_transition_matrices[d.index].shape[1] == len(d.transitions)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0: 1\n",
      "Index 1: 2\n",
      "                                                                                               3                                                                                                                                         \n",
      "      _________________________________________________________________________________________|____                                                                                                                                      \n",
      "     |                                                                                              4                                                                                                                                    \n",
      "     |                      ________________________________________________________________________|__________________________________________________________________________________________________________________________________   \n",
      "     |                     3                                                                                                                                                                                                           | \n",
      "     |         ____________|___________                                                                                                                                                                                                |  \n",
      "     |        |                        4                                                                                                                                                                                               | \n",
      "     |        |      __________________|_________________________________                                                                                                                                                              |  \n",
      "     |        |     |                                                    2                                                                                                                                                             | \n",
      "     |        |     |                                    ________________|_________________                                                                                                                                            |  \n",
      "     |        |     |                                   |                                  3                                                                                                                                           | \n",
      "     |        |     |                                   |                              ____|___                                                                                                                                        |  \n",
      "     |        |     |                                   |                             |        3                                                                                                                                       | \n",
      "     |        |     |                                   |                             |     ___|_________                                                                                                                              |  \n",
      "     |        |     |                                   |                             |    |             3                                                                                                                             | \n",
      "     |        |     |                                   |                             |    |    _________|_________________                                                                                                            |  \n",
      "     |        |     |                                   2                             |    |   |                           3                                                                                                           | \n",
      "     |        |     |                          _________|_________________________    |    |   |     ______________________|________________________________                                                                           |  \n",
      "     |        |     |                         2                                   |   |    |   |    |                                                       3                                                                          | \n",
      "     |        |     |           ______________|_______________________________    |   |    |   |    |     __________________________________________________|______________                                                            |  \n",
      "     |        |     |          2                                              |   |   |    |   |    |    |                                                                 4                                                           | \n",
      "     |        |     |       ___|___                                           |   |   |    |   |    |    |                  _______________________________________________|________________                                           |  \n",
      "     |        |     |      |       2                                          |   |   |    |   |    |    |                 |                                                                2                                          | \n",
      "     |        |     |      |    ___|___                                       |   |   |    |   |    |    |                 |                           _____________________________________|_______                                   |  \n",
      "     |        |     |      |   |       2                                      |   |   |    |   |    |    |                 |                          |                                             2                                  | \n",
      "     |        |     |      |   |    ___|________________                      |   |   |    |   |    |    |                 |                          |                                      _______|________________________          |  \n",
      "     |        |     |      |   |   |                    2                     |   |   |    |   |    |    |                 |                          |                                     2                                |         | \n",
      "     |        |     |      |   |   |    ________________|___                  |   |   |    |   |    |    |                 |                          |                               ______|_____________________           |         |  \n",
      "     |        |     |      |   |   |   |                    2                 |   |   |    |   |    |    |                 3                          |                              2                            |          |         | \n",
      "     |        |     |      |   |   |   |             _______|___              |   |   |    |   |    |    |    _____________|_____                     |                     _________|______________              |          |         |  \n",
      "     |        |     |      |   |   |   |            |           2             |   |   |    |   |    |    |   |                   3                    |                    2                        2             |          |         | \n",
      "     |        |     |      |   |   |   |            |        ___|___          |   |   |    |   |    |    |   |         __________|________            |            ________|_________        _______|___          |          |         |  \n",
      "     2        |     |      |   |   |   |            2       |       2         |   |   |    |   |    |    |   |        3                   2           |           1                  |      |           2         |          2         | \n",
      "  ___|___     |     |      |   |   |   |       _____|___    |    ___|____     |   |   |    |   |    |    |   |     ___|____           ____|_____      |      _____|________          |      |        ___|____     |     _____|____     |  \n",
      " 2       2    2     2      2   2   2   2      2         2   3   2        2    2   2   2    2   2    2    2   2    2        3         2          3     2     2              2         2      2       2        2    2    2          2    2 \n",
      " |       |    |     |      |   |   |   |      |         |   |   |        |    |   |   |    |   |    |    |   |    |        |         |          |     |     |              |         |      |       |        |    |    |          |    |  \n",
      "The     Rock  is destined  to  be the 21st Century      's new  ``     Conan  '' and that  he  's going  to make  a      splash     even     greater than Arnold     Schwarzenegger  ,  Jean-Claud Van     Damme  or Steven     Segal  . \n",
      "\n",
      "                                                                                                                                         4                                                                                                                                                                \n",
      "                                       __________________________________________________________________________________________________|_____________________________________________________________                                                                                                    \n",
      "                                      |                                                                                                                                                                2                                                                                                  \n",
      "                                      |                                                                                                                                                    ____________|________________________________________________________________________________________________   \n",
      "                                      |                                                                                                                                                   3                                                                                                             | \n",
      "                                      |                                                                        ___________________________________________________________________________|____________                                                                                                 |  \n",
      "                                      |                                                                       |                                                                                        2                                                                                                | \n",
      "                                      |                                                                       |             ___________________________________________________________________________|___________                                                                                     |  \n",
      "                                      |                                                                       |            |                                                                                       3                                                                                    | \n",
      "                                      |                                                                       |            |                    ___________________________________________________________________|____________________                                                                |  \n",
      "                                      4                                                                       |            |                   |                                                                                        2                                                               | \n",
      "                 _____________________|________________                                                       |            |                   |                          ______________________________________________________________|_____                                                          |  \n",
      "                |                                      2                                                      |            |                   |                         |                                                                    2                                                         | \n",
      "                |                                   ___|_______                                               |            |                   |                         |                 ___________________________________________________|___________                                              |  \n",
      "                |                                  |           2                                              |            |                   |                         |                |                                                               2                                             | \n",
      "                |                                  |        ___|____________                                  |            |                   |                         |                |                                    ___________________________|__________________                           |  \n",
      "                |                                  |       |                2                                 |            |                   |                         |                |                                   3                                              2                          | \n",
      "                |                                  |       |             ___|_________________                |            |                   |                         |                |                              _____|______________________            ____________|_____                     |  \n",
      "                4                                  |       |            2                     |               |            |                   |                         |                |                             2                            |          |                  2                    | \n",
      "  ______________|______                            |       |    ________|___                  |               |            |                   |                         |                |             ________________|_____                       |          |             _____|___________         |  \n",
      " |                     4                           |       |   |            2                 |               2            |                   2                         2                |            |                      2                      |          |            2                 |        | \n",
      " |       ______________|______                     |       |   |     _______|___              |            ___|___         |         __________|_______               ___|______          |            |            __________|_____                 |          |     _______|_____            |        |  \n",
      " |      |                     3                    2       |   |    |           2             2           |       2        |        2                  2             2          |         |            |           |                3                3          |    |             2           |        | \n",
      " |      |               ______|_______          ___|___    |   |    |        ___|____      ___|_____      |    ___|___     |     ___|____           ___|____      ___|___       |         |            |           |           _____|___       ______|____      |    |        _____|___        |        |  \n",
      " 2      3              2              2        2       2   2   2    2       2        2    2         2     2   2       2    2    2        2         2        2    2       1      3         2            2           2          2         2     2           2     2    2       2         2       2        2 \n",
      " |      |              |              |        |       |   |   |    |       |        |    |         |     |   |       |    |    |        |         |        |    |       |      |         |            |           |          |         |     |           |     |    |       |         |       |        |  \n",
      "The gorgeously     elaborate     continuation  of      `` The Lord  of     the     Rings  ''     trilogy  is  so     huge that  a      column      of     words can     not adequately describe co-writer/direct Peter     Jackson      's expanded     vision  of J.R.R. Tolkien      's Middle-earth  . \n",
      "                                                                                                                                                                                                       or                                                                                                 \n",
      "\n",
      "Last single word subtree 0: Example(tokens=['.'], tree=Tree('2', ['.']), label=2, transitions=[0], index=-1, loss=[], transition_matrix=None)\n",
      "Last single word subtree 1: Example(tokens=['.'], tree=Tree('2', ['.']), label=2, transitions=[0], index=-1, loss=[], transition_matrix=None)\n",
      "Update loss...\n",
      "Last single word subtree 0: Example(tokens=['.'], tree=Tree('2', ['.']), label=2, transitions=[0], index=-1, loss=[0.2], transition_matrix=None)\n",
      "Last single word subtree 1: Example(tokens=['.'], tree=Tree('2', ['.']), label=2, transitions=[0], index=-1, loss=[0.2], transition_matrix=None)\n",
      "Last single word subtree 0: Example(tokens=['Segal'], tree=Tree('2', ['Segal']), label=2, transitions=[0], index=-1, loss=[], transition_matrix=None)\n",
      "Last single word subtree 1: Example(tokens=['Middle-earth'], tree=Tree('2', ['Middle-earth']), label=2, transitions=[0], index=-1, loss=[], transition_matrix=None)\n",
      "Revert update...\n",
      "Last single word subtree 0: Example(tokens=['.'], tree=Tree('2', ['.']), label=2, transitions=[0], index=-1, loss=[], transition_matrix=None)\n",
      "Last single word subtree 1: Example(tokens=['.'], tree=Tree('2', ['.']), label=2, transitions=[0], index=-1, loss=[], transition_matrix=None)\n"
     ]
    }
   ],
   "source": [
    "# Test of previous function\n",
    "print(\"Index 0: \" + str(train_data[0].index))\n",
    "print(\"Index 1: \" + str(train_data[1].index))\n",
    "# print(\"Subtrees: \" + str(subtrees_train_data[train_data[1].index]))\n",
    "print(TreePrettyPrinter(train_data[0].tree))\n",
    "print(TreePrettyPrinter(train_data[1].tree))\n",
    "\n",
    "print(\"Last single word subtree 0: \" + str(subtrees_train_data[train_data[0].index][0][-1]))\n",
    "print(\"Last single word subtree 1: \" + str(subtrees_train_data[train_data[1].index][0][-1]))\n",
    "\n",
    "print(\"Update loss...\")\n",
    "subtrees_train_data[train_data[0].index][0][-1].loss.append(0.2)\n",
    "\n",
    "print(\"Last single word subtree 0: \" + str(subtrees_train_data[train_data[0].index][0][-1]))\n",
    "print(\"Last single word subtree 1: \" + str(subtrees_train_data[train_data[1].index][0][-1]))\n",
    "print(\"Last single word subtree 0: \" + str(subtrees_train_data[train_data[0].index][0][-2]))\n",
    "print(\"Last single word subtree 1: \" + str(subtrees_train_data[train_data[1].index][0][-2]))\n",
    "\n",
    "print(\"Revert update...\")\n",
    "del subtrees_train_data[train_data[1].index][0][-1].loss[0]\n",
    "\n",
    "print(\"Last single word subtree 0: \" + str(subtrees_train_data[train_data[0].index][0][-1]))\n",
    "print(\"Last single word subtree 1: \" + str(subtrees_train_data[train_data[1].index][0][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Depth 0\n",
      "==================================================\n",
      "Number of trees: 18278\n",
      "Distribution over classes:\n",
      " - Label 0: 1.21%\n",
      " - Label 1: 11.33%\n",
      " - Label 2: 74.22%\n",
      " - Label 3: 11.80%\n",
      " - Label 4: 1.44%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 1\n",
      "==================================================\n",
      "Number of trees: 26591\n",
      "Distribution over classes:\n",
      " - Label 0: 1.57%\n",
      " - Label 1: 11.17%\n",
      " - Label 2: 68.10%\n",
      " - Label 3: 16.26%\n",
      " - Label 4: 2.90%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 2\n",
      "==================================================\n",
      "Number of trees: 23109\n",
      "Distribution over classes:\n",
      " - Label 0: 2.46%\n",
      " - Label 1: 13.08%\n",
      " - Label 2: 60.47%\n",
      " - Label 3: 19.67%\n",
      " - Label 4: 4.32%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 3\n",
      "==================================================\n",
      "Number of trees: 17965\n",
      "Distribution over classes:\n",
      " - Label 0: 3.59%\n",
      " - Label 1: 15.61%\n",
      " - Label 2: 53.65%\n",
      " - Label 3: 21.76%\n",
      " - Label 4: 5.39%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 4\n",
      "==================================================\n",
      "Number of trees: 14235\n",
      "Distribution over classes:\n",
      " - Label 0: 4.24%\n",
      " - Label 1: 18.10%\n",
      " - Label 2: 47.92%\n",
      " - Label 3: 23.27%\n",
      " - Label 4: 6.48%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 5\n",
      "==================================================\n",
      "Number of trees: 11633\n",
      "Distribution over classes:\n",
      " - Label 0: 5.05%\n",
      " - Label 1: 20.85%\n",
      " - Label 2: 41.98%\n",
      " - Label 3: 24.94%\n",
      " - Label 4: 7.18%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 6\n",
      "==================================================\n",
      "Number of trees: 9756\n",
      "Distribution over classes:\n",
      " - Label 0: 6.29%\n",
      " - Label 1: 22.32%\n",
      " - Label 2: 38.28%\n",
      " - Label 3: 25.08%\n",
      " - Label 4: 8.02%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 7\n",
      "==================================================\n",
      "Number of trees: 8222\n",
      "Distribution over classes:\n",
      " - Label 0: 7.10%\n",
      " - Label 1: 24.42%\n",
      " - Label 2: 34.03%\n",
      " - Label 3: 25.63%\n",
      " - Label 4: 8.82%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 8\n",
      "==================================================\n",
      "Number of trees: 6887\n",
      "Distribution over classes:\n",
      " - Label 0: 8.29%\n",
      " - Label 1: 25.77%\n",
      " - Label 2: 30.87%\n",
      " - Label 3: 25.79%\n",
      " - Label 4: 9.28%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 9\n",
      "==================================================\n",
      "Number of trees: 5684\n",
      "Distribution over classes:\n",
      " - Label 0: 8.69%\n",
      " - Label 1: 26.14%\n",
      " - Label 2: 29.66%\n",
      " - Label 3: 25.76%\n",
      " - Label 4: 9.75%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 10\n",
      "==================================================\n",
      "Number of trees: 4532\n",
      "Distribution over classes:\n",
      " - Label 0: 9.71%\n",
      " - Label 1: 26.37%\n",
      " - Label 2: 28.24%\n",
      " - Label 3: 25.33%\n",
      " - Label 4: 10.35%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 11\n",
      "==================================================\n",
      "Number of trees: 3551\n",
      "Distribution over classes:\n",
      " - Label 0: 10.28%\n",
      " - Label 1: 26.13%\n",
      " - Label 2: 26.95%\n",
      " - Label 3: 25.91%\n",
      " - Label 4: 10.73%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 12\n",
      "==================================================\n",
      "Number of trees: 2693\n",
      "Distribution over classes:\n",
      " - Label 0: 11.10%\n",
      " - Label 1: 24.92%\n",
      " - Label 2: 26.03%\n",
      " - Label 3: 27.22%\n",
      " - Label 4: 10.73%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 13\n",
      "==================================================\n",
      "Number of trees: 1958\n",
      "Distribution over classes:\n",
      " - Label 0: 11.80%\n",
      " - Label 1: 27.78%\n",
      " - Label 2: 23.65%\n",
      " - Label 3: 25.59%\n",
      " - Label 4: 11.18%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 14\n",
      "==================================================\n",
      "Number of trees: 1412\n",
      "Distribution over classes:\n",
      " - Label 0: 13.03%\n",
      " - Label 1: 25.42%\n",
      " - Label 2: 23.37%\n",
      " - Label 3: 25.28%\n",
      " - Label 4: 12.89%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 15\n",
      "==================================================\n",
      "Number of trees: 979\n",
      "Distribution over classes:\n",
      " - Label 0: 14.81%\n",
      " - Label 1: 26.05%\n",
      " - Label 2: 21.25%\n",
      " - Label 3: 27.17%\n",
      " - Label 4: 10.73%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 16\n",
      "==================================================\n",
      "Number of trees: 642\n",
      "Distribution over classes:\n",
      " - Label 0: 13.71%\n",
      " - Label 1: 26.32%\n",
      " - Label 2: 22.12%\n",
      " - Label 3: 26.79%\n",
      " - Label 4: 11.06%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 17\n",
      "==================================================\n",
      "Number of trees: 431\n",
      "Distribution over classes:\n",
      " - Label 0: 15.55%\n",
      " - Label 1: 25.75%\n",
      " - Label 2: 21.35%\n",
      " - Label 3: 25.06%\n",
      " - Label 4: 12.30%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 18\n",
      "==================================================\n",
      "Number of trees: 275\n",
      "Distribution over classes:\n",
      " - Label 0: 13.09%\n",
      " - Label 1: 25.82%\n",
      " - Label 2: 23.64%\n",
      " - Label 3: 27.64%\n",
      " - Label 4: 9.82%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 19\n",
      "==================================================\n",
      "Number of trees: 177\n",
      "Distribution over classes:\n",
      " - Label 0: 16.95%\n",
      " - Label 1: 20.34%\n",
      " - Label 2: 17.51%\n",
      " - Label 3: 31.64%\n",
      " - Label 4: 13.56%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 20\n",
      "==================================================\n",
      "Number of trees: 109\n",
      "Distribution over classes:\n",
      " - Label 0: 14.68%\n",
      " - Label 1: 21.10%\n",
      " - Label 2: 18.35%\n",
      " - Label 3: 32.11%\n",
      " - Label 4: 13.76%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 21\n",
      "==================================================\n",
      "Number of trees: 60\n",
      "Distribution over classes:\n",
      " - Label 0: 18.33%\n",
      " - Label 1: 16.67%\n",
      " - Label 2: 25.00%\n",
      " - Label 3: 25.00%\n",
      " - Label 4: 15.00%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 22\n",
      "==================================================\n",
      "Number of trees: 42\n",
      "Distribution over classes:\n",
      " - Label 0: 21.43%\n",
      " - Label 1: 21.43%\n",
      " - Label 2: 23.81%\n",
      " - Label 3: 23.81%\n",
      " - Label 4: 9.52%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 23\n",
      "==================================================\n",
      "Number of trees: 24\n",
      "Distribution over classes:\n",
      " - Label 0: 8.33%\n",
      " - Label 1: 37.50%\n",
      " - Label 2: 16.67%\n",
      " - Label 3: 33.33%\n",
      " - Label 4: 4.17%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 24\n",
      "==================================================\n",
      "Number of trees: 16\n",
      "Distribution over classes:\n",
      " - Label 0: 12.50%\n",
      " - Label 1: 18.75%\n",
      " - Label 2: 18.75%\n",
      " - Label 3: 37.50%\n",
      " - Label 4: 12.50%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 25\n",
      "==================================================\n",
      "Number of trees: 7\n",
      "Distribution over classes:\n",
      " - Label 0: 14.29%\n",
      " - Label 1: 28.57%\n",
      " - Label 2: 14.29%\n",
      " - Label 3: 42.86%\n",
      " - Label 4: 0.00%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 26\n",
      "==================================================\n",
      "Number of trees: 5\n",
      "Distribution over classes:\n",
      " - Label 0: 0.00%\n",
      " - Label 1: 20.00%\n",
      " - Label 2: 20.00%\n",
      " - Label 3: 60.00%\n",
      " - Label 4: 0.00%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 27\n",
      "==================================================\n",
      "Number of trees: 2\n",
      "Distribution over classes:\n",
      " - Label 0: 0.00%\n",
      " - Label 1: 0.00%\n",
      " - Label 2: 50.00%\n",
      " - Label 3: 50.00%\n",
      " - Label 4: 0.00%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 28\n",
      "==================================================\n",
      "Number of trees: 1\n",
      "Distribution over classes:\n",
      " - Label 0: 0.00%\n",
      " - Label 1: 0.00%\n",
      " - Label 2: 0.00%\n",
      " - Label 3: 100.00%\n",
      " - Label 4: 0.00%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 29\n",
      "==================================================\n",
      "Number of trees: 1\n",
      "Distribution over classes:\n",
      " - Label 0: 0.00%\n",
      " - Label 1: 0.00%\n",
      " - Label 2: 0.00%\n",
      " - Label 3: 100.00%\n",
      " - Label 4: 0.00%\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Analyse sub-dataset\n",
    "lab_distribution = [[] for _ in range(5)]\n",
    "frequencies = [[] for _ in range(5)]\n",
    "number_examples = []\n",
    "for depth in range(max(all_subtrees.keys()) + 1):\n",
    "    if depth not in all_subtrees:\n",
    "        continue\n",
    "    subtrees = all_subtrees[depth]\n",
    "    number_examples.append(len(subtrees))\n",
    "    print(\"=\"*50)\n",
    "    print(\"Depth \" + str(depth))\n",
    "    print(\"=\"*50)\n",
    "    print(\"Number of trees: \" + str(number_examples[depth]))\n",
    "    print(\"Distribution over classes:\")\n",
    "    for lab in range(5):\n",
    "        subtrees_with_label = [t for t in subtrees if t.label==lab]\n",
    "        lab_distribution[lab].append(len(subtrees_with_label)*1.0/len(subtrees))\n",
    "        frequencies[lab].append(1)\n",
    "        print(\" - Label \" + str(lab) + \": %.2f\" % (100.0 * lab_distribution[lab][-1]) + \"%\")\n",
    "    print(\"=\"*50+\"\\n\")\n",
    "    \n",
    "plt.plot(number_examples)\n",
    "plt.title(\"Number of examples for different subtree depths\")\n",
    "plt.ylabel(\"Number of examples\")\n",
    "plt.xlabel(\"Depth\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "for i in range(5):\n",
    "    plt.plot([lab_distribution[i][j] * number_examples[j] for j in range(len(number_examples))], label=\"Class '\" + i2t[i] + \"'\")\n",
    "plt.title(\"Number of examples for different subtree depths splitted into classes\")\n",
    "plt.ylabel(\"Number of examples\")\n",
    "plt.xlabel(\"Depth\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()\n",
    "for i in range(5):\n",
    "    plt.plot(lab_distribution[i], label=\"Class '\" + i2t[i] + \"'\")\n",
    "plt.title(\"Class distribution for different subtree depths\")\n",
    "plt.ylabel(\"Proportion of class\")\n",
    "plt.xlabel(\"Depth\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dev_data_length = [len(data.tokens) for data in dev_data]\n",
    "train_data_length = [len(data.tokens) for data in train_data]\n",
    "test_data_length = [len(data.tokens) for data in test_data]\n",
    "\n",
    "# dev_data_length = [len(data.transitions) for data in dev_data]\n",
    "# train_data_length = [len(data.transitions) for data in train_data]\n",
    "# test_data_length = [len(data.transitions) for data in test_data]\n",
    "\n",
    "fig, ax = plt.subplots(1,3,figsize=(20,5))\n",
    "ax[0].bar(range(max(dev_data_length)), [sum([d==i for d in dev_data_length]) for i in range(max(dev_data_length))])\n",
    "ax[0].set_title(\"Evaluation dataset\")\n",
    "ax[1].bar(range(max(train_data_length)), [sum([d==i for d in train_data_length]) for i in range(max(train_data_length))])\n",
    "ax[1].set_title(\"Training dataset\")\n",
    "ax[2].bar(range(max(test_data_length)), [sum([d==i for d in test_data_length]) for i in range(max(test_data_length))])\n",
    "_ = ax[2].set_title(\"Test dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip training data 1 due to missing loss information\n",
      "Skip training data 2 due to missing loss information\n",
      "Skip training data 3 due to missing loss information\n",
      "Skip training data 4 due to missing loss information\n",
      "Skip training data 5 due to missing loss information\n",
      "Skip training data 6 due to missing loss information\n",
      "Skip training data 7 due to missing loss information\n",
      "Skip training data 8 due to missing loss information\n",
      "Skip training data 9 due to missing loss information\n",
      "Skip training data 10 due to missing loss information\n",
      "Skip training data 11 due to missing loss information\n",
      "Skip training data 12 due to missing loss information\n",
      "Skip training data 13 due to missing loss information\n",
      "Skip training data 14 due to missing loss information\n",
      "Skip training data 15 due to missing loss information\n",
      "Skip training data 16 due to missing loss information\n",
      "Skip training data 17 due to missing loss information\n",
      "Skip training data 18 due to missing loss information\n",
      "Skip training data 19 due to missing loss information\n",
      "Skip training data 20 due to missing loss information\n",
      "Skip training data 21 due to missing loss information\n",
      "Skip training data 22 due to missing loss information\n",
      "Skip training data 23 due to missing loss information\n",
      "Skip training data 24 due to missing loss information\n",
      "Skip training data 25 due to missing loss information\n",
      "Skip training data 26 due to missing loss information\n",
      "Skip training data 27 due to missing loss information\n",
      "Skip training data 28 due to missing loss information\n",
      "Skip training data 29 due to missing loss information\n",
      "Skip training data 30 due to missing loss information\n",
      "Skip training data 31 due to missing loss information\n",
      "Skip training data 32 due to missing loss information\n",
      "Skip training data 33 due to missing loss information\n",
      "Skip training data 34 due to missing loss information\n",
      "Skip training data 35 due to missing loss information\n",
      "Skip training data 36 due to missing loss information\n",
      "Skip training data 37 due to missing loss information\n",
      "Skip training data 38 due to missing loss information\n",
      "Skip training data 39 due to missing loss information\n",
      "Skip training data 40 due to missing loss information\n",
      "Skip training data 41 due to missing loss information\n",
      "Skip training data 42 due to missing loss information\n",
      "Skip training data 43 due to missing loss information\n",
      "Skip training data 44 due to missing loss information\n",
      "Skip training data 45 due to missing loss information\n",
      "Skip training data 46 due to missing loss information\n",
      "Skip training data 47 due to missing loss information\n",
      "Skip training data 48 due to missing loss information\n",
      "Skip training data 49 due to missing loss information\n",
      "Skip training data 50 due to missing loss information\n",
      "Skip training data 51 due to missing loss information\n",
      "Skip training data 52 due to missing loss information\n",
      "Skip training data 53 due to missing loss information\n",
      "Skip training data 54 due to missing loss information\n",
      "Skip training data 55 due to missing loss information\n",
      "Skip training data 56 due to missing loss information\n",
      "Skip training data 57 due to missing loss information\n",
      "Skip training data 58 due to missing loss information\n",
      "Skip training data 59 due to missing loss information\n",
      "Skip training data 60 due to missing loss information\n",
      "Skip training data 61 due to missing loss information\n",
      "Skip training data 62 due to missing loss information\n",
      "Skip training data 63 due to missing loss information\n",
      "Skip training data 64 due to missing loss information\n",
      "Skip training data 65 due to missing loss information\n",
      "Skip training data 66 due to missing loss information\n",
      "Skip training data 67 due to missing loss information\n",
      "Skip training data 68 due to missing loss information\n",
      "Skip training data 69 due to missing loss information\n",
      "Skip training data 70 due to missing loss information\n",
      "Skip training data 71 due to missing loss information\n",
      "Skip training data 72 due to missing loss information\n",
      "Skip training data 73 due to missing loss information\n",
      "Skip training data 74 due to missing loss information\n",
      "Skip training data 75 due to missing loss information\n",
      "Skip training data 76 due to missing loss information\n",
      "Skip training data 77 due to missing loss information\n",
      "Skip training data 78 due to missing loss information\n",
      "Skip training data 79 due to missing loss information\n",
      "Skip training data 80 due to missing loss information\n",
      "Skip training data 81 due to missing loss information\n",
      "Skip training data 82 due to missing loss information\n",
      "Skip training data 83 due to missing loss information\n",
      "Skip training data 84 due to missing loss information\n",
      "Skip training data 85 due to missing loss information\n",
      "Skip training data 86 due to missing loss information\n",
      "Skip training data 87 due to missing loss information\n",
      "Skip training data 88 due to missing loss information\n",
      "Skip training data 89 due to missing loss information\n",
      "Skip training data 90 due to missing loss information\n",
      "Skip training data 91 due to missing loss information\n",
      "Skip training data 92 due to missing loss information\n",
      "Skip training data 93 due to missing loss information\n",
      "Skip training data 94 due to missing loss information\n",
      "Skip training data 95 due to missing loss information\n",
      "Skip training data 96 due to missing loss information\n",
      "Skip training data 97 due to missing loss information\n",
      "Skip training data 98 due to missing loss information\n",
      "Skip training data 99 due to missing loss information\n",
      "Skip training data 100 due to missing loss information\n",
      "Skip training data 101 due to missing loss information\n",
      "Skip training data 102 due to missing loss information\n",
      "Skip training data 103 due to missing loss information\n",
      "Skip training data 104 due to missing loss information\n",
      "Skip training data 105 due to missing loss information\n",
      "Skip training data 106 due to missing loss information\n",
      "Skip training data 107 due to missing loss information\n",
      "Skip training data 108 due to missing loss information\n",
      "Skip training data 109 due to missing loss information\n",
      "Skip training data 110 due to missing loss information\n",
      "Skip training data 111 due to missing loss information\n",
      "Skip training data 112 due to missing loss information\n",
      "Skip training data 113 due to missing loss information\n",
      "Skip training data 114 due to missing loss information\n",
      "Skip training data 115 due to missing loss information\n",
      "Skip training data 116 due to missing loss information\n",
      "Skip training data 117 due to missing loss information\n",
      "Skip training data 118 due to missing loss information\n",
      "Skip training data 119 due to missing loss information\n",
      "Skip training data 120 due to missing loss information\n",
      "Skip training data 121 due to missing loss information\n",
      "Skip training data 122 due to missing loss information\n",
      "Skip training data 123 due to missing loss information\n",
      "Skip training data 124 due to missing loss information\n",
      "Skip training data 125 due to missing loss information\n",
      "Skip training data 126 due to missing loss information\n",
      "Skip training data 127 due to missing loss information\n",
      "Skip training data 128 due to missing loss information\n",
      "Skip training data 129 due to missing loss information\n",
      "Skip training data 130 due to missing loss information\n",
      "Skip training data 131 due to missing loss information\n",
      "Skip training data 132 due to missing loss information\n",
      "Skip training data 133 due to missing loss information\n",
      "Skip training data 134 due to missing loss information\n",
      "Skip training data 135 due to missing loss information\n",
      "Skip training data 136 due to missing loss information\n",
      "Skip training data 137 due to missing loss information\n",
      "Skip training data 138 due to missing loss information\n",
      "Skip training data 139 due to missing loss information\n",
      "Skip training data 140 due to missing loss information\n",
      "Skip training data 141 due to missing loss information\n",
      "Skip training data 142 due to missing loss information\n",
      "Skip training data 143 due to missing loss information\n",
      "Skip training data 144 due to missing loss information\n",
      "Skip training data 145 due to missing loss information\n",
      "Skip training data 146 due to missing loss information\n",
      "Skip training data 147 due to missing loss information\n",
      "Skip training data 148 due to missing loss information\n",
      "Skip training data 149 due to missing loss information\n",
      "Skip training data 150 due to missing loss information\n",
      "Skip training data 151 due to missing loss information\n",
      "Skip training data 152 due to missing loss information\n",
      "Skip training data 153 due to missing loss information\n",
      "Skip training data 154 due to missing loss information\n",
      "Skip training data 155 due to missing loss information\n",
      "Skip training data 156 due to missing loss information\n",
      "Skip training data 157 due to missing loss information\n",
      "Skip training data 158 due to missing loss information\n",
      "Skip training data 159 due to missing loss information\n",
      "Skip training data 160 due to missing loss information\n",
      "Skip training data 161 due to missing loss information\n",
      "Skip training data 162 due to missing loss information\n",
      "Skip training data 163 due to missing loss information\n",
      "Skip training data 164 due to missing loss information\n",
      "Skip training data 165 due to missing loss information\n",
      "Skip training data 166 due to missing loss information\n",
      "Skip training data 167 due to missing loss information\n",
      "Skip training data 168 due to missing loss information\n",
      "Skip training data 169 due to missing loss information\n",
      "Skip training data 170 due to missing loss information\n",
      "Skip training data 171 due to missing loss information\n",
      "Skip training data 172 due to missing loss information\n",
      "Skip training data 173 due to missing loss information\n",
      "Skip training data 174 due to missing loss information\n",
      "Skip training data 175 due to missing loss information\n",
      "Skip training data 176 due to missing loss information\n",
      "Skip training data 177 due to missing loss information\n",
      "Skip training data 178 due to missing loss information\n",
      "Skip training data 179 due to missing loss information\n",
      "Skip training data 180 due to missing loss information\n",
      "Skip training data 181 due to missing loss information\n",
      "Skip training data 182 due to missing loss information\n",
      "Skip training data 183 due to missing loss information\n",
      "Skip training data 184 due to missing loss information\n",
      "Skip training data 185 due to missing loss information\n",
      "Skip training data 186 due to missing loss information\n",
      "Skip training data 187 due to missing loss information\n",
      "Skip training data 188 due to missing loss information\n",
      "Skip training data 189 due to missing loss information\n",
      "Skip training data 190 due to missing loss information\n",
      "Skip training data 191 due to missing loss information\n",
      "Skip training data 192 due to missing loss information\n",
      "Skip training data 193 due to missing loss information\n",
      "Skip training data 194 due to missing loss information\n",
      "Skip training data 195 due to missing loss information\n",
      "Skip training data 196 due to missing loss information\n",
      "Skip training data 197 due to missing loss information\n",
      "Skip training data 198 due to missing loss information\n",
      "Skip training data 199 due to missing loss information\n",
      "Skip training data 200 due to missing loss information\n",
      "Skip training data 201 due to missing loss information\n",
      "Skip training data 202 due to missing loss information\n",
      "Skip training data 203 due to missing loss information\n",
      "Skip training data 204 due to missing loss information\n",
      "Skip training data 205 due to missing loss information\n",
      "Skip training data 206 due to missing loss information\n",
      "Skip training data 207 due to missing loss information\n",
      "Skip training data 208 due to missing loss information\n",
      "Skip training data 209 due to missing loss information\n",
      "Skip training data 210 due to missing loss information\n",
      "Skip training data 211 due to missing loss information\n",
      "Skip training data 212 due to missing loss information\n",
      "Skip training data 213 due to missing loss information\n",
      "Skip training data 214 due to missing loss information\n",
      "Skip training data 215 due to missing loss information\n",
      "Skip training data 216 due to missing loss information\n",
      "Skip training data 217 due to missing loss information\n",
      "Skip training data 218 due to missing loss information\n",
      "Skip training data 219 due to missing loss information\n",
      "Skip training data 220 due to missing loss information\n",
      "Skip training data 221 due to missing loss information\n",
      "Skip training data 222 due to missing loss information\n",
      "Skip training data 223 due to missing loss information\n",
      "Skip training data 224 due to missing loss information\n",
      "Skip training data 225 due to missing loss information\n",
      "Skip training data 226 due to missing loss information\n",
      "Skip training data 227 due to missing loss information\n",
      "Skip training data 228 due to missing loss information\n",
      "Skip training data 229 due to missing loss information\n",
      "Skip training data 230 due to missing loss information\n",
      "Skip training data 231 due to missing loss information\n",
      "Skip training data 232 due to missing loss information\n",
      "Skip training data 233 due to missing loss information\n",
      "Skip training data 234 due to missing loss information\n",
      "Skip training data 235 due to missing loss information\n",
      "Skip training data 236 due to missing loss information\n",
      "Skip training data 237 due to missing loss information\n",
      "Skip training data 238 due to missing loss information\n",
      "Skip training data 239 due to missing loss information\n",
      "Skip training data 240 due to missing loss information\n",
      "Skip training data 241 due to missing loss information\n",
      "Skip training data 242 due to missing loss information\n",
      "Skip training data 243 due to missing loss information\n",
      "Skip training data 244 due to missing loss information\n",
      "Skip training data 245 due to missing loss information\n",
      "Skip training data 246 due to missing loss information\n",
      "Skip training data 247 due to missing loss information\n",
      "Skip training data 248 due to missing loss information\n",
      "Skip training data 249 due to missing loss information\n",
      "Skip training data 250 due to missing loss information\n",
      "Skip training data 251 due to missing loss information\n",
      "Skip training data 252 due to missing loss information\n",
      "Skip training data 253 due to missing loss information\n",
      "Skip training data 254 due to missing loss information\n",
      "Skip training data 255 due to missing loss information\n",
      "Skip training data 256 due to missing loss information\n",
      "Skip training data 257 due to missing loss information\n",
      "Skip training data 258 due to missing loss information\n",
      "Skip training data 259 due to missing loss information\n",
      "Skip training data 260 due to missing loss information\n",
      "Skip training data 261 due to missing loss information\n",
      "Skip training data 262 due to missing loss information\n",
      "Skip training data 263 due to missing loss information\n",
      "Skip training data 264 due to missing loss information\n",
      "Skip training data 265 due to missing loss information\n",
      "Skip training data 266 due to missing loss information\n",
      "Skip training data 267 due to missing loss information\n",
      "Skip training data 268 due to missing loss information\n",
      "Skip training data 269 due to missing loss information\n",
      "Skip training data 270 due to missing loss information\n",
      "Skip training data 271 due to missing loss information\n",
      "Skip training data 272 due to missing loss information\n",
      "Skip training data 273 due to missing loss information\n",
      "Skip training data 274 due to missing loss information\n",
      "Skip training data 275 due to missing loss information\n",
      "Skip training data 276 due to missing loss information\n",
      "Skip training data 277 due to missing loss information\n",
      "Skip training data 278 due to missing loss information\n",
      "Skip training data 279 due to missing loss information\n",
      "Skip training data 280 due to missing loss information\n",
      "Skip training data 281 due to missing loss information\n",
      "Skip training data 282 due to missing loss information\n",
      "Skip training data 283 due to missing loss information\n",
      "Skip training data 284 due to missing loss information\n",
      "Skip training data 285 due to missing loss information\n",
      "Skip training data 286 due to missing loss information\n",
      "Skip training data 287 due to missing loss information\n",
      "Skip training data 288 due to missing loss information\n",
      "Skip training data 289 due to missing loss information\n",
      "Skip training data 290 due to missing loss information\n",
      "Skip training data 291 due to missing loss information\n",
      "Skip training data 292 due to missing loss information\n",
      "Skip training data 293 due to missing loss information\n",
      "Skip training data 294 due to missing loss information\n",
      "Skip training data 295 due to missing loss information\n",
      "Skip training data 296 due to missing loss information\n",
      "Skip training data 297 due to missing loss information\n",
      "Skip training data 298 due to missing loss information\n",
      "Skip training data 299 due to missing loss information\n",
      "Skip training data 300 due to missing loss information\n",
      "Skip training data 301 due to missing loss information\n",
      "Skip training data 302 due to missing loss information\n",
      "Skip training data 303 due to missing loss information\n",
      "Skip training data 304 due to missing loss information\n",
      "Skip training data 305 due to missing loss information\n",
      "Skip training data 306 due to missing loss information\n",
      "Skip training data 307 due to missing loss information\n",
      "Skip training data 308 due to missing loss information\n",
      "Skip training data 309 due to missing loss information\n",
      "Skip training data 310 due to missing loss information\n",
      "Skip training data 311 due to missing loss information\n",
      "Skip training data 312 due to missing loss information\n",
      "Skip training data 313 due to missing loss information\n",
      "Skip training data 314 due to missing loss information\n",
      "Skip training data 315 due to missing loss information\n",
      "Skip training data 316 due to missing loss information\n",
      "Skip training data 317 due to missing loss information\n",
      "Skip training data 318 due to missing loss information\n",
      "Skip training data 319 due to missing loss information\n",
      "Skip training data 320 due to missing loss information\n",
      "Skip training data 321 due to missing loss information\n",
      "Skip training data 322 due to missing loss information\n",
      "Skip training data 323 due to missing loss information\n",
      "Skip training data 324 due to missing loss information\n",
      "Skip training data 325 due to missing loss information\n",
      "Skip training data 326 due to missing loss information\n",
      "Skip training data 327 due to missing loss information\n",
      "Skip training data 328 due to missing loss information\n",
      "Skip training data 329 due to missing loss information\n",
      "Skip training data 330 due to missing loss information\n",
      "Skip training data 331 due to missing loss information\n",
      "Skip training data 332 due to missing loss information\n",
      "Skip training data 333 due to missing loss information\n",
      "Skip training data 334 due to missing loss information\n",
      "Skip training data 335 due to missing loss information\n",
      "Skip training data 336 due to missing loss information\n",
      "Skip training data 337 due to missing loss information\n",
      "Skip training data 338 due to missing loss information\n",
      "Skip training data 339 due to missing loss information\n",
      "Skip training data 340 due to missing loss information\n",
      "Skip training data 341 due to missing loss information\n",
      "Skip training data 342 due to missing loss information\n",
      "Skip training data 343 due to missing loss information\n",
      "Skip training data 344 due to missing loss information\n",
      "Skip training data 345 due to missing loss information\n",
      "Skip training data 346 due to missing loss information\n",
      "Skip training data 347 due to missing loss information\n",
      "Skip training data 348 due to missing loss information\n",
      "Skip training data 349 due to missing loss information\n",
      "Skip training data 350 due to missing loss information\n",
      "Skip training data 351 due to missing loss information\n",
      "Skip training data 352 due to missing loss information\n",
      "Skip training data 353 due to missing loss information\n",
      "Skip training data 354 due to missing loss information\n",
      "Skip training data 355 due to missing loss information\n",
      "Skip training data 356 due to missing loss information\n",
      "Skip training data 357 due to missing loss information\n",
      "Skip training data 358 due to missing loss information\n",
      "Skip training data 359 due to missing loss information\n",
      "Skip training data 360 due to missing loss information\n",
      "Skip training data 361 due to missing loss information\n",
      "Skip training data 362 due to missing loss information\n",
      "Skip training data 363 due to missing loss information\n",
      "Skip training data 364 due to missing loss information\n",
      "Skip training data 365 due to missing loss information\n",
      "Skip training data 366 due to missing loss information\n",
      "Skip training data 367 due to missing loss information\n",
      "Skip training data 368 due to missing loss information\n",
      "Skip training data 369 due to missing loss information\n",
      "Skip training data 370 due to missing loss information\n",
      "Skip training data 371 due to missing loss information\n",
      "Skip training data 372 due to missing loss information\n",
      "Skip training data 373 due to missing loss information\n",
      "Skip training data 374 due to missing loss information\n",
      "Skip training data 375 due to missing loss information\n",
      "Skip training data 376 due to missing loss information\n",
      "Skip training data 377 due to missing loss information\n",
      "Skip training data 378 due to missing loss information\n",
      "Skip training data 379 due to missing loss information\n",
      "Skip training data 380 due to missing loss information\n",
      "Skip training data 381 due to missing loss information\n",
      "Skip training data 382 due to missing loss information\n",
      "Skip training data 383 due to missing loss information\n",
      "Skip training data 384 due to missing loss information\n",
      "Skip training data 385 due to missing loss information\n",
      "Skip training data 386 due to missing loss information\n",
      "Skip training data 387 due to missing loss information\n",
      "Skip training data 388 due to missing loss information\n",
      "Skip training data 389 due to missing loss information\n",
      "Skip training data 390 due to missing loss information\n",
      "Skip training data 391 due to missing loss information\n",
      "Skip training data 392 due to missing loss information\n",
      "Skip training data 393 due to missing loss information\n",
      "Skip training data 394 due to missing loss information\n",
      "Skip training data 395 due to missing loss information\n",
      "Skip training data 396 due to missing loss information\n",
      "Skip training data 397 due to missing loss information\n",
      "Skip training data 398 due to missing loss information\n",
      "Skip training data 399 due to missing loss information\n",
      "Skip training data 400 due to missing loss information\n",
      "Skip training data 401 due to missing loss information\n",
      "Skip training data 402 due to missing loss information\n",
      "Skip training data 403 due to missing loss information\n",
      "Skip training data 404 due to missing loss information\n",
      "Skip training data 405 due to missing loss information\n",
      "Skip training data 406 due to missing loss information\n",
      "Skip training data 407 due to missing loss information\n",
      "Skip training data 408 due to missing loss information\n",
      "Skip training data 409 due to missing loss information\n",
      "Skip training data 410 due to missing loss information\n",
      "Skip training data 411 due to missing loss information\n",
      "Skip training data 412 due to missing loss information\n",
      "Skip training data 413 due to missing loss information\n",
      "Skip training data 414 due to missing loss information\n",
      "Skip training data 415 due to missing loss information\n",
      "Skip training data 416 due to missing loss information\n",
      "Skip training data 417 due to missing loss information\n",
      "Skip training data 418 due to missing loss information\n",
      "Skip training data 419 due to missing loss information\n",
      "Skip training data 420 due to missing loss information\n",
      "Skip training data 421 due to missing loss information\n",
      "Skip training data 422 due to missing loss information\n",
      "Skip training data 423 due to missing loss information\n",
      "Skip training data 424 due to missing loss information\n",
      "Skip training data 425 due to missing loss information\n",
      "Skip training data 426 due to missing loss information\n",
      "Skip training data 427 due to missing loss information\n",
      "Skip training data 428 due to missing loss information\n",
      "Skip training data 429 due to missing loss information\n",
      "Skip training data 430 due to missing loss information\n",
      "Skip training data 431 due to missing loss information\n",
      "Skip training data 432 due to missing loss information\n",
      "Skip training data 433 due to missing loss information\n",
      "Skip training data 434 due to missing loss information\n",
      "Skip training data 435 due to missing loss information\n",
      "Skip training data 436 due to missing loss information\n",
      "Skip training data 437 due to missing loss information\n",
      "Skip training data 438 due to missing loss information\n",
      "Skip training data 439 due to missing loss information\n",
      "Skip training data 440 due to missing loss information\n",
      "Skip training data 441 due to missing loss information\n",
      "Skip training data 442 due to missing loss information\n",
      "Skip training data 443 due to missing loss information\n",
      "Skip training data 444 due to missing loss information\n",
      "Skip training data 445 due to missing loss information\n",
      "Skip training data 446 due to missing loss information\n",
      "Skip training data 447 due to missing loss information\n",
      "Skip training data 448 due to missing loss information\n",
      "Skip training data 449 due to missing loss information\n",
      "Skip training data 450 due to missing loss information\n",
      "Skip training data 451 due to missing loss information\n",
      "Skip training data 452 due to missing loss information\n",
      "Skip training data 453 due to missing loss information\n",
      "Skip training data 454 due to missing loss information\n",
      "Skip training data 455 due to missing loss information\n",
      "Skip training data 456 due to missing loss information\n",
      "Skip training data 457 due to missing loss information\n",
      "Skip training data 458 due to missing loss information\n",
      "Skip training data 459 due to missing loss information\n",
      "Skip training data 460 due to missing loss information\n",
      "Skip training data 461 due to missing loss information\n",
      "Skip training data 462 due to missing loss information\n",
      "Skip training data 463 due to missing loss information\n",
      "Skip training data 464 due to missing loss information\n",
      "Skip training data 465 due to missing loss information\n",
      "Skip training data 466 due to missing loss information\n",
      "Skip training data 467 due to missing loss information\n",
      "Skip training data 468 due to missing loss information\n",
      "Skip training data 469 due to missing loss information\n",
      "Skip training data 470 due to missing loss information\n",
      "Skip training data 471 due to missing loss information\n",
      "Skip training data 472 due to missing loss information\n",
      "Skip training data 473 due to missing loss information\n",
      "Skip training data 474 due to missing loss information\n",
      "Skip training data 475 due to missing loss information\n",
      "Skip training data 476 due to missing loss information\n",
      "Skip training data 477 due to missing loss information\n",
      "Skip training data 478 due to missing loss information\n",
      "Skip training data 479 due to missing loss information\n",
      "Skip training data 480 due to missing loss information\n",
      "Skip training data 481 due to missing loss information\n",
      "Skip training data 482 due to missing loss information\n",
      "Skip training data 483 due to missing loss information\n",
      "Skip training data 484 due to missing loss information\n",
      "Skip training data 485 due to missing loss information\n",
      "Skip training data 486 due to missing loss information\n",
      "Skip training data 487 due to missing loss information\n",
      "Skip training data 488 due to missing loss information\n",
      "Skip training data 489 due to missing loss information\n",
      "Skip training data 490 due to missing loss information\n",
      "Skip training data 491 due to missing loss information\n",
      "Skip training data 492 due to missing loss information\n",
      "Skip training data 493 due to missing loss information\n",
      "Skip training data 494 due to missing loss information\n",
      "Skip training data 495 due to missing loss information\n",
      "Skip training data 496 due to missing loss information\n",
      "Skip training data 497 due to missing loss information\n",
      "Skip training data 498 due to missing loss information\n",
      "Skip training data 499 due to missing loss information\n",
      "Skip training data 500 due to missing loss information\n",
      "Skip training data 501 due to missing loss information\n",
      "Skip training data 502 due to missing loss information\n",
      "Skip training data 503 due to missing loss information\n",
      "Skip training data 504 due to missing loss information\n",
      "Skip training data 505 due to missing loss information\n",
      "Skip training data 506 due to missing loss information\n",
      "Skip training data 507 due to missing loss information\n",
      "Skip training data 508 due to missing loss information\n",
      "Skip training data 509 due to missing loss information\n",
      "Skip training data 510 due to missing loss information\n",
      "Skip training data 511 due to missing loss information\n",
      "Skip training data 512 due to missing loss information\n",
      "Skip training data 513 due to missing loss information\n",
      "Skip training data 514 due to missing loss information\n",
      "Skip training data 515 due to missing loss information\n",
      "Skip training data 516 due to missing loss information\n",
      "Skip training data 517 due to missing loss information\n",
      "Skip training data 518 due to missing loss information\n",
      "Skip training data 519 due to missing loss information\n",
      "Skip training data 520 due to missing loss information\n",
      "Skip training data 521 due to missing loss information\n",
      "Skip training data 522 due to missing loss information\n",
      "Skip training data 523 due to missing loss information\n",
      "Skip training data 524 due to missing loss information\n",
      "Skip training data 525 due to missing loss information\n",
      "Skip training data 526 due to missing loss information\n",
      "Skip training data 527 due to missing loss information\n",
      "Skip training data 528 due to missing loss information\n",
      "Skip training data 529 due to missing loss information\n",
      "Skip training data 530 due to missing loss information\n",
      "Skip training data 531 due to missing loss information\n",
      "Skip training data 532 due to missing loss information\n",
      "Skip training data 533 due to missing loss information\n",
      "Skip training data 534 due to missing loss information\n",
      "Skip training data 535 due to missing loss information\n",
      "Skip training data 536 due to missing loss information\n",
      "Skip training data 537 due to missing loss information\n",
      "Skip training data 538 due to missing loss information\n",
      "Skip training data 539 due to missing loss information\n",
      "Skip training data 540 due to missing loss information\n",
      "Skip training data 541 due to missing loss information\n",
      "Skip training data 542 due to missing loss information\n",
      "Skip training data 543 due to missing loss information\n",
      "Skip training data 544 due to missing loss information\n",
      "Skip training data 545 due to missing loss information\n",
      "Skip training data 546 due to missing loss information\n",
      "Skip training data 547 due to missing loss information\n",
      "Skip training data 548 due to missing loss information\n",
      "Skip training data 549 due to missing loss information\n",
      "Skip training data 550 due to missing loss information\n",
      "Skip training data 551 due to missing loss information\n",
      "Skip training data 552 due to missing loss information\n",
      "Skip training data 553 due to missing loss information\n",
      "Skip training data 554 due to missing loss information\n",
      "Skip training data 555 due to missing loss information\n",
      "Skip training data 556 due to missing loss information\n",
      "Skip training data 557 due to missing loss information\n",
      "Skip training data 558 due to missing loss information\n",
      "Skip training data 559 due to missing loss information\n",
      "Skip training data 560 due to missing loss information\n",
      "Skip training data 561 due to missing loss information\n",
      "Skip training data 562 due to missing loss information\n",
      "Skip training data 563 due to missing loss information\n",
      "Skip training data 564 due to missing loss information\n",
      "Skip training data 565 due to missing loss information\n",
      "Skip training data 566 due to missing loss information\n",
      "Skip training data 567 due to missing loss information\n",
      "Skip training data 568 due to missing loss information\n",
      "Skip training data 569 due to missing loss information\n",
      "Skip training data 570 due to missing loss information\n",
      "Skip training data 571 due to missing loss information\n",
      "Skip training data 572 due to missing loss information\n",
      "Skip training data 573 due to missing loss information\n",
      "Skip training data 574 due to missing loss information\n",
      "Skip training data 575 due to missing loss information\n",
      "Skip training data 576 due to missing loss information\n",
      "Skip training data 577 due to missing loss information\n",
      "Skip training data 578 due to missing loss information\n",
      "Skip training data 579 due to missing loss information\n",
      "Skip training data 580 due to missing loss information\n",
      "Skip training data 581 due to missing loss information\n",
      "Skip training data 582 due to missing loss information\n",
      "Skip training data 583 due to missing loss information\n",
      "Skip training data 584 due to missing loss information\n",
      "Skip training data 585 due to missing loss information\n",
      "Skip training data 586 due to missing loss information\n",
      "Skip training data 587 due to missing loss information\n",
      "Skip training data 588 due to missing loss information\n",
      "Skip training data 589 due to missing loss information\n",
      "Skip training data 590 due to missing loss information\n",
      "Skip training data 591 due to missing loss information\n",
      "Skip training data 592 due to missing loss information\n",
      "Skip training data 593 due to missing loss information\n",
      "Skip training data 594 due to missing loss information\n",
      "Skip training data 595 due to missing loss information\n",
      "Skip training data 596 due to missing loss information\n",
      "Skip training data 597 due to missing loss information\n",
      "Skip training data 598 due to missing loss information\n",
      "Skip training data 599 due to missing loss information\n",
      "Skip training data 600 due to missing loss information\n",
      "Skip training data 601 due to missing loss information\n",
      "Skip training data 602 due to missing loss information\n",
      "Skip training data 603 due to missing loss information\n",
      "Skip training data 604 due to missing loss information\n",
      "Skip training data 605 due to missing loss information\n",
      "Skip training data 606 due to missing loss information\n",
      "Skip training data 607 due to missing loss information\n",
      "Skip training data 608 due to missing loss information\n",
      "Skip training data 609 due to missing loss information\n",
      "Skip training data 610 due to missing loss information\n",
      "Skip training data 611 due to missing loss information\n",
      "Skip training data 612 due to missing loss information\n",
      "Skip training data 613 due to missing loss information\n",
      "Skip training data 614 due to missing loss information\n",
      "Skip training data 615 due to missing loss information\n",
      "Skip training data 616 due to missing loss information\n",
      "Skip training data 617 due to missing loss information\n",
      "Skip training data 618 due to missing loss information\n",
      "Skip training data 619 due to missing loss information\n",
      "Skip training data 620 due to missing loss information\n",
      "Skip training data 621 due to missing loss information\n",
      "Skip training data 622 due to missing loss information\n",
      "Skip training data 623 due to missing loss information\n",
      "Skip training data 624 due to missing loss information\n",
      "Skip training data 625 due to missing loss information\n",
      "Skip training data 626 due to missing loss information\n",
      "Skip training data 627 due to missing loss information\n",
      "Skip training data 628 due to missing loss information\n",
      "Skip training data 629 due to missing loss information\n",
      "Skip training data 630 due to missing loss information\n",
      "Skip training data 631 due to missing loss information\n",
      "Skip training data 632 due to missing loss information\n",
      "Skip training data 633 due to missing loss information\n",
      "Skip training data 634 due to missing loss information\n",
      "Skip training data 635 due to missing loss information\n",
      "Skip training data 636 due to missing loss information\n",
      "Skip training data 637 due to missing loss information\n",
      "Skip training data 638 due to missing loss information\n",
      "Skip training data 639 due to missing loss information\n",
      "Skip training data 640 due to missing loss information\n",
      "Skip training data 641 due to missing loss information\n",
      "Skip training data 642 due to missing loss information\n",
      "Skip training data 643 due to missing loss information\n",
      "Skip training data 644 due to missing loss information\n",
      "Skip training data 645 due to missing loss information\n",
      "Skip training data 646 due to missing loss information\n",
      "Skip training data 647 due to missing loss information\n",
      "Skip training data 648 due to missing loss information\n",
      "Skip training data 649 due to missing loss information\n",
      "Skip training data 650 due to missing loss information\n",
      "Skip training data 651 due to missing loss information\n",
      "Skip training data 652 due to missing loss information\n",
      "Skip training data 653 due to missing loss information\n",
      "Skip training data 654 due to missing loss information\n",
      "Skip training data 655 due to missing loss information\n",
      "Skip training data 656 due to missing loss information\n",
      "Skip training data 657 due to missing loss information\n",
      "Skip training data 658 due to missing loss information\n",
      "Skip training data 659 due to missing loss information\n",
      "Skip training data 660 due to missing loss information\n",
      "Skip training data 661 due to missing loss information\n",
      "Skip training data 662 due to missing loss information\n",
      "Skip training data 663 due to missing loss information\n",
      "Skip training data 664 due to missing loss information\n",
      "Skip training data 665 due to missing loss information\n",
      "Skip training data 666 due to missing loss information\n",
      "Skip training data 667 due to missing loss information\n",
      "Skip training data 668 due to missing loss information\n",
      "Skip training data 669 due to missing loss information\n",
      "Skip training data 670 due to missing loss information\n",
      "Skip training data 671 due to missing loss information\n",
      "Skip training data 672 due to missing loss information\n",
      "Skip training data 673 due to missing loss information\n",
      "Skip training data 674 due to missing loss information\n",
      "Skip training data 675 due to missing loss information\n",
      "Skip training data 676 due to missing loss information\n",
      "Skip training data 677 due to missing loss information\n",
      "Skip training data 678 due to missing loss information\n",
      "Skip training data 679 due to missing loss information\n",
      "Skip training data 680 due to missing loss information\n",
      "Skip training data 681 due to missing loss information\n",
      "Skip training data 682 due to missing loss information\n",
      "Skip training data 683 due to missing loss information\n",
      "Skip training data 684 due to missing loss information\n",
      "Skip training data 685 due to missing loss information\n",
      "Skip training data 686 due to missing loss information\n",
      "Skip training data 687 due to missing loss information\n",
      "Skip training data 688 due to missing loss information\n",
      "Skip training data 689 due to missing loss information\n",
      "Skip training data 690 due to missing loss information\n",
      "Skip training data 691 due to missing loss information\n",
      "Skip training data 692 due to missing loss information\n",
      "Skip training data 693 due to missing loss information\n",
      "Skip training data 694 due to missing loss information\n",
      "Skip training data 695 due to missing loss information\n",
      "Skip training data 696 due to missing loss information\n",
      "Skip training data 697 due to missing loss information\n",
      "Skip training data 698 due to missing loss information\n",
      "Skip training data 699 due to missing loss information\n",
      "Skip training data 700 due to missing loss information\n",
      "Skip training data 701 due to missing loss information\n",
      "Skip training data 702 due to missing loss information\n",
      "Skip training data 703 due to missing loss information\n",
      "Skip training data 704 due to missing loss information\n",
      "Skip training data 705 due to missing loss information\n",
      "Skip training data 706 due to missing loss information\n",
      "Skip training data 707 due to missing loss information\n",
      "Skip training data 708 due to missing loss information\n",
      "Skip training data 709 due to missing loss information\n",
      "Skip training data 710 due to missing loss information\n",
      "Skip training data 711 due to missing loss information\n",
      "Skip training data 712 due to missing loss information\n",
      "Skip training data 713 due to missing loss information\n",
      "Skip training data 714 due to missing loss information\n",
      "Skip training data 715 due to missing loss information\n",
      "Skip training data 716 due to missing loss information\n",
      "Skip training data 717 due to missing loss information\n",
      "Skip training data 718 due to missing loss information\n",
      "Skip training data 719 due to missing loss information\n",
      "Skip training data 720 due to missing loss information\n",
      "Skip training data 721 due to missing loss information\n",
      "Skip training data 722 due to missing loss information\n",
      "Skip training data 723 due to missing loss information\n",
      "Skip training data 724 due to missing loss information\n",
      "Skip training data 725 due to missing loss information\n",
      "Skip training data 726 due to missing loss information\n",
      "Skip training data 727 due to missing loss information\n",
      "Skip training data 728 due to missing loss information\n",
      "Skip training data 729 due to missing loss information\n",
      "Skip training data 730 due to missing loss information\n",
      "Skip training data 731 due to missing loss information\n",
      "Skip training data 732 due to missing loss information\n",
      "Skip training data 733 due to missing loss information\n",
      "Skip training data 734 due to missing loss information\n",
      "Skip training data 735 due to missing loss information\n",
      "Skip training data 736 due to missing loss information\n",
      "Skip training data 737 due to missing loss information\n",
      "Skip training data 738 due to missing loss information\n",
      "Skip training data 739 due to missing loss information\n",
      "Skip training data 740 due to missing loss information\n",
      "Skip training data 741 due to missing loss information\n",
      "Skip training data 742 due to missing loss information\n",
      "Skip training data 743 due to missing loss information\n",
      "Skip training data 744 due to missing loss information\n",
      "Skip training data 745 due to missing loss information\n",
      "Skip training data 746 due to missing loss information\n",
      "Skip training data 747 due to missing loss information\n",
      "Skip training data 748 due to missing loss information\n",
      "Skip training data 749 due to missing loss information\n",
      "Skip training data 750 due to missing loss information\n",
      "Skip training data 751 due to missing loss information\n",
      "Skip training data 752 due to missing loss information\n",
      "Skip training data 753 due to missing loss information\n",
      "Skip training data 754 due to missing loss information\n",
      "Skip training data 755 due to missing loss information\n",
      "Skip training data 756 due to missing loss information\n",
      "Skip training data 757 due to missing loss information\n",
      "Skip training data 758 due to missing loss information\n",
      "Skip training data 759 due to missing loss information\n",
      "Skip training data 760 due to missing loss information\n",
      "Skip training data 761 due to missing loss information\n",
      "Skip training data 762 due to missing loss information\n",
      "Skip training data 763 due to missing loss information\n",
      "Skip training data 764 due to missing loss information\n",
      "Skip training data 765 due to missing loss information\n",
      "Skip training data 766 due to missing loss information\n",
      "Skip training data 767 due to missing loss information\n",
      "Skip training data 768 due to missing loss information\n",
      "Skip training data 769 due to missing loss information\n",
      "Skip training data 770 due to missing loss information\n",
      "Skip training data 771 due to missing loss information\n",
      "Skip training data 772 due to missing loss information\n",
      "Skip training data 773 due to missing loss information\n",
      "Skip training data 774 due to missing loss information\n",
      "Skip training data 775 due to missing loss information\n",
      "Skip training data 776 due to missing loss information\n",
      "Skip training data 777 due to missing loss information\n",
      "Skip training data 778 due to missing loss information\n",
      "Skip training data 779 due to missing loss information\n",
      "Skip training data 780 due to missing loss information\n",
      "Skip training data 781 due to missing loss information\n",
      "Skip training data 782 due to missing loss information\n",
      "Skip training data 783 due to missing loss information\n",
      "Skip training data 784 due to missing loss information\n",
      "Skip training data 785 due to missing loss information\n",
      "Skip training data 786 due to missing loss information\n",
      "Skip training data 787 due to missing loss information\n",
      "Skip training data 788 due to missing loss information\n",
      "Skip training data 789 due to missing loss information\n",
      "Skip training data 790 due to missing loss information\n",
      "Skip training data 791 due to missing loss information\n",
      "Skip training data 792 due to missing loss information\n",
      "Skip training data 793 due to missing loss information\n",
      "Skip training data 794 due to missing loss information\n",
      "Skip training data 795 due to missing loss information\n",
      "Skip training data 796 due to missing loss information\n",
      "Skip training data 797 due to missing loss information\n",
      "Skip training data 798 due to missing loss information\n",
      "Skip training data 799 due to missing loss information\n",
      "Skip training data 800 due to missing loss information\n",
      "Skip training data 801 due to missing loss information\n",
      "Skip training data 802 due to missing loss information\n",
      "Skip training data 803 due to missing loss information\n",
      "Skip training data 804 due to missing loss information\n",
      "Skip training data 805 due to missing loss information\n",
      "Skip training data 806 due to missing loss information\n",
      "Skip training data 807 due to missing loss information\n",
      "Skip training data 808 due to missing loss information\n",
      "Skip training data 809 due to missing loss information\n",
      "Skip training data 810 due to missing loss information\n",
      "Skip training data 811 due to missing loss information\n",
      "Skip training data 812 due to missing loss information\n",
      "Skip training data 813 due to missing loss information\n",
      "Skip training data 814 due to missing loss information\n",
      "Skip training data 815 due to missing loss information\n",
      "Skip training data 816 due to missing loss information\n",
      "Skip training data 817 due to missing loss information\n",
      "Skip training data 818 due to missing loss information\n",
      "Skip training data 819 due to missing loss information\n",
      "Skip training data 820 due to missing loss information\n",
      "Skip training data 821 due to missing loss information\n",
      "Skip training data 822 due to missing loss information\n",
      "Skip training data 823 due to missing loss information\n",
      "Skip training data 824 due to missing loss information\n",
      "Skip training data 825 due to missing loss information\n",
      "Skip training data 826 due to missing loss information\n",
      "Skip training data 827 due to missing loss information\n",
      "Skip training data 828 due to missing loss information\n",
      "Skip training data 829 due to missing loss information\n",
      "Skip training data 830 due to missing loss information\n",
      "Skip training data 831 due to missing loss information\n",
      "Skip training data 832 due to missing loss information\n",
      "Skip training data 833 due to missing loss information\n",
      "Skip training data 834 due to missing loss information\n",
      "Skip training data 835 due to missing loss information\n",
      "Skip training data 836 due to missing loss information\n",
      "Skip training data 837 due to missing loss information\n",
      "Skip training data 838 due to missing loss information\n",
      "Skip training data 839 due to missing loss information\n",
      "Skip training data 840 due to missing loss information\n",
      "Skip training data 841 due to missing loss information\n",
      "Skip training data 842 due to missing loss information\n",
      "Skip training data 843 due to missing loss information\n",
      "Skip training data 844 due to missing loss information\n",
      "Skip training data 845 due to missing loss information\n",
      "Skip training data 846 due to missing loss information\n",
      "Skip training data 847 due to missing loss information\n",
      "Skip training data 848 due to missing loss information\n",
      "Skip training data 849 due to missing loss information\n",
      "Skip training data 850 due to missing loss information\n",
      "Skip training data 851 due to missing loss information\n",
      "Skip training data 852 due to missing loss information\n",
      "Skip training data 853 due to missing loss information\n",
      "Skip training data 854 due to missing loss information\n",
      "Skip training data 855 due to missing loss information\n",
      "Skip training data 856 due to missing loss information\n",
      "Skip training data 857 due to missing loss information\n",
      "Skip training data 858 due to missing loss information\n",
      "Skip training data 859 due to missing loss information\n",
      "Skip training data 860 due to missing loss information\n",
      "Skip training data 861 due to missing loss information\n",
      "Skip training data 862 due to missing loss information\n",
      "Skip training data 863 due to missing loss information\n",
      "Skip training data 864 due to missing loss information\n",
      "Skip training data 865 due to missing loss information\n",
      "Skip training data 866 due to missing loss information\n",
      "Skip training data 867 due to missing loss information\n",
      "Skip training data 868 due to missing loss information\n",
      "Skip training data 869 due to missing loss information\n",
      "Skip training data 870 due to missing loss information\n",
      "Skip training data 871 due to missing loss information\n",
      "Skip training data 872 due to missing loss information\n",
      "Skip training data 873 due to missing loss information\n",
      "Skip training data 874 due to missing loss information\n",
      "Skip training data 875 due to missing loss information\n",
      "Skip training data 876 due to missing loss information\n",
      "Skip training data 877 due to missing loss information\n",
      "Skip training data 878 due to missing loss information\n",
      "Skip training data 879 due to missing loss information\n",
      "Skip training data 880 due to missing loss information\n",
      "Skip training data 881 due to missing loss information\n",
      "Skip training data 882 due to missing loss information\n",
      "Skip training data 883 due to missing loss information\n",
      "Skip training data 884 due to missing loss information\n",
      "Skip training data 885 due to missing loss information\n",
      "Skip training data 886 due to missing loss information\n",
      "Skip training data 887 due to missing loss information\n",
      "Skip training data 888 due to missing loss information\n",
      "Skip training data 889 due to missing loss information\n",
      "Skip training data 890 due to missing loss information\n",
      "Skip training data 891 due to missing loss information\n",
      "Skip training data 892 due to missing loss information\n",
      "Skip training data 893 due to missing loss information\n",
      "Skip training data 894 due to missing loss information\n",
      "Skip training data 895 due to missing loss information\n",
      "Skip training data 896 due to missing loss information\n",
      "Skip training data 897 due to missing loss information\n",
      "Skip training data 898 due to missing loss information\n",
      "Skip training data 899 due to missing loss information\n",
      "Skip training data 900 due to missing loss information\n",
      "Skip training data 901 due to missing loss information\n",
      "Skip training data 902 due to missing loss information\n",
      "Skip training data 903 due to missing loss information\n",
      "Skip training data 904 due to missing loss information\n",
      "Skip training data 905 due to missing loss information\n",
      "Skip training data 906 due to missing loss information\n",
      "Skip training data 907 due to missing loss information\n",
      "Skip training data 908 due to missing loss information\n",
      "Skip training data 909 due to missing loss information\n",
      "Skip training data 910 due to missing loss information\n",
      "Skip training data 911 due to missing loss information\n",
      "Skip training data 912 due to missing loss information\n",
      "Skip training data 913 due to missing loss information\n",
      "Skip training data 914 due to missing loss information\n",
      "Skip training data 915 due to missing loss information\n",
      "Skip training data 916 due to missing loss information\n",
      "Skip training data 917 due to missing loss information\n",
      "Skip training data 918 due to missing loss information\n",
      "Skip training data 919 due to missing loss information\n",
      "Skip training data 920 due to missing loss information\n",
      "Skip training data 921 due to missing loss information\n",
      "Skip training data 922 due to missing loss information\n",
      "Skip training data 923 due to missing loss information\n",
      "Skip training data 924 due to missing loss information\n",
      "Skip training data 925 due to missing loss information\n",
      "Skip training data 926 due to missing loss information\n",
      "Skip training data 927 due to missing loss information\n",
      "Skip training data 928 due to missing loss information\n",
      "Skip training data 929 due to missing loss information\n",
      "Skip training data 930 due to missing loss information\n",
      "Skip training data 931 due to missing loss information\n",
      "Skip training data 932 due to missing loss information\n",
      "Skip training data 933 due to missing loss information\n",
      "Skip training data 934 due to missing loss information\n",
      "Skip training data 935 due to missing loss information\n",
      "Skip training data 936 due to missing loss information\n",
      "Skip training data 937 due to missing loss information\n",
      "Skip training data 938 due to missing loss information\n",
      "Skip training data 939 due to missing loss information\n",
      "Skip training data 940 due to missing loss information\n",
      "Skip training data 941 due to missing loss information\n",
      "Skip training data 942 due to missing loss information\n",
      "Skip training data 943 due to missing loss information\n",
      "Skip training data 944 due to missing loss information\n",
      "Skip training data 945 due to missing loss information\n",
      "Skip training data 946 due to missing loss information\n",
      "Skip training data 947 due to missing loss information\n",
      "Skip training data 948 due to missing loss information\n",
      "Skip training data 949 due to missing loss information\n",
      "Skip training data 950 due to missing loss information\n",
      "Skip training data 951 due to missing loss information\n",
      "Skip training data 952 due to missing loss information\n",
      "Skip training data 953 due to missing loss information\n",
      "Skip training data 954 due to missing loss information\n",
      "Skip training data 955 due to missing loss information\n",
      "Skip training data 956 due to missing loss information\n",
      "Skip training data 957 due to missing loss information\n",
      "Skip training data 958 due to missing loss information\n",
      "Skip training data 959 due to missing loss information\n",
      "Skip training data 960 due to missing loss information\n",
      "Skip training data 961 due to missing loss information\n",
      "Skip training data 962 due to missing loss information\n",
      "Skip training data 963 due to missing loss information\n",
      "Skip training data 964 due to missing loss information\n",
      "Skip training data 965 due to missing loss information\n",
      "Skip training data 966 due to missing loss information\n",
      "Skip training data 967 due to missing loss information\n",
      "Skip training data 968 due to missing loss information\n",
      "Skip training data 969 due to missing loss information\n",
      "Skip training data 970 due to missing loss information\n",
      "Skip training data 971 due to missing loss information\n",
      "Skip training data 972 due to missing loss information\n",
      "Skip training data 973 due to missing loss information\n",
      "Skip training data 974 due to missing loss information\n",
      "Skip training data 975 due to missing loss information\n",
      "Skip training data 976 due to missing loss information\n",
      "Skip training data 977 due to missing loss information\n",
      "Skip training data 978 due to missing loss information\n",
      "Skip training data 979 due to missing loss information\n",
      "Skip training data 980 due to missing loss information\n",
      "Skip training data 981 due to missing loss information\n",
      "Skip training data 982 due to missing loss information\n",
      "Skip training data 983 due to missing loss information\n",
      "Skip training data 984 due to missing loss information\n",
      "Skip training data 985 due to missing loss information\n",
      "Skip training data 986 due to missing loss information\n",
      "Skip training data 987 due to missing loss information\n",
      "Skip training data 988 due to missing loss information\n",
      "Skip training data 989 due to missing loss information\n",
      "Skip training data 990 due to missing loss information\n",
      "Skip training data 991 due to missing loss information\n",
      "Skip training data 992 due to missing loss information\n",
      "Skip training data 993 due to missing loss information\n",
      "Skip training data 994 due to missing loss information\n",
      "Skip training data 995 due to missing loss information\n",
      "Skip training data 996 due to missing loss information\n",
      "Skip training data 997 due to missing loss information\n",
      "Skip training data 998 due to missing loss information\n",
      "Skip training data 999 due to missing loss information\n",
      "Skip training data 1000 due to missing loss information\n",
      "Skip training data 1001 due to missing loss information\n",
      "Skip training data 1002 due to missing loss information\n",
      "Skip training data 1003 due to missing loss information\n",
      "Skip training data 1004 due to missing loss information\n",
      "Skip training data 1005 due to missing loss information\n",
      "Skip training data 1006 due to missing loss information\n",
      "Skip training data 1007 due to missing loss information\n",
      "Skip training data 1008 due to missing loss information\n",
      "Skip training data 1009 due to missing loss information\n",
      "Skip training data 1010 due to missing loss information\n",
      "Skip training data 1011 due to missing loss information\n",
      "Skip training data 1012 due to missing loss information\n",
      "Skip training data 1013 due to missing loss information\n",
      "Skip training data 1014 due to missing loss information\n",
      "Skip training data 1015 due to missing loss information\n",
      "Skip training data 1016 due to missing loss information\n",
      "Skip training data 1017 due to missing loss information\n",
      "Skip training data 1018 due to missing loss information\n",
      "Skip training data 1019 due to missing loss information\n",
      "Skip training data 1020 due to missing loss information\n",
      "Skip training data 1021 due to missing loss information\n",
      "Skip training data 1022 due to missing loss information\n",
      "Skip training data 1023 due to missing loss information\n",
      "Skip training data 1024 due to missing loss information\n",
      "Skip training data 1025 due to missing loss information\n",
      "Skip training data 1026 due to missing loss information\n",
      "Skip training data 1027 due to missing loss information\n",
      "Skip training data 1028 due to missing loss information\n",
      "Skip training data 1029 due to missing loss information\n",
      "Skip training data 1030 due to missing loss information\n",
      "Skip training data 1031 due to missing loss information\n",
      "Skip training data 1032 due to missing loss information\n",
      "Skip training data 1033 due to missing loss information\n",
      "Skip training data 1034 due to missing loss information\n",
      "Skip training data 1035 due to missing loss information\n",
      "Skip training data 1036 due to missing loss information\n",
      "Skip training data 1037 due to missing loss information\n",
      "Skip training data 1038 due to missing loss information\n",
      "Skip training data 1039 due to missing loss information\n",
      "Skip training data 1040 due to missing loss information\n",
      "Skip training data 1041 due to missing loss information\n",
      "Skip training data 1042 due to missing loss information\n",
      "Skip training data 1043 due to missing loss information\n",
      "Skip training data 1044 due to missing loss information\n",
      "Skip training data 1045 due to missing loss information\n",
      "Skip training data 1046 due to missing loss information\n",
      "Skip training data 1047 due to missing loss information\n",
      "Skip training data 1048 due to missing loss information\n",
      "Skip training data 1049 due to missing loss information\n",
      "Skip training data 1050 due to missing loss information\n",
      "Skip training data 1051 due to missing loss information\n",
      "Skip training data 1052 due to missing loss information\n",
      "Skip training data 1053 due to missing loss information\n",
      "Skip training data 1054 due to missing loss information\n",
      "Skip training data 1055 due to missing loss information\n",
      "Skip training data 1056 due to missing loss information\n",
      "Skip training data 1057 due to missing loss information\n",
      "Skip training data 1058 due to missing loss information\n",
      "Skip training data 1059 due to missing loss information\n",
      "Skip training data 1060 due to missing loss information\n",
      "Skip training data 1061 due to missing loss information\n",
      "Skip training data 1062 due to missing loss information\n",
      "Skip training data 1063 due to missing loss information\n",
      "Skip training data 1064 due to missing loss information\n",
      "Skip training data 1065 due to missing loss information\n",
      "Skip training data 1066 due to missing loss information\n",
      "Skip training data 1067 due to missing loss information\n",
      "Skip training data 1068 due to missing loss information\n",
      "Skip training data 1069 due to missing loss information\n",
      "Skip training data 1070 due to missing loss information\n",
      "Skip training data 1071 due to missing loss information\n",
      "Skip training data 1072 due to missing loss information\n",
      "Skip training data 1073 due to missing loss information\n",
      "Skip training data 1074 due to missing loss information\n",
      "Skip training data 1075 due to missing loss information\n",
      "Skip training data 1076 due to missing loss information\n",
      "Skip training data 1077 due to missing loss information\n",
      "Skip training data 1078 due to missing loss information\n",
      "Skip training data 1079 due to missing loss information\n",
      "Skip training data 1080 due to missing loss information\n",
      "Skip training data 1081 due to missing loss information\n",
      "Skip training data 1082 due to missing loss information\n",
      "Skip training data 1083 due to missing loss information\n",
      "Skip training data 1084 due to missing loss information\n",
      "Skip training data 1085 due to missing loss information\n",
      "Skip training data 1086 due to missing loss information\n",
      "Skip training data 1087 due to missing loss information\n",
      "Skip training data 1088 due to missing loss information\n",
      "Skip training data 1089 due to missing loss information\n",
      "Skip training data 1090 due to missing loss information\n",
      "Skip training data 1091 due to missing loss information\n",
      "Skip training data 1092 due to missing loss information\n",
      "Skip training data 1093 due to missing loss information\n",
      "Skip training data 1094 due to missing loss information\n",
      "Skip training data 1095 due to missing loss information\n",
      "Skip training data 1096 due to missing loss information\n",
      "Skip training data 1097 due to missing loss information\n",
      "Skip training data 1098 due to missing loss information\n",
      "Skip training data 1099 due to missing loss information\n",
      "Skip training data 1100 due to missing loss information\n",
      "Skip training data 1101 due to missing loss information\n",
      "Skip training data 1102 due to missing loss information\n",
      "Skip training data 1103 due to missing loss information\n",
      "Skip training data 1104 due to missing loss information\n",
      "Skip training data 1105 due to missing loss information\n",
      "Skip training data 1106 due to missing loss information\n",
      "Skip training data 1107 due to missing loss information\n",
      "Skip training data 1108 due to missing loss information\n",
      "Skip training data 1109 due to missing loss information\n",
      "Skip training data 1110 due to missing loss information\n",
      "Skip training data 1111 due to missing loss information\n",
      "Skip training data 1112 due to missing loss information\n",
      "Skip training data 1113 due to missing loss information\n",
      "Skip training data 1114 due to missing loss information\n",
      "Skip training data 1115 due to missing loss information\n",
      "Skip training data 1116 due to missing loss information\n",
      "Skip training data 1117 due to missing loss information\n",
      "Skip training data 1118 due to missing loss information\n",
      "Skip training data 1119 due to missing loss information\n",
      "Skip training data 1120 due to missing loss information\n",
      "Skip training data 1121 due to missing loss information\n",
      "Skip training data 1122 due to missing loss information\n",
      "Skip training data 1123 due to missing loss information\n",
      "Skip training data 1124 due to missing loss information\n",
      "Skip training data 1125 due to missing loss information\n",
      "Skip training data 1126 due to missing loss information\n",
      "Skip training data 1127 due to missing loss information\n",
      "Skip training data 1128 due to missing loss information\n",
      "Skip training data 1129 due to missing loss information\n",
      "Skip training data 1130 due to missing loss information\n",
      "Skip training data 1131 due to missing loss information\n",
      "Skip training data 1132 due to missing loss information\n",
      "Skip training data 1133 due to missing loss information\n",
      "Skip training data 1134 due to missing loss information\n",
      "Skip training data 1135 due to missing loss information\n",
      "Skip training data 1136 due to missing loss information\n",
      "Skip training data 1137 due to missing loss information\n",
      "Skip training data 1138 due to missing loss information\n",
      "Skip training data 1139 due to missing loss information\n",
      "Skip training data 1140 due to missing loss information\n",
      "Skip training data 1141 due to missing loss information\n",
      "Skip training data 1142 due to missing loss information\n",
      "Skip training data 1143 due to missing loss information\n",
      "Skip training data 1144 due to missing loss information\n",
      "Skip training data 1145 due to missing loss information\n",
      "Skip training data 1146 due to missing loss information\n",
      "Skip training data 1147 due to missing loss information\n",
      "Skip training data 1148 due to missing loss information\n",
      "Skip training data 1149 due to missing loss information\n",
      "Skip training data 1150 due to missing loss information\n",
      "Skip training data 1151 due to missing loss information\n",
      "Skip training data 1152 due to missing loss information\n",
      "Skip training data 1153 due to missing loss information\n",
      "Skip training data 1154 due to missing loss information\n",
      "Skip training data 1155 due to missing loss information\n",
      "Skip training data 1156 due to missing loss information\n",
      "Skip training data 1157 due to missing loss information\n",
      "Skip training data 1158 due to missing loss information\n",
      "Skip training data 1159 due to missing loss information\n",
      "Skip training data 1160 due to missing loss information\n",
      "Skip training data 1161 due to missing loss information\n",
      "Skip training data 1162 due to missing loss information\n",
      "Skip training data 1163 due to missing loss information\n",
      "Skip training data 1164 due to missing loss information\n",
      "Skip training data 1165 due to missing loss information\n",
      "Skip training data 1166 due to missing loss information\n",
      "Skip training data 1167 due to missing loss information\n",
      "Skip training data 1168 due to missing loss information\n",
      "Skip training data 1169 due to missing loss information\n",
      "Skip training data 1170 due to missing loss information\n",
      "Skip training data 1171 due to missing loss information\n",
      "Skip training data 1172 due to missing loss information\n",
      "Skip training data 1173 due to missing loss information\n",
      "Skip training data 1174 due to missing loss information\n",
      "Skip training data 1175 due to missing loss information\n",
      "Skip training data 1176 due to missing loss information\n",
      "Skip training data 1177 due to missing loss information\n",
      "Skip training data 1178 due to missing loss information\n",
      "Skip training data 1179 due to missing loss information\n",
      "Skip training data 1180 due to missing loss information\n",
      "Skip training data 1181 due to missing loss information\n",
      "Skip training data 1182 due to missing loss information\n",
      "Skip training data 1183 due to missing loss information\n",
      "Skip training data 1184 due to missing loss information\n",
      "Skip training data 1185 due to missing loss information\n",
      "Skip training data 1186 due to missing loss information\n",
      "Skip training data 1187 due to missing loss information\n",
      "Skip training data 1188 due to missing loss information\n",
      "Skip training data 1189 due to missing loss information\n",
      "Skip training data 1190 due to missing loss information\n",
      "Skip training data 1191 due to missing loss information\n",
      "Skip training data 1192 due to missing loss information\n",
      "Skip training data 1193 due to missing loss information\n",
      "Skip training data 1194 due to missing loss information\n",
      "Skip training data 1195 due to missing loss information\n",
      "Skip training data 1196 due to missing loss information\n",
      "Skip training data 1197 due to missing loss information\n",
      "Skip training data 1198 due to missing loss information\n",
      "Skip training data 1199 due to missing loss information\n",
      "Skip training data 1200 due to missing loss information\n",
      "Skip training data 1201 due to missing loss information\n",
      "Skip training data 1202 due to missing loss information\n",
      "Skip training data 1203 due to missing loss information\n",
      "Skip training data 1204 due to missing loss information\n",
      "Skip training data 1205 due to missing loss information\n",
      "Skip training data 1206 due to missing loss information\n",
      "Skip training data 1207 due to missing loss information\n",
      "Skip training data 1208 due to missing loss information\n",
      "Skip training data 1209 due to missing loss information\n",
      "Skip training data 1210 due to missing loss information\n",
      "Skip training data 1211 due to missing loss information\n",
      "Skip training data 1212 due to missing loss information\n",
      "Skip training data 1213 due to missing loss information\n",
      "Skip training data 1214 due to missing loss information\n",
      "Skip training data 1215 due to missing loss information\n",
      "Skip training data 1216 due to missing loss information\n",
      "Skip training data 1217 due to missing loss information\n",
      "Skip training data 1218 due to missing loss information\n",
      "Skip training data 1219 due to missing loss information\n",
      "Skip training data 1220 due to missing loss information\n",
      "Skip training data 1221 due to missing loss information\n",
      "Skip training data 1222 due to missing loss information\n",
      "Skip training data 1223 due to missing loss information\n",
      "Skip training data 1224 due to missing loss information\n",
      "Skip training data 1225 due to missing loss information\n",
      "Skip training data 1226 due to missing loss information\n",
      "Skip training data 1227 due to missing loss information\n",
      "Skip training data 1228 due to missing loss information\n",
      "Skip training data 1229 due to missing loss information\n",
      "Skip training data 1230 due to missing loss information\n",
      "Skip training data 1231 due to missing loss information\n",
      "Skip training data 1232 due to missing loss information\n",
      "Skip training data 1233 due to missing loss information\n",
      "Skip training data 1234 due to missing loss information\n",
      "Skip training data 1235 due to missing loss information\n",
      "Skip training data 1236 due to missing loss information\n",
      "Skip training data 1237 due to missing loss information\n",
      "Skip training data 1238 due to missing loss information\n",
      "Skip training data 1239 due to missing loss information\n",
      "Skip training data 1240 due to missing loss information\n",
      "Skip training data 1241 due to missing loss information\n",
      "Skip training data 1242 due to missing loss information\n",
      "Skip training data 1243 due to missing loss information\n",
      "Skip training data 1244 due to missing loss information\n",
      "Skip training data 1245 due to missing loss information\n",
      "Skip training data 1246 due to missing loss information\n",
      "Skip training data 1247 due to missing loss information\n",
      "Skip training data 1248 due to missing loss information\n",
      "Skip training data 1249 due to missing loss information\n",
      "Skip training data 1250 due to missing loss information\n",
      "Skip training data 1251 due to missing loss information\n",
      "Skip training data 1252 due to missing loss information\n",
      "Skip training data 1253 due to missing loss information\n",
      "Skip training data 1254 due to missing loss information\n",
      "Skip training data 1255 due to missing loss information\n",
      "Skip training data 1256 due to missing loss information\n",
      "Skip training data 1257 due to missing loss information\n",
      "Skip training data 1258 due to missing loss information\n",
      "Skip training data 1259 due to missing loss information\n",
      "Skip training data 1260 due to missing loss information\n",
      "Skip training data 1261 due to missing loss information\n",
      "Skip training data 1262 due to missing loss information\n",
      "Skip training data 1263 due to missing loss information\n",
      "Skip training data 1264 due to missing loss information\n",
      "Skip training data 1265 due to missing loss information\n",
      "Skip training data 1266 due to missing loss information\n",
      "Skip training data 1267 due to missing loss information\n",
      "Skip training data 1268 due to missing loss information\n",
      "Skip training data 1269 due to missing loss information\n",
      "Skip training data 1270 due to missing loss information\n",
      "Skip training data 1271 due to missing loss information\n",
      "Skip training data 1272 due to missing loss information\n",
      "Skip training data 1273 due to missing loss information\n",
      "Skip training data 1274 due to missing loss information\n",
      "Skip training data 1275 due to missing loss information\n",
      "Skip training data 1276 due to missing loss information\n",
      "Skip training data 1277 due to missing loss information\n",
      "Skip training data 1278 due to missing loss information\n",
      "Skip training data 1279 due to missing loss information\n",
      "Skip training data 1280 due to missing loss information\n",
      "Skip training data 1281 due to missing loss information\n",
      "Skip training data 1282 due to missing loss information\n",
      "Skip training data 1283 due to missing loss information\n",
      "Skip training data 1284 due to missing loss information\n",
      "Skip training data 1285 due to missing loss information\n",
      "Skip training data 1286 due to missing loss information\n",
      "Skip training data 1287 due to missing loss information\n",
      "Skip training data 1288 due to missing loss information\n",
      "Skip training data 1289 due to missing loss information\n",
      "Skip training data 1290 due to missing loss information\n",
      "Skip training data 1291 due to missing loss information\n",
      "Skip training data 1292 due to missing loss information\n",
      "Skip training data 1293 due to missing loss information\n",
      "Skip training data 1294 due to missing loss information\n",
      "Skip training data 1295 due to missing loss information\n",
      "Skip training data 1296 due to missing loss information\n",
      "Skip training data 1297 due to missing loss information\n",
      "Skip training data 1298 due to missing loss information\n",
      "Skip training data 1299 due to missing loss information\n",
      "Skip training data 1300 due to missing loss information\n",
      "Skip training data 1301 due to missing loss information\n",
      "Skip training data 1302 due to missing loss information\n",
      "Skip training data 1303 due to missing loss information\n",
      "Skip training data 1304 due to missing loss information\n",
      "Skip training data 1305 due to missing loss information\n",
      "Skip training data 1306 due to missing loss information\n",
      "Skip training data 1307 due to missing loss information\n",
      "Skip training data 1308 due to missing loss information\n",
      "Skip training data 1309 due to missing loss information\n",
      "Skip training data 1310 due to missing loss information\n",
      "Skip training data 1311 due to missing loss information\n",
      "Skip training data 1312 due to missing loss information\n",
      "Skip training data 1313 due to missing loss information\n",
      "Skip training data 1314 due to missing loss information\n",
      "Skip training data 1315 due to missing loss information\n",
      "Skip training data 1316 due to missing loss information\n",
      "Skip training data 1317 due to missing loss information\n",
      "Skip training data 1318 due to missing loss information\n",
      "Skip training data 1319 due to missing loss information\n",
      "Skip training data 1320 due to missing loss information\n",
      "Skip training data 1321 due to missing loss information\n",
      "Skip training data 1322 due to missing loss information\n",
      "Skip training data 1323 due to missing loss information\n",
      "Skip training data 1324 due to missing loss information\n",
      "Skip training data 1325 due to missing loss information\n",
      "Skip training data 1326 due to missing loss information\n",
      "Skip training data 1327 due to missing loss information\n",
      "Skip training data 1328 due to missing loss information\n",
      "Skip training data 1329 due to missing loss information\n",
      "Skip training data 1330 due to missing loss information\n",
      "Skip training data 1331 due to missing loss information\n",
      "Skip training data 1332 due to missing loss information\n",
      "Skip training data 1333 due to missing loss information\n",
      "Skip training data 1334 due to missing loss information\n",
      "Skip training data 1335 due to missing loss information\n",
      "Skip training data 1336 due to missing loss information\n",
      "Skip training data 1337 due to missing loss information\n",
      "Skip training data 1338 due to missing loss information\n",
      "Skip training data 1339 due to missing loss information\n",
      "Skip training data 1340 due to missing loss information\n",
      "Skip training data 1341 due to missing loss information\n",
      "Skip training data 1342 due to missing loss information\n",
      "Skip training data 1343 due to missing loss information\n",
      "Skip training data 1344 due to missing loss information\n",
      "Skip training data 1345 due to missing loss information\n",
      "Skip training data 1346 due to missing loss information\n",
      "Skip training data 1347 due to missing loss information\n",
      "Skip training data 1348 due to missing loss information\n",
      "Skip training data 1349 due to missing loss information\n",
      "Skip training data 1350 due to missing loss information\n",
      "Skip training data 1351 due to missing loss information\n",
      "Skip training data 1352 due to missing loss information\n",
      "Skip training data 1353 due to missing loss information\n",
      "Skip training data 1354 due to missing loss information\n",
      "Skip training data 1355 due to missing loss information\n",
      "Skip training data 1356 due to missing loss information\n",
      "Skip training data 1357 due to missing loss information\n",
      "Skip training data 1358 due to missing loss information\n",
      "Skip training data 1359 due to missing loss information\n",
      "Skip training data 1360 due to missing loss information\n",
      "Skip training data 1361 due to missing loss information\n",
      "Skip training data 1362 due to missing loss information\n",
      "Skip training data 1363 due to missing loss information\n",
      "Skip training data 1364 due to missing loss information\n",
      "Skip training data 1365 due to missing loss information\n",
      "Skip training data 1366 due to missing loss information\n",
      "Skip training data 1367 due to missing loss information\n",
      "Skip training data 1368 due to missing loss information\n",
      "Skip training data 1369 due to missing loss information\n",
      "Skip training data 1370 due to missing loss information\n",
      "Skip training data 1371 due to missing loss information\n",
      "Skip training data 1372 due to missing loss information\n",
      "Skip training data 1373 due to missing loss information\n",
      "Skip training data 1374 due to missing loss information\n",
      "Skip training data 1375 due to missing loss information\n",
      "Skip training data 1376 due to missing loss information\n",
      "Skip training data 1377 due to missing loss information\n",
      "Skip training data 1378 due to missing loss information\n",
      "Skip training data 1379 due to missing loss information\n",
      "Skip training data 1380 due to missing loss information\n",
      "Skip training data 1381 due to missing loss information\n",
      "Skip training data 1382 due to missing loss information\n",
      "Skip training data 1383 due to missing loss information\n",
      "Skip training data 1384 due to missing loss information\n",
      "Skip training data 1385 due to missing loss information\n",
      "Skip training data 1386 due to missing loss information\n",
      "Skip training data 1387 due to missing loss information\n",
      "Skip training data 1388 due to missing loss information\n",
      "Skip training data 1389 due to missing loss information\n",
      "Skip training data 1390 due to missing loss information\n",
      "Skip training data 1391 due to missing loss information\n",
      "Skip training data 1392 due to missing loss information\n",
      "Skip training data 1393 due to missing loss information\n",
      "Skip training data 1394 due to missing loss information\n",
      "Skip training data 1395 due to missing loss information\n",
      "Skip training data 1396 due to missing loss information\n",
      "Skip training data 1397 due to missing loss information\n",
      "Skip training data 1398 due to missing loss information\n",
      "Skip training data 1399 due to missing loss information\n",
      "Skip training data 1400 due to missing loss information\n",
      "Skip training data 1401 due to missing loss information\n",
      "Skip training data 1402 due to missing loss information\n",
      "Skip training data 1403 due to missing loss information\n",
      "Skip training data 1404 due to missing loss information\n",
      "Skip training data 1405 due to missing loss information\n",
      "Skip training data 1406 due to missing loss information\n",
      "Skip training data 1407 due to missing loss information\n",
      "Skip training data 1408 due to missing loss information\n",
      "Skip training data 1409 due to missing loss information\n",
      "Skip training data 1410 due to missing loss information\n",
      "Skip training data 1411 due to missing loss information\n",
      "Skip training data 1412 due to missing loss information\n",
      "Skip training data 1413 due to missing loss information\n",
      "Skip training data 1414 due to missing loss information\n",
      "Skip training data 1415 due to missing loss information\n",
      "Skip training data 1416 due to missing loss information\n",
      "Skip training data 1417 due to missing loss information\n",
      "Skip training data 1418 due to missing loss information\n",
      "Skip training data 1419 due to missing loss information\n",
      "Skip training data 1420 due to missing loss information\n",
      "Skip training data 1421 due to missing loss information\n",
      "Skip training data 1422 due to missing loss information\n",
      "Skip training data 1423 due to missing loss information\n",
      "Skip training data 1424 due to missing loss information\n",
      "Skip training data 1425 due to missing loss information\n",
      "Skip training data 1426 due to missing loss information\n",
      "Skip training data 1427 due to missing loss information\n",
      "Skip training data 1428 due to missing loss information\n",
      "Skip training data 1429 due to missing loss information\n",
      "Skip training data 1430 due to missing loss information\n",
      "Skip training data 1431 due to missing loss information\n",
      "Skip training data 1432 due to missing loss information\n",
      "Skip training data 1433 due to missing loss information\n",
      "Skip training data 1434 due to missing loss information\n",
      "Skip training data 1435 due to missing loss information\n",
      "Skip training data 1436 due to missing loss information\n",
      "Skip training data 1437 due to missing loss information\n",
      "Skip training data 1438 due to missing loss information\n",
      "Skip training data 1439 due to missing loss information\n",
      "Skip training data 1440 due to missing loss information\n",
      "Skip training data 1441 due to missing loss information\n",
      "Skip training data 1442 due to missing loss information\n",
      "Skip training data 1443 due to missing loss information\n",
      "Skip training data 1444 due to missing loss information\n",
      "Skip training data 1445 due to missing loss information\n",
      "Skip training data 1446 due to missing loss information\n",
      "Skip training data 1447 due to missing loss information\n",
      "Skip training data 1448 due to missing loss information\n",
      "Skip training data 1449 due to missing loss information\n",
      "Skip training data 1450 due to missing loss information\n",
      "Skip training data 1451 due to missing loss information\n",
      "Skip training data 1452 due to missing loss information\n",
      "Skip training data 1453 due to missing loss information\n",
      "Skip training data 1454 due to missing loss information\n",
      "Skip training data 1455 due to missing loss information\n",
      "Skip training data 1456 due to missing loss information\n",
      "Skip training data 1457 due to missing loss information\n",
      "Skip training data 1458 due to missing loss information\n",
      "Skip training data 1459 due to missing loss information\n",
      "Skip training data 1460 due to missing loss information\n",
      "Skip training data 1461 due to missing loss information\n",
      "Skip training data 1462 due to missing loss information\n",
      "Skip training data 1463 due to missing loss information\n",
      "Skip training data 1464 due to missing loss information\n",
      "Skip training data 1465 due to missing loss information\n",
      "Skip training data 1466 due to missing loss information\n",
      "Skip training data 1467 due to missing loss information\n",
      "Skip training data 1468 due to missing loss information\n",
      "Skip training data 1469 due to missing loss information\n",
      "Skip training data 1470 due to missing loss information\n",
      "Skip training data 1471 due to missing loss information\n",
      "Skip training data 1472 due to missing loss information\n",
      "Skip training data 1473 due to missing loss information\n",
      "Skip training data 1474 due to missing loss information\n",
      "Skip training data 1475 due to missing loss information\n",
      "Skip training data 1476 due to missing loss information\n",
      "Skip training data 1477 due to missing loss information\n",
      "Skip training data 1478 due to missing loss information\n",
      "Skip training data 1479 due to missing loss information\n",
      "Skip training data 1480 due to missing loss information\n",
      "Skip training data 1481 due to missing loss information\n",
      "Skip training data 1482 due to missing loss information\n",
      "Skip training data 1483 due to missing loss information\n",
      "Skip training data 1484 due to missing loss information\n",
      "Skip training data 1485 due to missing loss information\n",
      "Skip training data 1486 due to missing loss information\n",
      "Skip training data 1487 due to missing loss information\n",
      "Skip training data 1488 due to missing loss information\n",
      "Skip training data 1489 due to missing loss information\n",
      "Skip training data 1490 due to missing loss information\n",
      "Skip training data 1491 due to missing loss information\n",
      "Skip training data 1492 due to missing loss information\n",
      "Skip training data 1493 due to missing loss information\n",
      "Skip training data 1494 due to missing loss information\n",
      "Skip training data 1495 due to missing loss information\n",
      "Skip training data 1496 due to missing loss information\n",
      "Skip training data 1497 due to missing loss information\n",
      "Skip training data 1498 due to missing loss information\n",
      "Skip training data 1499 due to missing loss information\n",
      "Skip training data 1500 due to missing loss information\n",
      "Skip training data 1501 due to missing loss information\n",
      "Skip training data 1502 due to missing loss information\n",
      "Skip training data 1503 due to missing loss information\n",
      "Skip training data 1504 due to missing loss information\n",
      "Skip training data 1505 due to missing loss information\n",
      "Skip training data 1506 due to missing loss information\n",
      "Skip training data 1507 due to missing loss information\n",
      "Skip training data 1508 due to missing loss information\n",
      "Skip training data 1509 due to missing loss information\n",
      "Skip training data 1510 due to missing loss information\n",
      "Skip training data 1511 due to missing loss information\n",
      "Skip training data 1512 due to missing loss information\n",
      "Skip training data 1513 due to missing loss information\n",
      "Skip training data 1514 due to missing loss information\n",
      "Skip training data 1515 due to missing loss information\n",
      "Skip training data 1516 due to missing loss information\n",
      "Skip training data 1517 due to missing loss information\n",
      "Skip training data 1518 due to missing loss information\n",
      "Skip training data 1519 due to missing loss information\n",
      "Skip training data 1520 due to missing loss information\n",
      "Skip training data 1521 due to missing loss information\n",
      "Skip training data 1522 due to missing loss information\n",
      "Skip training data 1523 due to missing loss information\n",
      "Skip training data 1524 due to missing loss information\n",
      "Skip training data 1525 due to missing loss information\n",
      "Skip training data 1526 due to missing loss information\n",
      "Skip training data 1527 due to missing loss information\n",
      "Skip training data 1528 due to missing loss information\n",
      "Skip training data 1529 due to missing loss information\n",
      "Skip training data 1530 due to missing loss information\n",
      "Skip training data 1531 due to missing loss information\n",
      "Skip training data 1532 due to missing loss information\n",
      "Skip training data 1533 due to missing loss information\n",
      "Skip training data 1534 due to missing loss information\n",
      "Skip training data 1535 due to missing loss information\n",
      "Skip training data 1536 due to missing loss information\n",
      "Skip training data 1537 due to missing loss information\n",
      "Skip training data 1538 due to missing loss information\n",
      "Skip training data 1539 due to missing loss information\n",
      "Skip training data 1540 due to missing loss information\n",
      "Skip training data 1541 due to missing loss information\n",
      "Skip training data 1542 due to missing loss information\n",
      "Skip training data 1543 due to missing loss information\n",
      "Skip training data 1544 due to missing loss information\n",
      "Skip training data 1545 due to missing loss information\n",
      "Skip training data 1546 due to missing loss information\n",
      "Skip training data 1547 due to missing loss information\n",
      "Skip training data 1548 due to missing loss information\n",
      "Skip training data 1549 due to missing loss information\n",
      "Skip training data 1550 due to missing loss information\n",
      "Skip training data 1551 due to missing loss information\n",
      "Skip training data 1552 due to missing loss information\n",
      "Skip training data 1553 due to missing loss information\n",
      "Skip training data 1554 due to missing loss information\n",
      "Skip training data 1555 due to missing loss information\n",
      "Skip training data 1556 due to missing loss information\n",
      "Skip training data 1557 due to missing loss information\n",
      "Skip training data 1558 due to missing loss information\n",
      "Skip training data 1559 due to missing loss information\n",
      "Skip training data 1560 due to missing loss information\n",
      "Skip training data 1561 due to missing loss information\n",
      "Skip training data 1562 due to missing loss information\n",
      "Skip training data 1563 due to missing loss information\n",
      "Skip training data 1564 due to missing loss information\n",
      "Skip training data 1565 due to missing loss information\n",
      "Skip training data 1566 due to missing loss information\n",
      "Skip training data 1567 due to missing loss information\n",
      "Skip training data 1568 due to missing loss information\n",
      "Skip training data 1569 due to missing loss information\n",
      "Skip training data 1570 due to missing loss information\n",
      "Skip training data 1571 due to missing loss information\n",
      "Skip training data 1572 due to missing loss information\n",
      "Skip training data 1573 due to missing loss information\n",
      "Skip training data 1574 due to missing loss information\n",
      "Skip training data 1575 due to missing loss information\n",
      "Skip training data 1576 due to missing loss information\n",
      "Skip training data 1577 due to missing loss information\n",
      "Skip training data 1578 due to missing loss information\n",
      "Skip training data 1579 due to missing loss information\n",
      "Skip training data 1580 due to missing loss information\n",
      "Skip training data 1581 due to missing loss information\n",
      "Skip training data 1582 due to missing loss information\n",
      "Skip training data 1583 due to missing loss information\n",
      "Skip training data 1584 due to missing loss information\n",
      "Skip training data 1585 due to missing loss information\n",
      "Skip training data 1586 due to missing loss information\n",
      "Skip training data 1587 due to missing loss information\n",
      "Skip training data 1588 due to missing loss information\n",
      "Skip training data 1589 due to missing loss information\n",
      "Skip training data 1590 due to missing loss information\n",
      "Skip training data 1591 due to missing loss information\n",
      "Skip training data 1592 due to missing loss information\n",
      "Skip training data 1593 due to missing loss information\n",
      "Skip training data 1594 due to missing loss information\n",
      "Skip training data 1595 due to missing loss information\n",
      "Skip training data 1596 due to missing loss information\n",
      "Skip training data 1597 due to missing loss information\n",
      "Skip training data 1598 due to missing loss information\n",
      "Skip training data 1599 due to missing loss information\n",
      "Skip training data 1600 due to missing loss information\n",
      "Skip training data 1601 due to missing loss information\n",
      "Skip training data 1602 due to missing loss information\n",
      "Skip training data 1603 due to missing loss information\n",
      "Skip training data 1604 due to missing loss information\n",
      "Skip training data 1605 due to missing loss information\n",
      "Skip training data 1606 due to missing loss information\n",
      "Skip training data 1607 due to missing loss information\n",
      "Skip training data 1608 due to missing loss information\n",
      "Skip training data 1609 due to missing loss information\n",
      "Skip training data 1610 due to missing loss information\n",
      "Skip training data 1611 due to missing loss information\n",
      "Skip training data 1612 due to missing loss information\n",
      "Skip training data 1613 due to missing loss information\n",
      "Skip training data 1614 due to missing loss information\n",
      "Skip training data 1615 due to missing loss information\n",
      "Skip training data 1616 due to missing loss information\n",
      "Skip training data 1617 due to missing loss information\n",
      "Skip training data 1618 due to missing loss information\n",
      "Skip training data 1619 due to missing loss information\n",
      "Skip training data 1620 due to missing loss information\n",
      "Skip training data 1621 due to missing loss information\n",
      "Skip training data 1622 due to missing loss information\n",
      "Skip training data 1623 due to missing loss information\n",
      "Skip training data 1624 due to missing loss information\n",
      "Skip training data 1625 due to missing loss information\n",
      "Skip training data 1626 due to missing loss information\n",
      "Skip training data 1627 due to missing loss information\n",
      "Skip training data 1628 due to missing loss information\n",
      "Skip training data 1629 due to missing loss information\n",
      "Skip training data 1630 due to missing loss information\n",
      "Skip training data 1631 due to missing loss information\n",
      "Skip training data 1632 due to missing loss information\n",
      "Skip training data 1633 due to missing loss information\n",
      "Skip training data 1634 due to missing loss information\n",
      "Skip training data 1635 due to missing loss information\n",
      "Skip training data 1636 due to missing loss information\n",
      "Skip training data 1637 due to missing loss information\n",
      "Skip training data 1638 due to missing loss information\n",
      "Skip training data 1639 due to missing loss information\n",
      "Skip training data 1640 due to missing loss information\n",
      "Skip training data 1641 due to missing loss information\n",
      "Skip training data 1642 due to missing loss information\n",
      "Skip training data 1643 due to missing loss information\n",
      "Skip training data 1644 due to missing loss information\n",
      "Skip training data 1645 due to missing loss information\n",
      "Skip training data 1646 due to missing loss information\n",
      "Skip training data 1647 due to missing loss information\n",
      "Skip training data 1648 due to missing loss information\n",
      "Skip training data 1649 due to missing loss information\n",
      "Skip training data 1650 due to missing loss information\n",
      "Skip training data 1651 due to missing loss information\n",
      "Skip training data 1652 due to missing loss information\n",
      "Skip training data 1653 due to missing loss information\n",
      "Skip training data 1654 due to missing loss information\n",
      "Skip training data 1655 due to missing loss information\n",
      "Skip training data 1656 due to missing loss information\n",
      "Skip training data 1657 due to missing loss information\n",
      "Skip training data 1658 due to missing loss information\n",
      "Skip training data 1659 due to missing loss information\n",
      "Skip training data 1660 due to missing loss information\n",
      "Skip training data 1661 due to missing loss information\n",
      "Skip training data 1662 due to missing loss information\n",
      "Skip training data 1663 due to missing loss information\n",
      "Skip training data 1664 due to missing loss information\n",
      "Skip training data 1665 due to missing loss information\n",
      "Skip training data 1666 due to missing loss information\n",
      "Skip training data 1667 due to missing loss information\n",
      "Skip training data 1668 due to missing loss information\n",
      "Skip training data 1669 due to missing loss information\n",
      "Skip training data 1670 due to missing loss information\n",
      "Skip training data 1671 due to missing loss information\n",
      "Skip training data 1672 due to missing loss information\n",
      "Skip training data 1673 due to missing loss information\n",
      "Skip training data 1674 due to missing loss information\n",
      "Skip training data 1675 due to missing loss information\n",
      "Skip training data 1676 due to missing loss information\n",
      "Skip training data 1677 due to missing loss information\n",
      "Skip training data 1678 due to missing loss information\n",
      "Skip training data 1679 due to missing loss information\n",
      "Skip training data 1680 due to missing loss information\n",
      "Skip training data 1681 due to missing loss information\n",
      "Skip training data 1682 due to missing loss information\n",
      "Skip training data 1683 due to missing loss information\n",
      "Skip training data 1684 due to missing loss information\n",
      "Skip training data 1685 due to missing loss information\n",
      "Skip training data 1686 due to missing loss information\n",
      "Skip training data 1687 due to missing loss information\n",
      "Skip training data 1688 due to missing loss information\n",
      "Skip training data 1689 due to missing loss information\n",
      "Skip training data 1690 due to missing loss information\n",
      "Skip training data 1691 due to missing loss information\n",
      "Skip training data 1692 due to missing loss information\n",
      "Skip training data 1693 due to missing loss information\n",
      "Skip training data 1694 due to missing loss information\n",
      "Skip training data 1695 due to missing loss information\n",
      "Skip training data 1696 due to missing loss information\n",
      "Skip training data 1697 due to missing loss information\n",
      "Skip training data 1698 due to missing loss information\n",
      "Skip training data 1699 due to missing loss information\n",
      "Skip training data 1700 due to missing loss information\n",
      "Skip training data 1701 due to missing loss information\n",
      "Skip training data 1702 due to missing loss information\n",
      "Skip training data 1703 due to missing loss information\n",
      "Skip training data 1704 due to missing loss information\n",
      "Skip training data 1705 due to missing loss information\n",
      "Skip training data 1706 due to missing loss information\n",
      "Skip training data 1707 due to missing loss information\n",
      "Skip training data 1708 due to missing loss information\n",
      "Skip training data 1709 due to missing loss information\n",
      "Skip training data 1710 due to missing loss information\n",
      "Skip training data 1711 due to missing loss information\n",
      "Skip training data 1712 due to missing loss information\n",
      "Skip training data 1713 due to missing loss information\n",
      "Skip training data 1714 due to missing loss information\n",
      "Skip training data 1715 due to missing loss information\n",
      "Skip training data 1716 due to missing loss information\n",
      "Skip training data 1717 due to missing loss information\n",
      "Skip training data 1718 due to missing loss information\n",
      "Skip training data 1719 due to missing loss information\n",
      "Skip training data 1720 due to missing loss information\n",
      "Skip training data 1721 due to missing loss information\n",
      "Skip training data 1722 due to missing loss information\n",
      "Skip training data 1723 due to missing loss information\n",
      "Skip training data 1724 due to missing loss information\n",
      "Skip training data 1725 due to missing loss information\n",
      "Skip training data 1726 due to missing loss information\n",
      "Skip training data 1727 due to missing loss information\n",
      "Skip training data 1728 due to missing loss information\n",
      "Skip training data 1729 due to missing loss information\n",
      "Skip training data 1730 due to missing loss information\n",
      "Skip training data 1731 due to missing loss information\n",
      "Skip training data 1732 due to missing loss information\n",
      "Skip training data 1733 due to missing loss information\n",
      "Skip training data 1734 due to missing loss information\n",
      "Skip training data 1735 due to missing loss information\n",
      "Skip training data 1736 due to missing loss information\n",
      "Skip training data 1737 due to missing loss information\n",
      "Skip training data 1738 due to missing loss information\n",
      "Skip training data 1739 due to missing loss information\n",
      "Skip training data 1740 due to missing loss information\n",
      "Skip training data 1741 due to missing loss information\n",
      "Skip training data 1742 due to missing loss information\n",
      "Skip training data 1743 due to missing loss information\n",
      "Skip training data 1744 due to missing loss information\n",
      "Skip training data 1745 due to missing loss information\n",
      "Skip training data 1746 due to missing loss information\n",
      "Skip training data 1747 due to missing loss information\n",
      "Skip training data 1748 due to missing loss information\n",
      "Skip training data 1749 due to missing loss information\n",
      "Skip training data 1750 due to missing loss information\n",
      "Skip training data 1751 due to missing loss information\n",
      "Skip training data 1752 due to missing loss information\n",
      "Skip training data 1753 due to missing loss information\n",
      "Skip training data 1754 due to missing loss information\n",
      "Skip training data 1755 due to missing loss information\n",
      "Skip training data 1756 due to missing loss information\n",
      "Skip training data 1757 due to missing loss information\n",
      "Skip training data 1758 due to missing loss information\n",
      "Skip training data 1759 due to missing loss information\n",
      "Skip training data 1760 due to missing loss information\n",
      "Skip training data 1761 due to missing loss information\n",
      "Skip training data 1762 due to missing loss information\n",
      "Skip training data 1763 due to missing loss information\n",
      "Skip training data 1764 due to missing loss information\n",
      "Skip training data 1765 due to missing loss information\n",
      "Skip training data 1766 due to missing loss information\n",
      "Skip training data 1767 due to missing loss information\n",
      "Skip training data 1768 due to missing loss information\n",
      "Skip training data 1769 due to missing loss information\n",
      "Skip training data 1770 due to missing loss information\n",
      "Skip training data 1771 due to missing loss information\n",
      "Skip training data 1772 due to missing loss information\n",
      "Skip training data 1773 due to missing loss information\n",
      "Skip training data 1774 due to missing loss information\n",
      "Skip training data 1775 due to missing loss information\n",
      "Skip training data 1776 due to missing loss information\n",
      "Skip training data 1777 due to missing loss information\n",
      "Skip training data 1778 due to missing loss information\n",
      "Skip training data 1779 due to missing loss information\n",
      "Skip training data 1780 due to missing loss information\n",
      "Skip training data 1781 due to missing loss information\n",
      "Skip training data 1782 due to missing loss information\n",
      "Skip training data 1783 due to missing loss information\n",
      "Skip training data 1784 due to missing loss information\n",
      "Skip training data 1785 due to missing loss information\n",
      "Skip training data 1786 due to missing loss information\n",
      "Skip training data 1787 due to missing loss information\n",
      "Skip training data 1788 due to missing loss information\n",
      "Skip training data 1789 due to missing loss information\n",
      "Skip training data 1790 due to missing loss information\n",
      "Skip training data 1791 due to missing loss information\n",
      "Skip training data 1792 due to missing loss information\n",
      "Skip training data 1793 due to missing loss information\n",
      "Skip training data 1794 due to missing loss information\n",
      "Skip training data 1795 due to missing loss information\n",
      "Skip training data 1796 due to missing loss information\n",
      "Skip training data 1797 due to missing loss information\n",
      "Skip training data 1798 due to missing loss information\n",
      "Skip training data 1799 due to missing loss information\n",
      "Skip training data 1800 due to missing loss information\n",
      "Skip training data 1801 due to missing loss information\n",
      "Skip training data 1802 due to missing loss information\n",
      "Skip training data 1803 due to missing loss information\n",
      "Skip training data 1804 due to missing loss information\n",
      "Skip training data 1805 due to missing loss information\n",
      "Skip training data 1806 due to missing loss information\n",
      "Skip training data 1807 due to missing loss information\n",
      "Skip training data 1808 due to missing loss information\n",
      "Skip training data 1809 due to missing loss information\n",
      "Skip training data 1810 due to missing loss information\n",
      "Skip training data 1811 due to missing loss information\n",
      "Skip training data 1812 due to missing loss information\n",
      "Skip training data 1813 due to missing loss information\n",
      "Skip training data 1814 due to missing loss information\n",
      "Skip training data 1815 due to missing loss information\n",
      "Skip training data 1816 due to missing loss information\n",
      "Skip training data 1817 due to missing loss information\n",
      "Skip training data 1818 due to missing loss information\n",
      "Skip training data 1819 due to missing loss information\n",
      "Skip training data 1820 due to missing loss information\n",
      "Skip training data 1821 due to missing loss information\n",
      "Skip training data 1822 due to missing loss information\n",
      "Skip training data 1823 due to missing loss information\n",
      "Skip training data 1824 due to missing loss information\n",
      "Skip training data 1825 due to missing loss information\n",
      "Skip training data 1826 due to missing loss information\n",
      "Skip training data 1827 due to missing loss information\n",
      "Skip training data 1828 due to missing loss information\n",
      "Skip training data 1829 due to missing loss information\n",
      "Skip training data 1830 due to missing loss information\n",
      "Skip training data 1831 due to missing loss information\n",
      "Skip training data 1832 due to missing loss information\n",
      "Skip training data 1833 due to missing loss information\n",
      "Skip training data 1834 due to missing loss information\n",
      "Skip training data 1835 due to missing loss information\n",
      "Skip training data 1836 due to missing loss information\n",
      "Skip training data 1837 due to missing loss information\n",
      "Skip training data 1838 due to missing loss information\n",
      "Skip training data 1839 due to missing loss information\n",
      "Skip training data 1840 due to missing loss information\n",
      "Skip training data 1841 due to missing loss information\n",
      "Skip training data 1842 due to missing loss information\n",
      "Skip training data 1843 due to missing loss information\n",
      "Skip training data 1844 due to missing loss information\n",
      "Skip training data 1845 due to missing loss information\n",
      "Skip training data 1846 due to missing loss information\n",
      "Skip training data 1847 due to missing loss information\n",
      "Skip training data 1848 due to missing loss information\n",
      "Skip training data 1849 due to missing loss information\n",
      "Skip training data 1850 due to missing loss information\n",
      "Skip training data 1851 due to missing loss information\n",
      "Skip training data 1852 due to missing loss information\n",
      "Skip training data 1853 due to missing loss information\n",
      "Skip training data 1854 due to missing loss information\n",
      "Skip training data 1855 due to missing loss information\n",
      "Skip training data 1856 due to missing loss information\n",
      "Skip training data 1857 due to missing loss information\n",
      "Skip training data 1858 due to missing loss information\n",
      "Skip training data 1859 due to missing loss information\n",
      "Skip training data 1860 due to missing loss information\n",
      "Skip training data 1861 due to missing loss information\n",
      "Skip training data 1862 due to missing loss information\n",
      "Skip training data 1863 due to missing loss information\n",
      "Skip training data 1864 due to missing loss information\n",
      "Skip training data 1865 due to missing loss information\n",
      "Skip training data 1866 due to missing loss information\n",
      "Skip training data 1867 due to missing loss information\n",
      "Skip training data 1868 due to missing loss information\n",
      "Skip training data 1869 due to missing loss information\n",
      "Skip training data 1870 due to missing loss information\n",
      "Skip training data 1871 due to missing loss information\n",
      "Skip training data 1872 due to missing loss information\n",
      "Skip training data 1873 due to missing loss information\n",
      "Skip training data 1874 due to missing loss information\n",
      "Skip training data 1875 due to missing loss information\n",
      "Skip training data 1876 due to missing loss information\n",
      "Skip training data 1877 due to missing loss information\n",
      "Skip training data 1878 due to missing loss information\n",
      "Skip training data 1879 due to missing loss information\n",
      "Skip training data 1880 due to missing loss information\n",
      "Skip training data 1881 due to missing loss information\n",
      "Skip training data 1882 due to missing loss information\n",
      "Skip training data 1883 due to missing loss information\n",
      "Skip training data 1884 due to missing loss information\n",
      "Skip training data 1885 due to missing loss information\n",
      "Skip training data 1886 due to missing loss information\n",
      "Skip training data 1887 due to missing loss information\n",
      "Skip training data 1888 due to missing loss information\n",
      "Skip training data 1889 due to missing loss information\n",
      "Skip training data 1890 due to missing loss information\n",
      "Skip training data 1891 due to missing loss information\n",
      "Skip training data 1892 due to missing loss information\n",
      "Skip training data 1893 due to missing loss information\n",
      "Skip training data 1894 due to missing loss information\n",
      "Skip training data 1895 due to missing loss information\n",
      "Skip training data 1896 due to missing loss information\n",
      "Skip training data 1897 due to missing loss information\n",
      "Skip training data 1898 due to missing loss information\n",
      "Skip training data 1899 due to missing loss information\n",
      "Skip training data 1900 due to missing loss information\n",
      "Skip training data 1901 due to missing loss information\n",
      "Skip training data 1902 due to missing loss information\n",
      "Skip training data 1903 due to missing loss information\n",
      "Skip training data 1904 due to missing loss information\n",
      "Skip training data 1905 due to missing loss information\n",
      "Skip training data 1906 due to missing loss information\n",
      "Skip training data 1907 due to missing loss information\n",
      "Skip training data 1908 due to missing loss information\n",
      "Skip training data 1909 due to missing loss information\n",
      "Skip training data 1910 due to missing loss information\n",
      "Skip training data 1911 due to missing loss information\n",
      "Skip training data 1912 due to missing loss information\n",
      "Skip training data 1913 due to missing loss information\n",
      "Skip training data 1914 due to missing loss information\n",
      "Skip training data 1915 due to missing loss information\n",
      "Skip training data 1916 due to missing loss information\n",
      "Skip training data 1917 due to missing loss information\n",
      "Skip training data 1918 due to missing loss information\n",
      "Skip training data 1919 due to missing loss information\n",
      "Skip training data 1920 due to missing loss information\n",
      "Skip training data 1921 due to missing loss information\n",
      "Skip training data 1922 due to missing loss information\n",
      "Skip training data 1923 due to missing loss information\n",
      "Skip training data 1924 due to missing loss information\n",
      "Skip training data 1925 due to missing loss information\n",
      "Skip training data 1926 due to missing loss information\n",
      "Skip training data 1927 due to missing loss information\n",
      "Skip training data 1928 due to missing loss information\n",
      "Skip training data 1929 due to missing loss information\n",
      "Skip training data 1930 due to missing loss information\n",
      "Skip training data 1931 due to missing loss information\n",
      "Skip training data 1932 due to missing loss information\n",
      "Skip training data 1933 due to missing loss information\n",
      "Skip training data 1934 due to missing loss information\n",
      "Skip training data 1935 due to missing loss information\n",
      "Skip training data 1936 due to missing loss information\n",
      "Skip training data 1937 due to missing loss information\n",
      "Skip training data 1938 due to missing loss information\n",
      "Skip training data 1939 due to missing loss information\n",
      "Skip training data 1940 due to missing loss information\n",
      "Skip training data 1941 due to missing loss information\n",
      "Skip training data 1942 due to missing loss information\n",
      "Skip training data 1943 due to missing loss information\n",
      "Skip training data 1944 due to missing loss information\n",
      "Skip training data 1945 due to missing loss information\n",
      "Skip training data 1946 due to missing loss information\n",
      "Skip training data 1947 due to missing loss information\n",
      "Skip training data 1948 due to missing loss information\n",
      "Skip training data 1949 due to missing loss information\n",
      "Skip training data 1950 due to missing loss information\n",
      "Skip training data 1951 due to missing loss information\n",
      "Skip training data 1952 due to missing loss information\n",
      "Skip training data 1953 due to missing loss information\n",
      "Skip training data 1954 due to missing loss information\n",
      "Skip training data 1955 due to missing loss information\n",
      "Skip training data 1956 due to missing loss information\n",
      "Skip training data 1957 due to missing loss information\n",
      "Skip training data 1958 due to missing loss information\n",
      "Skip training data 1959 due to missing loss information\n",
      "Skip training data 1960 due to missing loss information\n",
      "Skip training data 1961 due to missing loss information\n",
      "Skip training data 1962 due to missing loss information\n",
      "Skip training data 1963 due to missing loss information\n",
      "Skip training data 1964 due to missing loss information\n",
      "Skip training data 1965 due to missing loss information\n",
      "Skip training data 1966 due to missing loss information\n",
      "Skip training data 1967 due to missing loss information\n",
      "Skip training data 1968 due to missing loss information\n",
      "Skip training data 1969 due to missing loss information\n",
      "Skip training data 1970 due to missing loss information\n",
      "Skip training data 1971 due to missing loss information\n",
      "Skip training data 1972 due to missing loss information\n",
      "Skip training data 1973 due to missing loss information\n",
      "Skip training data 1974 due to missing loss information\n",
      "Skip training data 1975 due to missing loss information\n",
      "Skip training data 1976 due to missing loss information\n",
      "Skip training data 1977 due to missing loss information\n",
      "Skip training data 1978 due to missing loss information\n",
      "Skip training data 1979 due to missing loss information\n",
      "Skip training data 1980 due to missing loss information\n",
      "Skip training data 1981 due to missing loss information\n",
      "Skip training data 1982 due to missing loss information\n",
      "Skip training data 1983 due to missing loss information\n",
      "Skip training data 1984 due to missing loss information\n",
      "Skip training data 1985 due to missing loss information\n",
      "Skip training data 1986 due to missing loss information\n",
      "Skip training data 1987 due to missing loss information\n",
      "Skip training data 1988 due to missing loss information\n",
      "Skip training data 1989 due to missing loss information\n",
      "Skip training data 1990 due to missing loss information\n",
      "Skip training data 1991 due to missing loss information\n",
      "Skip training data 1992 due to missing loss information\n",
      "Skip training data 1993 due to missing loss information\n",
      "Skip training data 1994 due to missing loss information\n",
      "Skip training data 1995 due to missing loss information\n",
      "Skip training data 1996 due to missing loss information\n",
      "Skip training data 1997 due to missing loss information\n",
      "Skip training data 1998 due to missing loss information\n",
      "Skip training data 1999 due to missing loss information\n",
      "Skip training data 2000 due to missing loss information\n",
      "Skip training data 2001 due to missing loss information\n",
      "Skip training data 2002 due to missing loss information\n",
      "Skip training data 2003 due to missing loss information\n",
      "Skip training data 2004 due to missing loss information\n",
      "Skip training data 2005 due to missing loss information\n",
      "Skip training data 2006 due to missing loss information\n",
      "Skip training data 2007 due to missing loss information\n",
      "Skip training data 2008 due to missing loss information\n",
      "Skip training data 2009 due to missing loss information\n",
      "Skip training data 2010 due to missing loss information\n",
      "Skip training data 2011 due to missing loss information\n",
      "Skip training data 2012 due to missing loss information\n",
      "Skip training data 2013 due to missing loss information\n",
      "Skip training data 2014 due to missing loss information\n",
      "Skip training data 2015 due to missing loss information\n",
      "Skip training data 2016 due to missing loss information\n",
      "Skip training data 2017 due to missing loss information\n",
      "Skip training data 2018 due to missing loss information\n",
      "Skip training data 2019 due to missing loss information\n",
      "Skip training data 2020 due to missing loss information\n",
      "Skip training data 2021 due to missing loss information\n",
      "Skip training data 2022 due to missing loss information\n",
      "Skip training data 2023 due to missing loss information\n",
      "Skip training data 2024 due to missing loss information\n",
      "Skip training data 2025 due to missing loss information\n",
      "Skip training data 2026 due to missing loss information\n",
      "Skip training data 2027 due to missing loss information\n",
      "Skip training data 2028 due to missing loss information\n",
      "Skip training data 2029 due to missing loss information\n",
      "Skip training data 2030 due to missing loss information\n",
      "Skip training data 2031 due to missing loss information\n",
      "Skip training data 2032 due to missing loss information\n",
      "Skip training data 2033 due to missing loss information\n",
      "Skip training data 2034 due to missing loss information\n",
      "Skip training data 2035 due to missing loss information\n",
      "Skip training data 2036 due to missing loss information\n",
      "Skip training data 2037 due to missing loss information\n",
      "Skip training data 2038 due to missing loss information\n",
      "Skip training data 2039 due to missing loss information\n",
      "Skip training data 2040 due to missing loss information\n",
      "Skip training data 2041 due to missing loss information\n",
      "Skip training data 2042 due to missing loss information\n",
      "Skip training data 2043 due to missing loss information\n",
      "Skip training data 2044 due to missing loss information\n",
      "Skip training data 2045 due to missing loss information\n",
      "Skip training data 2046 due to missing loss information\n",
      "Skip training data 2047 due to missing loss information\n",
      "Skip training data 2048 due to missing loss information\n",
      "Skip training data 2049 due to missing loss information\n",
      "Skip training data 2050 due to missing loss information\n",
      "Skip training data 2051 due to missing loss information\n",
      "Skip training data 2052 due to missing loss information\n",
      "Skip training data 2053 due to missing loss information\n",
      "Skip training data 2054 due to missing loss information\n",
      "Skip training data 2055 due to missing loss information\n",
      "Skip training data 2056 due to missing loss information\n",
      "Skip training data 2057 due to missing loss information\n",
      "Skip training data 2058 due to missing loss information\n",
      "Skip training data 2059 due to missing loss information\n",
      "Skip training data 2060 due to missing loss information\n",
      "Skip training data 2061 due to missing loss information\n",
      "Skip training data 2062 due to missing loss information\n",
      "Skip training data 2063 due to missing loss information\n",
      "Skip training data 2064 due to missing loss information\n",
      "Skip training data 2065 due to missing loss information\n",
      "Skip training data 2066 due to missing loss information\n",
      "Skip training data 2067 due to missing loss information\n",
      "Skip training data 2068 due to missing loss information\n",
      "Skip training data 2069 due to missing loss information\n",
      "Skip training data 2070 due to missing loss information\n",
      "Skip training data 2071 due to missing loss information\n",
      "Skip training data 2072 due to missing loss information\n",
      "Skip training data 2073 due to missing loss information\n",
      "Skip training data 2074 due to missing loss information\n",
      "Skip training data 2075 due to missing loss information\n",
      "Skip training data 2076 due to missing loss information\n",
      "Skip training data 2077 due to missing loss information\n",
      "Skip training data 2078 due to missing loss information\n",
      "Skip training data 2079 due to missing loss information\n",
      "Skip training data 2080 due to missing loss information\n",
      "Skip training data 2081 due to missing loss information\n",
      "Skip training data 2082 due to missing loss information\n",
      "Skip training data 2083 due to missing loss information\n",
      "Skip training data 2084 due to missing loss information\n",
      "Skip training data 2085 due to missing loss information\n",
      "Skip training data 2086 due to missing loss information\n",
      "Skip training data 2087 due to missing loss information\n",
      "Skip training data 2088 due to missing loss information\n",
      "Skip training data 2089 due to missing loss information\n",
      "Skip training data 2090 due to missing loss information\n",
      "Skip training data 2091 due to missing loss information\n",
      "Skip training data 2092 due to missing loss information\n",
      "Skip training data 2093 due to missing loss information\n",
      "Skip training data 2094 due to missing loss information\n",
      "Skip training data 2095 due to missing loss information\n",
      "Skip training data 2096 due to missing loss information\n",
      "Skip training data 2097 due to missing loss information\n",
      "Skip training data 2098 due to missing loss information\n",
      "Skip training data 2099 due to missing loss information\n",
      "Skip training data 2100 due to missing loss information\n",
      "Skip training data 2101 due to missing loss information\n",
      "Skip training data 2102 due to missing loss information\n",
      "Skip training data 2103 due to missing loss information\n",
      "Skip training data 2104 due to missing loss information\n",
      "Skip training data 2105 due to missing loss information\n",
      "Skip training data 2106 due to missing loss information\n",
      "Skip training data 2107 due to missing loss information\n",
      "Skip training data 2108 due to missing loss information\n",
      "Skip training data 2109 due to missing loss information\n",
      "Skip training data 2110 due to missing loss information\n",
      "Skip training data 2111 due to missing loss information\n",
      "Skip training data 2112 due to missing loss information\n",
      "Skip training data 2113 due to missing loss information\n",
      "Skip training data 2114 due to missing loss information\n",
      "Skip training data 2115 due to missing loss information\n",
      "Skip training data 2116 due to missing loss information\n",
      "Skip training data 2117 due to missing loss information\n",
      "Skip training data 2118 due to missing loss information\n",
      "Skip training data 2119 due to missing loss information\n",
      "Skip training data 2120 due to missing loss information\n",
      "Skip training data 2121 due to missing loss information\n",
      "Skip training data 2122 due to missing loss information\n",
      "Skip training data 2123 due to missing loss information\n",
      "Skip training data 2124 due to missing loss information\n",
      "Skip training data 2125 due to missing loss information\n",
      "Skip training data 2126 due to missing loss information\n",
      "Skip training data 2127 due to missing loss information\n",
      "Skip training data 2128 due to missing loss information\n",
      "Skip training data 2129 due to missing loss information\n",
      "Skip training data 2130 due to missing loss information\n",
      "Skip training data 2131 due to missing loss information\n",
      "Skip training data 2132 due to missing loss information\n",
      "Skip training data 2133 due to missing loss information\n",
      "Skip training data 2134 due to missing loss information\n",
      "Skip training data 2135 due to missing loss information\n",
      "Skip training data 2136 due to missing loss information\n",
      "Skip training data 2137 due to missing loss information\n",
      "Skip training data 2138 due to missing loss information\n",
      "Skip training data 2139 due to missing loss information\n",
      "Skip training data 2140 due to missing loss information\n",
      "Skip training data 2141 due to missing loss information\n",
      "Skip training data 2142 due to missing loss information\n",
      "Skip training data 2143 due to missing loss information\n",
      "Skip training data 2144 due to missing loss information\n",
      "Skip training data 2145 due to missing loss information\n",
      "Skip training data 2146 due to missing loss information\n",
      "Skip training data 2147 due to missing loss information\n",
      "Skip training data 2148 due to missing loss information\n",
      "Skip training data 2149 due to missing loss information\n",
      "Skip training data 2150 due to missing loss information\n",
      "Skip training data 2151 due to missing loss information\n",
      "Skip training data 2152 due to missing loss information\n",
      "Skip training data 2153 due to missing loss information\n",
      "Skip training data 2154 due to missing loss information\n",
      "Skip training data 2155 due to missing loss information\n",
      "Skip training data 2156 due to missing loss information\n",
      "Skip training data 2157 due to missing loss information\n",
      "Skip training data 2158 due to missing loss information\n",
      "Skip training data 2159 due to missing loss information\n",
      "Skip training data 2160 due to missing loss information\n",
      "Skip training data 2161 due to missing loss information\n",
      "Skip training data 2162 due to missing loss information\n",
      "Skip training data 2163 due to missing loss information\n",
      "Skip training data 2164 due to missing loss information\n",
      "Skip training data 2165 due to missing loss information\n",
      "Skip training data 2166 due to missing loss information\n",
      "Skip training data 2167 due to missing loss information\n",
      "Skip training data 2168 due to missing loss information\n",
      "Skip training data 2169 due to missing loss information\n",
      "Skip training data 2170 due to missing loss information\n",
      "Skip training data 2171 due to missing loss information\n",
      "Skip training data 2172 due to missing loss information\n",
      "Skip training data 2173 due to missing loss information\n",
      "Skip training data 2174 due to missing loss information\n",
      "Skip training data 2175 due to missing loss information\n",
      "Skip training data 2176 due to missing loss information\n",
      "Skip training data 2177 due to missing loss information\n",
      "Skip training data 2178 due to missing loss information\n",
      "Skip training data 2179 due to missing loss information\n",
      "Skip training data 2180 due to missing loss information\n",
      "Skip training data 2181 due to missing loss information\n",
      "Skip training data 2182 due to missing loss information\n",
      "Skip training data 2183 due to missing loss information\n",
      "Skip training data 2184 due to missing loss information\n",
      "Skip training data 2185 due to missing loss information\n",
      "Skip training data 2186 due to missing loss information\n",
      "Skip training data 2187 due to missing loss information\n",
      "Skip training data 2188 due to missing loss information\n",
      "Skip training data 2189 due to missing loss information\n",
      "Skip training data 2190 due to missing loss information\n",
      "Skip training data 2191 due to missing loss information\n",
      "Skip training data 2192 due to missing loss information\n",
      "Skip training data 2193 due to missing loss information\n",
      "Skip training data 2194 due to missing loss information\n",
      "Skip training data 2195 due to missing loss information\n",
      "Skip training data 2196 due to missing loss information\n",
      "Skip training data 2197 due to missing loss information\n",
      "Skip training data 2198 due to missing loss information\n",
      "Skip training data 2199 due to missing loss information\n",
      "Skip training data 2200 due to missing loss information\n",
      "Skip training data 2201 due to missing loss information\n",
      "Skip training data 2202 due to missing loss information\n",
      "Skip training data 2203 due to missing loss information\n",
      "Skip training data 2204 due to missing loss information\n",
      "Skip training data 2205 due to missing loss information\n",
      "Skip training data 2206 due to missing loss information\n",
      "Skip training data 2207 due to missing loss information\n",
      "Skip training data 2208 due to missing loss information\n",
      "Skip training data 2209 due to missing loss information\n",
      "Skip training data 2210 due to missing loss information\n",
      "Skip training data 2211 due to missing loss information\n",
      "Skip training data 2212 due to missing loss information\n",
      "Skip training data 2213 due to missing loss information\n",
      "Skip training data 2214 due to missing loss information\n",
      "Skip training data 2215 due to missing loss information\n",
      "Skip training data 2216 due to missing loss information\n",
      "Skip training data 2217 due to missing loss information\n",
      "Skip training data 2218 due to missing loss information\n",
      "Skip training data 2219 due to missing loss information\n",
      "Skip training data 2220 due to missing loss information\n",
      "Skip training data 2221 due to missing loss information\n",
      "Skip training data 2222 due to missing loss information\n",
      "Skip training data 2223 due to missing loss information\n",
      "Skip training data 2224 due to missing loss information\n",
      "Skip training data 2225 due to missing loss information\n",
      "Skip training data 2226 due to missing loss information\n",
      "Skip training data 2227 due to missing loss information\n",
      "Skip training data 2228 due to missing loss information\n",
      "Skip training data 2229 due to missing loss information\n",
      "Skip training data 2230 due to missing loss information\n",
      "Skip training data 2231 due to missing loss information\n",
      "Skip training data 2232 due to missing loss information\n",
      "Skip training data 2233 due to missing loss information\n",
      "Skip training data 2234 due to missing loss information\n",
      "Skip training data 2235 due to missing loss information\n",
      "Skip training data 2236 due to missing loss information\n",
      "Skip training data 2237 due to missing loss information\n",
      "Skip training data 2238 due to missing loss information\n",
      "Skip training data 2239 due to missing loss information\n",
      "Skip training data 2240 due to missing loss information\n",
      "Skip training data 2241 due to missing loss information\n",
      "Skip training data 2242 due to missing loss information\n",
      "Skip training data 2243 due to missing loss information\n",
      "Skip training data 2244 due to missing loss information\n",
      "Skip training data 2245 due to missing loss information\n",
      "Skip training data 2246 due to missing loss information\n",
      "Skip training data 2247 due to missing loss information\n",
      "Skip training data 2248 due to missing loss information\n",
      "Skip training data 2249 due to missing loss information\n",
      "Skip training data 2250 due to missing loss information\n",
      "Skip training data 2251 due to missing loss information\n",
      "Skip training data 2252 due to missing loss information\n",
      "Skip training data 2253 due to missing loss information\n",
      "Skip training data 2254 due to missing loss information\n",
      "Skip training data 2255 due to missing loss information\n",
      "Skip training data 2256 due to missing loss information\n",
      "Skip training data 2257 due to missing loss information\n",
      "Skip training data 2258 due to missing loss information\n",
      "Skip training data 2259 due to missing loss information\n",
      "Skip training data 2260 due to missing loss information\n",
      "Skip training data 2261 due to missing loss information\n",
      "Skip training data 2262 due to missing loss information\n",
      "Skip training data 2263 due to missing loss information\n",
      "Skip training data 2264 due to missing loss information\n",
      "Skip training data 2265 due to missing loss information\n",
      "Skip training data 2266 due to missing loss information\n",
      "Skip training data 2267 due to missing loss information\n",
      "Skip training data 2268 due to missing loss information\n",
      "Skip training data 2269 due to missing loss information\n",
      "Skip training data 2270 due to missing loss information\n",
      "Skip training data 2271 due to missing loss information\n",
      "Skip training data 2272 due to missing loss information\n",
      "Skip training data 2273 due to missing loss information\n",
      "Skip training data 2274 due to missing loss information\n",
      "Skip training data 2275 due to missing loss information\n",
      "Skip training data 2276 due to missing loss information\n",
      "Skip training data 2277 due to missing loss information\n",
      "Skip training data 2278 due to missing loss information\n",
      "Skip training data 2279 due to missing loss information\n",
      "Skip training data 2280 due to missing loss information\n",
      "Skip training data 2281 due to missing loss information\n",
      "Skip training data 2282 due to missing loss information\n",
      "Skip training data 2283 due to missing loss information\n",
      "Skip training data 2284 due to missing loss information\n",
      "Skip training data 2285 due to missing loss information\n",
      "Skip training data 2286 due to missing loss information\n",
      "Skip training data 2287 due to missing loss information\n",
      "Skip training data 2288 due to missing loss information\n",
      "Skip training data 2289 due to missing loss information\n",
      "Skip training data 2290 due to missing loss information\n",
      "Skip training data 2291 due to missing loss information\n",
      "Skip training data 2292 due to missing loss information\n",
      "Skip training data 2293 due to missing loss information\n",
      "Skip training data 2294 due to missing loss information\n",
      "Skip training data 2295 due to missing loss information\n",
      "Skip training data 2296 due to missing loss information\n",
      "Skip training data 2297 due to missing loss information\n",
      "Skip training data 2298 due to missing loss information\n",
      "Skip training data 2299 due to missing loss information\n",
      "Skip training data 2300 due to missing loss information\n",
      "Skip training data 2301 due to missing loss information\n",
      "Skip training data 2302 due to missing loss information\n",
      "Skip training data 2303 due to missing loss information\n",
      "Skip training data 2304 due to missing loss information\n",
      "Skip training data 2305 due to missing loss information\n",
      "Skip training data 2306 due to missing loss information\n",
      "Skip training data 2307 due to missing loss information\n",
      "Skip training data 2308 due to missing loss information\n",
      "Skip training data 2309 due to missing loss information\n",
      "Skip training data 2310 due to missing loss information\n",
      "Skip training data 2311 due to missing loss information\n",
      "Skip training data 2312 due to missing loss information\n",
      "Skip training data 2313 due to missing loss information\n",
      "Skip training data 2314 due to missing loss information\n",
      "Skip training data 2315 due to missing loss information\n",
      "Skip training data 2316 due to missing loss information\n",
      "Skip training data 2317 due to missing loss information\n",
      "Skip training data 2318 due to missing loss information\n",
      "Skip training data 2319 due to missing loss information\n",
      "Skip training data 2320 due to missing loss information\n",
      "Skip training data 2321 due to missing loss information\n",
      "Skip training data 2322 due to missing loss information\n",
      "Skip training data 2323 due to missing loss information\n",
      "Skip training data 2324 due to missing loss information\n",
      "Skip training data 2325 due to missing loss information\n",
      "Skip training data 2326 due to missing loss information\n",
      "Skip training data 2327 due to missing loss information\n",
      "Skip training data 2328 due to missing loss information\n",
      "Skip training data 2329 due to missing loss information\n",
      "Skip training data 2330 due to missing loss information\n",
      "Skip training data 2331 due to missing loss information\n",
      "Skip training data 2332 due to missing loss information\n",
      "Skip training data 2333 due to missing loss information\n",
      "Skip training data 2334 due to missing loss information\n",
      "Skip training data 2335 due to missing loss information\n",
      "Skip training data 2336 due to missing loss information\n",
      "Skip training data 2337 due to missing loss information\n",
      "Skip training data 2338 due to missing loss information\n",
      "Skip training data 2339 due to missing loss information\n",
      "Skip training data 2340 due to missing loss information\n",
      "Skip training data 2341 due to missing loss information\n",
      "Skip training data 2342 due to missing loss information\n",
      "Skip training data 2343 due to missing loss information\n",
      "Skip training data 2344 due to missing loss information\n",
      "Skip training data 2345 due to missing loss information\n",
      "Skip training data 2346 due to missing loss information\n",
      "Skip training data 2347 due to missing loss information\n",
      "Skip training data 2348 due to missing loss information\n",
      "Skip training data 2349 due to missing loss information\n",
      "Skip training data 2350 due to missing loss information\n",
      "Skip training data 2351 due to missing loss information\n",
      "Skip training data 2352 due to missing loss information\n",
      "Skip training data 2353 due to missing loss information\n",
      "Skip training data 2354 due to missing loss information\n",
      "Skip training data 2355 due to missing loss information\n",
      "Skip training data 2356 due to missing loss information\n",
      "Skip training data 2357 due to missing loss information\n",
      "Skip training data 2358 due to missing loss information\n",
      "Skip training data 2359 due to missing loss information\n",
      "Skip training data 2360 due to missing loss information\n",
      "Skip training data 2361 due to missing loss information\n",
      "Skip training data 2362 due to missing loss information\n",
      "Skip training data 2363 due to missing loss information\n",
      "Skip training data 2364 due to missing loss information\n",
      "Skip training data 2365 due to missing loss information\n",
      "Skip training data 2366 due to missing loss information\n",
      "Skip training data 2367 due to missing loss information\n",
      "Skip training data 2368 due to missing loss information\n",
      "Skip training data 2369 due to missing loss information\n",
      "Skip training data 2370 due to missing loss information\n",
      "Skip training data 2371 due to missing loss information\n",
      "Skip training data 2372 due to missing loss information\n",
      "Skip training data 2373 due to missing loss information\n",
      "Skip training data 2374 due to missing loss information\n",
      "Skip training data 2375 due to missing loss information\n",
      "Skip training data 2376 due to missing loss information\n",
      "Skip training data 2377 due to missing loss information\n",
      "Skip training data 2378 due to missing loss information\n",
      "Skip training data 2379 due to missing loss information\n",
      "Skip training data 2380 due to missing loss information\n",
      "Skip training data 2381 due to missing loss information\n",
      "Skip training data 2382 due to missing loss information\n",
      "Skip training data 2383 due to missing loss information\n",
      "Skip training data 2384 due to missing loss information\n",
      "Skip training data 2385 due to missing loss information\n",
      "Skip training data 2386 due to missing loss information\n",
      "Skip training data 2387 due to missing loss information\n",
      "Skip training data 2388 due to missing loss information\n",
      "Skip training data 2389 due to missing loss information\n",
      "Skip training data 2390 due to missing loss information\n",
      "Skip training data 2391 due to missing loss information\n",
      "Skip training data 2392 due to missing loss information\n",
      "Skip training data 2393 due to missing loss information\n",
      "Skip training data 2394 due to missing loss information\n",
      "Skip training data 2395 due to missing loss information\n",
      "Skip training data 2396 due to missing loss information\n",
      "Skip training data 2397 due to missing loss information\n",
      "Skip training data 2398 due to missing loss information\n",
      "Skip training data 2399 due to missing loss information\n",
      "Skip training data 2400 due to missing loss information\n",
      "Skip training data 2401 due to missing loss information\n",
      "Skip training data 2402 due to missing loss information\n",
      "Skip training data 2403 due to missing loss information\n",
      "Skip training data 2404 due to missing loss information\n",
      "Skip training data 2405 due to missing loss information\n",
      "Skip training data 2406 due to missing loss information\n",
      "Skip training data 2407 due to missing loss information\n",
      "Skip training data 2408 due to missing loss information\n",
      "Skip training data 2409 due to missing loss information\n",
      "Skip training data 2410 due to missing loss information\n",
      "Skip training data 2411 due to missing loss information\n",
      "Skip training data 2412 due to missing loss information\n",
      "Skip training data 2413 due to missing loss information\n",
      "Skip training data 2414 due to missing loss information\n",
      "Skip training data 2415 due to missing loss information\n",
      "Skip training data 2416 due to missing loss information\n",
      "Skip training data 2417 due to missing loss information\n",
      "Skip training data 2418 due to missing loss information\n",
      "Skip training data 2419 due to missing loss information\n",
      "Skip training data 2420 due to missing loss information\n",
      "Skip training data 2421 due to missing loss information\n",
      "Skip training data 2422 due to missing loss information\n",
      "Skip training data 2423 due to missing loss information\n",
      "Skip training data 2424 due to missing loss information\n",
      "Skip training data 2425 due to missing loss information\n",
      "Skip training data 2426 due to missing loss information\n",
      "Skip training data 2427 due to missing loss information\n",
      "Skip training data 2428 due to missing loss information\n",
      "Skip training data 2429 due to missing loss information\n",
      "Skip training data 2430 due to missing loss information\n",
      "Skip training data 2431 due to missing loss information\n",
      "Skip training data 2432 due to missing loss information\n",
      "Skip training data 2433 due to missing loss information\n",
      "Skip training data 2434 due to missing loss information\n",
      "Skip training data 2435 due to missing loss information\n",
      "Skip training data 2436 due to missing loss information\n",
      "Skip training data 2437 due to missing loss information\n",
      "Skip training data 2438 due to missing loss information\n",
      "Skip training data 2439 due to missing loss information\n",
      "Skip training data 2440 due to missing loss information\n",
      "Skip training data 2441 due to missing loss information\n",
      "Skip training data 2442 due to missing loss information\n",
      "Skip training data 2443 due to missing loss information\n",
      "Skip training data 2444 due to missing loss information\n",
      "Skip training data 2445 due to missing loss information\n",
      "Skip training data 2446 due to missing loss information\n",
      "Skip training data 2447 due to missing loss information\n",
      "Skip training data 2448 due to missing loss information\n",
      "Skip training data 2449 due to missing loss information\n",
      "Skip training data 2450 due to missing loss information\n",
      "Skip training data 2451 due to missing loss information\n",
      "Skip training data 2452 due to missing loss information\n",
      "Skip training data 2453 due to missing loss information\n",
      "Skip training data 2454 due to missing loss information\n",
      "Skip training data 2455 due to missing loss information\n",
      "Skip training data 2456 due to missing loss information\n",
      "Skip training data 2457 due to missing loss information\n",
      "Skip training data 2458 due to missing loss information\n",
      "Skip training data 2459 due to missing loss information\n",
      "Skip training data 2460 due to missing loss information\n",
      "Skip training data 2461 due to missing loss information\n",
      "Skip training data 2462 due to missing loss information\n",
      "Skip training data 2463 due to missing loss information\n",
      "Skip training data 2464 due to missing loss information\n",
      "Skip training data 2465 due to missing loss information\n",
      "Skip training data 2466 due to missing loss information\n",
      "Skip training data 2467 due to missing loss information\n",
      "Skip training data 2468 due to missing loss information\n",
      "Skip training data 2469 due to missing loss information\n",
      "Skip training data 2470 due to missing loss information\n",
      "Skip training data 2471 due to missing loss information\n",
      "Skip training data 2472 due to missing loss information\n",
      "Skip training data 2473 due to missing loss information\n",
      "Skip training data 2474 due to missing loss information\n",
      "Skip training data 2475 due to missing loss information\n",
      "Skip training data 2476 due to missing loss information\n",
      "Skip training data 2477 due to missing loss information\n",
      "Skip training data 2478 due to missing loss information\n",
      "Skip training data 2479 due to missing loss information\n",
      "Skip training data 2480 due to missing loss information\n",
      "Skip training data 2481 due to missing loss information\n",
      "Skip training data 2482 due to missing loss information\n",
      "Skip training data 2483 due to missing loss information\n",
      "Skip training data 2484 due to missing loss information\n",
      "Skip training data 2485 due to missing loss information\n",
      "Skip training data 2486 due to missing loss information\n",
      "Skip training data 2487 due to missing loss information\n",
      "Skip training data 2488 due to missing loss information\n",
      "Skip training data 2489 due to missing loss information\n",
      "Skip training data 2490 due to missing loss information\n",
      "Skip training data 2491 due to missing loss information\n",
      "Skip training data 2492 due to missing loss information\n",
      "Skip training data 2493 due to missing loss information\n",
      "Skip training data 2494 due to missing loss information\n",
      "Skip training data 2495 due to missing loss information\n",
      "Skip training data 2496 due to missing loss information\n",
      "Skip training data 2497 due to missing loss information\n",
      "Skip training data 2498 due to missing loss information\n",
      "Skip training data 2499 due to missing loss information\n",
      "Skip training data 2500 due to missing loss information\n",
      "Skip training data 2501 due to missing loss information\n",
      "Skip training data 2502 due to missing loss information\n",
      "Skip training data 2503 due to missing loss information\n",
      "Skip training data 2504 due to missing loss information\n",
      "Skip training data 2505 due to missing loss information\n",
      "Skip training data 2506 due to missing loss information\n",
      "Skip training data 2507 due to missing loss information\n",
      "Skip training data 2508 due to missing loss information\n",
      "Skip training data 2509 due to missing loss information\n",
      "Skip training data 2510 due to missing loss information\n",
      "Skip training data 2511 due to missing loss information\n",
      "Skip training data 2512 due to missing loss information\n",
      "Skip training data 2513 due to missing loss information\n",
      "Skip training data 2514 due to missing loss information\n",
      "Skip training data 2515 due to missing loss information\n",
      "Skip training data 2516 due to missing loss information\n",
      "Skip training data 2517 due to missing loss information\n",
      "Skip training data 2518 due to missing loss information\n",
      "Skip training data 2519 due to missing loss information\n",
      "Skip training data 2520 due to missing loss information\n",
      "Skip training data 2521 due to missing loss information\n",
      "Skip training data 2522 due to missing loss information\n",
      "Skip training data 2523 due to missing loss information\n",
      "Skip training data 2524 due to missing loss information\n",
      "Skip training data 2525 due to missing loss information\n",
      "Skip training data 2526 due to missing loss information\n",
      "Skip training data 2527 due to missing loss information\n",
      "Skip training data 2528 due to missing loss information\n",
      "Skip training data 2529 due to missing loss information\n",
      "Skip training data 2530 due to missing loss information\n",
      "Skip training data 2531 due to missing loss information\n",
      "Skip training data 2532 due to missing loss information\n",
      "Skip training data 2533 due to missing loss information\n",
      "Skip training data 2534 due to missing loss information\n",
      "Skip training data 2535 due to missing loss information\n",
      "Skip training data 2536 due to missing loss information\n",
      "Skip training data 2537 due to missing loss information\n",
      "Skip training data 2538 due to missing loss information\n",
      "Skip training data 2539 due to missing loss information\n",
      "Skip training data 2540 due to missing loss information\n",
      "Skip training data 2541 due to missing loss information\n",
      "Skip training data 2542 due to missing loss information\n",
      "Skip training data 2543 due to missing loss information\n",
      "Skip training data 2544 due to missing loss information\n",
      "Skip training data 2545 due to missing loss information\n",
      "Skip training data 2546 due to missing loss information\n",
      "Skip training data 2547 due to missing loss information\n",
      "Skip training data 2548 due to missing loss information\n",
      "Skip training data 2549 due to missing loss information\n",
      "Skip training data 2550 due to missing loss information\n",
      "Skip training data 2551 due to missing loss information\n",
      "Skip training data 2552 due to missing loss information\n",
      "Skip training data 2553 due to missing loss information\n",
      "Skip training data 2554 due to missing loss information\n",
      "Skip training data 2555 due to missing loss information\n",
      "Skip training data 2556 due to missing loss information\n",
      "Skip training data 2557 due to missing loss information\n",
      "Skip training data 2558 due to missing loss information\n",
      "Skip training data 2559 due to missing loss information\n",
      "Skip training data 2560 due to missing loss information\n",
      "Skip training data 2561 due to missing loss information\n",
      "Skip training data 2562 due to missing loss information\n",
      "Skip training data 2563 due to missing loss information\n",
      "Skip training data 2564 due to missing loss information\n",
      "Skip training data 2565 due to missing loss information\n",
      "Skip training data 2566 due to missing loss information\n",
      "Skip training data 2567 due to missing loss information\n",
      "Skip training data 2568 due to missing loss information\n",
      "Skip training data 2569 due to missing loss information\n",
      "Skip training data 2570 due to missing loss information\n",
      "Skip training data 2571 due to missing loss information\n",
      "Skip training data 2572 due to missing loss information\n",
      "Skip training data 2573 due to missing loss information\n",
      "Skip training data 2574 due to missing loss information\n",
      "Skip training data 2575 due to missing loss information\n",
      "Skip training data 2576 due to missing loss information\n",
      "Skip training data 2577 due to missing loss information\n",
      "Skip training data 2578 due to missing loss information\n",
      "Skip training data 2579 due to missing loss information\n",
      "Skip training data 2580 due to missing loss information\n",
      "Skip training data 2581 due to missing loss information\n",
      "Skip training data 2582 due to missing loss information\n",
      "Skip training data 2583 due to missing loss information\n",
      "Skip training data 2584 due to missing loss information\n",
      "Skip training data 2585 due to missing loss information\n",
      "Skip training data 2586 due to missing loss information\n",
      "Skip training data 2587 due to missing loss information\n",
      "Skip training data 2588 due to missing loss information\n",
      "Skip training data 2589 due to missing loss information\n",
      "Skip training data 2590 due to missing loss information\n",
      "Skip training data 2591 due to missing loss information\n",
      "Skip training data 2592 due to missing loss information\n",
      "Skip training data 2593 due to missing loss information\n",
      "Skip training data 2594 due to missing loss information\n",
      "Skip training data 2595 due to missing loss information\n",
      "Skip training data 2596 due to missing loss information\n",
      "Skip training data 2597 due to missing loss information\n",
      "Skip training data 2598 due to missing loss information\n",
      "Skip training data 2599 due to missing loss information\n",
      "Skip training data 2600 due to missing loss information\n",
      "Skip training data 2601 due to missing loss information\n",
      "Skip training data 2602 due to missing loss information\n",
      "Skip training data 2603 due to missing loss information\n",
      "Skip training data 2604 due to missing loss information\n",
      "Skip training data 2605 due to missing loss information\n",
      "Skip training data 2606 due to missing loss information\n",
      "Skip training data 2607 due to missing loss information\n",
      "Skip training data 2608 due to missing loss information\n",
      "Skip training data 2609 due to missing loss information\n",
      "Skip training data 2610 due to missing loss information\n",
      "Skip training data 2611 due to missing loss information\n",
      "Skip training data 2612 due to missing loss information\n",
      "Skip training data 2613 due to missing loss information\n",
      "Skip training data 2614 due to missing loss information\n",
      "Skip training data 2615 due to missing loss information\n",
      "Skip training data 2616 due to missing loss information\n",
      "Skip training data 2617 due to missing loss information\n",
      "Skip training data 2618 due to missing loss information\n",
      "Skip training data 2619 due to missing loss information\n",
      "Skip training data 2620 due to missing loss information\n",
      "Skip training data 2621 due to missing loss information\n",
      "Skip training data 2622 due to missing loss information\n",
      "Skip training data 2623 due to missing loss information\n",
      "Skip training data 2624 due to missing loss information\n",
      "Skip training data 2625 due to missing loss information\n",
      "Skip training data 2626 due to missing loss information\n",
      "Skip training data 2627 due to missing loss information\n",
      "Skip training data 2628 due to missing loss information\n",
      "Skip training data 2629 due to missing loss information\n",
      "Skip training data 2630 due to missing loss information\n",
      "Skip training data 2631 due to missing loss information\n",
      "Skip training data 2632 due to missing loss information\n",
      "Skip training data 2633 due to missing loss information\n",
      "Skip training data 2634 due to missing loss information\n",
      "Skip training data 2635 due to missing loss information\n",
      "Skip training data 2636 due to missing loss information\n",
      "Skip training data 2637 due to missing loss information\n",
      "Skip training data 2638 due to missing loss information\n",
      "Skip training data 2639 due to missing loss information\n",
      "Skip training data 2640 due to missing loss information\n",
      "Skip training data 2641 due to missing loss information\n",
      "Skip training data 2642 due to missing loss information\n",
      "Skip training data 2643 due to missing loss information\n",
      "Skip training data 2644 due to missing loss information\n",
      "Skip training data 2645 due to missing loss information\n",
      "Skip training data 2646 due to missing loss information\n",
      "Skip training data 2647 due to missing loss information\n",
      "Skip training data 2648 due to missing loss information\n",
      "Skip training data 2649 due to missing loss information\n",
      "Skip training data 2650 due to missing loss information\n",
      "Skip training data 2651 due to missing loss information\n",
      "Skip training data 2652 due to missing loss information\n",
      "Skip training data 2653 due to missing loss information\n",
      "Skip training data 2654 due to missing loss information\n",
      "Skip training data 2655 due to missing loss information\n",
      "Skip training data 2656 due to missing loss information\n",
      "Skip training data 2657 due to missing loss information\n",
      "Skip training data 2658 due to missing loss information\n",
      "Skip training data 2659 due to missing loss information\n",
      "Skip training data 2660 due to missing loss information\n",
      "Skip training data 2661 due to missing loss information\n",
      "Skip training data 2662 due to missing loss information\n",
      "Skip training data 2663 due to missing loss information\n",
      "Skip training data 2664 due to missing loss information\n",
      "Skip training data 2665 due to missing loss information\n",
      "Skip training data 2666 due to missing loss information\n",
      "Skip training data 2667 due to missing loss information\n",
      "Skip training data 2668 due to missing loss information\n",
      "Skip training data 2669 due to missing loss information\n",
      "Skip training data 2670 due to missing loss information\n",
      "Skip training data 2671 due to missing loss information\n",
      "Skip training data 2672 due to missing loss information\n",
      "Skip training data 2673 due to missing loss information\n",
      "Skip training data 2674 due to missing loss information\n",
      "Skip training data 2675 due to missing loss information\n",
      "Skip training data 2676 due to missing loss information\n",
      "Skip training data 2677 due to missing loss information\n",
      "Skip training data 2678 due to missing loss information\n",
      "Skip training data 2679 due to missing loss information\n",
      "Skip training data 2680 due to missing loss information\n",
      "Skip training data 2681 due to missing loss information\n",
      "Skip training data 2682 due to missing loss information\n",
      "Skip training data 2683 due to missing loss information\n",
      "Skip training data 2684 due to missing loss information\n",
      "Skip training data 2685 due to missing loss information\n",
      "Skip training data 2686 due to missing loss information\n",
      "Skip training data 2687 due to missing loss information\n",
      "Skip training data 2688 due to missing loss information\n",
      "Skip training data 2689 due to missing loss information\n",
      "Skip training data 2690 due to missing loss information\n",
      "Skip training data 2691 due to missing loss information\n",
      "Skip training data 2692 due to missing loss information\n",
      "Skip training data 2693 due to missing loss information\n",
      "Skip training data 2694 due to missing loss information\n",
      "Skip training data 2695 due to missing loss information\n",
      "Skip training data 2696 due to missing loss information\n",
      "Skip training data 2697 due to missing loss information\n",
      "Skip training data 2698 due to missing loss information\n",
      "Skip training data 2699 due to missing loss information\n",
      "Skip training data 2700 due to missing loss information\n",
      "Skip training data 2701 due to missing loss information\n",
      "Skip training data 2702 due to missing loss information\n",
      "Skip training data 2703 due to missing loss information\n",
      "Skip training data 2704 due to missing loss information\n",
      "Skip training data 2705 due to missing loss information\n",
      "Skip training data 2706 due to missing loss information\n",
      "Skip training data 2707 due to missing loss information\n",
      "Skip training data 2708 due to missing loss information\n",
      "Skip training data 2709 due to missing loss information\n",
      "Skip training data 2710 due to missing loss information\n",
      "Skip training data 2711 due to missing loss information\n",
      "Skip training data 2712 due to missing loss information\n",
      "Skip training data 2713 due to missing loss information\n",
      "Skip training data 2714 due to missing loss information\n",
      "Skip training data 2715 due to missing loss information\n",
      "Skip training data 2716 due to missing loss information\n",
      "Skip training data 2717 due to missing loss information\n",
      "Skip training data 2718 due to missing loss information\n",
      "Skip training data 2719 due to missing loss information\n",
      "Skip training data 2720 due to missing loss information\n",
      "Skip training data 2721 due to missing loss information\n",
      "Skip training data 2722 due to missing loss information\n",
      "Skip training data 2723 due to missing loss information\n",
      "Skip training data 2724 due to missing loss information\n",
      "Skip training data 2725 due to missing loss information\n",
      "Skip training data 2726 due to missing loss information\n",
      "Skip training data 2727 due to missing loss information\n",
      "Skip training data 2728 due to missing loss information\n",
      "Skip training data 2729 due to missing loss information\n",
      "Skip training data 2730 due to missing loss information\n",
      "Skip training data 2731 due to missing loss information\n",
      "Skip training data 2732 due to missing loss information\n",
      "Skip training data 2733 due to missing loss information\n",
      "Skip training data 2734 due to missing loss information\n",
      "Skip training data 2735 due to missing loss information\n",
      "Skip training data 2736 due to missing loss information\n",
      "Skip training data 2737 due to missing loss information\n",
      "Skip training data 2738 due to missing loss information\n",
      "Skip training data 2739 due to missing loss information\n",
      "Skip training data 2740 due to missing loss information\n",
      "Skip training data 2741 due to missing loss information\n",
      "Skip training data 2742 due to missing loss information\n",
      "Skip training data 2743 due to missing loss information\n",
      "Skip training data 2744 due to missing loss information\n",
      "Skip training data 2745 due to missing loss information\n",
      "Skip training data 2746 due to missing loss information\n",
      "Skip training data 2747 due to missing loss information\n",
      "Skip training data 2748 due to missing loss information\n",
      "Skip training data 2749 due to missing loss information\n",
      "Skip training data 2750 due to missing loss information\n",
      "Skip training data 2751 due to missing loss information\n",
      "Skip training data 2752 due to missing loss information\n",
      "Skip training data 2753 due to missing loss information\n",
      "Skip training data 2754 due to missing loss information\n",
      "Skip training data 2755 due to missing loss information\n",
      "Skip training data 2756 due to missing loss information\n",
      "Skip training data 2757 due to missing loss information\n",
      "Skip training data 2758 due to missing loss information\n",
      "Skip training data 2759 due to missing loss information\n",
      "Skip training data 2760 due to missing loss information\n",
      "Skip training data 2761 due to missing loss information\n",
      "Skip training data 2762 due to missing loss information\n",
      "Skip training data 2763 due to missing loss information\n",
      "Skip training data 2764 due to missing loss information\n",
      "Skip training data 2765 due to missing loss information\n",
      "Skip training data 2766 due to missing loss information\n",
      "Skip training data 2767 due to missing loss information\n",
      "Skip training data 2768 due to missing loss information\n",
      "Skip training data 2769 due to missing loss information\n",
      "Skip training data 2770 due to missing loss information\n",
      "Skip training data 2771 due to missing loss information\n",
      "Skip training data 2772 due to missing loss information\n",
      "Skip training data 2773 due to missing loss information\n",
      "Skip training data 2774 due to missing loss information\n",
      "Skip training data 2775 due to missing loss information\n",
      "Skip training data 2776 due to missing loss information\n",
      "Skip training data 2777 due to missing loss information\n",
      "Skip training data 2778 due to missing loss information\n",
      "Skip training data 2779 due to missing loss information\n",
      "Skip training data 2780 due to missing loss information\n",
      "Skip training data 2781 due to missing loss information\n",
      "Skip training data 2782 due to missing loss information\n",
      "Skip training data 2783 due to missing loss information\n",
      "Skip training data 2784 due to missing loss information\n",
      "Skip training data 2785 due to missing loss information\n",
      "Skip training data 2786 due to missing loss information\n",
      "Skip training data 2787 due to missing loss information\n",
      "Skip training data 2788 due to missing loss information\n",
      "Skip training data 2789 due to missing loss information\n",
      "Skip training data 2790 due to missing loss information\n",
      "Skip training data 2791 due to missing loss information\n",
      "Skip training data 2792 due to missing loss information\n",
      "Skip training data 2793 due to missing loss information\n",
      "Skip training data 2794 due to missing loss information\n",
      "Skip training data 2795 due to missing loss information\n",
      "Skip training data 2796 due to missing loss information\n",
      "Skip training data 2797 due to missing loss information\n",
      "Skip training data 2798 due to missing loss information\n",
      "Skip training data 2799 due to missing loss information\n",
      "Skip training data 2800 due to missing loss information\n",
      "Skip training data 2801 due to missing loss information\n",
      "Skip training data 2802 due to missing loss information\n",
      "Skip training data 2803 due to missing loss information\n",
      "Skip training data 2804 due to missing loss information\n",
      "Skip training data 2805 due to missing loss information\n",
      "Skip training data 2806 due to missing loss information\n",
      "Skip training data 2807 due to missing loss information\n",
      "Skip training data 2808 due to missing loss information\n",
      "Skip training data 2809 due to missing loss information\n",
      "Skip training data 2810 due to missing loss information\n",
      "Skip training data 2811 due to missing loss information\n",
      "Skip training data 2812 due to missing loss information\n",
      "Skip training data 2813 due to missing loss information\n",
      "Skip training data 2814 due to missing loss information\n",
      "Skip training data 2815 due to missing loss information\n",
      "Skip training data 2816 due to missing loss information\n",
      "Skip training data 2817 due to missing loss information\n",
      "Skip training data 2818 due to missing loss information\n",
      "Skip training data 2819 due to missing loss information\n",
      "Skip training data 2820 due to missing loss information\n",
      "Skip training data 2821 due to missing loss information\n",
      "Skip training data 2822 due to missing loss information\n",
      "Skip training data 2823 due to missing loss information\n",
      "Skip training data 2824 due to missing loss information\n",
      "Skip training data 2825 due to missing loss information\n",
      "Skip training data 2826 due to missing loss information\n",
      "Skip training data 2827 due to missing loss information\n",
      "Skip training data 2828 due to missing loss information\n",
      "Skip training data 2829 due to missing loss information\n",
      "Skip training data 2830 due to missing loss information\n",
      "Skip training data 2831 due to missing loss information\n",
      "Skip training data 2832 due to missing loss information\n",
      "Skip training data 2833 due to missing loss information\n",
      "Skip training data 2834 due to missing loss information\n",
      "Skip training data 2835 due to missing loss information\n",
      "Skip training data 2836 due to missing loss information\n",
      "Skip training data 2837 due to missing loss information\n",
      "Skip training data 2838 due to missing loss information\n",
      "Skip training data 2839 due to missing loss information\n",
      "Skip training data 2840 due to missing loss information\n",
      "Skip training data 2841 due to missing loss information\n",
      "Skip training data 2842 due to missing loss information\n",
      "Skip training data 2843 due to missing loss information\n",
      "Skip training data 2844 due to missing loss information\n",
      "Skip training data 2845 due to missing loss information\n",
      "Skip training data 2846 due to missing loss information\n",
      "Skip training data 2847 due to missing loss information\n",
      "Skip training data 2848 due to missing loss information\n",
      "Skip training data 2849 due to missing loss information\n",
      "Skip training data 2850 due to missing loss information\n",
      "Skip training data 2851 due to missing loss information\n",
      "Skip training data 2852 due to missing loss information\n",
      "Skip training data 2853 due to missing loss information\n",
      "Skip training data 2854 due to missing loss information\n",
      "Skip training data 2855 due to missing loss information\n",
      "Skip training data 2856 due to missing loss information\n",
      "Skip training data 2857 due to missing loss information\n",
      "Skip training data 2858 due to missing loss information\n",
      "Skip training data 2859 due to missing loss information\n",
      "Skip training data 2860 due to missing loss information\n",
      "Skip training data 2861 due to missing loss information\n",
      "Skip training data 2862 due to missing loss information\n",
      "Skip training data 2863 due to missing loss information\n",
      "Skip training data 2864 due to missing loss information\n",
      "Skip training data 2865 due to missing loss information\n",
      "Skip training data 2866 due to missing loss information\n",
      "Skip training data 2867 due to missing loss information\n",
      "Skip training data 2868 due to missing loss information\n",
      "Skip training data 2869 due to missing loss information\n",
      "Skip training data 2870 due to missing loss information\n",
      "Skip training data 2871 due to missing loss information\n",
      "Skip training data 2872 due to missing loss information\n",
      "Skip training data 2873 due to missing loss information\n",
      "Skip training data 2874 due to missing loss information\n",
      "Skip training data 2875 due to missing loss information\n",
      "Skip training data 2876 due to missing loss information\n",
      "Skip training data 2877 due to missing loss information\n",
      "Skip training data 2878 due to missing loss information\n",
      "Skip training data 2879 due to missing loss information\n",
      "Skip training data 2880 due to missing loss information\n",
      "Skip training data 2881 due to missing loss information\n",
      "Skip training data 2882 due to missing loss information\n",
      "Skip training data 2883 due to missing loss information\n",
      "Skip training data 2884 due to missing loss information\n",
      "Skip training data 2885 due to missing loss information\n",
      "Skip training data 2886 due to missing loss information\n",
      "Skip training data 2887 due to missing loss information\n",
      "Skip training data 2888 due to missing loss information\n",
      "Skip training data 2889 due to missing loss information\n",
      "Skip training data 2890 due to missing loss information\n",
      "Skip training data 2891 due to missing loss information\n",
      "Skip training data 2892 due to missing loss information\n",
      "Skip training data 2893 due to missing loss information\n",
      "Skip training data 2894 due to missing loss information\n",
      "Skip training data 2895 due to missing loss information\n",
      "Skip training data 2896 due to missing loss information\n",
      "Skip training data 2897 due to missing loss information\n",
      "Skip training data 2898 due to missing loss information\n",
      "Skip training data 2899 due to missing loss information\n",
      "Skip training data 2900 due to missing loss information\n",
      "Skip training data 2901 due to missing loss information\n",
      "Skip training data 2902 due to missing loss information\n",
      "Skip training data 2903 due to missing loss information\n",
      "Skip training data 2904 due to missing loss information\n",
      "Skip training data 2905 due to missing loss information\n",
      "Skip training data 2906 due to missing loss information\n",
      "Skip training data 2907 due to missing loss information\n",
      "Skip training data 2908 due to missing loss information\n",
      "Skip training data 2909 due to missing loss information\n",
      "Skip training data 2910 due to missing loss information\n",
      "Skip training data 2911 due to missing loss information\n",
      "Skip training data 2912 due to missing loss information\n",
      "Skip training data 2913 due to missing loss information\n",
      "Skip training data 2914 due to missing loss information\n",
      "Skip training data 2915 due to missing loss information\n",
      "Skip training data 2916 due to missing loss information\n",
      "Skip training data 2917 due to missing loss information\n",
      "Skip training data 2918 due to missing loss information\n",
      "Skip training data 2919 due to missing loss information\n",
      "Skip training data 2920 due to missing loss information\n",
      "Skip training data 2921 due to missing loss information\n",
      "Skip training data 2922 due to missing loss information\n",
      "Skip training data 2923 due to missing loss information\n",
      "Skip training data 2924 due to missing loss information\n",
      "Skip training data 2925 due to missing loss information\n",
      "Skip training data 2926 due to missing loss information\n",
      "Skip training data 2927 due to missing loss information\n",
      "Skip training data 2928 due to missing loss information\n",
      "Skip training data 2929 due to missing loss information\n",
      "Skip training data 2930 due to missing loss information\n",
      "Skip training data 2931 due to missing loss information\n",
      "Skip training data 2932 due to missing loss information\n",
      "Skip training data 2933 due to missing loss information\n",
      "Skip training data 2934 due to missing loss information\n",
      "Skip training data 2935 due to missing loss information\n",
      "Skip training data 2936 due to missing loss information\n",
      "Skip training data 2937 due to missing loss information\n",
      "Skip training data 2938 due to missing loss information\n",
      "Skip training data 2939 due to missing loss information\n",
      "Skip training data 2940 due to missing loss information\n",
      "Skip training data 2941 due to missing loss information\n",
      "Skip training data 2942 due to missing loss information\n",
      "Skip training data 2943 due to missing loss information\n",
      "Skip training data 2944 due to missing loss information\n",
      "Skip training data 2945 due to missing loss information\n",
      "Skip training data 2946 due to missing loss information\n",
      "Skip training data 2947 due to missing loss information\n",
      "Skip training data 2948 due to missing loss information\n",
      "Skip training data 2949 due to missing loss information\n",
      "Skip training data 2950 due to missing loss information\n",
      "Skip training data 2951 due to missing loss information\n",
      "Skip training data 2952 due to missing loss information\n",
      "Skip training data 2953 due to missing loss information\n",
      "Skip training data 2954 due to missing loss information\n",
      "Skip training data 2955 due to missing loss information\n",
      "Skip training data 2956 due to missing loss information\n",
      "Skip training data 2957 due to missing loss information\n",
      "Skip training data 2958 due to missing loss information\n",
      "Skip training data 2959 due to missing loss information\n",
      "Skip training data 2960 due to missing loss information\n",
      "Skip training data 2961 due to missing loss information\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip training data 2962 due to missing loss information\n",
      "Skip training data 2963 due to missing loss information\n",
      "Skip training data 2964 due to missing loss information\n",
      "Skip training data 2965 due to missing loss information\n",
      "Skip training data 2966 due to missing loss information\n",
      "Skip training data 2967 due to missing loss information\n",
      "Skip training data 2968 due to missing loss information\n",
      "Skip training data 2969 due to missing loss information\n",
      "Skip training data 2970 due to missing loss information\n",
      "Skip training data 2971 due to missing loss information\n",
      "Skip training data 2972 due to missing loss information\n",
      "Skip training data 2973 due to missing loss information\n",
      "Skip training data 2974 due to missing loss information\n",
      "Skip training data 2975 due to missing loss information\n",
      "Skip training data 2976 due to missing loss information\n",
      "Skip training data 2977 due to missing loss information\n",
      "Skip training data 2978 due to missing loss information\n",
      "Skip training data 2979 due to missing loss information\n",
      "Skip training data 2980 due to missing loss information\n",
      "Skip training data 2981 due to missing loss information\n",
      "Skip training data 2982 due to missing loss information\n",
      "Skip training data 2983 due to missing loss information\n",
      "Skip training data 2984 due to missing loss information\n",
      "Skip training data 2985 due to missing loss information\n",
      "Skip training data 2986 due to missing loss information\n",
      "Skip training data 2987 due to missing loss information\n",
      "Skip training data 2988 due to missing loss information\n",
      "Skip training data 2989 due to missing loss information\n",
      "Skip training data 2990 due to missing loss information\n",
      "Skip training data 2991 due to missing loss information\n",
      "Skip training data 2992 due to missing loss information\n",
      "Skip training data 2993 due to missing loss information\n",
      "Skip training data 2994 due to missing loss information\n",
      "Skip training data 2995 due to missing loss information\n",
      "Skip training data 2996 due to missing loss information\n",
      "Skip training data 2997 due to missing loss information\n",
      "Skip training data 2998 due to missing loss information\n",
      "Skip training data 2999 due to missing loss information\n",
      "Skip training data 3000 due to missing loss information\n",
      "Skip training data 3001 due to missing loss information\n",
      "Skip training data 3002 due to missing loss information\n",
      "Skip training data 3003 due to missing loss information\n",
      "Skip training data 3004 due to missing loss information\n",
      "Skip training data 3005 due to missing loss information\n",
      "Skip training data 3006 due to missing loss information\n",
      "Skip training data 3007 due to missing loss information\n",
      "Skip training data 3008 due to missing loss information\n",
      "Skip training data 3009 due to missing loss information\n",
      "Skip training data 3010 due to missing loss information\n",
      "Skip training data 3011 due to missing loss information\n",
      "Skip training data 3012 due to missing loss information\n",
      "Skip training data 3013 due to missing loss information\n",
      "Skip training data 3014 due to missing loss information\n",
      "Skip training data 3015 due to missing loss information\n",
      "Skip training data 3016 due to missing loss information\n",
      "Skip training data 3017 due to missing loss information\n",
      "Skip training data 3018 due to missing loss information\n",
      "Skip training data 3019 due to missing loss information\n",
      "Skip training data 3020 due to missing loss information\n",
      "Skip training data 3021 due to missing loss information\n",
      "Skip training data 3022 due to missing loss information\n",
      "Skip training data 3023 due to missing loss information\n",
      "Skip training data 3024 due to missing loss information\n",
      "Skip training data 3025 due to missing loss information\n",
      "Skip training data 3026 due to missing loss information\n",
      "Skip training data 3027 due to missing loss information\n",
      "Skip training data 3028 due to missing loss information\n",
      "Skip training data 3029 due to missing loss information\n",
      "Skip training data 3030 due to missing loss information\n",
      "Skip training data 3031 due to missing loss information\n",
      "Skip training data 3032 due to missing loss information\n",
      "Skip training data 3033 due to missing loss information\n",
      "Skip training data 3034 due to missing loss information\n",
      "Skip training data 3035 due to missing loss information\n",
      "Skip training data 3036 due to missing loss information\n",
      "Skip training data 3037 due to missing loss information\n",
      "Skip training data 3038 due to missing loss information\n",
      "Skip training data 3039 due to missing loss information\n",
      "Skip training data 3040 due to missing loss information\n",
      "Skip training data 3041 due to missing loss information\n",
      "Skip training data 3042 due to missing loss information\n",
      "Skip training data 3043 due to missing loss information\n",
      "Skip training data 3044 due to missing loss information\n",
      "Skip training data 3045 due to missing loss information\n",
      "Skip training data 3046 due to missing loss information\n",
      "Skip training data 3047 due to missing loss information\n",
      "Skip training data 3048 due to missing loss information\n",
      "Skip training data 3049 due to missing loss information\n",
      "Skip training data 3050 due to missing loss information\n",
      "Skip training data 3051 due to missing loss information\n",
      "Skip training data 3052 due to missing loss information\n",
      "Skip training data 3053 due to missing loss information\n",
      "Skip training data 3054 due to missing loss information\n",
      "Skip training data 3055 due to missing loss information\n",
      "Skip training data 3056 due to missing loss information\n",
      "Skip training data 3057 due to missing loss information\n",
      "Skip training data 3058 due to missing loss information\n",
      "Skip training data 3059 due to missing loss information\n",
      "Skip training data 3060 due to missing loss information\n",
      "Skip training data 3061 due to missing loss information\n",
      "Skip training data 3062 due to missing loss information\n",
      "Skip training data 3063 due to missing loss information\n",
      "Skip training data 3064 due to missing loss information\n",
      "Skip training data 3065 due to missing loss information\n",
      "Skip training data 3066 due to missing loss information\n",
      "Skip training data 3067 due to missing loss information\n",
      "Skip training data 3068 due to missing loss information\n",
      "Skip training data 3069 due to missing loss information\n",
      "Skip training data 3070 due to missing loss information\n",
      "Skip training data 3071 due to missing loss information\n",
      "Skip training data 3072 due to missing loss information\n",
      "Skip training data 3073 due to missing loss information\n",
      "Skip training data 3074 due to missing loss information\n",
      "Skip training data 3075 due to missing loss information\n",
      "Skip training data 3076 due to missing loss information\n",
      "Skip training data 3077 due to missing loss information\n",
      "Skip training data 3078 due to missing loss information\n",
      "Skip training data 3079 due to missing loss information\n",
      "Skip training data 3080 due to missing loss information\n",
      "Skip training data 3081 due to missing loss information\n",
      "Skip training data 3082 due to missing loss information\n",
      "Skip training data 3083 due to missing loss information\n",
      "Skip training data 3084 due to missing loss information\n",
      "Skip training data 3085 due to missing loss information\n",
      "Skip training data 3086 due to missing loss information\n",
      "Skip training data 3087 due to missing loss information\n",
      "Skip training data 3088 due to missing loss information\n",
      "Skip training data 3089 due to missing loss information\n",
      "Skip training data 3090 due to missing loss information\n",
      "Skip training data 3091 due to missing loss information\n",
      "Skip training data 3092 due to missing loss information\n",
      "Skip training data 3093 due to missing loss information\n",
      "Skip training data 3094 due to missing loss information\n",
      "Skip training data 3095 due to missing loss information\n",
      "Skip training data 3096 due to missing loss information\n",
      "Skip training data 3097 due to missing loss information\n",
      "Skip training data 3098 due to missing loss information\n",
      "Skip training data 3099 due to missing loss information\n",
      "Skip training data 3100 due to missing loss information\n",
      "Skip training data 3101 due to missing loss information\n",
      "Skip training data 3102 due to missing loss information\n",
      "Skip training data 3103 due to missing loss information\n",
      "Skip training data 3104 due to missing loss information\n",
      "Skip training data 3105 due to missing loss information\n",
      "Skip training data 3106 due to missing loss information\n",
      "Skip training data 3107 due to missing loss information\n",
      "Skip training data 3108 due to missing loss information\n",
      "Skip training data 3109 due to missing loss information\n",
      "Skip training data 3110 due to missing loss information\n",
      "Skip training data 3111 due to missing loss information\n",
      "Skip training data 3112 due to missing loss information\n",
      "Skip training data 3113 due to missing loss information\n",
      "Skip training data 3114 due to missing loss information\n",
      "Skip training data 3115 due to missing loss information\n",
      "Skip training data 3116 due to missing loss information\n",
      "Skip training data 3117 due to missing loss information\n",
      "Skip training data 3118 due to missing loss information\n",
      "Skip training data 3119 due to missing loss information\n",
      "Skip training data 3120 due to missing loss information\n",
      "Skip training data 3121 due to missing loss information\n",
      "Skip training data 3122 due to missing loss information\n",
      "Skip training data 3123 due to missing loss information\n",
      "Skip training data 3124 due to missing loss information\n",
      "Skip training data 3125 due to missing loss information\n",
      "Skip training data 3126 due to missing loss information\n",
      "Skip training data 3127 due to missing loss information\n",
      "Skip training data 3128 due to missing loss information\n",
      "Skip training data 3129 due to missing loss information\n",
      "Skip training data 3130 due to missing loss information\n",
      "Skip training data 3131 due to missing loss information\n",
      "Skip training data 3132 due to missing loss information\n",
      "Skip training data 3133 due to missing loss information\n",
      "Skip training data 3134 due to missing loss information\n",
      "Skip training data 3135 due to missing loss information\n",
      "Skip training data 3136 due to missing loss information\n",
      "Skip training data 3137 due to missing loss information\n",
      "Skip training data 3138 due to missing loss information\n",
      "Skip training data 3139 due to missing loss information\n",
      "Skip training data 3140 due to missing loss information\n",
      "Skip training data 3141 due to missing loss information\n",
      "Skip training data 3142 due to missing loss information\n",
      "Skip training data 3143 due to missing loss information\n",
      "Skip training data 3144 due to missing loss information\n",
      "Skip training data 3145 due to missing loss information\n",
      "Skip training data 3146 due to missing loss information\n",
      "Skip training data 3147 due to missing loss information\n",
      "Skip training data 3148 due to missing loss information\n",
      "Skip training data 3149 due to missing loss information\n",
      "Skip training data 3150 due to missing loss information\n",
      "Skip training data 3151 due to missing loss information\n",
      "Skip training data 3152 due to missing loss information\n",
      "Skip training data 3153 due to missing loss information\n",
      "Skip training data 3154 due to missing loss information\n",
      "Skip training data 3155 due to missing loss information\n",
      "Skip training data 3156 due to missing loss information\n",
      "Skip training data 3157 due to missing loss information\n",
      "Skip training data 3158 due to missing loss information\n",
      "Skip training data 3159 due to missing loss information\n",
      "Skip training data 3160 due to missing loss information\n",
      "Skip training data 3161 due to missing loss information\n",
      "Skip training data 3162 due to missing loss information\n",
      "Skip training data 3163 due to missing loss information\n",
      "Skip training data 3164 due to missing loss information\n",
      "Skip training data 3165 due to missing loss information\n",
      "Skip training data 3166 due to missing loss information\n",
      "Skip training data 3167 due to missing loss information\n",
      "Skip training data 3168 due to missing loss information\n",
      "Skip training data 3169 due to missing loss information\n",
      "Skip training data 3170 due to missing loss information\n",
      "Skip training data 3171 due to missing loss information\n",
      "Skip training data 3172 due to missing loss information\n",
      "Skip training data 3173 due to missing loss information\n",
      "Skip training data 3174 due to missing loss information\n",
      "Skip training data 3175 due to missing loss information\n",
      "Skip training data 3176 due to missing loss information\n",
      "Skip training data 3177 due to missing loss information\n",
      "Skip training data 3178 due to missing loss information\n",
      "Skip training data 3179 due to missing loss information\n",
      "Skip training data 3180 due to missing loss information\n",
      "Skip training data 3181 due to missing loss information\n",
      "Skip training data 3182 due to missing loss information\n",
      "Skip training data 3183 due to missing loss information\n",
      "Skip training data 3184 due to missing loss information\n",
      "Skip training data 3185 due to missing loss information\n",
      "Skip training data 3186 due to missing loss information\n",
      "Skip training data 3187 due to missing loss information\n",
      "Skip training data 3188 due to missing loss information\n",
      "Skip training data 3189 due to missing loss information\n",
      "Skip training data 3190 due to missing loss information\n",
      "Skip training data 3191 due to missing loss information\n",
      "Skip training data 3192 due to missing loss information\n",
      "Skip training data 3193 due to missing loss information\n",
      "Skip training data 3194 due to missing loss information\n",
      "Skip training data 3195 due to missing loss information\n",
      "Skip training data 3196 due to missing loss information\n",
      "Skip training data 3197 due to missing loss information\n",
      "Skip training data 3198 due to missing loss information\n",
      "Skip training data 3199 due to missing loss information\n",
      "Skip training data 3200 due to missing loss information\n",
      "Skip training data 3201 due to missing loss information\n",
      "Skip training data 3202 due to missing loss information\n",
      "Skip training data 3203 due to missing loss information\n",
      "Skip training data 3204 due to missing loss information\n",
      "Skip training data 3205 due to missing loss information\n",
      "Skip training data 3206 due to missing loss information\n",
      "Skip training data 3207 due to missing loss information\n",
      "Skip training data 3208 due to missing loss information\n",
      "Skip training data 3209 due to missing loss information\n",
      "Skip training data 3210 due to missing loss information\n",
      "Skip training data 3211 due to missing loss information\n",
      "Skip training data 3212 due to missing loss information\n",
      "Skip training data 3213 due to missing loss information\n",
      "Skip training data 3214 due to missing loss information\n",
      "Skip training data 3215 due to missing loss information\n",
      "Skip training data 3216 due to missing loss information\n",
      "Skip training data 3217 due to missing loss information\n",
      "Skip training data 3218 due to missing loss information\n",
      "Skip training data 3219 due to missing loss information\n",
      "Skip training data 3220 due to missing loss information\n",
      "Skip training data 3221 due to missing loss information\n",
      "Skip training data 3222 due to missing loss information\n",
      "Skip training data 3223 due to missing loss information\n",
      "Skip training data 3224 due to missing loss information\n",
      "Skip training data 3225 due to missing loss information\n",
      "Skip training data 3226 due to missing loss information\n",
      "Skip training data 3227 due to missing loss information\n",
      "Skip training data 3228 due to missing loss information\n",
      "Skip training data 3229 due to missing loss information\n",
      "Skip training data 3230 due to missing loss information\n",
      "Skip training data 3231 due to missing loss information\n",
      "Skip training data 3232 due to missing loss information\n",
      "Skip training data 3233 due to missing loss information\n",
      "Skip training data 3234 due to missing loss information\n",
      "Skip training data 3235 due to missing loss information\n",
      "Skip training data 3236 due to missing loss information\n",
      "Skip training data 3237 due to missing loss information\n",
      "Skip training data 3238 due to missing loss information\n",
      "Skip training data 3239 due to missing loss information\n",
      "Skip training data 3240 due to missing loss information\n",
      "Skip training data 3241 due to missing loss information\n",
      "Skip training data 3242 due to missing loss information\n",
      "Skip training data 3243 due to missing loss information\n",
      "Skip training data 3244 due to missing loss information\n",
      "Skip training data 3245 due to missing loss information\n",
      "Skip training data 3246 due to missing loss information\n",
      "Skip training data 3247 due to missing loss information\n",
      "Skip training data 3248 due to missing loss information\n",
      "Skip training data 3249 due to missing loss information\n",
      "Skip training data 3250 due to missing loss information\n",
      "Skip training data 3251 due to missing loss information\n",
      "Skip training data 3252 due to missing loss information\n",
      "Skip training data 3253 due to missing loss information\n",
      "Skip training data 3254 due to missing loss information\n",
      "Skip training data 3255 due to missing loss information\n",
      "Skip training data 3256 due to missing loss information\n",
      "Skip training data 3257 due to missing loss information\n",
      "Skip training data 3258 due to missing loss information\n",
      "Skip training data 3259 due to missing loss information\n",
      "Skip training data 3260 due to missing loss information\n",
      "Skip training data 3261 due to missing loss information\n",
      "Skip training data 3262 due to missing loss information\n",
      "Skip training data 3263 due to missing loss information\n",
      "Skip training data 3264 due to missing loss information\n",
      "Skip training data 3265 due to missing loss information\n",
      "Skip training data 3266 due to missing loss information\n",
      "Skip training data 3267 due to missing loss information\n",
      "Skip training data 3268 due to missing loss information\n",
      "Skip training data 3269 due to missing loss information\n",
      "Skip training data 3270 due to missing loss information\n",
      "Skip training data 3271 due to missing loss information\n",
      "Skip training data 3272 due to missing loss information\n",
      "Skip training data 3273 due to missing loss information\n",
      "Skip training data 3274 due to missing loss information\n",
      "Skip training data 3275 due to missing loss information\n",
      "Skip training data 3276 due to missing loss information\n",
      "Skip training data 3277 due to missing loss information\n",
      "Skip training data 3278 due to missing loss information\n",
      "Skip training data 3279 due to missing loss information\n",
      "Skip training data 3280 due to missing loss information\n",
      "Skip training data 3281 due to missing loss information\n",
      "Skip training data 3282 due to missing loss information\n",
      "Skip training data 3283 due to missing loss information\n",
      "Skip training data 3284 due to missing loss information\n",
      "Skip training data 3285 due to missing loss information\n",
      "Skip training data 3286 due to missing loss information\n",
      "Skip training data 3287 due to missing loss information\n",
      "Skip training data 3288 due to missing loss information\n",
      "Skip training data 3289 due to missing loss information\n",
      "Skip training data 3290 due to missing loss information\n",
      "Skip training data 3291 due to missing loss information\n",
      "Skip training data 3292 due to missing loss information\n",
      "Skip training data 3293 due to missing loss information\n",
      "Skip training data 3294 due to missing loss information\n",
      "Skip training data 3295 due to missing loss information\n",
      "Skip training data 3296 due to missing loss information\n",
      "Skip training data 3297 due to missing loss information\n",
      "Skip training data 3298 due to missing loss information\n",
      "Skip training data 3299 due to missing loss information\n",
      "Skip training data 3300 due to missing loss information\n",
      "Skip training data 3301 due to missing loss information\n",
      "Skip training data 3302 due to missing loss information\n",
      "Skip training data 3303 due to missing loss information\n",
      "Skip training data 3304 due to missing loss information\n",
      "Skip training data 3305 due to missing loss information\n",
      "Skip training data 3306 due to missing loss information\n",
      "Skip training data 3307 due to missing loss information\n",
      "Skip training data 3308 due to missing loss information\n",
      "Skip training data 3309 due to missing loss information\n",
      "Skip training data 3310 due to missing loss information\n",
      "Skip training data 3311 due to missing loss information\n",
      "Skip training data 3312 due to missing loss information\n",
      "Skip training data 3313 due to missing loss information\n",
      "Skip training data 3314 due to missing loss information\n",
      "Skip training data 3315 due to missing loss information\n",
      "Skip training data 3316 due to missing loss information\n",
      "Skip training data 3317 due to missing loss information\n",
      "Skip training data 3318 due to missing loss information\n",
      "Skip training data 3319 due to missing loss information\n",
      "Skip training data 3320 due to missing loss information\n",
      "Skip training data 3321 due to missing loss information\n",
      "Skip training data 3322 due to missing loss information\n",
      "Skip training data 3323 due to missing loss information\n",
      "Skip training data 3324 due to missing loss information\n",
      "Skip training data 3325 due to missing loss information\n",
      "Skip training data 3326 due to missing loss information\n",
      "Skip training data 3327 due to missing loss information\n",
      "Skip training data 3328 due to missing loss information\n",
      "Skip training data 3329 due to missing loss information\n",
      "Skip training data 3330 due to missing loss information\n",
      "Skip training data 3331 due to missing loss information\n",
      "Skip training data 3332 due to missing loss information\n",
      "Skip training data 3333 due to missing loss information\n",
      "Skip training data 3334 due to missing loss information\n",
      "Skip training data 3335 due to missing loss information\n",
      "Skip training data 3336 due to missing loss information\n",
      "Skip training data 3337 due to missing loss information\n",
      "Skip training data 3338 due to missing loss information\n",
      "Skip training data 3339 due to missing loss information\n",
      "Skip training data 3340 due to missing loss information\n",
      "Skip training data 3341 due to missing loss information\n",
      "Skip training data 3342 due to missing loss information\n",
      "Skip training data 3343 due to missing loss information\n",
      "Skip training data 3344 due to missing loss information\n",
      "Skip training data 3345 due to missing loss information\n",
      "Skip training data 3346 due to missing loss information\n",
      "Skip training data 3347 due to missing loss information\n",
      "Skip training data 3348 due to missing loss information\n",
      "Skip training data 3349 due to missing loss information\n",
      "Skip training data 3350 due to missing loss information\n",
      "Skip training data 3351 due to missing loss information\n",
      "Skip training data 3352 due to missing loss information\n",
      "Skip training data 3353 due to missing loss information\n",
      "Skip training data 3354 due to missing loss information\n",
      "Skip training data 3355 due to missing loss information\n",
      "Skip training data 3356 due to missing loss information\n",
      "Skip training data 3357 due to missing loss information\n",
      "Skip training data 3358 due to missing loss information\n",
      "Skip training data 3359 due to missing loss information\n",
      "Skip training data 3360 due to missing loss information\n",
      "Skip training data 3361 due to missing loss information\n",
      "Skip training data 3362 due to missing loss information\n",
      "Skip training data 3363 due to missing loss information\n",
      "Skip training data 3364 due to missing loss information\n",
      "Skip training data 3365 due to missing loss information\n",
      "Skip training data 3366 due to missing loss information\n",
      "Skip training data 3367 due to missing loss information\n",
      "Skip training data 3368 due to missing loss information\n",
      "Skip training data 3369 due to missing loss information\n",
      "Skip training data 3370 due to missing loss information\n",
      "Skip training data 3371 due to missing loss information\n",
      "Skip training data 3372 due to missing loss information\n",
      "Skip training data 3373 due to missing loss information\n",
      "Skip training data 3374 due to missing loss information\n",
      "Skip training data 3375 due to missing loss information\n",
      "Skip training data 3376 due to missing loss information\n",
      "Skip training data 3377 due to missing loss information\n",
      "Skip training data 3378 due to missing loss information\n",
      "Skip training data 3379 due to missing loss information\n",
      "Skip training data 3380 due to missing loss information\n",
      "Skip training data 3381 due to missing loss information\n",
      "Skip training data 3382 due to missing loss information\n",
      "Skip training data 3383 due to missing loss information\n",
      "Skip training data 3384 due to missing loss information\n",
      "Skip training data 3385 due to missing loss information\n",
      "Skip training data 3386 due to missing loss information\n",
      "Skip training data 3387 due to missing loss information\n",
      "Skip training data 3388 due to missing loss information\n",
      "Skip training data 3389 due to missing loss information\n",
      "Skip training data 3390 due to missing loss information\n",
      "Skip training data 3391 due to missing loss information\n",
      "Skip training data 3392 due to missing loss information\n",
      "Skip training data 3393 due to missing loss information\n",
      "Skip training data 3394 due to missing loss information\n",
      "Skip training data 3395 due to missing loss information\n",
      "Skip training data 3396 due to missing loss information\n",
      "Skip training data 3397 due to missing loss information\n",
      "Skip training data 3398 due to missing loss information\n",
      "Skip training data 3399 due to missing loss information\n",
      "Skip training data 3400 due to missing loss information\n",
      "Skip training data 3401 due to missing loss information\n",
      "Skip training data 3402 due to missing loss information\n",
      "Skip training data 3403 due to missing loss information\n",
      "Skip training data 3404 due to missing loss information\n",
      "Skip training data 3405 due to missing loss information\n",
      "Skip training data 3406 due to missing loss information\n",
      "Skip training data 3407 due to missing loss information\n",
      "Skip training data 3408 due to missing loss information\n",
      "Skip training data 3409 due to missing loss information\n",
      "Skip training data 3410 due to missing loss information\n",
      "Skip training data 3411 due to missing loss information\n",
      "Skip training data 3412 due to missing loss information\n",
      "Skip training data 3413 due to missing loss information\n",
      "Skip training data 3414 due to missing loss information\n",
      "Skip training data 3415 due to missing loss information\n",
      "Skip training data 3416 due to missing loss information\n",
      "Skip training data 3417 due to missing loss information\n",
      "Skip training data 3418 due to missing loss information\n",
      "Skip training data 3419 due to missing loss information\n",
      "Skip training data 3420 due to missing loss information\n",
      "Skip training data 3421 due to missing loss information\n",
      "Skip training data 3422 due to missing loss information\n",
      "Skip training data 3423 due to missing loss information\n",
      "Skip training data 3424 due to missing loss information\n",
      "Skip training data 3425 due to missing loss information\n",
      "Skip training data 3426 due to missing loss information\n",
      "Skip training data 3427 due to missing loss information\n",
      "Skip training data 3428 due to missing loss information\n",
      "Skip training data 3429 due to missing loss information\n",
      "Skip training data 3430 due to missing loss information\n",
      "Skip training data 3431 due to missing loss information\n",
      "Skip training data 3432 due to missing loss information\n",
      "Skip training data 3433 due to missing loss information\n",
      "Skip training data 3434 due to missing loss information\n",
      "Skip training data 3435 due to missing loss information\n",
      "Skip training data 3436 due to missing loss information\n",
      "Skip training data 3437 due to missing loss information\n",
      "Skip training data 3438 due to missing loss information\n",
      "Skip training data 3439 due to missing loss information\n",
      "Skip training data 3440 due to missing loss information\n",
      "Skip training data 3441 due to missing loss information\n",
      "Skip training data 3442 due to missing loss information\n",
      "Skip training data 3443 due to missing loss information\n",
      "Skip training data 3444 due to missing loss information\n",
      "Skip training data 3445 due to missing loss information\n",
      "Skip training data 3446 due to missing loss information\n",
      "Skip training data 3447 due to missing loss information\n",
      "Skip training data 3448 due to missing loss information\n",
      "Skip training data 3449 due to missing loss information\n",
      "Skip training data 3450 due to missing loss information\n",
      "Skip training data 3451 due to missing loss information\n",
      "Skip training data 3452 due to missing loss information\n",
      "Skip training data 3453 due to missing loss information\n",
      "Skip training data 3454 due to missing loss information\n",
      "Skip training data 3455 due to missing loss information\n",
      "Skip training data 3456 due to missing loss information\n",
      "Skip training data 3457 due to missing loss information\n",
      "Skip training data 3458 due to missing loss information\n",
      "Skip training data 3459 due to missing loss information\n",
      "Skip training data 3460 due to missing loss information\n",
      "Skip training data 3461 due to missing loss information\n",
      "Skip training data 3462 due to missing loss information\n",
      "Skip training data 3463 due to missing loss information\n",
      "Skip training data 3464 due to missing loss information\n",
      "Skip training data 3465 due to missing loss information\n",
      "Skip training data 3466 due to missing loss information\n",
      "Skip training data 3467 due to missing loss information\n",
      "Skip training data 3468 due to missing loss information\n",
      "Skip training data 3469 due to missing loss information\n",
      "Skip training data 3470 due to missing loss information\n",
      "Skip training data 3471 due to missing loss information\n",
      "Skip training data 3472 due to missing loss information\n",
      "Skip training data 3473 due to missing loss information\n",
      "Skip training data 3474 due to missing loss information\n",
      "Skip training data 3475 due to missing loss information\n",
      "Skip training data 3476 due to missing loss information\n",
      "Skip training data 3477 due to missing loss information\n",
      "Skip training data 3478 due to missing loss information\n",
      "Skip training data 3479 due to missing loss information\n",
      "Skip training data 3480 due to missing loss information\n",
      "Skip training data 3481 due to missing loss information\n",
      "Skip training data 3482 due to missing loss information\n",
      "Skip training data 3483 due to missing loss information\n",
      "Skip training data 3484 due to missing loss information\n",
      "Skip training data 3485 due to missing loss information\n",
      "Skip training data 3486 due to missing loss information\n",
      "Skip training data 3487 due to missing loss information\n",
      "Skip training data 3488 due to missing loss information\n",
      "Skip training data 3489 due to missing loss information\n",
      "Skip training data 3490 due to missing loss information\n",
      "Skip training data 3491 due to missing loss information\n",
      "Skip training data 3492 due to missing loss information\n",
      "Skip training data 3493 due to missing loss information\n",
      "Skip training data 3494 due to missing loss information\n",
      "Skip training data 3495 due to missing loss information\n",
      "Skip training data 3496 due to missing loss information\n",
      "Skip training data 3497 due to missing loss information\n",
      "Skip training data 3498 due to missing loss information\n",
      "Skip training data 3499 due to missing loss information\n",
      "Skip training data 3500 due to missing loss information\n",
      "Skip training data 3501 due to missing loss information\n",
      "Skip training data 3502 due to missing loss information\n",
      "Skip training data 3503 due to missing loss information\n",
      "Skip training data 3504 due to missing loss information\n",
      "Skip training data 3505 due to missing loss information\n",
      "Skip training data 3506 due to missing loss information\n",
      "Skip training data 3507 due to missing loss information\n",
      "Skip training data 3508 due to missing loss information\n",
      "Skip training data 3509 due to missing loss information\n",
      "Skip training data 3510 due to missing loss information\n",
      "Skip training data 3511 due to missing loss information\n",
      "Skip training data 3512 due to missing loss information\n",
      "Skip training data 3513 due to missing loss information\n",
      "Skip training data 3514 due to missing loss information\n",
      "Skip training data 3515 due to missing loss information\n",
      "Skip training data 3516 due to missing loss information\n",
      "Skip training data 3517 due to missing loss information\n",
      "Skip training data 3518 due to missing loss information\n",
      "Skip training data 3519 due to missing loss information\n",
      "Skip training data 3520 due to missing loss information\n",
      "Skip training data 3521 due to missing loss information\n",
      "Skip training data 3522 due to missing loss information\n",
      "Skip training data 3523 due to missing loss information\n",
      "Skip training data 3524 due to missing loss information\n",
      "Skip training data 3525 due to missing loss information\n",
      "Skip training data 3526 due to missing loss information\n",
      "Skip training data 3527 due to missing loss information\n",
      "Skip training data 3528 due to missing loss information\n",
      "Skip training data 3529 due to missing loss information\n",
      "Skip training data 3530 due to missing loss information\n",
      "Skip training data 3531 due to missing loss information\n",
      "Skip training data 3532 due to missing loss information\n",
      "Skip training data 3533 due to missing loss information\n",
      "Skip training data 3534 due to missing loss information\n",
      "Skip training data 3535 due to missing loss information\n",
      "Skip training data 3536 due to missing loss information\n",
      "Skip training data 3537 due to missing loss information\n",
      "Skip training data 3538 due to missing loss information\n",
      "Skip training data 3539 due to missing loss information\n",
      "Skip training data 3540 due to missing loss information\n",
      "Skip training data 3541 due to missing loss information\n",
      "Skip training data 3542 due to missing loss information\n",
      "Skip training data 3543 due to missing loss information\n",
      "Skip training data 3544 due to missing loss information\n",
      "Skip training data 3545 due to missing loss information\n",
      "Skip training data 3546 due to missing loss information\n",
      "Skip training data 3547 due to missing loss information\n",
      "Skip training data 3548 due to missing loss information\n",
      "Skip training data 3549 due to missing loss information\n",
      "Skip training data 3550 due to missing loss information\n",
      "Skip training data 3551 due to missing loss information\n",
      "Skip training data 3552 due to missing loss information\n",
      "Skip training data 3553 due to missing loss information\n",
      "Skip training data 3554 due to missing loss information\n",
      "Skip training data 3555 due to missing loss information\n",
      "Skip training data 3556 due to missing loss information\n",
      "Skip training data 3557 due to missing loss information\n",
      "Skip training data 3558 due to missing loss information\n",
      "Skip training data 3559 due to missing loss information\n",
      "Skip training data 3560 due to missing loss information\n",
      "Skip training data 3561 due to missing loss information\n",
      "Skip training data 3562 due to missing loss information\n",
      "Skip training data 3563 due to missing loss information\n",
      "Skip training data 3564 due to missing loss information\n",
      "Skip training data 3565 due to missing loss information\n",
      "Skip training data 3566 due to missing loss information\n",
      "Skip training data 3567 due to missing loss information\n",
      "Skip training data 3568 due to missing loss information\n",
      "Skip training data 3569 due to missing loss information\n",
      "Skip training data 3570 due to missing loss information\n",
      "Skip training data 3571 due to missing loss information\n",
      "Skip training data 3572 due to missing loss information\n",
      "Skip training data 3573 due to missing loss information\n",
      "Skip training data 3574 due to missing loss information\n",
      "Skip training data 3575 due to missing loss information\n",
      "Skip training data 3576 due to missing loss information\n",
      "Skip training data 3577 due to missing loss information\n",
      "Skip training data 3578 due to missing loss information\n",
      "Skip training data 3579 due to missing loss information\n",
      "Skip training data 3580 due to missing loss information\n",
      "Skip training data 3581 due to missing loss information\n",
      "Skip training data 3582 due to missing loss information\n",
      "Skip training data 3583 due to missing loss information\n",
      "Skip training data 3584 due to missing loss information\n",
      "Skip training data 3585 due to missing loss information\n",
      "Skip training data 3586 due to missing loss information\n",
      "Skip training data 3587 due to missing loss information\n",
      "Skip training data 3588 due to missing loss information\n",
      "Skip training data 3589 due to missing loss information\n",
      "Skip training data 3590 due to missing loss information\n",
      "Skip training data 3591 due to missing loss information\n",
      "Skip training data 3592 due to missing loss information\n",
      "Skip training data 3593 due to missing loss information\n",
      "Skip training data 3594 due to missing loss information\n",
      "Skip training data 3595 due to missing loss information\n",
      "Skip training data 3596 due to missing loss information\n",
      "Skip training data 3597 due to missing loss information\n",
      "Skip training data 3598 due to missing loss information\n",
      "Skip training data 3599 due to missing loss information\n",
      "Skip training data 3600 due to missing loss information\n",
      "Skip training data 3601 due to missing loss information\n",
      "Skip training data 3602 due to missing loss information\n",
      "Skip training data 3603 due to missing loss information\n",
      "Skip training data 3604 due to missing loss information\n",
      "Skip training data 3605 due to missing loss information\n",
      "Skip training data 3606 due to missing loss information\n",
      "Skip training data 3607 due to missing loss information\n",
      "Skip training data 3608 due to missing loss information\n",
      "Skip training data 3609 due to missing loss information\n",
      "Skip training data 3610 due to missing loss information\n",
      "Skip training data 3611 due to missing loss information\n",
      "Skip training data 3612 due to missing loss information\n",
      "Skip training data 3613 due to missing loss information\n",
      "Skip training data 3614 due to missing loss information\n",
      "Skip training data 3615 due to missing loss information\n",
      "Skip training data 3616 due to missing loss information\n",
      "Skip training data 3617 due to missing loss information\n",
      "Skip training data 3618 due to missing loss information\n",
      "Skip training data 3619 due to missing loss information\n",
      "Skip training data 3620 due to missing loss information\n",
      "Skip training data 3621 due to missing loss information\n",
      "Skip training data 3622 due to missing loss information\n",
      "Skip training data 3623 due to missing loss information\n",
      "Skip training data 3624 due to missing loss information\n",
      "Skip training data 3625 due to missing loss information\n",
      "Skip training data 3626 due to missing loss information\n",
      "Skip training data 3627 due to missing loss information\n",
      "Skip training data 3628 due to missing loss information\n",
      "Skip training data 3629 due to missing loss information\n",
      "Skip training data 3630 due to missing loss information\n",
      "Skip training data 3631 due to missing loss information\n",
      "Skip training data 3632 due to missing loss information\n",
      "Skip training data 3633 due to missing loss information\n",
      "Skip training data 3634 due to missing loss information\n",
      "Skip training data 3635 due to missing loss information\n",
      "Skip training data 3636 due to missing loss information\n",
      "Skip training data 3637 due to missing loss information\n",
      "Skip training data 3638 due to missing loss information\n",
      "Skip training data 3639 due to missing loss information\n",
      "Skip training data 3640 due to missing loss information\n",
      "Skip training data 3641 due to missing loss information\n",
      "Skip training data 3642 due to missing loss information\n",
      "Skip training data 3643 due to missing loss information\n",
      "Skip training data 3644 due to missing loss information\n",
      "Skip training data 3645 due to missing loss information\n",
      "Skip training data 3646 due to missing loss information\n",
      "Skip training data 3647 due to missing loss information\n",
      "Skip training data 3648 due to missing loss information\n",
      "Skip training data 3649 due to missing loss information\n",
      "Skip training data 3650 due to missing loss information\n",
      "Skip training data 3651 due to missing loss information\n",
      "Skip training data 3652 due to missing loss information\n",
      "Skip training data 3653 due to missing loss information\n",
      "Skip training data 3654 due to missing loss information\n",
      "Skip training data 3655 due to missing loss information\n",
      "Skip training data 3656 due to missing loss information\n",
      "Skip training data 3657 due to missing loss information\n",
      "Skip training data 3658 due to missing loss information\n",
      "Skip training data 3659 due to missing loss information\n",
      "Skip training data 3660 due to missing loss information\n",
      "Skip training data 3661 due to missing loss information\n",
      "Skip training data 3662 due to missing loss information\n",
      "Skip training data 3663 due to missing loss information\n",
      "Skip training data 3664 due to missing loss information\n",
      "Skip training data 3665 due to missing loss information\n",
      "Skip training data 3666 due to missing loss information\n",
      "Skip training data 3667 due to missing loss information\n",
      "Skip training data 3668 due to missing loss information\n",
      "Skip training data 3669 due to missing loss information\n",
      "Skip training data 3670 due to missing loss information\n",
      "Skip training data 3671 due to missing loss information\n",
      "Skip training data 3672 due to missing loss information\n",
      "Skip training data 3673 due to missing loss information\n",
      "Skip training data 3674 due to missing loss information\n",
      "Skip training data 3675 due to missing loss information\n",
      "Skip training data 3676 due to missing loss information\n",
      "Skip training data 3677 due to missing loss information\n",
      "Skip training data 3678 due to missing loss information\n",
      "Skip training data 3679 due to missing loss information\n",
      "Skip training data 3680 due to missing loss information\n",
      "Skip training data 3681 due to missing loss information\n",
      "Skip training data 3682 due to missing loss information\n",
      "Skip training data 3683 due to missing loss information\n",
      "Skip training data 3684 due to missing loss information\n",
      "Skip training data 3685 due to missing loss information\n",
      "Skip training data 3686 due to missing loss information\n",
      "Skip training data 3687 due to missing loss information\n",
      "Skip training data 3688 due to missing loss information\n",
      "Skip training data 3689 due to missing loss information\n",
      "Skip training data 3690 due to missing loss information\n",
      "Skip training data 3691 due to missing loss information\n",
      "Skip training data 3692 due to missing loss information\n",
      "Skip training data 3693 due to missing loss information\n",
      "Skip training data 3694 due to missing loss information\n",
      "Skip training data 3695 due to missing loss information\n",
      "Skip training data 3696 due to missing loss information\n",
      "Skip training data 3697 due to missing loss information\n",
      "Skip training data 3698 due to missing loss information\n",
      "Skip training data 3699 due to missing loss information\n",
      "Skip training data 3700 due to missing loss information\n",
      "Skip training data 3701 due to missing loss information\n",
      "Skip training data 3702 due to missing loss information\n",
      "Skip training data 3703 due to missing loss information\n",
      "Skip training data 3704 due to missing loss information\n",
      "Skip training data 3705 due to missing loss information\n",
      "Skip training data 3706 due to missing loss information\n",
      "Skip training data 3707 due to missing loss information\n",
      "Skip training data 3708 due to missing loss information\n",
      "Skip training data 3709 due to missing loss information\n",
      "Skip training data 3710 due to missing loss information\n",
      "Skip training data 3711 due to missing loss information\n",
      "Skip training data 3712 due to missing loss information\n",
      "Skip training data 3713 due to missing loss information\n",
      "Skip training data 3714 due to missing loss information\n",
      "Skip training data 3715 due to missing loss information\n",
      "Skip training data 3716 due to missing loss information\n",
      "Skip training data 3717 due to missing loss information\n",
      "Skip training data 3718 due to missing loss information\n",
      "Skip training data 3719 due to missing loss information\n",
      "Skip training data 3720 due to missing loss information\n",
      "Skip training data 3721 due to missing loss information\n",
      "Skip training data 3722 due to missing loss information\n",
      "Skip training data 3723 due to missing loss information\n",
      "Skip training data 3724 due to missing loss information\n",
      "Skip training data 3725 due to missing loss information\n",
      "Skip training data 3726 due to missing loss information\n",
      "Skip training data 3727 due to missing loss information\n",
      "Skip training data 3728 due to missing loss information\n",
      "Skip training data 3729 due to missing loss information\n",
      "Skip training data 3730 due to missing loss information\n",
      "Skip training data 3731 due to missing loss information\n",
      "Skip training data 3732 due to missing loss information\n",
      "Skip training data 3733 due to missing loss information\n",
      "Skip training data 3734 due to missing loss information\n",
      "Skip training data 3735 due to missing loss information\n",
      "Skip training data 3736 due to missing loss information\n",
      "Skip training data 3737 due to missing loss information\n",
      "Skip training data 3738 due to missing loss information\n",
      "Skip training data 3739 due to missing loss information\n",
      "Skip training data 3740 due to missing loss information\n",
      "Skip training data 3741 due to missing loss information\n",
      "Skip training data 3742 due to missing loss information\n",
      "Skip training data 3743 due to missing loss information\n",
      "Skip training data 3744 due to missing loss information\n",
      "Skip training data 3745 due to missing loss information\n",
      "Skip training data 3746 due to missing loss information\n",
      "Skip training data 3747 due to missing loss information\n",
      "Skip training data 3748 due to missing loss information\n",
      "Skip training data 3749 due to missing loss information\n",
      "Skip training data 3750 due to missing loss information\n",
      "Skip training data 3751 due to missing loss information\n",
      "Skip training data 3752 due to missing loss information\n",
      "Skip training data 3753 due to missing loss information\n",
      "Skip training data 3754 due to missing loss information\n",
      "Skip training data 3755 due to missing loss information\n",
      "Skip training data 3756 due to missing loss information\n",
      "Skip training data 3757 due to missing loss information\n",
      "Skip training data 3758 due to missing loss information\n",
      "Skip training data 3759 due to missing loss information\n",
      "Skip training data 3760 due to missing loss information\n",
      "Skip training data 3761 due to missing loss information\n",
      "Skip training data 3762 due to missing loss information\n",
      "Skip training data 3763 due to missing loss information\n",
      "Skip training data 3764 due to missing loss information\n",
      "Skip training data 3765 due to missing loss information\n",
      "Skip training data 3766 due to missing loss information\n",
      "Skip training data 3767 due to missing loss information\n",
      "Skip training data 3768 due to missing loss information\n",
      "Skip training data 3769 due to missing loss information\n",
      "Skip training data 3770 due to missing loss information\n",
      "Skip training data 3771 due to missing loss information\n",
      "Skip training data 3772 due to missing loss information\n",
      "Skip training data 3773 due to missing loss information\n",
      "Skip training data 3774 due to missing loss information\n",
      "Skip training data 3775 due to missing loss information\n",
      "Skip training data 3776 due to missing loss information\n",
      "Skip training data 3777 due to missing loss information\n",
      "Skip training data 3778 due to missing loss information\n",
      "Skip training data 3779 due to missing loss information\n",
      "Skip training data 3780 due to missing loss information\n",
      "Skip training data 3781 due to missing loss information\n",
      "Skip training data 3782 due to missing loss information\n",
      "Skip training data 3783 due to missing loss information\n",
      "Skip training data 3784 due to missing loss information\n",
      "Skip training data 3785 due to missing loss information\n",
      "Skip training data 3786 due to missing loss information\n",
      "Skip training data 3787 due to missing loss information\n",
      "Skip training data 3788 due to missing loss information\n",
      "Skip training data 3789 due to missing loss information\n",
      "Skip training data 3790 due to missing loss information\n",
      "Skip training data 3791 due to missing loss information\n",
      "Skip training data 3792 due to missing loss information\n",
      "Skip training data 3793 due to missing loss information\n",
      "Skip training data 3794 due to missing loss information\n",
      "Skip training data 3795 due to missing loss information\n",
      "Skip training data 3796 due to missing loss information\n",
      "Skip training data 3797 due to missing loss information\n",
      "Skip training data 3798 due to missing loss information\n",
      "Skip training data 3799 due to missing loss information\n",
      "Skip training data 3800 due to missing loss information\n",
      "Skip training data 3801 due to missing loss information\n",
      "Skip training data 3802 due to missing loss information\n",
      "Skip training data 3803 due to missing loss information\n",
      "Skip training data 3804 due to missing loss information\n",
      "Skip training data 3805 due to missing loss information\n",
      "Skip training data 3806 due to missing loss information\n",
      "Skip training data 3807 due to missing loss information\n",
      "Skip training data 3808 due to missing loss information\n",
      "Skip training data 3809 due to missing loss information\n",
      "Skip training data 3810 due to missing loss information\n",
      "Skip training data 3811 due to missing loss information\n",
      "Skip training data 3812 due to missing loss information\n",
      "Skip training data 3813 due to missing loss information\n",
      "Skip training data 3814 due to missing loss information\n",
      "Skip training data 3815 due to missing loss information\n",
      "Skip training data 3816 due to missing loss information\n",
      "Skip training data 3817 due to missing loss information\n",
      "Skip training data 3818 due to missing loss information\n",
      "Skip training data 3819 due to missing loss information\n",
      "Skip training data 3820 due to missing loss information\n",
      "Skip training data 3821 due to missing loss information\n",
      "Skip training data 3822 due to missing loss information\n",
      "Skip training data 3823 due to missing loss information\n",
      "Skip training data 3824 due to missing loss information\n",
      "Skip training data 3825 due to missing loss information\n",
      "Skip training data 3826 due to missing loss information\n",
      "Skip training data 3827 due to missing loss information\n",
      "Skip training data 3828 due to missing loss information\n",
      "Skip training data 3829 due to missing loss information\n",
      "Skip training data 3830 due to missing loss information\n",
      "Skip training data 3831 due to missing loss information\n",
      "Skip training data 3832 due to missing loss information\n",
      "Skip training data 3833 due to missing loss information\n",
      "Skip training data 3834 due to missing loss information\n",
      "Skip training data 3835 due to missing loss information\n",
      "Skip training data 3836 due to missing loss information\n",
      "Skip training data 3837 due to missing loss information\n",
      "Skip training data 3838 due to missing loss information\n",
      "Skip training data 3839 due to missing loss information\n",
      "Skip training data 3840 due to missing loss information\n",
      "Skip training data 3841 due to missing loss information\n",
      "Skip training data 3842 due to missing loss information\n",
      "Skip training data 3843 due to missing loss information\n",
      "Skip training data 3844 due to missing loss information\n",
      "Skip training data 3845 due to missing loss information\n",
      "Skip training data 3846 due to missing loss information\n",
      "Skip training data 3847 due to missing loss information\n",
      "Skip training data 3848 due to missing loss information\n",
      "Skip training data 3849 due to missing loss information\n",
      "Skip training data 3850 due to missing loss information\n",
      "Skip training data 3851 due to missing loss information\n",
      "Skip training data 3852 due to missing loss information\n",
      "Skip training data 3853 due to missing loss information\n",
      "Skip training data 3854 due to missing loss information\n",
      "Skip training data 3855 due to missing loss information\n",
      "Skip training data 3856 due to missing loss information\n",
      "Skip training data 3857 due to missing loss information\n",
      "Skip training data 3858 due to missing loss information\n",
      "Skip training data 3859 due to missing loss information\n",
      "Skip training data 3860 due to missing loss information\n",
      "Skip training data 3861 due to missing loss information\n",
      "Skip training data 3862 due to missing loss information\n",
      "Skip training data 3863 due to missing loss information\n",
      "Skip training data 3864 due to missing loss information\n",
      "Skip training data 3865 due to missing loss information\n",
      "Skip training data 3866 due to missing loss information\n",
      "Skip training data 3867 due to missing loss information\n",
      "Skip training data 3868 due to missing loss information\n",
      "Skip training data 3869 due to missing loss information\n",
      "Skip training data 3870 due to missing loss information\n",
      "Skip training data 3871 due to missing loss information\n",
      "Skip training data 3872 due to missing loss information\n",
      "Skip training data 3873 due to missing loss information\n",
      "Skip training data 3874 due to missing loss information\n",
      "Skip training data 3875 due to missing loss information\n",
      "Skip training data 3876 due to missing loss information\n",
      "Skip training data 3877 due to missing loss information\n",
      "Skip training data 3878 due to missing loss information\n",
      "Skip training data 3879 due to missing loss information\n",
      "Skip training data 3880 due to missing loss information\n",
      "Skip training data 3881 due to missing loss information\n",
      "Skip training data 3882 due to missing loss information\n",
      "Skip training data 3883 due to missing loss information\n",
      "Skip training data 3884 due to missing loss information\n",
      "Skip training data 3885 due to missing loss information\n",
      "Skip training data 3886 due to missing loss information\n",
      "Skip training data 3887 due to missing loss information\n",
      "Skip training data 3888 due to missing loss information\n",
      "Skip training data 3889 due to missing loss information\n",
      "Skip training data 3890 due to missing loss information\n",
      "Skip training data 3891 due to missing loss information\n",
      "Skip training data 3892 due to missing loss information\n",
      "Skip training data 3893 due to missing loss information\n",
      "Skip training data 3894 due to missing loss information\n",
      "Skip training data 3895 due to missing loss information\n",
      "Skip training data 3896 due to missing loss information\n",
      "Skip training data 3897 due to missing loss information\n",
      "Skip training data 3898 due to missing loss information\n",
      "Skip training data 3899 due to missing loss information\n",
      "Skip training data 3900 due to missing loss information\n",
      "Skip training data 3901 due to missing loss information\n",
      "Skip training data 3902 due to missing loss information\n",
      "Skip training data 3903 due to missing loss information\n",
      "Skip training data 3904 due to missing loss information\n",
      "Skip training data 3905 due to missing loss information\n",
      "Skip training data 3906 due to missing loss information\n",
      "Skip training data 3907 due to missing loss information\n",
      "Skip training data 3908 due to missing loss information\n",
      "Skip training data 3909 due to missing loss information\n",
      "Skip training data 3910 due to missing loss information\n",
      "Skip training data 3911 due to missing loss information\n",
      "Skip training data 3912 due to missing loss information\n",
      "Skip training data 3913 due to missing loss information\n",
      "Skip training data 3914 due to missing loss information\n",
      "Skip training data 3915 due to missing loss information\n",
      "Skip training data 3916 due to missing loss information\n",
      "Skip training data 3917 due to missing loss information\n",
      "Skip training data 3918 due to missing loss information\n",
      "Skip training data 3919 due to missing loss information\n",
      "Skip training data 3920 due to missing loss information\n",
      "Skip training data 3921 due to missing loss information\n",
      "Skip training data 3922 due to missing loss information\n",
      "Skip training data 3923 due to missing loss information\n",
      "Skip training data 3924 due to missing loss information\n",
      "Skip training data 3925 due to missing loss information\n",
      "Skip training data 3926 due to missing loss information\n",
      "Skip training data 3927 due to missing loss information\n",
      "Skip training data 3928 due to missing loss information\n",
      "Skip training data 3929 due to missing loss information\n",
      "Skip training data 3930 due to missing loss information\n",
      "Skip training data 3931 due to missing loss information\n",
      "Skip training data 3932 due to missing loss information\n",
      "Skip training data 3933 due to missing loss information\n",
      "Skip training data 3934 due to missing loss information\n",
      "Skip training data 3935 due to missing loss information\n",
      "Skip training data 3936 due to missing loss information\n",
      "Skip training data 3937 due to missing loss information\n",
      "Skip training data 3938 due to missing loss information\n",
      "Skip training data 3939 due to missing loss information\n",
      "Skip training data 3940 due to missing loss information\n",
      "Skip training data 3941 due to missing loss information\n",
      "Skip training data 3942 due to missing loss information\n",
      "Skip training data 3943 due to missing loss information\n",
      "Skip training data 3944 due to missing loss information\n",
      "Skip training data 3945 due to missing loss information\n",
      "Skip training data 3946 due to missing loss information\n",
      "Skip training data 3947 due to missing loss information\n",
      "Skip training data 3948 due to missing loss information\n",
      "Skip training data 3949 due to missing loss information\n",
      "Skip training data 3950 due to missing loss information\n",
      "Skip training data 3951 due to missing loss information\n",
      "Skip training data 3952 due to missing loss information\n",
      "Skip training data 3953 due to missing loss information\n",
      "Skip training data 3954 due to missing loss information\n",
      "Skip training data 3955 due to missing loss information\n",
      "Skip training data 3956 due to missing loss information\n",
      "Skip training data 3957 due to missing loss information\n",
      "Skip training data 3958 due to missing loss information\n",
      "Skip training data 3959 due to missing loss information\n",
      "Skip training data 3960 due to missing loss information\n",
      "Skip training data 3961 due to missing loss information\n",
      "Skip training data 3962 due to missing loss information\n",
      "Skip training data 3963 due to missing loss information\n",
      "Skip training data 3964 due to missing loss information\n",
      "Skip training data 3965 due to missing loss information\n",
      "Skip training data 3966 due to missing loss information\n",
      "Skip training data 3967 due to missing loss information\n",
      "Skip training data 3968 due to missing loss information\n",
      "Skip training data 3969 due to missing loss information\n",
      "Skip training data 3970 due to missing loss information\n",
      "Skip training data 3971 due to missing loss information\n",
      "Skip training data 3972 due to missing loss information\n",
      "Skip training data 3973 due to missing loss information\n",
      "Skip training data 3974 due to missing loss information\n",
      "Skip training data 3975 due to missing loss information\n",
      "Skip training data 3976 due to missing loss information\n",
      "Skip training data 3977 due to missing loss information\n",
      "Skip training data 3978 due to missing loss information\n",
      "Skip training data 3979 due to missing loss information\n",
      "Skip training data 3980 due to missing loss information\n",
      "Skip training data 3981 due to missing loss information\n",
      "Skip training data 3982 due to missing loss information\n",
      "Skip training data 3983 due to missing loss information\n",
      "Skip training data 3984 due to missing loss information\n",
      "Skip training data 3985 due to missing loss information\n",
      "Skip training data 3986 due to missing loss information\n",
      "Skip training data 3987 due to missing loss information\n",
      "Skip training data 3988 due to missing loss information\n",
      "Skip training data 3989 due to missing loss information\n",
      "Skip training data 3990 due to missing loss information\n",
      "Skip training data 3991 due to missing loss information\n",
      "Skip training data 3992 due to missing loss information\n",
      "Skip training data 3993 due to missing loss information\n",
      "Skip training data 3994 due to missing loss information\n",
      "Skip training data 3995 due to missing loss information\n",
      "Skip training data 3996 due to missing loss information\n",
      "Skip training data 3997 due to missing loss information\n",
      "Skip training data 3998 due to missing loss information\n",
      "Skip training data 3999 due to missing loss information\n",
      "Skip training data 4000 due to missing loss information\n",
      "Skip training data 4001 due to missing loss information\n",
      "Skip training data 4002 due to missing loss information\n",
      "Skip training data 4003 due to missing loss information\n",
      "Skip training data 4004 due to missing loss information\n",
      "Skip training data 4005 due to missing loss information\n",
      "Skip training data 4006 due to missing loss information\n",
      "Skip training data 4007 due to missing loss information\n",
      "Skip training data 4008 due to missing loss information\n",
      "Skip training data 4009 due to missing loss information\n",
      "Skip training data 4010 due to missing loss information\n",
      "Skip training data 4011 due to missing loss information\n",
      "Skip training data 4012 due to missing loss information\n",
      "Skip training data 4013 due to missing loss information\n",
      "Skip training data 4014 due to missing loss information\n",
      "Skip training data 4015 due to missing loss information\n",
      "Skip training data 4016 due to missing loss information\n",
      "Skip training data 4017 due to missing loss information\n",
      "Skip training data 4018 due to missing loss information\n",
      "Skip training data 4019 due to missing loss information\n",
      "Skip training data 4020 due to missing loss information\n",
      "Skip training data 4021 due to missing loss information\n",
      "Skip training data 4022 due to missing loss information\n",
      "Skip training data 4023 due to missing loss information\n",
      "Skip training data 4024 due to missing loss information\n",
      "Skip training data 4025 due to missing loss information\n",
      "Skip training data 4026 due to missing loss information\n",
      "Skip training data 4027 due to missing loss information\n",
      "Skip training data 4028 due to missing loss information\n",
      "Skip training data 4029 due to missing loss information\n",
      "Skip training data 4030 due to missing loss information\n",
      "Skip training data 4031 due to missing loss information\n",
      "Skip training data 4032 due to missing loss information\n",
      "Skip training data 4033 due to missing loss information\n",
      "Skip training data 4034 due to missing loss information\n",
      "Skip training data 4035 due to missing loss information\n",
      "Skip training data 4036 due to missing loss information\n",
      "Skip training data 4037 due to missing loss information\n",
      "Skip training data 4038 due to missing loss information\n",
      "Skip training data 4039 due to missing loss information\n",
      "Skip training data 4040 due to missing loss information\n",
      "Skip training data 4041 due to missing loss information\n",
      "Skip training data 4042 due to missing loss information\n",
      "Skip training data 4043 due to missing loss information\n",
      "Skip training data 4044 due to missing loss information\n",
      "Skip training data 4045 due to missing loss information\n",
      "Skip training data 4046 due to missing loss information\n",
      "Skip training data 4047 due to missing loss information\n",
      "Skip training data 4048 due to missing loss information\n",
      "Skip training data 4049 due to missing loss information\n",
      "Skip training data 4050 due to missing loss information\n",
      "Skip training data 4051 due to missing loss information\n",
      "Skip training data 4052 due to missing loss information\n",
      "Skip training data 4053 due to missing loss information\n",
      "Skip training data 4054 due to missing loss information\n",
      "Skip training data 4055 due to missing loss information\n",
      "Skip training data 4056 due to missing loss information\n",
      "Skip training data 4057 due to missing loss information\n",
      "Skip training data 4058 due to missing loss information\n",
      "Skip training data 4059 due to missing loss information\n",
      "Skip training data 4060 due to missing loss information\n",
      "Skip training data 4061 due to missing loss information\n",
      "Skip training data 4062 due to missing loss information\n",
      "Skip training data 4063 due to missing loss information\n",
      "Skip training data 4064 due to missing loss information\n",
      "Skip training data 4065 due to missing loss information\n",
      "Skip training data 4066 due to missing loss information\n",
      "Skip training data 4067 due to missing loss information\n",
      "Skip training data 4068 due to missing loss information\n",
      "Skip training data 4069 due to missing loss information\n",
      "Skip training data 4070 due to missing loss information\n",
      "Skip training data 4071 due to missing loss information\n",
      "Skip training data 4072 due to missing loss information\n",
      "Skip training data 4073 due to missing loss information\n",
      "Skip training data 4074 due to missing loss information\n",
      "Skip training data 4075 due to missing loss information\n",
      "Skip training data 4076 due to missing loss information\n",
      "Skip training data 4077 due to missing loss information\n",
      "Skip training data 4078 due to missing loss information\n",
      "Skip training data 4079 due to missing loss information\n",
      "Skip training data 4080 due to missing loss information\n",
      "Skip training data 4081 due to missing loss information\n",
      "Skip training data 4082 due to missing loss information\n",
      "Skip training data 4083 due to missing loss information\n",
      "Skip training data 4084 due to missing loss information\n",
      "Skip training data 4085 due to missing loss information\n",
      "Skip training data 4086 due to missing loss information\n",
      "Skip training data 4087 due to missing loss information\n",
      "Skip training data 4088 due to missing loss information\n",
      "Skip training data 4089 due to missing loss information\n",
      "Skip training data 4090 due to missing loss information\n",
      "Skip training data 4091 due to missing loss information\n",
      "Skip training data 4092 due to missing loss information\n",
      "Skip training data 4093 due to missing loss information\n",
      "Skip training data 4094 due to missing loss information\n",
      "Skip training data 4095 due to missing loss information\n",
      "Skip training data 4096 due to missing loss information\n",
      "Skip training data 4097 due to missing loss information\n",
      "Skip training data 4098 due to missing loss information\n",
      "Skip training data 4099 due to missing loss information\n",
      "Skip training data 4100 due to missing loss information\n",
      "Skip training data 4101 due to missing loss information\n",
      "Skip training data 4102 due to missing loss information\n",
      "Skip training data 4103 due to missing loss information\n",
      "Skip training data 4104 due to missing loss information\n",
      "Skip training data 4105 due to missing loss information\n",
      "Skip training data 4106 due to missing loss information\n",
      "Skip training data 4107 due to missing loss information\n",
      "Skip training data 4108 due to missing loss information\n",
      "Skip training data 4109 due to missing loss information\n",
      "Skip training data 4110 due to missing loss information\n",
      "Skip training data 4111 due to missing loss information\n",
      "Skip training data 4112 due to missing loss information\n",
      "Skip training data 4113 due to missing loss information\n",
      "Skip training data 4114 due to missing loss information\n",
      "Skip training data 4115 due to missing loss information\n",
      "Skip training data 4116 due to missing loss information\n",
      "Skip training data 4117 due to missing loss information\n",
      "Skip training data 4118 due to missing loss information\n",
      "Skip training data 4119 due to missing loss information\n",
      "Skip training data 4120 due to missing loss information\n",
      "Skip training data 4121 due to missing loss information\n",
      "Skip training data 4122 due to missing loss information\n",
      "Skip training data 4123 due to missing loss information\n",
      "Skip training data 4124 due to missing loss information\n",
      "Skip training data 4125 due to missing loss information\n",
      "Skip training data 4126 due to missing loss information\n",
      "Skip training data 4127 due to missing loss information\n",
      "Skip training data 4128 due to missing loss information\n",
      "Skip training data 4129 due to missing loss information\n",
      "Skip training data 4130 due to missing loss information\n",
      "Skip training data 4131 due to missing loss information\n",
      "Skip training data 4132 due to missing loss information\n",
      "Skip training data 4133 due to missing loss information\n",
      "Skip training data 4134 due to missing loss information\n",
      "Skip training data 4135 due to missing loss information\n",
      "Skip training data 4136 due to missing loss information\n",
      "Skip training data 4137 due to missing loss information\n",
      "Skip training data 4138 due to missing loss information\n",
      "Skip training data 4139 due to missing loss information\n",
      "Skip training data 4140 due to missing loss information\n",
      "Skip training data 4141 due to missing loss information\n",
      "Skip training data 4142 due to missing loss information\n",
      "Skip training data 4143 due to missing loss information\n",
      "Skip training data 4144 due to missing loss information\n",
      "Skip training data 4145 due to missing loss information\n",
      "Skip training data 4146 due to missing loss information\n",
      "Skip training data 4147 due to missing loss information\n",
      "Skip training data 4148 due to missing loss information\n",
      "Skip training data 4149 due to missing loss information\n",
      "Skip training data 4150 due to missing loss information\n",
      "Skip training data 4151 due to missing loss information\n",
      "Skip training data 4152 due to missing loss information\n",
      "Skip training data 4153 due to missing loss information\n",
      "Skip training data 4154 due to missing loss information\n",
      "Skip training data 4155 due to missing loss information\n",
      "Skip training data 4156 due to missing loss information\n",
      "Skip training data 4157 due to missing loss information\n",
      "Skip training data 4158 due to missing loss information\n",
      "Skip training data 4159 due to missing loss information\n",
      "Skip training data 4160 due to missing loss information\n",
      "Skip training data 4161 due to missing loss information\n",
      "Skip training data 4162 due to missing loss information\n",
      "Skip training data 4163 due to missing loss information\n",
      "Skip training data 4164 due to missing loss information\n",
      "Skip training data 4165 due to missing loss information\n",
      "Skip training data 4166 due to missing loss information\n",
      "Skip training data 4167 due to missing loss information\n",
      "Skip training data 4168 due to missing loss information\n",
      "Skip training data 4169 due to missing loss information\n",
      "Skip training data 4170 due to missing loss information\n",
      "Skip training data 4171 due to missing loss information\n",
      "Skip training data 4172 due to missing loss information\n",
      "Skip training data 4173 due to missing loss information\n",
      "Skip training data 4174 due to missing loss information\n",
      "Skip training data 4175 due to missing loss information\n",
      "Skip training data 4176 due to missing loss information\n",
      "Skip training data 4177 due to missing loss information\n",
      "Skip training data 4178 due to missing loss information\n",
      "Skip training data 4179 due to missing loss information\n",
      "Skip training data 4180 due to missing loss information\n",
      "Skip training data 4181 due to missing loss information\n",
      "Skip training data 4182 due to missing loss information\n",
      "Skip training data 4183 due to missing loss information\n",
      "Skip training data 4184 due to missing loss information\n",
      "Skip training data 4185 due to missing loss information\n",
      "Skip training data 4186 due to missing loss information\n",
      "Skip training data 4187 due to missing loss information\n",
      "Skip training data 4188 due to missing loss information\n",
      "Skip training data 4189 due to missing loss information\n",
      "Skip training data 4190 due to missing loss information\n",
      "Skip training data 4191 due to missing loss information\n",
      "Skip training data 4192 due to missing loss information\n",
      "Skip training data 4193 due to missing loss information\n",
      "Skip training data 4194 due to missing loss information\n",
      "Skip training data 4195 due to missing loss information\n",
      "Skip training data 4196 due to missing loss information\n",
      "Skip training data 4197 due to missing loss information\n",
      "Skip training data 4198 due to missing loss information\n",
      "Skip training data 4199 due to missing loss information\n",
      "Skip training data 4200 due to missing loss information\n",
      "Skip training data 4201 due to missing loss information\n",
      "Skip training data 4202 due to missing loss information\n",
      "Skip training data 4203 due to missing loss information\n",
      "Skip training data 4204 due to missing loss information\n",
      "Skip training data 4205 due to missing loss information\n",
      "Skip training data 4206 due to missing loss information\n",
      "Skip training data 4207 due to missing loss information\n",
      "Skip training data 4208 due to missing loss information\n",
      "Skip training data 4209 due to missing loss information\n",
      "Skip training data 4210 due to missing loss information\n",
      "Skip training data 4211 due to missing loss information\n",
      "Skip training data 4212 due to missing loss information\n",
      "Skip training data 4213 due to missing loss information\n",
      "Skip training data 4214 due to missing loss information\n",
      "Skip training data 4215 due to missing loss information\n",
      "Skip training data 4216 due to missing loss information\n",
      "Skip training data 4217 due to missing loss information\n",
      "Skip training data 4218 due to missing loss information\n",
      "Skip training data 4219 due to missing loss information\n",
      "Skip training data 4220 due to missing loss information\n",
      "Skip training data 4221 due to missing loss information\n",
      "Skip training data 4222 due to missing loss information\n",
      "Skip training data 4223 due to missing loss information\n",
      "Skip training data 4224 due to missing loss information\n",
      "Skip training data 4225 due to missing loss information\n",
      "Skip training data 4226 due to missing loss information\n",
      "Skip training data 4227 due to missing loss information\n",
      "Skip training data 4228 due to missing loss information\n",
      "Skip training data 4229 due to missing loss information\n",
      "Skip training data 4230 due to missing loss information\n",
      "Skip training data 4231 due to missing loss information\n",
      "Skip training data 4232 due to missing loss information\n",
      "Skip training data 4233 due to missing loss information\n",
      "Skip training data 4234 due to missing loss information\n",
      "Skip training data 4235 due to missing loss information\n",
      "Skip training data 4236 due to missing loss information\n",
      "Skip training data 4237 due to missing loss information\n",
      "Skip training data 4238 due to missing loss information\n",
      "Skip training data 4239 due to missing loss information\n",
      "Skip training data 4240 due to missing loss information\n",
      "Skip training data 4241 due to missing loss information\n",
      "Skip training data 4242 due to missing loss information\n",
      "Skip training data 4243 due to missing loss information\n",
      "Skip training data 4244 due to missing loss information\n",
      "Skip training data 4245 due to missing loss information\n",
      "Skip training data 4246 due to missing loss information\n",
      "Skip training data 4247 due to missing loss information\n",
      "Skip training data 4248 due to missing loss information\n",
      "Skip training data 4249 due to missing loss information\n",
      "Skip training data 4250 due to missing loss information\n",
      "Skip training data 4251 due to missing loss information\n",
      "Skip training data 4252 due to missing loss information\n",
      "Skip training data 4253 due to missing loss information\n",
      "Skip training data 4254 due to missing loss information\n",
      "Skip training data 4255 due to missing loss information\n",
      "Skip training data 4256 due to missing loss information\n",
      "Skip training data 4257 due to missing loss information\n",
      "Skip training data 4258 due to missing loss information\n",
      "Skip training data 4259 due to missing loss information\n",
      "Skip training data 4260 due to missing loss information\n",
      "Skip training data 4261 due to missing loss information\n",
      "Skip training data 4262 due to missing loss information\n",
      "Skip training data 4263 due to missing loss information\n",
      "Skip training data 4264 due to missing loss information\n",
      "Skip training data 4265 due to missing loss information\n",
      "Skip training data 4266 due to missing loss information\n",
      "Skip training data 4267 due to missing loss information\n",
      "Skip training data 4268 due to missing loss information\n",
      "Skip training data 4269 due to missing loss information\n",
      "Skip training data 4270 due to missing loss information\n",
      "Skip training data 4271 due to missing loss information\n",
      "Skip training data 4272 due to missing loss information\n",
      "Skip training data 4273 due to missing loss information\n",
      "Skip training data 4274 due to missing loss information\n",
      "Skip training data 4275 due to missing loss information\n",
      "Skip training data 4276 due to missing loss information\n",
      "Skip training data 4277 due to missing loss information\n",
      "Skip training data 4278 due to missing loss information\n",
      "Skip training data 4279 due to missing loss information\n",
      "Skip training data 4280 due to missing loss information\n",
      "Skip training data 4281 due to missing loss information\n",
      "Skip training data 4282 due to missing loss information\n",
      "Skip training data 4283 due to missing loss information\n",
      "Skip training data 4284 due to missing loss information\n",
      "Skip training data 4285 due to missing loss information\n",
      "Skip training data 4286 due to missing loss information\n",
      "Skip training data 4287 due to missing loss information\n",
      "Skip training data 4288 due to missing loss information\n",
      "Skip training data 4289 due to missing loss information\n",
      "Skip training data 4290 due to missing loss information\n",
      "Skip training data 4291 due to missing loss information\n",
      "Skip training data 4292 due to missing loss information\n",
      "Skip training data 4293 due to missing loss information\n",
      "Skip training data 4294 due to missing loss information\n",
      "Skip training data 4295 due to missing loss information\n",
      "Skip training data 4296 due to missing loss information\n",
      "Skip training data 4297 due to missing loss information\n",
      "Skip training data 4298 due to missing loss information\n",
      "Skip training data 4299 due to missing loss information\n",
      "Skip training data 4300 due to missing loss information\n",
      "Skip training data 4301 due to missing loss information\n",
      "Skip training data 4302 due to missing loss information\n",
      "Skip training data 4303 due to missing loss information\n",
      "Skip training data 4304 due to missing loss information\n",
      "Skip training data 4305 due to missing loss information\n",
      "Skip training data 4306 due to missing loss information\n",
      "Skip training data 4307 due to missing loss information\n",
      "Skip training data 4308 due to missing loss information\n",
      "Skip training data 4309 due to missing loss information\n",
      "Skip training data 4310 due to missing loss information\n",
      "Skip training data 4311 due to missing loss information\n",
      "Skip training data 4312 due to missing loss information\n",
      "Skip training data 4313 due to missing loss information\n",
      "Skip training data 4314 due to missing loss information\n",
      "Skip training data 4315 due to missing loss information\n",
      "Skip training data 4316 due to missing loss information\n",
      "Skip training data 4317 due to missing loss information\n",
      "Skip training data 4318 due to missing loss information\n",
      "Skip training data 4319 due to missing loss information\n",
      "Skip training data 4320 due to missing loss information\n",
      "Skip training data 4321 due to missing loss information\n",
      "Skip training data 4322 due to missing loss information\n",
      "Skip training data 4323 due to missing loss information\n",
      "Skip training data 4324 due to missing loss information\n",
      "Skip training data 4325 due to missing loss information\n",
      "Skip training data 4326 due to missing loss information\n",
      "Skip training data 4327 due to missing loss information\n",
      "Skip training data 4328 due to missing loss information\n",
      "Skip training data 4329 due to missing loss information\n",
      "Skip training data 4330 due to missing loss information\n",
      "Skip training data 4331 due to missing loss information\n",
      "Skip training data 4332 due to missing loss information\n",
      "Skip training data 4333 due to missing loss information\n",
      "Skip training data 4334 due to missing loss information\n",
      "Skip training data 4335 due to missing loss information\n",
      "Skip training data 4336 due to missing loss information\n",
      "Skip training data 4337 due to missing loss information\n",
      "Skip training data 4338 due to missing loss information\n",
      "Skip training data 4339 due to missing loss information\n",
      "Skip training data 4340 due to missing loss information\n",
      "Skip training data 4341 due to missing loss information\n",
      "Skip training data 4342 due to missing loss information\n",
      "Skip training data 4343 due to missing loss information\n",
      "Skip training data 4344 due to missing loss information\n",
      "Skip training data 4345 due to missing loss information\n",
      "Skip training data 4346 due to missing loss information\n",
      "Skip training data 4347 due to missing loss information\n",
      "Skip training data 4348 due to missing loss information\n",
      "Skip training data 4349 due to missing loss information\n",
      "Skip training data 4350 due to missing loss information\n",
      "Skip training data 4351 due to missing loss information\n",
      "Skip training data 4352 due to missing loss information\n",
      "Skip training data 4353 due to missing loss information\n",
      "Skip training data 4354 due to missing loss information\n",
      "Skip training data 4355 due to missing loss information\n",
      "Skip training data 4356 due to missing loss information\n",
      "Skip training data 4357 due to missing loss information\n",
      "Skip training data 4358 due to missing loss information\n",
      "Skip training data 4359 due to missing loss information\n",
      "Skip training data 4360 due to missing loss information\n",
      "Skip training data 4361 due to missing loss information\n",
      "Skip training data 4362 due to missing loss information\n",
      "Skip training data 4363 due to missing loss information\n",
      "Skip training data 4364 due to missing loss information\n",
      "Skip training data 4365 due to missing loss information\n",
      "Skip training data 4366 due to missing loss information\n",
      "Skip training data 4367 due to missing loss information\n",
      "Skip training data 4368 due to missing loss information\n",
      "Skip training data 4369 due to missing loss information\n",
      "Skip training data 4370 due to missing loss information\n",
      "Skip training data 4371 due to missing loss information\n",
      "Skip training data 4372 due to missing loss information\n",
      "Skip training data 4373 due to missing loss information\n",
      "Skip training data 4374 due to missing loss information\n",
      "Skip training data 4375 due to missing loss information\n",
      "Skip training data 4376 due to missing loss information\n",
      "Skip training data 4377 due to missing loss information\n",
      "Skip training data 4378 due to missing loss information\n",
      "Skip training data 4379 due to missing loss information\n",
      "Skip training data 4380 due to missing loss information\n",
      "Skip training data 4381 due to missing loss information\n",
      "Skip training data 4382 due to missing loss information\n",
      "Skip training data 4383 due to missing loss information\n",
      "Skip training data 4384 due to missing loss information\n",
      "Skip training data 4385 due to missing loss information\n",
      "Skip training data 4386 due to missing loss information\n",
      "Skip training data 4387 due to missing loss information\n",
      "Skip training data 4388 due to missing loss information\n",
      "Skip training data 4389 due to missing loss information\n",
      "Skip training data 4390 due to missing loss information\n",
      "Skip training data 4391 due to missing loss information\n",
      "Skip training data 4392 due to missing loss information\n",
      "Skip training data 4393 due to missing loss information\n",
      "Skip training data 4394 due to missing loss information\n",
      "Skip training data 4395 due to missing loss information\n",
      "Skip training data 4396 due to missing loss information\n",
      "Skip training data 4397 due to missing loss information\n",
      "Skip training data 4398 due to missing loss information\n",
      "Skip training data 4399 due to missing loss information\n",
      "Skip training data 4400 due to missing loss information\n",
      "Skip training data 4401 due to missing loss information\n",
      "Skip training data 4402 due to missing loss information\n",
      "Skip training data 4403 due to missing loss information\n",
      "Skip training data 4404 due to missing loss information\n",
      "Skip training data 4405 due to missing loss information\n",
      "Skip training data 4406 due to missing loss information\n",
      "Skip training data 4407 due to missing loss information\n",
      "Skip training data 4408 due to missing loss information\n",
      "Skip training data 4409 due to missing loss information\n",
      "Skip training data 4410 due to missing loss information\n",
      "Skip training data 4411 due to missing loss information\n",
      "Skip training data 4412 due to missing loss information\n",
      "Skip training data 4413 due to missing loss information\n",
      "Skip training data 4414 due to missing loss information\n",
      "Skip training data 4415 due to missing loss information\n",
      "Skip training data 4416 due to missing loss information\n",
      "Skip training data 4417 due to missing loss information\n",
      "Skip training data 4418 due to missing loss information\n",
      "Skip training data 4419 due to missing loss information\n",
      "Skip training data 4420 due to missing loss information\n",
      "Skip training data 4421 due to missing loss information\n",
      "Skip training data 4422 due to missing loss information\n",
      "Skip training data 4423 due to missing loss information\n",
      "Skip training data 4424 due to missing loss information\n",
      "Skip training data 4425 due to missing loss information\n",
      "Skip training data 4426 due to missing loss information\n",
      "Skip training data 4427 due to missing loss information\n",
      "Skip training data 4428 due to missing loss information\n",
      "Skip training data 4429 due to missing loss information\n",
      "Skip training data 4430 due to missing loss information\n",
      "Skip training data 4431 due to missing loss information\n",
      "Skip training data 4432 due to missing loss information\n",
      "Skip training data 4433 due to missing loss information\n",
      "Skip training data 4434 due to missing loss information\n",
      "Skip training data 4435 due to missing loss information\n",
      "Skip training data 4436 due to missing loss information\n",
      "Skip training data 4437 due to missing loss information\n",
      "Skip training data 4438 due to missing loss information\n",
      "Skip training data 4439 due to missing loss information\n",
      "Skip training data 4440 due to missing loss information\n",
      "Skip training data 4441 due to missing loss information\n",
      "Skip training data 4442 due to missing loss information\n",
      "Skip training data 4443 due to missing loss information\n",
      "Skip training data 4444 due to missing loss information\n",
      "Skip training data 4445 due to missing loss information\n",
      "Skip training data 4446 due to missing loss information\n",
      "Skip training data 4447 due to missing loss information\n",
      "Skip training data 4448 due to missing loss information\n",
      "Skip training data 4449 due to missing loss information\n",
      "Skip training data 4450 due to missing loss information\n",
      "Skip training data 4451 due to missing loss information\n",
      "Skip training data 4452 due to missing loss information\n",
      "Skip training data 4453 due to missing loss information\n",
      "Skip training data 4454 due to missing loss information\n",
      "Skip training data 4455 due to missing loss information\n",
      "Skip training data 4456 due to missing loss information\n",
      "Skip training data 4457 due to missing loss information\n",
      "Skip training data 4458 due to missing loss information\n",
      "Skip training data 4459 due to missing loss information\n",
      "Skip training data 4460 due to missing loss information\n",
      "Skip training data 4461 due to missing loss information\n",
      "Skip training data 4462 due to missing loss information\n",
      "Skip training data 4463 due to missing loss information\n",
      "Skip training data 4464 due to missing loss information\n",
      "Skip training data 4465 due to missing loss information\n",
      "Skip training data 4466 due to missing loss information\n",
      "Skip training data 4467 due to missing loss information\n",
      "Skip training data 4468 due to missing loss information\n",
      "Skip training data 4469 due to missing loss information\n",
      "Skip training data 4470 due to missing loss information\n",
      "Skip training data 4471 due to missing loss information\n",
      "Skip training data 4472 due to missing loss information\n",
      "Skip training data 4473 due to missing loss information\n",
      "Skip training data 4474 due to missing loss information\n",
      "Skip training data 4475 due to missing loss information\n",
      "Skip training data 4476 due to missing loss information\n",
      "Skip training data 4477 due to missing loss information\n",
      "Skip training data 4478 due to missing loss information\n",
      "Skip training data 4479 due to missing loss information\n",
      "Skip training data 4480 due to missing loss information\n",
      "Skip training data 4481 due to missing loss information\n",
      "Skip training data 4482 due to missing loss information\n",
      "Skip training data 4483 due to missing loss information\n",
      "Skip training data 4484 due to missing loss information\n",
      "Skip training data 4485 due to missing loss information\n",
      "Skip training data 4486 due to missing loss information\n",
      "Skip training data 4487 due to missing loss information\n",
      "Skip training data 4488 due to missing loss information\n",
      "Skip training data 4489 due to missing loss information\n",
      "Skip training data 4490 due to missing loss information\n",
      "Skip training data 4491 due to missing loss information\n",
      "Skip training data 4492 due to missing loss information\n",
      "Skip training data 4493 due to missing loss information\n",
      "Skip training data 4494 due to missing loss information\n",
      "Skip training data 4495 due to missing loss information\n",
      "Skip training data 4496 due to missing loss information\n",
      "Skip training data 4497 due to missing loss information\n",
      "Skip training data 4498 due to missing loss information\n",
      "Skip training data 4499 due to missing loss information\n",
      "Skip training data 4500 due to missing loss information\n",
      "Skip training data 4501 due to missing loss information\n",
      "Skip training data 4502 due to missing loss information\n",
      "Skip training data 4503 due to missing loss information\n",
      "Skip training data 4504 due to missing loss information\n",
      "Skip training data 4505 due to missing loss information\n",
      "Skip training data 4506 due to missing loss information\n",
      "Skip training data 4507 due to missing loss information\n",
      "Skip training data 4508 due to missing loss information\n",
      "Skip training data 4509 due to missing loss information\n",
      "Skip training data 4510 due to missing loss information\n",
      "Skip training data 4511 due to missing loss information\n",
      "Skip training data 4512 due to missing loss information\n",
      "Skip training data 4513 due to missing loss information\n",
      "Skip training data 4514 due to missing loss information\n",
      "Skip training data 4515 due to missing loss information\n",
      "Skip training data 4516 due to missing loss information\n",
      "Skip training data 4517 due to missing loss information\n",
      "Skip training data 4518 due to missing loss information\n",
      "Skip training data 4519 due to missing loss information\n",
      "Skip training data 4520 due to missing loss information\n",
      "Skip training data 4521 due to missing loss information\n",
      "Skip training data 4522 due to missing loss information\n",
      "Skip training data 4523 due to missing loss information\n",
      "Skip training data 4524 due to missing loss information\n",
      "Skip training data 4525 due to missing loss information\n",
      "Skip training data 4526 due to missing loss information\n",
      "Skip training data 4527 due to missing loss information\n",
      "Skip training data 4528 due to missing loss information\n",
      "Skip training data 4529 due to missing loss information\n",
      "Skip training data 4530 due to missing loss information\n",
      "Skip training data 4531 due to missing loss information\n",
      "Skip training data 4532 due to missing loss information\n",
      "Skip training data 4533 due to missing loss information\n",
      "Skip training data 4534 due to missing loss information\n",
      "Skip training data 4535 due to missing loss information\n",
      "Skip training data 4536 due to missing loss information\n",
      "Skip training data 4537 due to missing loss information\n",
      "Skip training data 4538 due to missing loss information\n",
      "Skip training data 4539 due to missing loss information\n",
      "Skip training data 4540 due to missing loss information\n",
      "Skip training data 4541 due to missing loss information\n",
      "Skip training data 4542 due to missing loss information\n",
      "Skip training data 4543 due to missing loss information\n",
      "Skip training data 4544 due to missing loss information\n",
      "Skip training data 4545 due to missing loss information\n",
      "Skip training data 4546 due to missing loss information\n",
      "Skip training data 4547 due to missing loss information\n",
      "Skip training data 4548 due to missing loss information\n",
      "Skip training data 4549 due to missing loss information\n",
      "Skip training data 4550 due to missing loss information\n",
      "Skip training data 4551 due to missing loss information\n",
      "Skip training data 4552 due to missing loss information\n",
      "Skip training data 4553 due to missing loss information\n",
      "Skip training data 4554 due to missing loss information\n",
      "Skip training data 4555 due to missing loss information\n",
      "Skip training data 4556 due to missing loss information\n",
      "Skip training data 4557 due to missing loss information\n",
      "Skip training data 4558 due to missing loss information\n",
      "Skip training data 4559 due to missing loss information\n",
      "Skip training data 4560 due to missing loss information\n",
      "Skip training data 4561 due to missing loss information\n",
      "Skip training data 4562 due to missing loss information\n",
      "Skip training data 4563 due to missing loss information\n",
      "Skip training data 4564 due to missing loss information\n",
      "Skip training data 4565 due to missing loss information\n",
      "Skip training data 4566 due to missing loss information\n",
      "Skip training data 4567 due to missing loss information\n",
      "Skip training data 4568 due to missing loss information\n",
      "Skip training data 4569 due to missing loss information\n",
      "Skip training data 4570 due to missing loss information\n",
      "Skip training data 4571 due to missing loss information\n",
      "Skip training data 4572 due to missing loss information\n",
      "Skip training data 4573 due to missing loss information\n",
      "Skip training data 4574 due to missing loss information\n",
      "Skip training data 4575 due to missing loss information\n",
      "Skip training data 4576 due to missing loss information\n",
      "Skip training data 4577 due to missing loss information\n",
      "Skip training data 4578 due to missing loss information\n",
      "Skip training data 4579 due to missing loss information\n",
      "Skip training data 4580 due to missing loss information\n",
      "Skip training data 4581 due to missing loss information\n",
      "Skip training data 4582 due to missing loss information\n",
      "Skip training data 4583 due to missing loss information\n",
      "Skip training data 4584 due to missing loss information\n",
      "Skip training data 4585 due to missing loss information\n",
      "Skip training data 4586 due to missing loss information\n",
      "Skip training data 4587 due to missing loss information\n",
      "Skip training data 4588 due to missing loss information\n",
      "Skip training data 4589 due to missing loss information\n",
      "Skip training data 4590 due to missing loss information\n",
      "Skip training data 4591 due to missing loss information\n",
      "Skip training data 4592 due to missing loss information\n",
      "Skip training data 4593 due to missing loss information\n",
      "Skip training data 4594 due to missing loss information\n",
      "Skip training data 4595 due to missing loss information\n",
      "Skip training data 4596 due to missing loss information\n",
      "Skip training data 4597 due to missing loss information\n",
      "Skip training data 4598 due to missing loss information\n",
      "Skip training data 4599 due to missing loss information\n",
      "Skip training data 4600 due to missing loss information\n",
      "Skip training data 4601 due to missing loss information\n",
      "Skip training data 4602 due to missing loss information\n",
      "Skip training data 4603 due to missing loss information\n",
      "Skip training data 4604 due to missing loss information\n",
      "Skip training data 4605 due to missing loss information\n",
      "Skip training data 4606 due to missing loss information\n",
      "Skip training data 4607 due to missing loss information\n",
      "Skip training data 4608 due to missing loss information\n",
      "Skip training data 4609 due to missing loss information\n",
      "Skip training data 4610 due to missing loss information\n",
      "Skip training data 4611 due to missing loss information\n",
      "Skip training data 4612 due to missing loss information\n",
      "Skip training data 4613 due to missing loss information\n",
      "Skip training data 4614 due to missing loss information\n",
      "Skip training data 4615 due to missing loss information\n",
      "Skip training data 4616 due to missing loss information\n",
      "Skip training data 4617 due to missing loss information\n",
      "Skip training data 4618 due to missing loss information\n",
      "Skip training data 4619 due to missing loss information\n",
      "Skip training data 4620 due to missing loss information\n",
      "Skip training data 4621 due to missing loss information\n",
      "Skip training data 4622 due to missing loss information\n",
      "Skip training data 4623 due to missing loss information\n",
      "Skip training data 4624 due to missing loss information\n",
      "Skip training data 4625 due to missing loss information\n",
      "Skip training data 4626 due to missing loss information\n",
      "Skip training data 4627 due to missing loss information\n",
      "Skip training data 4628 due to missing loss information\n",
      "Skip training data 4629 due to missing loss information\n",
      "Skip training data 4630 due to missing loss information\n",
      "Skip training data 4631 due to missing loss information\n",
      "Skip training data 4632 due to missing loss information\n",
      "Skip training data 4633 due to missing loss information\n",
      "Skip training data 4634 due to missing loss information\n",
      "Skip training data 4635 due to missing loss information\n",
      "Skip training data 4636 due to missing loss information\n",
      "Skip training data 4637 due to missing loss information\n",
      "Skip training data 4638 due to missing loss information\n",
      "Skip training data 4639 due to missing loss information\n",
      "Skip training data 4640 due to missing loss information\n",
      "Skip training data 4641 due to missing loss information\n",
      "Skip training data 4642 due to missing loss information\n",
      "Skip training data 4643 due to missing loss information\n",
      "Skip training data 4644 due to missing loss information\n",
      "Skip training data 4645 due to missing loss information\n",
      "Skip training data 4646 due to missing loss information\n",
      "Skip training data 4647 due to missing loss information\n",
      "Skip training data 4648 due to missing loss information\n",
      "Skip training data 4649 due to missing loss information\n",
      "Skip training data 4650 due to missing loss information\n",
      "Skip training data 4651 due to missing loss information\n",
      "Skip training data 4652 due to missing loss information\n",
      "Skip training data 4653 due to missing loss information\n",
      "Skip training data 4654 due to missing loss information\n",
      "Skip training data 4655 due to missing loss information\n",
      "Skip training data 4656 due to missing loss information\n",
      "Skip training data 4657 due to missing loss information\n",
      "Skip training data 4658 due to missing loss information\n",
      "Skip training data 4659 due to missing loss information\n",
      "Skip training data 4660 due to missing loss information\n",
      "Skip training data 4661 due to missing loss information\n",
      "Skip training data 4662 due to missing loss information\n",
      "Skip training data 4663 due to missing loss information\n",
      "Skip training data 4664 due to missing loss information\n",
      "Skip training data 4665 due to missing loss information\n",
      "Skip training data 4666 due to missing loss information\n",
      "Skip training data 4667 due to missing loss information\n",
      "Skip training data 4668 due to missing loss information\n",
      "Skip training data 4669 due to missing loss information\n",
      "Skip training data 4670 due to missing loss information\n",
      "Skip training data 4671 due to missing loss information\n",
      "Skip training data 4672 due to missing loss information\n",
      "Skip training data 4673 due to missing loss information\n",
      "Skip training data 4674 due to missing loss information\n",
      "Skip training data 4675 due to missing loss information\n",
      "Skip training data 4676 due to missing loss information\n",
      "Skip training data 4677 due to missing loss information\n",
      "Skip training data 4678 due to missing loss information\n",
      "Skip training data 4679 due to missing loss information\n",
      "Skip training data 4680 due to missing loss information\n",
      "Skip training data 4681 due to missing loss information\n",
      "Skip training data 4682 due to missing loss information\n",
      "Skip training data 4683 due to missing loss information\n",
      "Skip training data 4684 due to missing loss information\n",
      "Skip training data 4685 due to missing loss information\n",
      "Skip training data 4686 due to missing loss information\n",
      "Skip training data 4687 due to missing loss information\n",
      "Skip training data 4688 due to missing loss information\n",
      "Skip training data 4689 due to missing loss information\n",
      "Skip training data 4690 due to missing loss information\n",
      "Skip training data 4691 due to missing loss information\n",
      "Skip training data 4692 due to missing loss information\n",
      "Skip training data 4693 due to missing loss information\n",
      "Skip training data 4694 due to missing loss information\n",
      "Skip training data 4695 due to missing loss information\n",
      "Skip training data 4696 due to missing loss information\n",
      "Skip training data 4697 due to missing loss information\n",
      "Skip training data 4698 due to missing loss information\n",
      "Skip training data 4699 due to missing loss information\n",
      "Skip training data 4700 due to missing loss information\n",
      "Skip training data 4701 due to missing loss information\n",
      "Skip training data 4702 due to missing loss information\n",
      "Skip training data 4703 due to missing loss information\n",
      "Skip training data 4704 due to missing loss information\n",
      "Skip training data 4705 due to missing loss information\n",
      "Skip training data 4706 due to missing loss information\n",
      "Skip training data 4707 due to missing loss information\n",
      "Skip training data 4708 due to missing loss information\n",
      "Skip training data 4709 due to missing loss information\n",
      "Skip training data 4710 due to missing loss information\n",
      "Skip training data 4711 due to missing loss information\n",
      "Skip training data 4712 due to missing loss information\n",
      "Skip training data 4713 due to missing loss information\n",
      "Skip training data 4714 due to missing loss information\n",
      "Skip training data 4715 due to missing loss information\n",
      "Skip training data 4716 due to missing loss information\n",
      "Skip training data 4717 due to missing loss information\n",
      "Skip training data 4718 due to missing loss information\n",
      "Skip training data 4719 due to missing loss information\n",
      "Skip training data 4720 due to missing loss information\n",
      "Skip training data 4721 due to missing loss information\n",
      "Skip training data 4722 due to missing loss information\n",
      "Skip training data 4723 due to missing loss information\n",
      "Skip training data 4724 due to missing loss information\n",
      "Skip training data 4725 due to missing loss information\n",
      "Skip training data 4726 due to missing loss information\n",
      "Skip training data 4727 due to missing loss information\n",
      "Skip training data 4728 due to missing loss information\n",
      "Skip training data 4729 due to missing loss information\n",
      "Skip training data 4730 due to missing loss information\n",
      "Skip training data 4731 due to missing loss information\n",
      "Skip training data 4732 due to missing loss information\n",
      "Skip training data 4733 due to missing loss information\n",
      "Skip training data 4734 due to missing loss information\n",
      "Skip training data 4735 due to missing loss information\n",
      "Skip training data 4736 due to missing loss information\n",
      "Skip training data 4737 due to missing loss information\n",
      "Skip training data 4738 due to missing loss information\n",
      "Skip training data 4739 due to missing loss information\n",
      "Skip training data 4740 due to missing loss information\n",
      "Skip training data 4741 due to missing loss information\n",
      "Skip training data 4742 due to missing loss information\n",
      "Skip training data 4743 due to missing loss information\n",
      "Skip training data 4744 due to missing loss information\n",
      "Skip training data 4745 due to missing loss information\n",
      "Skip training data 4746 due to missing loss information\n",
      "Skip training data 4747 due to missing loss information\n",
      "Skip training data 4748 due to missing loss information\n",
      "Skip training data 4749 due to missing loss information\n",
      "Skip training data 4750 due to missing loss information\n",
      "Skip training data 4751 due to missing loss information\n",
      "Skip training data 4752 due to missing loss information\n",
      "Skip training data 4753 due to missing loss information\n",
      "Skip training data 4754 due to missing loss information\n",
      "Skip training data 4755 due to missing loss information\n",
      "Skip training data 4756 due to missing loss information\n",
      "Skip training data 4757 due to missing loss information\n",
      "Skip training data 4758 due to missing loss information\n",
      "Skip training data 4759 due to missing loss information\n",
      "Skip training data 4760 due to missing loss information\n",
      "Skip training data 4761 due to missing loss information\n",
      "Skip training data 4762 due to missing loss information\n",
      "Skip training data 4763 due to missing loss information\n",
      "Skip training data 4764 due to missing loss information\n",
      "Skip training data 4765 due to missing loss information\n",
      "Skip training data 4766 due to missing loss information\n",
      "Skip training data 4767 due to missing loss information\n",
      "Skip training data 4768 due to missing loss information\n",
      "Skip training data 4769 due to missing loss information\n",
      "Skip training data 4770 due to missing loss information\n",
      "Skip training data 4771 due to missing loss information\n",
      "Skip training data 4772 due to missing loss information\n",
      "Skip training data 4773 due to missing loss information\n",
      "Skip training data 4774 due to missing loss information\n",
      "Skip training data 4775 due to missing loss information\n",
      "Skip training data 4776 due to missing loss information\n",
      "Skip training data 4777 due to missing loss information\n",
      "Skip training data 4778 due to missing loss information\n",
      "Skip training data 4779 due to missing loss information\n",
      "Skip training data 4780 due to missing loss information\n",
      "Skip training data 4781 due to missing loss information\n",
      "Skip training data 4782 due to missing loss information\n",
      "Skip training data 4783 due to missing loss information\n",
      "Skip training data 4784 due to missing loss information\n",
      "Skip training data 4785 due to missing loss information\n",
      "Skip training data 4786 due to missing loss information\n",
      "Skip training data 4787 due to missing loss information\n",
      "Skip training data 4788 due to missing loss information\n",
      "Skip training data 4789 due to missing loss information\n",
      "Skip training data 4790 due to missing loss information\n",
      "Skip training data 4791 due to missing loss information\n",
      "Skip training data 4792 due to missing loss information\n",
      "Skip training data 4793 due to missing loss information\n",
      "Skip training data 4794 due to missing loss information\n",
      "Skip training data 4795 due to missing loss information\n",
      "Skip training data 4796 due to missing loss information\n",
      "Skip training data 4797 due to missing loss information\n",
      "Skip training data 4798 due to missing loss information\n",
      "Skip training data 4799 due to missing loss information\n",
      "Skip training data 4800 due to missing loss information\n",
      "Skip training data 4801 due to missing loss information\n",
      "Skip training data 4802 due to missing loss information\n",
      "Skip training data 4803 due to missing loss information\n",
      "Skip training data 4804 due to missing loss information\n",
      "Skip training data 4805 due to missing loss information\n",
      "Skip training data 4806 due to missing loss information\n",
      "Skip training data 4807 due to missing loss information\n",
      "Skip training data 4808 due to missing loss information\n",
      "Skip training data 4809 due to missing loss information\n",
      "Skip training data 4810 due to missing loss information\n",
      "Skip training data 4811 due to missing loss information\n",
      "Skip training data 4812 due to missing loss information\n",
      "Skip training data 4813 due to missing loss information\n",
      "Skip training data 4814 due to missing loss information\n",
      "Skip training data 4815 due to missing loss information\n",
      "Skip training data 4816 due to missing loss information\n",
      "Skip training data 4817 due to missing loss information\n",
      "Skip training data 4818 due to missing loss information\n",
      "Skip training data 4819 due to missing loss information\n",
      "Skip training data 4820 due to missing loss information\n",
      "Skip training data 4821 due to missing loss information\n",
      "Skip training data 4822 due to missing loss information\n",
      "Skip training data 4823 due to missing loss information\n",
      "Skip training data 4824 due to missing loss information\n",
      "Skip training data 4825 due to missing loss information\n",
      "Skip training data 4826 due to missing loss information\n",
      "Skip training data 4827 due to missing loss information\n",
      "Skip training data 4828 due to missing loss information\n",
      "Skip training data 4829 due to missing loss information\n",
      "Skip training data 4830 due to missing loss information\n",
      "Skip training data 4831 due to missing loss information\n",
      "Skip training data 4832 due to missing loss information\n",
      "Skip training data 4833 due to missing loss information\n",
      "Skip training data 4834 due to missing loss information\n",
      "Skip training data 4835 due to missing loss information\n",
      "Skip training data 4836 due to missing loss information\n",
      "Skip training data 4837 due to missing loss information\n",
      "Skip training data 4838 due to missing loss information\n",
      "Skip training data 4839 due to missing loss information\n",
      "Skip training data 4840 due to missing loss information\n",
      "Skip training data 4841 due to missing loss information\n",
      "Skip training data 4842 due to missing loss information\n",
      "Skip training data 4843 due to missing loss information\n",
      "Skip training data 4844 due to missing loss information\n",
      "Skip training data 4845 due to missing loss information\n",
      "Skip training data 4846 due to missing loss information\n",
      "Skip training data 4847 due to missing loss information\n",
      "Skip training data 4848 due to missing loss information\n",
      "Skip training data 4849 due to missing loss information\n",
      "Skip training data 4850 due to missing loss information\n",
      "Skip training data 4851 due to missing loss information\n",
      "Skip training data 4852 due to missing loss information\n",
      "Skip training data 4853 due to missing loss information\n",
      "Skip training data 4854 due to missing loss information\n",
      "Skip training data 4855 due to missing loss information\n",
      "Skip training data 4856 due to missing loss information\n",
      "Skip training data 4857 due to missing loss information\n",
      "Skip training data 4858 due to missing loss information\n",
      "Skip training data 4859 due to missing loss information\n",
      "Skip training data 4860 due to missing loss information\n",
      "Skip training data 4861 due to missing loss information\n",
      "Skip training data 4862 due to missing loss information\n",
      "Skip training data 4863 due to missing loss information\n",
      "Skip training data 4864 due to missing loss information\n",
      "Skip training data 4865 due to missing loss information\n",
      "Skip training data 4866 due to missing loss information\n",
      "Skip training data 4867 due to missing loss information\n",
      "Skip training data 4868 due to missing loss information\n",
      "Skip training data 4869 due to missing loss information\n",
      "Skip training data 4870 due to missing loss information\n",
      "Skip training data 4871 due to missing loss information\n",
      "Skip training data 4872 due to missing loss information\n",
      "Skip training data 4873 due to missing loss information\n",
      "Skip training data 4874 due to missing loss information\n",
      "Skip training data 4875 due to missing loss information\n",
      "Skip training data 4876 due to missing loss information\n",
      "Skip training data 4877 due to missing loss information\n",
      "Skip training data 4878 due to missing loss information\n",
      "Skip training data 4879 due to missing loss information\n",
      "Skip training data 4880 due to missing loss information\n",
      "Skip training data 4881 due to missing loss information\n",
      "Skip training data 4882 due to missing loss information\n",
      "Skip training data 4883 due to missing loss information\n",
      "Skip training data 4884 due to missing loss information\n",
      "Skip training data 4885 due to missing loss information\n",
      "Skip training data 4886 due to missing loss information\n",
      "Skip training data 4887 due to missing loss information\n",
      "Skip training data 4888 due to missing loss information\n",
      "Skip training data 4889 due to missing loss information\n",
      "Skip training data 4890 due to missing loss information\n",
      "Skip training data 4891 due to missing loss information\n",
      "Skip training data 4892 due to missing loss information\n",
      "Skip training data 4893 due to missing loss information\n",
      "Skip training data 4894 due to missing loss information\n",
      "Skip training data 4895 due to missing loss information\n",
      "Skip training data 4896 due to missing loss information\n",
      "Skip training data 4897 due to missing loss information\n",
      "Skip training data 4898 due to missing loss information\n",
      "Skip training data 4899 due to missing loss information\n",
      "Skip training data 4900 due to missing loss information\n",
      "Skip training data 4901 due to missing loss information\n",
      "Skip training data 4902 due to missing loss information\n",
      "Skip training data 4903 due to missing loss information\n",
      "Skip training data 4904 due to missing loss information\n",
      "Skip training data 4905 due to missing loss information\n",
      "Skip training data 4906 due to missing loss information\n",
      "Skip training data 4907 due to missing loss information\n",
      "Skip training data 4908 due to missing loss information\n",
      "Skip training data 4909 due to missing loss information\n",
      "Skip training data 4910 due to missing loss information\n",
      "Skip training data 4911 due to missing loss information\n",
      "Skip training data 4912 due to missing loss information\n",
      "Skip training data 4913 due to missing loss information\n",
      "Skip training data 4914 due to missing loss information\n",
      "Skip training data 4915 due to missing loss information\n",
      "Skip training data 4916 due to missing loss information\n",
      "Skip training data 4917 due to missing loss information\n",
      "Skip training data 4918 due to missing loss information\n",
      "Skip training data 4919 due to missing loss information\n",
      "Skip training data 4920 due to missing loss information\n",
      "Skip training data 4921 due to missing loss information\n",
      "Skip training data 4922 due to missing loss information\n",
      "Skip training data 4923 due to missing loss information\n",
      "Skip training data 4924 due to missing loss information\n",
      "Skip training data 4925 due to missing loss information\n",
      "Skip training data 4926 due to missing loss information\n",
      "Skip training data 4927 due to missing loss information\n",
      "Skip training data 4928 due to missing loss information\n",
      "Skip training data 4929 due to missing loss information\n",
      "Skip training data 4930 due to missing loss information\n",
      "Skip training data 4931 due to missing loss information\n",
      "Skip training data 4932 due to missing loss information\n",
      "Skip training data 4933 due to missing loss information\n",
      "Skip training data 4934 due to missing loss information\n",
      "Skip training data 4935 due to missing loss information\n",
      "Skip training data 4936 due to missing loss information\n",
      "Skip training data 4937 due to missing loss information\n",
      "Skip training data 4938 due to missing loss information\n",
      "Skip training data 4939 due to missing loss information\n",
      "Skip training data 4940 due to missing loss information\n",
      "Skip training data 4941 due to missing loss information\n",
      "Skip training data 4942 due to missing loss information\n",
      "Skip training data 4943 due to missing loss information\n",
      "Skip training data 4944 due to missing loss information\n",
      "Skip training data 4945 due to missing loss information\n",
      "Skip training data 4946 due to missing loss information\n",
      "Skip training data 4947 due to missing loss information\n",
      "Skip training data 4948 due to missing loss information\n",
      "Skip training data 4949 due to missing loss information\n",
      "Skip training data 4950 due to missing loss information\n",
      "Skip training data 4951 due to missing loss information\n",
      "Skip training data 4952 due to missing loss information\n",
      "Skip training data 4953 due to missing loss information\n",
      "Skip training data 4954 due to missing loss information\n",
      "Skip training data 4955 due to missing loss information\n",
      "Skip training data 4956 due to missing loss information\n",
      "Skip training data 4957 due to missing loss information\n",
      "Skip training data 4958 due to missing loss information\n",
      "Skip training data 4959 due to missing loss information\n",
      "Skip training data 4960 due to missing loss information\n",
      "Skip training data 4961 due to missing loss information\n",
      "Skip training data 4962 due to missing loss information\n",
      "Skip training data 4963 due to missing loss information\n",
      "Skip training data 4964 due to missing loss information\n",
      "Skip training data 4965 due to missing loss information\n",
      "Skip training data 4966 due to missing loss information\n",
      "Skip training data 4967 due to missing loss information\n",
      "Skip training data 4968 due to missing loss information\n",
      "Skip training data 4969 due to missing loss information\n",
      "Skip training data 4970 due to missing loss information\n",
      "Skip training data 4971 due to missing loss information\n",
      "Skip training data 4972 due to missing loss information\n",
      "Skip training data 4973 due to missing loss information\n",
      "Skip training data 4974 due to missing loss information\n",
      "Skip training data 4975 due to missing loss information\n",
      "Skip training data 4976 due to missing loss information\n",
      "Skip training data 4977 due to missing loss information\n",
      "Skip training data 4978 due to missing loss information\n",
      "Skip training data 4979 due to missing loss information\n",
      "Skip training data 4980 due to missing loss information\n",
      "Skip training data 4981 due to missing loss information\n",
      "Skip training data 4982 due to missing loss information\n",
      "Skip training data 4983 due to missing loss information\n",
      "Skip training data 4984 due to missing loss information\n",
      "Skip training data 4985 due to missing loss information\n",
      "Skip training data 4986 due to missing loss information\n",
      "Skip training data 4987 due to missing loss information\n",
      "Skip training data 4988 due to missing loss information\n",
      "Skip training data 4989 due to missing loss information\n",
      "Skip training data 4990 due to missing loss information\n",
      "Skip training data 4991 due to missing loss information\n",
      "Skip training data 4992 due to missing loss information\n",
      "Skip training data 4993 due to missing loss information\n",
      "Skip training data 4994 due to missing loss information\n",
      "Skip training data 4995 due to missing loss information\n",
      "Skip training data 4996 due to missing loss information\n",
      "Skip training data 4997 due to missing loss information\n",
      "Skip training data 4998 due to missing loss information\n",
      "Skip training data 4999 due to missing loss information\n",
      "Skip training data 5000 due to missing loss information\n",
      "Skip training data 5001 due to missing loss information\n",
      "Skip training data 5002 due to missing loss information\n",
      "Skip training data 5003 due to missing loss information\n",
      "Skip training data 5004 due to missing loss information\n",
      "Skip training data 5005 due to missing loss information\n",
      "Skip training data 5006 due to missing loss information\n",
      "Skip training data 5007 due to missing loss information\n",
      "Skip training data 5008 due to missing loss information\n",
      "Skip training data 5009 due to missing loss information\n",
      "Skip training data 5010 due to missing loss information\n",
      "Skip training data 5011 due to missing loss information\n",
      "Skip training data 5012 due to missing loss information\n",
      "Skip training data 5013 due to missing loss information\n",
      "Skip training data 5014 due to missing loss information\n",
      "Skip training data 5015 due to missing loss information\n",
      "Skip training data 5016 due to missing loss information\n",
      "Skip training data 5017 due to missing loss information\n",
      "Skip training data 5018 due to missing loss information\n",
      "Skip training data 5019 due to missing loss information\n",
      "Skip training data 5020 due to missing loss information\n",
      "Skip training data 5021 due to missing loss information\n",
      "Skip training data 5022 due to missing loss information\n",
      "Skip training data 5023 due to missing loss information\n",
      "Skip training data 5024 due to missing loss information\n",
      "Skip training data 5025 due to missing loss information\n",
      "Skip training data 5026 due to missing loss information\n",
      "Skip training data 5027 due to missing loss information\n",
      "Skip training data 5028 due to missing loss information\n",
      "Skip training data 5029 due to missing loss information\n",
      "Skip training data 5030 due to missing loss information\n",
      "Skip training data 5031 due to missing loss information\n",
      "Skip training data 5032 due to missing loss information\n",
      "Skip training data 5033 due to missing loss information\n",
      "Skip training data 5034 due to missing loss information\n",
      "Skip training data 5035 due to missing loss information\n",
      "Skip training data 5036 due to missing loss information\n",
      "Skip training data 5037 due to missing loss information\n",
      "Skip training data 5038 due to missing loss information\n",
      "Skip training data 5039 due to missing loss information\n",
      "Skip training data 5040 due to missing loss information\n",
      "Skip training data 5041 due to missing loss information\n",
      "Skip training data 5042 due to missing loss information\n",
      "Skip training data 5043 due to missing loss information\n",
      "Skip training data 5044 due to missing loss information\n",
      "Skip training data 5045 due to missing loss information\n",
      "Skip training data 5046 due to missing loss information\n",
      "Skip training data 5047 due to missing loss information\n",
      "Skip training data 5048 due to missing loss information\n",
      "Skip training data 5049 due to missing loss information\n",
      "Skip training data 5050 due to missing loss information\n",
      "Skip training data 5051 due to missing loss information\n",
      "Skip training data 5052 due to missing loss information\n",
      "Skip training data 5053 due to missing loss information\n",
      "Skip training data 5054 due to missing loss information\n",
      "Skip training data 5055 due to missing loss information\n",
      "Skip training data 5056 due to missing loss information\n",
      "Skip training data 5057 due to missing loss information\n",
      "Skip training data 5058 due to missing loss information\n",
      "Skip training data 5059 due to missing loss information\n",
      "Skip training data 5060 due to missing loss information\n",
      "Skip training data 5061 due to missing loss information\n",
      "Skip training data 5062 due to missing loss information\n",
      "Skip training data 5063 due to missing loss information\n",
      "Skip training data 5064 due to missing loss information\n",
      "Skip training data 5065 due to missing loss information\n",
      "Skip training data 5066 due to missing loss information\n",
      "Skip training data 5067 due to missing loss information\n",
      "Skip training data 5068 due to missing loss information\n",
      "Skip training data 5069 due to missing loss information\n",
      "Skip training data 5070 due to missing loss information\n",
      "Skip training data 5071 due to missing loss information\n",
      "Skip training data 5072 due to missing loss information\n",
      "Skip training data 5073 due to missing loss information\n",
      "Skip training data 5074 due to missing loss information\n",
      "Skip training data 5075 due to missing loss information\n",
      "Skip training data 5076 due to missing loss information\n",
      "Skip training data 5077 due to missing loss information\n",
      "Skip training data 5078 due to missing loss information\n",
      "Skip training data 5079 due to missing loss information\n",
      "Skip training data 5080 due to missing loss information\n",
      "Skip training data 5081 due to missing loss information\n",
      "Skip training data 5082 due to missing loss information\n",
      "Skip training data 5083 due to missing loss information\n",
      "Skip training data 5084 due to missing loss information\n",
      "Skip training data 5085 due to missing loss information\n",
      "Skip training data 5086 due to missing loss information\n",
      "Skip training data 5087 due to missing loss information\n",
      "Skip training data 5088 due to missing loss information\n",
      "Skip training data 5089 due to missing loss information\n",
      "Skip training data 5090 due to missing loss information\n",
      "Skip training data 5091 due to missing loss information\n",
      "Skip training data 5092 due to missing loss information\n",
      "Skip training data 5093 due to missing loss information\n",
      "Skip training data 5094 due to missing loss information\n",
      "Skip training data 5095 due to missing loss information\n",
      "Skip training data 5096 due to missing loss information\n",
      "Skip training data 5097 due to missing loss information\n",
      "Skip training data 5098 due to missing loss information\n",
      "Skip training data 5099 due to missing loss information\n",
      "Skip training data 5100 due to missing loss information\n",
      "Skip training data 5101 due to missing loss information\n",
      "Skip training data 5102 due to missing loss information\n",
      "Skip training data 5103 due to missing loss information\n",
      "Skip training data 5104 due to missing loss information\n",
      "Skip training data 5105 due to missing loss information\n",
      "Skip training data 5106 due to missing loss information\n",
      "Skip training data 5107 due to missing loss information\n",
      "Skip training data 5108 due to missing loss information\n",
      "Skip training data 5109 due to missing loss information\n",
      "Skip training data 5110 due to missing loss information\n",
      "Skip training data 5111 due to missing loss information\n",
      "Skip training data 5112 due to missing loss information\n",
      "Skip training data 5113 due to missing loss information\n",
      "Skip training data 5114 due to missing loss information\n",
      "Skip training data 5115 due to missing loss information\n",
      "Skip training data 5116 due to missing loss information\n",
      "Skip training data 5117 due to missing loss information\n",
      "Skip training data 5118 due to missing loss information\n",
      "Skip training data 5119 due to missing loss information\n",
      "Skip training data 5120 due to missing loss information\n",
      "Skip training data 5121 due to missing loss information\n",
      "Skip training data 5122 due to missing loss information\n",
      "Skip training data 5123 due to missing loss information\n",
      "Skip training data 5124 due to missing loss information\n",
      "Skip training data 5125 due to missing loss information\n",
      "Skip training data 5126 due to missing loss information\n",
      "Skip training data 5127 due to missing loss information\n",
      "Skip training data 5128 due to missing loss information\n",
      "Skip training data 5129 due to missing loss information\n",
      "Skip training data 5130 due to missing loss information\n",
      "Skip training data 5131 due to missing loss information\n",
      "Skip training data 5132 due to missing loss information\n",
      "Skip training data 5133 due to missing loss information\n",
      "Skip training data 5134 due to missing loss information\n",
      "Skip training data 5135 due to missing loss information\n",
      "Skip training data 5136 due to missing loss information\n",
      "Skip training data 5137 due to missing loss information\n",
      "Skip training data 5138 due to missing loss information\n",
      "Skip training data 5139 due to missing loss information\n",
      "Skip training data 5140 due to missing loss information\n",
      "Skip training data 5141 due to missing loss information\n",
      "Skip training data 5142 due to missing loss information\n",
      "Skip training data 5143 due to missing loss information\n",
      "Skip training data 5144 due to missing loss information\n",
      "Skip training data 5145 due to missing loss information\n",
      "Skip training data 5146 due to missing loss information\n",
      "Skip training data 5147 due to missing loss information\n",
      "Skip training data 5148 due to missing loss information\n",
      "Skip training data 5149 due to missing loss information\n",
      "Skip training data 5150 due to missing loss information\n",
      "Skip training data 5151 due to missing loss information\n",
      "Skip training data 5152 due to missing loss information\n",
      "Skip training data 5153 due to missing loss information\n",
      "Skip training data 5154 due to missing loss information\n",
      "Skip training data 5155 due to missing loss information\n",
      "Skip training data 5156 due to missing loss information\n",
      "Skip training data 5157 due to missing loss information\n",
      "Skip training data 5158 due to missing loss information\n",
      "Skip training data 5159 due to missing loss information\n",
      "Skip training data 5160 due to missing loss information\n",
      "Skip training data 5161 due to missing loss information\n",
      "Skip training data 5162 due to missing loss information\n",
      "Skip training data 5163 due to missing loss information\n",
      "Skip training data 5164 due to missing loss information\n",
      "Skip training data 5165 due to missing loss information\n",
      "Skip training data 5166 due to missing loss information\n",
      "Skip training data 5167 due to missing loss information\n",
      "Skip training data 5168 due to missing loss information\n",
      "Skip training data 5169 due to missing loss information\n",
      "Skip training data 5170 due to missing loss information\n",
      "Skip training data 5171 due to missing loss information\n",
      "Skip training data 5172 due to missing loss information\n",
      "Skip training data 5173 due to missing loss information\n",
      "Skip training data 5174 due to missing loss information\n",
      "Skip training data 5175 due to missing loss information\n",
      "Skip training data 5176 due to missing loss information\n",
      "Skip training data 5177 due to missing loss information\n",
      "Skip training data 5178 due to missing loss information\n",
      "Skip training data 5179 due to missing loss information\n",
      "Skip training data 5180 due to missing loss information\n",
      "Skip training data 5181 due to missing loss information\n",
      "Skip training data 5182 due to missing loss information\n",
      "Skip training data 5183 due to missing loss information\n",
      "Skip training data 5184 due to missing loss information\n",
      "Skip training data 5185 due to missing loss information\n",
      "Skip training data 5186 due to missing loss information\n",
      "Skip training data 5187 due to missing loss information\n",
      "Skip training data 5188 due to missing loss information\n",
      "Skip training data 5189 due to missing loss information\n",
      "Skip training data 5190 due to missing loss information\n",
      "Skip training data 5191 due to missing loss information\n",
      "Skip training data 5192 due to missing loss information\n",
      "Skip training data 5193 due to missing loss information\n",
      "Skip training data 5194 due to missing loss information\n",
      "Skip training data 5195 due to missing loss information\n",
      "Skip training data 5196 due to missing loss information\n",
      "Skip training data 5197 due to missing loss information\n",
      "Skip training data 5198 due to missing loss information\n",
      "Skip training data 5199 due to missing loss information\n",
      "Skip training data 5200 due to missing loss information\n",
      "Skip training data 5201 due to missing loss information\n",
      "Skip training data 5202 due to missing loss information\n",
      "Skip training data 5203 due to missing loss information\n",
      "Skip training data 5204 due to missing loss information\n",
      "Skip training data 5205 due to missing loss information\n",
      "Skip training data 5206 due to missing loss information\n",
      "Skip training data 5207 due to missing loss information\n",
      "Skip training data 5208 due to missing loss information\n",
      "Skip training data 5209 due to missing loss information\n",
      "Skip training data 5210 due to missing loss information\n",
      "Skip training data 5211 due to missing loss information\n",
      "Skip training data 5212 due to missing loss information\n",
      "Skip training data 5213 due to missing loss information\n",
      "Skip training data 5214 due to missing loss information\n",
      "Skip training data 5215 due to missing loss information\n",
      "Skip training data 5216 due to missing loss information\n",
      "Skip training data 5217 due to missing loss information\n",
      "Skip training data 5218 due to missing loss information\n",
      "Skip training data 5219 due to missing loss information\n",
      "Skip training data 5220 due to missing loss information\n",
      "Skip training data 5221 due to missing loss information\n",
      "Skip training data 5222 due to missing loss information\n",
      "Skip training data 5223 due to missing loss information\n",
      "Skip training data 5224 due to missing loss information\n",
      "Skip training data 5225 due to missing loss information\n",
      "Skip training data 5226 due to missing loss information\n",
      "Skip training data 5227 due to missing loss information\n",
      "Skip training data 5228 due to missing loss information\n",
      "Skip training data 5229 due to missing loss information\n",
      "Skip training data 5230 due to missing loss information\n",
      "Skip training data 5231 due to missing loss information\n",
      "Skip training data 5232 due to missing loss information\n",
      "Skip training data 5233 due to missing loss information\n",
      "Skip training data 5234 due to missing loss information\n",
      "Skip training data 5235 due to missing loss information\n",
      "Skip training data 5236 due to missing loss information\n",
      "Skip training data 5237 due to missing loss information\n",
      "Skip training data 5238 due to missing loss information\n",
      "Skip training data 5239 due to missing loss information\n",
      "Skip training data 5240 due to missing loss information\n",
      "Skip training data 5241 due to missing loss information\n",
      "Skip training data 5242 due to missing loss information\n",
      "Skip training data 5243 due to missing loss information\n",
      "Skip training data 5244 due to missing loss information\n",
      "Skip training data 5245 due to missing loss information\n",
      "Skip training data 5246 due to missing loss information\n",
      "Skip training data 5247 due to missing loss information\n",
      "Skip training data 5248 due to missing loss information\n",
      "Skip training data 5249 due to missing loss information\n",
      "Skip training data 5250 due to missing loss information\n",
      "Skip training data 5251 due to missing loss information\n",
      "Skip training data 5252 due to missing loss information\n",
      "Skip training data 5253 due to missing loss information\n",
      "Skip training data 5254 due to missing loss information\n",
      "Skip training data 5255 due to missing loss information\n",
      "Skip training data 5256 due to missing loss information\n",
      "Skip training data 5257 due to missing loss information\n",
      "Skip training data 5258 due to missing loss information\n",
      "Skip training data 5259 due to missing loss information\n",
      "Skip training data 5260 due to missing loss information\n",
      "Skip training data 5261 due to missing loss information\n",
      "Skip training data 5262 due to missing loss information\n",
      "Skip training data 5263 due to missing loss information\n",
      "Skip training data 5264 due to missing loss information\n",
      "Skip training data 5265 due to missing loss information\n",
      "Skip training data 5266 due to missing loss information\n",
      "Skip training data 5267 due to missing loss information\n",
      "Skip training data 5268 due to missing loss information\n",
      "Skip training data 5269 due to missing loss information\n",
      "Skip training data 5270 due to missing loss information\n",
      "Skip training data 5271 due to missing loss information\n",
      "Skip training data 5272 due to missing loss information\n",
      "Skip training data 5273 due to missing loss information\n",
      "Skip training data 5274 due to missing loss information\n",
      "Skip training data 5275 due to missing loss information\n",
      "Skip training data 5276 due to missing loss information\n",
      "Skip training data 5277 due to missing loss information\n",
      "Skip training data 5278 due to missing loss information\n",
      "Skip training data 5279 due to missing loss information\n",
      "Skip training data 5280 due to missing loss information\n",
      "Skip training data 5281 due to missing loss information\n",
      "Skip training data 5282 due to missing loss information\n",
      "Skip training data 5283 due to missing loss information\n",
      "Skip training data 5284 due to missing loss information\n",
      "Skip training data 5285 due to missing loss information\n",
      "Skip training data 5286 due to missing loss information\n",
      "Skip training data 5287 due to missing loss information\n",
      "Skip training data 5288 due to missing loss information\n",
      "Skip training data 5289 due to missing loss information\n",
      "Skip training data 5290 due to missing loss information\n",
      "Skip training data 5291 due to missing loss information\n",
      "Skip training data 5292 due to missing loss information\n",
      "Skip training data 5293 due to missing loss information\n",
      "Skip training data 5294 due to missing loss information\n",
      "Skip training data 5295 due to missing loss information\n",
      "Skip training data 5296 due to missing loss information\n",
      "Skip training data 5297 due to missing loss information\n",
      "Skip training data 5298 due to missing loss information\n",
      "Skip training data 5299 due to missing loss information\n",
      "Skip training data 5300 due to missing loss information\n",
      "Skip training data 5301 due to missing loss information\n",
      "Skip training data 5302 due to missing loss information\n",
      "Skip training data 5303 due to missing loss information\n",
      "Skip training data 5304 due to missing loss information\n",
      "Skip training data 5305 due to missing loss information\n",
      "Skip training data 5306 due to missing loss information\n",
      "Skip training data 5307 due to missing loss information\n",
      "Skip training data 5308 due to missing loss information\n",
      "Skip training data 5309 due to missing loss information\n",
      "Skip training data 5310 due to missing loss information\n",
      "Skip training data 5311 due to missing loss information\n",
      "Skip training data 5312 due to missing loss information\n",
      "Skip training data 5313 due to missing loss information\n",
      "Skip training data 5314 due to missing loss information\n",
      "Skip training data 5315 due to missing loss information\n",
      "Skip training data 5316 due to missing loss information\n",
      "Skip training data 5317 due to missing loss information\n",
      "Skip training data 5318 due to missing loss information\n",
      "Skip training data 5319 due to missing loss information\n",
      "Skip training data 5320 due to missing loss information\n",
      "Skip training data 5321 due to missing loss information\n",
      "Skip training data 5322 due to missing loss information\n",
      "Skip training data 5323 due to missing loss information\n",
      "Skip training data 5324 due to missing loss information\n",
      "Skip training data 5325 due to missing loss information\n",
      "Skip training data 5326 due to missing loss information\n",
      "Skip training data 5327 due to missing loss information\n",
      "Skip training data 5328 due to missing loss information\n",
      "Skip training data 5329 due to missing loss information\n",
      "Skip training data 5330 due to missing loss information\n",
      "Skip training data 5331 due to missing loss information\n",
      "Skip training data 5332 due to missing loss information\n",
      "Skip training data 5333 due to missing loss information\n",
      "Skip training data 5334 due to missing loss information\n",
      "Skip training data 5335 due to missing loss information\n",
      "Skip training data 5336 due to missing loss information\n",
      "Skip training data 5337 due to missing loss information\n",
      "Skip training data 5338 due to missing loss information\n",
      "Skip training data 5339 due to missing loss information\n",
      "Skip training data 5340 due to missing loss information\n",
      "Skip training data 5341 due to missing loss information\n",
      "Skip training data 5342 due to missing loss information\n",
      "Skip training data 5343 due to missing loss information\n",
      "Skip training data 5344 due to missing loss information\n",
      "Skip training data 5345 due to missing loss information\n",
      "Skip training data 5346 due to missing loss information\n",
      "Skip training data 5347 due to missing loss information\n",
      "Skip training data 5348 due to missing loss information\n",
      "Skip training data 5349 due to missing loss information\n",
      "Skip training data 5350 due to missing loss information\n",
      "Skip training data 5351 due to missing loss information\n",
      "Skip training data 5352 due to missing loss information\n",
      "Skip training data 5353 due to missing loss information\n",
      "Skip training data 5354 due to missing loss information\n",
      "Skip training data 5355 due to missing loss information\n",
      "Skip training data 5356 due to missing loss information\n",
      "Skip training data 5357 due to missing loss information\n",
      "Skip training data 5358 due to missing loss information\n",
      "Skip training data 5359 due to missing loss information\n",
      "Skip training data 5360 due to missing loss information\n",
      "Skip training data 5361 due to missing loss information\n",
      "Skip training data 5362 due to missing loss information\n",
      "Skip training data 5363 due to missing loss information\n",
      "Skip training data 5364 due to missing loss information\n",
      "Skip training data 5365 due to missing loss information\n",
      "Skip training data 5366 due to missing loss information\n",
      "Skip training data 5367 due to missing loss information\n",
      "Skip training data 5368 due to missing loss information\n",
      "Skip training data 5369 due to missing loss information\n",
      "Skip training data 5370 due to missing loss information\n",
      "Skip training data 5371 due to missing loss information\n",
      "Skip training data 5372 due to missing loss information\n",
      "Skip training data 5373 due to missing loss information\n",
      "Skip training data 5374 due to missing loss information\n",
      "Skip training data 5375 due to missing loss information\n",
      "Skip training data 5376 due to missing loss information\n",
      "Skip training data 5377 due to missing loss information\n",
      "Skip training data 5378 due to missing loss information\n",
      "Skip training data 5379 due to missing loss information\n",
      "Skip training data 5380 due to missing loss information\n",
      "Skip training data 5381 due to missing loss information\n",
      "Skip training data 5382 due to missing loss information\n",
      "Skip training data 5383 due to missing loss information\n",
      "Skip training data 5384 due to missing loss information\n",
      "Skip training data 5385 due to missing loss information\n",
      "Skip training data 5386 due to missing loss information\n",
      "Skip training data 5387 due to missing loss information\n",
      "Skip training data 5388 due to missing loss information\n",
      "Skip training data 5389 due to missing loss information\n",
      "Skip training data 5390 due to missing loss information\n",
      "Skip training data 5391 due to missing loss information\n",
      "Skip training data 5392 due to missing loss information\n",
      "Skip training data 5393 due to missing loss information\n",
      "Skip training data 5394 due to missing loss information\n",
      "Skip training data 5395 due to missing loss information\n",
      "Skip training data 5396 due to missing loss information\n",
      "Skip training data 5397 due to missing loss information\n",
      "Skip training data 5398 due to missing loss information\n",
      "Skip training data 5399 due to missing loss information\n",
      "Skip training data 5400 due to missing loss information\n",
      "Skip training data 5401 due to missing loss information\n",
      "Skip training data 5402 due to missing loss information\n",
      "Skip training data 5403 due to missing loss information\n",
      "Skip training data 5404 due to missing loss information\n",
      "Skip training data 5405 due to missing loss information\n",
      "Skip training data 5406 due to missing loss information\n",
      "Skip training data 5407 due to missing loss information\n",
      "Skip training data 5408 due to missing loss information\n",
      "Skip training data 5409 due to missing loss information\n",
      "Skip training data 5410 due to missing loss information\n",
      "Skip training data 5411 due to missing loss information\n",
      "Skip training data 5412 due to missing loss information\n",
      "Skip training data 5413 due to missing loss information\n",
      "Skip training data 5414 due to missing loss information\n",
      "Skip training data 5415 due to missing loss information\n",
      "Skip training data 5416 due to missing loss information\n",
      "Skip training data 5417 due to missing loss information\n",
      "Skip training data 5418 due to missing loss information\n",
      "Skip training data 5419 due to missing loss information\n",
      "Skip training data 5420 due to missing loss information\n",
      "Skip training data 5421 due to missing loss information\n",
      "Skip training data 5422 due to missing loss information\n",
      "Skip training data 5423 due to missing loss information\n",
      "Skip training data 5424 due to missing loss information\n",
      "Skip training data 5425 due to missing loss information\n",
      "Skip training data 5426 due to missing loss information\n",
      "Skip training data 5427 due to missing loss information\n",
      "Skip training data 5428 due to missing loss information\n",
      "Skip training data 5429 due to missing loss information\n",
      "Skip training data 5430 due to missing loss information\n",
      "Skip training data 5431 due to missing loss information\n",
      "Skip training data 5432 due to missing loss information\n",
      "Skip training data 5433 due to missing loss information\n",
      "Skip training data 5434 due to missing loss information\n",
      "Skip training data 5435 due to missing loss information\n",
      "Skip training data 5436 due to missing loss information\n",
      "Skip training data 5437 due to missing loss information\n",
      "Skip training data 5438 due to missing loss information\n",
      "Skip training data 5439 due to missing loss information\n",
      "Skip training data 5440 due to missing loss information\n",
      "Skip training data 5441 due to missing loss information\n",
      "Skip training data 5442 due to missing loss information\n",
      "Skip training data 5443 due to missing loss information\n",
      "Skip training data 5444 due to missing loss information\n",
      "Skip training data 5445 due to missing loss information\n",
      "Skip training data 5446 due to missing loss information\n",
      "Skip training data 5447 due to missing loss information\n",
      "Skip training data 5448 due to missing loss information\n",
      "Skip training data 5449 due to missing loss information\n",
      "Skip training data 5450 due to missing loss information\n",
      "Skip training data 5451 due to missing loss information\n",
      "Skip training data 5452 due to missing loss information\n",
      "Skip training data 5453 due to missing loss information\n",
      "Skip training data 5454 due to missing loss information\n",
      "Skip training data 5455 due to missing loss information\n",
      "Skip training data 5456 due to missing loss information\n",
      "Skip training data 5457 due to missing loss information\n",
      "Skip training data 5458 due to missing loss information\n",
      "Skip training data 5459 due to missing loss information\n",
      "Skip training data 5460 due to missing loss information\n",
      "Skip training data 5461 due to missing loss information\n",
      "Skip training data 5462 due to missing loss information\n",
      "Skip training data 5463 due to missing loss information\n",
      "Skip training data 5464 due to missing loss information\n",
      "Skip training data 5465 due to missing loss information\n",
      "Skip training data 5466 due to missing loss information\n",
      "Skip training data 5467 due to missing loss information\n",
      "Skip training data 5468 due to missing loss information\n",
      "Skip training data 5469 due to missing loss information\n",
      "Skip training data 5470 due to missing loss information\n",
      "Skip training data 5471 due to missing loss information\n",
      "Skip training data 5472 due to missing loss information\n",
      "Skip training data 5473 due to missing loss information\n",
      "Skip training data 5474 due to missing loss information\n",
      "Skip training data 5475 due to missing loss information\n",
      "Skip training data 5476 due to missing loss information\n",
      "Skip training data 5477 due to missing loss information\n",
      "Skip training data 5478 due to missing loss information\n",
      "Skip training data 5479 due to missing loss information\n",
      "Skip training data 5480 due to missing loss information\n",
      "Skip training data 5481 due to missing loss information\n",
      "Skip training data 5482 due to missing loss information\n",
      "Skip training data 5483 due to missing loss information\n",
      "Skip training data 5484 due to missing loss information\n",
      "Skip training data 5485 due to missing loss information\n",
      "Skip training data 5486 due to missing loss information\n",
      "Skip training data 5487 due to missing loss information\n",
      "Skip training data 5488 due to missing loss information\n",
      "Skip training data 5489 due to missing loss information\n",
      "Skip training data 5490 due to missing loss information\n",
      "Skip training data 5491 due to missing loss information\n",
      "Skip training data 5492 due to missing loss information\n",
      "Skip training data 5493 due to missing loss information\n",
      "Skip training data 5494 due to missing loss information\n",
      "Skip training data 5495 due to missing loss information\n",
      "Skip training data 5496 due to missing loss information\n",
      "Skip training data 5497 due to missing loss information\n",
      "Skip training data 5498 due to missing loss information\n",
      "Skip training data 5499 due to missing loss information\n",
      "Skip training data 5500 due to missing loss information\n",
      "Skip training data 5501 due to missing loss information\n",
      "Skip training data 5502 due to missing loss information\n",
      "Skip training data 5503 due to missing loss information\n",
      "Skip training data 5504 due to missing loss information\n",
      "Skip training data 5505 due to missing loss information\n",
      "Skip training data 5506 due to missing loss information\n",
      "Skip training data 5507 due to missing loss information\n",
      "Skip training data 5508 due to missing loss information\n",
      "Skip training data 5509 due to missing loss information\n",
      "Skip training data 5510 due to missing loss information\n",
      "Skip training data 5511 due to missing loss information\n",
      "Skip training data 5512 due to missing loss information\n",
      "Skip training data 5513 due to missing loss information\n",
      "Skip training data 5514 due to missing loss information\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip training data 5515 due to missing loss information\n",
      "Skip training data 5516 due to missing loss information\n",
      "Skip training data 5517 due to missing loss information\n",
      "Skip training data 5518 due to missing loss information\n",
      "Skip training data 5519 due to missing loss information\n",
      "Skip training data 5520 due to missing loss information\n",
      "Skip training data 5521 due to missing loss information\n",
      "Skip training data 5522 due to missing loss information\n",
      "Skip training data 5523 due to missing loss information\n",
      "Skip training data 5524 due to missing loss information\n",
      "Skip training data 5525 due to missing loss information\n",
      "Skip training data 5526 due to missing loss information\n",
      "Skip training data 5527 due to missing loss information\n",
      "Skip training data 5528 due to missing loss information\n",
      "Skip training data 5529 due to missing loss information\n",
      "Skip training data 5530 due to missing loss information\n",
      "Skip training data 5531 due to missing loss information\n",
      "Skip training data 5532 due to missing loss information\n",
      "Skip training data 5533 due to missing loss information\n",
      "Skip training data 5534 due to missing loss information\n",
      "Skip training data 5535 due to missing loss information\n",
      "Skip training data 5536 due to missing loss information\n",
      "Skip training data 5537 due to missing loss information\n",
      "Skip training data 5538 due to missing loss information\n",
      "Skip training data 5539 due to missing loss information\n",
      "Skip training data 5540 due to missing loss information\n",
      "Skip training data 5541 due to missing loss information\n",
      "Skip training data 5542 due to missing loss information\n",
      "Skip training data 5543 due to missing loss information\n",
      "Skip training data 5544 due to missing loss information\n",
      "Skip training data 5545 due to missing loss information\n",
      "Skip training data 5546 due to missing loss information\n",
      "Skip training data 5547 due to missing loss information\n",
      "Skip training data 5548 due to missing loss information\n",
      "Skip training data 5549 due to missing loss information\n",
      "Skip training data 5550 due to missing loss information\n",
      "Skip training data 5551 due to missing loss information\n",
      "Skip training data 5552 due to missing loss information\n",
      "Skip training data 5553 due to missing loss information\n",
      "Skip training data 5554 due to missing loss information\n",
      "Skip training data 5555 due to missing loss information\n",
      "Skip training data 5556 due to missing loss information\n",
      "Skip training data 5557 due to missing loss information\n",
      "Skip training data 5558 due to missing loss information\n",
      "Skip training data 5559 due to missing loss information\n",
      "Skip training data 5560 due to missing loss information\n",
      "Skip training data 5561 due to missing loss information\n",
      "Skip training data 5562 due to missing loss information\n",
      "Skip training data 5563 due to missing loss information\n",
      "Skip training data 5564 due to missing loss information\n",
      "Skip training data 5565 due to missing loss information\n",
      "Skip training data 5566 due to missing loss information\n",
      "Skip training data 5567 due to missing loss information\n",
      "Skip training data 5568 due to missing loss information\n",
      "Skip training data 5569 due to missing loss information\n",
      "Skip training data 5570 due to missing loss information\n",
      "Skip training data 5571 due to missing loss information\n",
      "Skip training data 5572 due to missing loss information\n",
      "Skip training data 5573 due to missing loss information\n",
      "Skip training data 5574 due to missing loss information\n",
      "Skip training data 5575 due to missing loss information\n",
      "Skip training data 5576 due to missing loss information\n",
      "Skip training data 5577 due to missing loss information\n",
      "Skip training data 5578 due to missing loss information\n",
      "Skip training data 5579 due to missing loss information\n",
      "Skip training data 5580 due to missing loss information\n",
      "Skip training data 5581 due to missing loss information\n",
      "Skip training data 5582 due to missing loss information\n",
      "Skip training data 5583 due to missing loss information\n",
      "Skip training data 5584 due to missing loss information\n",
      "Skip training data 5585 due to missing loss information\n",
      "Skip training data 5586 due to missing loss information\n",
      "Skip training data 5587 due to missing loss information\n",
      "Skip training data 5588 due to missing loss information\n",
      "Skip training data 5589 due to missing loss information\n",
      "Skip training data 5590 due to missing loss information\n",
      "Skip training data 5591 due to missing loss information\n",
      "Skip training data 5592 due to missing loss information\n",
      "Skip training data 5593 due to missing loss information\n",
      "Skip training data 5594 due to missing loss information\n",
      "Skip training data 5595 due to missing loss information\n",
      "Skip training data 5596 due to missing loss information\n",
      "Skip training data 5597 due to missing loss information\n",
      "Skip training data 5598 due to missing loss information\n",
      "Skip training data 5599 due to missing loss information\n",
      "Skip training data 5600 due to missing loss information\n",
      "Skip training data 5601 due to missing loss information\n",
      "Skip training data 5602 due to missing loss information\n",
      "Skip training data 5603 due to missing loss information\n",
      "Skip training data 5604 due to missing loss information\n",
      "Skip training data 5605 due to missing loss information\n",
      "Skip training data 5606 due to missing loss information\n",
      "Skip training data 5607 due to missing loss information\n",
      "Skip training data 5608 due to missing loss information\n",
      "Skip training data 5609 due to missing loss information\n",
      "Skip training data 5610 due to missing loss information\n",
      "Skip training data 5611 due to missing loss information\n",
      "Skip training data 5612 due to missing loss information\n",
      "Skip training data 5613 due to missing loss information\n",
      "Skip training data 5614 due to missing loss information\n",
      "Skip training data 5615 due to missing loss information\n",
      "Skip training data 5616 due to missing loss information\n",
      "Skip training data 5617 due to missing loss information\n",
      "Skip training data 5618 due to missing loss information\n",
      "Skip training data 5619 due to missing loss information\n",
      "Skip training data 5620 due to missing loss information\n",
      "Skip training data 5621 due to missing loss information\n",
      "Skip training data 5622 due to missing loss information\n",
      "Skip training data 5623 due to missing loss information\n",
      "Skip training data 5624 due to missing loss information\n",
      "Skip training data 5625 due to missing loss information\n",
      "Skip training data 5626 due to missing loss information\n",
      "Skip training data 5627 due to missing loss information\n",
      "Skip training data 5628 due to missing loss information\n",
      "Skip training data 5629 due to missing loss information\n",
      "Skip training data 5630 due to missing loss information\n",
      "Skip training data 5631 due to missing loss information\n",
      "Skip training data 5632 due to missing loss information\n",
      "Skip training data 5633 due to missing loss information\n",
      "Skip training data 5634 due to missing loss information\n",
      "Skip training data 5635 due to missing loss information\n",
      "Skip training data 5636 due to missing loss information\n",
      "Skip training data 5637 due to missing loss information\n",
      "Skip training data 5638 due to missing loss information\n",
      "Skip training data 5639 due to missing loss information\n",
      "Skip training data 5640 due to missing loss information\n",
      "Skip training data 5641 due to missing loss information\n",
      "Skip training data 5642 due to missing loss information\n",
      "Skip training data 5643 due to missing loss information\n",
      "Skip training data 5644 due to missing loss information\n",
      "Skip training data 5645 due to missing loss information\n",
      "Skip training data 5646 due to missing loss information\n",
      "Skip training data 5647 due to missing loss information\n",
      "Skip training data 5648 due to missing loss information\n",
      "Skip training data 5649 due to missing loss information\n",
      "Skip training data 5650 due to missing loss information\n",
      "Skip training data 5651 due to missing loss information\n",
      "Skip training data 5652 due to missing loss information\n",
      "Skip training data 5653 due to missing loss information\n",
      "Skip training data 5654 due to missing loss information\n",
      "Skip training data 5655 due to missing loss information\n",
      "Skip training data 5656 due to missing loss information\n",
      "Skip training data 5657 due to missing loss information\n",
      "Skip training data 5658 due to missing loss information\n",
      "Skip training data 5659 due to missing loss information\n",
      "Skip training data 5660 due to missing loss information\n",
      "Skip training data 5661 due to missing loss information\n",
      "Skip training data 5662 due to missing loss information\n",
      "Skip training data 5663 due to missing loss information\n",
      "Skip training data 5664 due to missing loss information\n",
      "Skip training data 5665 due to missing loss information\n",
      "Skip training data 5666 due to missing loss information\n",
      "Skip training data 5667 due to missing loss information\n",
      "Skip training data 5668 due to missing loss information\n",
      "Skip training data 5669 due to missing loss information\n",
      "Skip training data 5670 due to missing loss information\n",
      "Skip training data 5671 due to missing loss information\n",
      "Skip training data 5672 due to missing loss information\n",
      "Skip training data 5673 due to missing loss information\n",
      "Skip training data 5674 due to missing loss information\n",
      "Skip training data 5675 due to missing loss information\n",
      "Skip training data 5676 due to missing loss information\n",
      "Skip training data 5677 due to missing loss information\n",
      "Skip training data 5678 due to missing loss information\n",
      "Skip training data 5679 due to missing loss information\n",
      "Skip training data 5680 due to missing loss information\n",
      "Skip training data 5681 due to missing loss information\n",
      "Skip training data 5682 due to missing loss information\n",
      "Skip training data 5683 due to missing loss information\n",
      "Skip training data 5684 due to missing loss information\n",
      "Skip training data 5685 due to missing loss information\n",
      "Skip training data 5686 due to missing loss information\n",
      "Skip training data 5687 due to missing loss information\n",
      "Skip training data 5688 due to missing loss information\n",
      "Skip training data 5689 due to missing loss information\n",
      "Skip training data 5690 due to missing loss information\n",
      "Skip training data 5691 due to missing loss information\n",
      "Skip training data 5692 due to missing loss information\n",
      "Skip training data 5693 due to missing loss information\n",
      "Skip training data 5694 due to missing loss information\n",
      "Skip training data 5695 due to missing loss information\n",
      "Skip training data 5696 due to missing loss information\n",
      "Skip training data 5697 due to missing loss information\n",
      "Skip training data 5698 due to missing loss information\n",
      "Skip training data 5699 due to missing loss information\n",
      "Skip training data 5700 due to missing loss information\n",
      "Skip training data 5701 due to missing loss information\n",
      "Skip training data 5702 due to missing loss information\n",
      "Skip training data 5703 due to missing loss information\n",
      "Skip training data 5704 due to missing loss information\n",
      "Skip training data 5705 due to missing loss information\n",
      "Skip training data 5706 due to missing loss information\n",
      "Skip training data 5707 due to missing loss information\n",
      "Skip training data 5708 due to missing loss information\n",
      "Skip training data 5709 due to missing loss information\n",
      "Skip training data 5710 due to missing loss information\n",
      "Skip training data 5711 due to missing loss information\n",
      "Skip training data 5712 due to missing loss information\n",
      "Skip training data 5713 due to missing loss information\n",
      "Skip training data 5714 due to missing loss information\n",
      "Skip training data 5715 due to missing loss information\n",
      "Skip training data 5716 due to missing loss information\n",
      "Skip training data 5717 due to missing loss information\n",
      "Skip training data 5718 due to missing loss information\n",
      "Skip training data 5719 due to missing loss information\n",
      "Skip training data 5720 due to missing loss information\n",
      "Skip training data 5721 due to missing loss information\n",
      "Skip training data 5722 due to missing loss information\n",
      "Skip training data 5723 due to missing loss information\n",
      "Skip training data 5724 due to missing loss information\n",
      "Skip training data 5725 due to missing loss information\n",
      "Skip training data 5726 due to missing loss information\n",
      "Skip training data 5727 due to missing loss information\n",
      "Skip training data 5728 due to missing loss information\n",
      "Skip training data 5729 due to missing loss information\n",
      "Skip training data 5730 due to missing loss information\n",
      "Skip training data 5731 due to missing loss information\n",
      "Skip training data 5732 due to missing loss information\n",
      "Skip training data 5733 due to missing loss information\n",
      "Skip training data 5734 due to missing loss information\n",
      "Skip training data 5735 due to missing loss information\n",
      "Skip training data 5736 due to missing loss information\n",
      "Skip training data 5737 due to missing loss information\n",
      "Skip training data 5738 due to missing loss information\n",
      "Skip training data 5739 due to missing loss information\n",
      "Skip training data 5740 due to missing loss information\n",
      "Skip training data 5741 due to missing loss information\n",
      "Skip training data 5742 due to missing loss information\n",
      "Skip training data 5743 due to missing loss information\n",
      "Skip training data 5744 due to missing loss information\n",
      "Skip training data 5745 due to missing loss information\n",
      "Skip training data 5746 due to missing loss information\n",
      "Skip training data 5747 due to missing loss information\n",
      "Skip training data 5748 due to missing loss information\n",
      "Skip training data 5749 due to missing loss information\n",
      "Skip training data 5750 due to missing loss information\n",
      "Skip training data 5751 due to missing loss information\n",
      "Skip training data 5752 due to missing loss information\n",
      "Skip training data 5753 due to missing loss information\n",
      "Skip training data 5754 due to missing loss information\n",
      "Skip training data 5755 due to missing loss information\n",
      "Skip training data 5756 due to missing loss information\n",
      "Skip training data 5757 due to missing loss information\n",
      "Skip training data 5758 due to missing loss information\n",
      "Skip training data 5759 due to missing loss information\n",
      "Skip training data 5760 due to missing loss information\n",
      "Skip training data 5761 due to missing loss information\n",
      "Skip training data 5762 due to missing loss information\n",
      "Skip training data 5763 due to missing loss information\n",
      "Skip training data 5764 due to missing loss information\n",
      "Skip training data 5765 due to missing loss information\n",
      "Skip training data 5766 due to missing loss information\n",
      "Skip training data 5767 due to missing loss information\n",
      "Skip training data 5768 due to missing loss information\n",
      "Skip training data 5769 due to missing loss information\n",
      "Skip training data 5770 due to missing loss information\n",
      "Skip training data 5771 due to missing loss information\n",
      "Skip training data 5772 due to missing loss information\n",
      "Skip training data 5773 due to missing loss information\n",
      "Skip training data 5774 due to missing loss information\n",
      "Skip training data 5775 due to missing loss information\n",
      "Skip training data 5776 due to missing loss information\n",
      "Skip training data 5777 due to missing loss information\n",
      "Skip training data 5778 due to missing loss information\n",
      "Skip training data 5779 due to missing loss information\n",
      "Skip training data 5780 due to missing loss information\n",
      "Skip training data 5781 due to missing loss information\n",
      "Skip training data 5782 due to missing loss information\n",
      "Skip training data 5783 due to missing loss information\n",
      "Skip training data 5784 due to missing loss information\n",
      "Skip training data 5785 due to missing loss information\n",
      "Skip training data 5786 due to missing loss information\n",
      "Skip training data 5787 due to missing loss information\n",
      "Skip training data 5788 due to missing loss information\n",
      "Skip training data 5789 due to missing loss information\n",
      "Skip training data 5790 due to missing loss information\n",
      "Skip training data 5791 due to missing loss information\n",
      "Skip training data 5792 due to missing loss information\n",
      "Skip training data 5793 due to missing loss information\n",
      "Skip training data 5794 due to missing loss information\n",
      "Skip training data 5795 due to missing loss information\n",
      "Skip training data 5796 due to missing loss information\n",
      "Skip training data 5797 due to missing loss information\n",
      "Skip training data 5798 due to missing loss information\n",
      "Skip training data 5799 due to missing loss information\n",
      "Skip training data 5800 due to missing loss information\n",
      "Skip training data 5801 due to missing loss information\n",
      "Skip training data 5802 due to missing loss information\n",
      "Skip training data 5803 due to missing loss information\n",
      "Skip training data 5804 due to missing loss information\n",
      "Skip training data 5805 due to missing loss information\n",
      "Skip training data 5806 due to missing loss information\n",
      "Skip training data 5807 due to missing loss information\n",
      "Skip training data 5808 due to missing loss information\n",
      "Skip training data 5809 due to missing loss information\n",
      "Skip training data 5810 due to missing loss information\n",
      "Skip training data 5811 due to missing loss information\n",
      "Skip training data 5812 due to missing loss information\n",
      "Skip training data 5813 due to missing loss information\n",
      "Skip training data 5814 due to missing loss information\n",
      "Skip training data 5815 due to missing loss information\n",
      "Skip training data 5816 due to missing loss information\n",
      "Skip training data 5817 due to missing loss information\n",
      "Skip training data 5818 due to missing loss information\n",
      "Skip training data 5819 due to missing loss information\n",
      "Skip training data 5820 due to missing loss information\n",
      "Skip training data 5821 due to missing loss information\n",
      "Skip training data 5822 due to missing loss information\n",
      "Skip training data 5823 due to missing loss information\n",
      "Skip training data 5824 due to missing loss information\n",
      "Skip training data 5825 due to missing loss information\n",
      "Skip training data 5826 due to missing loss information\n",
      "Skip training data 5827 due to missing loss information\n",
      "Skip training data 5828 due to missing loss information\n",
      "Skip training data 5829 due to missing loss information\n",
      "Skip training data 5830 due to missing loss information\n",
      "Skip training data 5831 due to missing loss information\n",
      "Skip training data 5832 due to missing loss information\n",
      "Skip training data 5833 due to missing loss information\n",
      "Skip training data 5834 due to missing loss information\n",
      "Skip training data 5835 due to missing loss information\n",
      "Skip training data 5836 due to missing loss information\n",
      "Skip training data 5837 due to missing loss information\n",
      "Skip training data 5838 due to missing loss information\n",
      "Skip training data 5839 due to missing loss information\n",
      "Skip training data 5840 due to missing loss information\n",
      "Skip training data 5841 due to missing loss information\n",
      "Skip training data 5842 due to missing loss information\n",
      "Skip training data 5843 due to missing loss information\n",
      "Skip training data 5844 due to missing loss information\n",
      "Skip training data 5845 due to missing loss information\n",
      "Skip training data 5846 due to missing loss information\n",
      "Skip training data 5847 due to missing loss information\n",
      "Skip training data 5848 due to missing loss information\n",
      "Skip training data 5849 due to missing loss information\n",
      "Skip training data 5850 due to missing loss information\n",
      "Skip training data 5851 due to missing loss information\n",
      "Skip training data 5852 due to missing loss information\n",
      "Skip training data 5853 due to missing loss information\n",
      "Skip training data 5854 due to missing loss information\n",
      "Skip training data 5855 due to missing loss information\n",
      "Skip training data 5856 due to missing loss information\n",
      "Skip training data 5857 due to missing loss information\n",
      "Skip training data 5858 due to missing loss information\n",
      "Skip training data 5859 due to missing loss information\n",
      "Skip training data 5860 due to missing loss information\n",
      "Skip training data 5861 due to missing loss information\n",
      "Skip training data 5862 due to missing loss information\n",
      "Skip training data 5863 due to missing loss information\n",
      "Skip training data 5864 due to missing loss information\n",
      "Skip training data 5865 due to missing loss information\n",
      "Skip training data 5866 due to missing loss information\n",
      "Skip training data 5867 due to missing loss information\n",
      "Skip training data 5868 due to missing loss information\n",
      "Skip training data 5869 due to missing loss information\n",
      "Skip training data 5870 due to missing loss information\n",
      "Skip training data 5871 due to missing loss information\n",
      "Skip training data 5872 due to missing loss information\n",
      "Skip training data 5873 due to missing loss information\n",
      "Skip training data 5874 due to missing loss information\n",
      "Skip training data 5875 due to missing loss information\n",
      "Skip training data 5876 due to missing loss information\n",
      "Skip training data 5877 due to missing loss information\n",
      "Skip training data 5878 due to missing loss information\n",
      "Skip training data 5879 due to missing loss information\n",
      "Skip training data 5880 due to missing loss information\n",
      "Skip training data 5881 due to missing loss information\n",
      "Skip training data 5882 due to missing loss information\n",
      "Skip training data 5883 due to missing loss information\n",
      "Skip training data 5884 due to missing loss information\n",
      "Skip training data 5885 due to missing loss information\n",
      "Skip training data 5886 due to missing loss information\n",
      "Skip training data 5887 due to missing loss information\n",
      "Skip training data 5888 due to missing loss information\n",
      "Skip training data 5889 due to missing loss information\n",
      "Skip training data 5890 due to missing loss information\n",
      "Skip training data 5891 due to missing loss information\n",
      "Skip training data 5892 due to missing loss information\n",
      "Skip training data 5893 due to missing loss information\n",
      "Skip training data 5894 due to missing loss information\n",
      "Skip training data 5895 due to missing loss information\n",
      "Skip training data 5896 due to missing loss information\n",
      "Skip training data 5897 due to missing loss information\n",
      "Skip training data 5898 due to missing loss information\n",
      "Skip training data 5899 due to missing loss information\n",
      "Skip training data 5900 due to missing loss information\n",
      "Skip training data 5901 due to missing loss information\n",
      "Skip training data 5902 due to missing loss information\n",
      "Skip training data 5903 due to missing loss information\n",
      "Skip training data 5904 due to missing loss information\n",
      "Skip training data 5905 due to missing loss information\n",
      "Skip training data 5906 due to missing loss information\n",
      "Skip training data 5907 due to missing loss information\n",
      "Skip training data 5908 due to missing loss information\n",
      "Skip training data 5909 due to missing loss information\n",
      "Skip training data 5910 due to missing loss information\n",
      "Skip training data 5911 due to missing loss information\n",
      "Skip training data 5912 due to missing loss information\n",
      "Skip training data 5913 due to missing loss information\n",
      "Skip training data 5914 due to missing loss information\n",
      "Skip training data 5915 due to missing loss information\n",
      "Skip training data 5916 due to missing loss information\n",
      "Skip training data 5917 due to missing loss information\n",
      "Skip training data 5918 due to missing loss information\n",
      "Skip training data 5919 due to missing loss information\n",
      "Skip training data 5920 due to missing loss information\n",
      "Skip training data 5921 due to missing loss information\n",
      "Skip training data 5922 due to missing loss information\n",
      "Skip training data 5923 due to missing loss information\n",
      "Skip training data 5924 due to missing loss information\n",
      "Skip training data 5925 due to missing loss information\n",
      "Skip training data 5926 due to missing loss information\n",
      "Skip training data 5927 due to missing loss information\n",
      "Skip training data 5928 due to missing loss information\n",
      "Skip training data 5929 due to missing loss information\n",
      "Skip training data 5930 due to missing loss information\n",
      "Skip training data 5931 due to missing loss information\n",
      "Skip training data 5932 due to missing loss information\n",
      "Skip training data 5933 due to missing loss information\n",
      "Skip training data 5934 due to missing loss information\n",
      "Skip training data 5935 due to missing loss information\n",
      "Skip training data 5936 due to missing loss information\n",
      "Skip training data 5937 due to missing loss information\n",
      "Skip training data 5938 due to missing loss information\n",
      "Skip training data 5939 due to missing loss information\n",
      "Skip training data 5940 due to missing loss information\n",
      "Skip training data 5941 due to missing loss information\n",
      "Skip training data 5942 due to missing loss information\n",
      "Skip training data 5943 due to missing loss information\n",
      "Skip training data 5944 due to missing loss information\n",
      "Skip training data 5945 due to missing loss information\n",
      "Skip training data 5946 due to missing loss information\n",
      "Skip training data 5947 due to missing loss information\n",
      "Skip training data 5948 due to missing loss information\n",
      "Skip training data 5949 due to missing loss information\n",
      "Skip training data 5950 due to missing loss information\n",
      "Skip training data 5951 due to missing loss information\n",
      "Skip training data 5952 due to missing loss information\n",
      "Skip training data 5953 due to missing loss information\n",
      "Skip training data 5954 due to missing loss information\n",
      "Skip training data 5955 due to missing loss information\n",
      "Skip training data 5956 due to missing loss information\n",
      "Skip training data 5957 due to missing loss information\n",
      "Skip training data 5958 due to missing loss information\n",
      "Skip training data 5959 due to missing loss information\n",
      "Skip training data 5960 due to missing loss information\n",
      "Skip training data 5961 due to missing loss information\n",
      "Skip training data 5962 due to missing loss information\n",
      "Skip training data 5963 due to missing loss information\n",
      "Skip training data 5964 due to missing loss information\n",
      "Skip training data 5965 due to missing loss information\n",
      "Skip training data 5966 due to missing loss information\n",
      "Skip training data 5967 due to missing loss information\n",
      "Skip training data 5968 due to missing loss information\n",
      "Skip training data 5969 due to missing loss information\n",
      "Skip training data 5970 due to missing loss information\n",
      "Skip training data 5971 due to missing loss information\n",
      "Skip training data 5972 due to missing loss information\n",
      "Skip training data 5973 due to missing loss information\n",
      "Skip training data 5974 due to missing loss information\n",
      "Skip training data 5975 due to missing loss information\n",
      "Skip training data 5976 due to missing loss information\n",
      "Skip training data 5977 due to missing loss information\n",
      "Skip training data 5978 due to missing loss information\n",
      "Skip training data 5979 due to missing loss information\n",
      "Skip training data 5980 due to missing loss information\n",
      "Skip training data 5981 due to missing loss information\n",
      "Skip training data 5982 due to missing loss information\n",
      "Skip training data 5983 due to missing loss information\n",
      "Skip training data 5984 due to missing loss information\n",
      "Skip training data 5985 due to missing loss information\n",
      "Skip training data 5986 due to missing loss information\n",
      "Skip training data 5987 due to missing loss information\n",
      "Skip training data 5988 due to missing loss information\n",
      "Skip training data 5989 due to missing loss information\n",
      "Skip training data 5990 due to missing loss information\n",
      "Skip training data 5991 due to missing loss information\n",
      "Skip training data 5992 due to missing loss information\n",
      "Skip training data 5993 due to missing loss information\n",
      "Skip training data 5994 due to missing loss information\n",
      "Skip training data 5995 due to missing loss information\n",
      "Skip training data 5996 due to missing loss information\n",
      "Skip training data 5997 due to missing loss information\n",
      "Skip training data 5998 due to missing loss information\n",
      "Skip training data 5999 due to missing loss information\n",
      "Skip training data 6000 due to missing loss information\n",
      "Skip training data 6001 due to missing loss information\n",
      "Skip training data 6002 due to missing loss information\n",
      "Skip training data 6003 due to missing loss information\n",
      "Skip training data 6004 due to missing loss information\n",
      "Skip training data 6005 due to missing loss information\n",
      "Skip training data 6006 due to missing loss information\n",
      "Skip training data 6007 due to missing loss information\n",
      "Skip training data 6008 due to missing loss information\n",
      "Skip training data 6009 due to missing loss information\n",
      "Skip training data 6010 due to missing loss information\n",
      "Skip training data 6011 due to missing loss information\n",
      "Skip training data 6012 due to missing loss information\n",
      "Skip training data 6013 due to missing loss information\n",
      "Skip training data 6014 due to missing loss information\n",
      "Skip training data 6015 due to missing loss information\n",
      "Skip training data 6016 due to missing loss information\n",
      "Skip training data 6017 due to missing loss information\n",
      "Skip training data 6018 due to missing loss information\n",
      "Skip training data 6019 due to missing loss information\n",
      "Skip training data 6020 due to missing loss information\n",
      "Skip training data 6021 due to missing loss information\n",
      "Skip training data 6022 due to missing loss information\n",
      "Skip training data 6023 due to missing loss information\n",
      "Skip training data 6024 due to missing loss information\n",
      "Skip training data 6025 due to missing loss information\n",
      "Skip training data 6026 due to missing loss information\n",
      "Skip training data 6027 due to missing loss information\n",
      "Skip training data 6028 due to missing loss information\n",
      "Skip training data 6029 due to missing loss information\n",
      "Skip training data 6030 due to missing loss information\n",
      "Skip training data 6031 due to missing loss information\n",
      "Skip training data 6032 due to missing loss information\n",
      "Skip training data 6033 due to missing loss information\n",
      "Skip training data 6034 due to missing loss information\n",
      "Skip training data 6035 due to missing loss information\n",
      "Skip training data 6036 due to missing loss information\n",
      "Skip training data 6037 due to missing loss information\n",
      "Skip training data 6038 due to missing loss information\n",
      "Skip training data 6039 due to missing loss information\n",
      "Skip training data 6040 due to missing loss information\n",
      "Skip training data 6041 due to missing loss information\n",
      "Skip training data 6042 due to missing loss information\n",
      "Skip training data 6043 due to missing loss information\n",
      "Skip training data 6044 due to missing loss information\n",
      "Skip training data 6045 due to missing loss information\n",
      "Skip training data 6046 due to missing loss information\n",
      "Skip training data 6047 due to missing loss information\n",
      "Skip training data 6048 due to missing loss information\n",
      "Skip training data 6049 due to missing loss information\n",
      "Skip training data 6050 due to missing loss information\n",
      "Skip training data 6051 due to missing loss information\n",
      "Skip training data 6052 due to missing loss information\n",
      "Skip training data 6053 due to missing loss information\n",
      "Skip training data 6054 due to missing loss information\n",
      "Skip training data 6055 due to missing loss information\n",
      "Skip training data 6056 due to missing loss information\n",
      "Skip training data 6057 due to missing loss information\n",
      "Skip training data 6058 due to missing loss information\n",
      "Skip training data 6059 due to missing loss information\n",
      "Skip training data 6060 due to missing loss information\n",
      "Skip training data 6061 due to missing loss information\n",
      "Skip training data 6062 due to missing loss information\n",
      "Skip training data 6063 due to missing loss information\n",
      "Skip training data 6064 due to missing loss information\n",
      "Skip training data 6065 due to missing loss information\n",
      "Skip training data 6066 due to missing loss information\n",
      "Skip training data 6067 due to missing loss information\n",
      "Skip training data 6068 due to missing loss information\n",
      "Skip training data 6069 due to missing loss information\n",
      "Skip training data 6070 due to missing loss information\n",
      "Skip training data 6071 due to missing loss information\n",
      "Skip training data 6072 due to missing loss information\n",
      "Skip training data 6073 due to missing loss information\n",
      "Skip training data 6074 due to missing loss information\n",
      "Skip training data 6075 due to missing loss information\n",
      "Skip training data 6076 due to missing loss information\n",
      "Skip training data 6077 due to missing loss information\n",
      "Skip training data 6078 due to missing loss information\n",
      "Skip training data 6079 due to missing loss information\n",
      "Skip training data 6080 due to missing loss information\n",
      "Skip training data 6081 due to missing loss information\n",
      "Skip training data 6082 due to missing loss information\n",
      "Skip training data 6083 due to missing loss information\n",
      "Skip training data 6084 due to missing loss information\n",
      "Skip training data 6085 due to missing loss information\n",
      "Skip training data 6086 due to missing loss information\n",
      "Skip training data 6087 due to missing loss information\n",
      "Skip training data 6088 due to missing loss information\n",
      "Skip training data 6089 due to missing loss information\n",
      "Skip training data 6090 due to missing loss information\n",
      "Skip training data 6091 due to missing loss information\n",
      "Skip training data 6092 due to missing loss information\n",
      "Skip training data 6093 due to missing loss information\n",
      "Skip training data 6094 due to missing loss information\n",
      "Skip training data 6095 due to missing loss information\n",
      "Skip training data 6096 due to missing loss information\n",
      "Skip training data 6097 due to missing loss information\n",
      "Skip training data 6098 due to missing loss information\n",
      "Skip training data 6099 due to missing loss information\n",
      "Skip training data 6100 due to missing loss information\n",
      "Skip training data 6101 due to missing loss information\n",
      "Skip training data 6102 due to missing loss information\n",
      "Skip training data 6103 due to missing loss information\n",
      "Skip training data 6104 due to missing loss information\n",
      "Skip training data 6105 due to missing loss information\n",
      "Skip training data 6106 due to missing loss information\n",
      "Skip training data 6107 due to missing loss information\n",
      "Skip training data 6108 due to missing loss information\n",
      "Skip training data 6109 due to missing loss information\n",
      "Skip training data 6110 due to missing loss information\n",
      "Skip training data 6111 due to missing loss information\n",
      "Skip training data 6112 due to missing loss information\n",
      "Skip training data 6113 due to missing loss information\n",
      "Skip training data 6114 due to missing loss information\n",
      "Skip training data 6115 due to missing loss information\n",
      "Skip training data 6116 due to missing loss information\n",
      "Skip training data 6117 due to missing loss information\n",
      "Skip training data 6118 due to missing loss information\n",
      "Skip training data 6119 due to missing loss information\n",
      "Skip training data 6120 due to missing loss information\n",
      "Skip training data 6121 due to missing loss information\n",
      "Skip training data 6122 due to missing loss information\n",
      "Skip training data 6123 due to missing loss information\n",
      "Skip training data 6124 due to missing loss information\n",
      "Skip training data 6125 due to missing loss information\n",
      "Skip training data 6126 due to missing loss information\n",
      "Skip training data 6127 due to missing loss information\n",
      "Skip training data 6128 due to missing loss information\n",
      "Skip training data 6129 due to missing loss information\n",
      "Skip training data 6130 due to missing loss information\n",
      "Skip training data 6131 due to missing loss information\n",
      "Skip training data 6132 due to missing loss information\n",
      "Skip training data 6133 due to missing loss information\n",
      "Skip training data 6134 due to missing loss information\n",
      "Skip training data 6135 due to missing loss information\n",
      "Skip training data 6136 due to missing loss information\n",
      "Skip training data 6137 due to missing loss information\n",
      "Skip training data 6138 due to missing loss information\n",
      "Skip training data 6139 due to missing loss information\n",
      "Skip training data 6140 due to missing loss information\n",
      "Skip training data 6141 due to missing loss information\n",
      "Skip training data 6142 due to missing loss information\n",
      "Skip training data 6143 due to missing loss information\n",
      "Skip training data 6144 due to missing loss information\n",
      "Skip training data 6145 due to missing loss information\n",
      "Skip training data 6146 due to missing loss information\n",
      "Skip training data 6147 due to missing loss information\n",
      "Skip training data 6148 due to missing loss information\n",
      "Skip training data 6149 due to missing loss information\n",
      "Skip training data 6150 due to missing loss information\n",
      "Skip training data 6151 due to missing loss information\n",
      "Skip training data 6152 due to missing loss information\n",
      "Skip training data 6153 due to missing loss information\n",
      "Skip training data 6154 due to missing loss information\n",
      "Skip training data 6155 due to missing loss information\n",
      "Skip training data 6156 due to missing loss information\n",
      "Skip training data 6157 due to missing loss information\n",
      "Skip training data 6158 due to missing loss information\n",
      "Skip training data 6159 due to missing loss information\n",
      "Skip training data 6160 due to missing loss information\n",
      "Skip training data 6161 due to missing loss information\n",
      "Skip training data 6162 due to missing loss information\n",
      "Skip training data 6163 due to missing loss information\n",
      "Skip training data 6164 due to missing loss information\n",
      "Skip training data 6165 due to missing loss information\n",
      "Skip training data 6166 due to missing loss information\n",
      "Skip training data 6167 due to missing loss information\n",
      "Skip training data 6168 due to missing loss information\n",
      "Skip training data 6169 due to missing loss information\n",
      "Skip training data 6170 due to missing loss information\n",
      "Skip training data 6171 due to missing loss information\n",
      "Skip training data 6172 due to missing loss information\n",
      "Skip training data 6173 due to missing loss information\n",
      "Skip training data 6174 due to missing loss information\n",
      "Skip training data 6175 due to missing loss information\n",
      "Skip training data 6176 due to missing loss information\n",
      "Skip training data 6177 due to missing loss information\n",
      "Skip training data 6178 due to missing loss information\n",
      "Skip training data 6179 due to missing loss information\n",
      "Skip training data 6180 due to missing loss information\n",
      "Skip training data 6181 due to missing loss information\n",
      "Skip training data 6182 due to missing loss information\n",
      "Skip training data 6183 due to missing loss information\n",
      "Skip training data 6184 due to missing loss information\n",
      "Skip training data 6185 due to missing loss information\n",
      "Skip training data 6186 due to missing loss information\n",
      "Skip training data 6187 due to missing loss information\n",
      "Skip training data 6188 due to missing loss information\n",
      "Skip training data 6189 due to missing loss information\n",
      "Skip training data 6190 due to missing loss information\n",
      "Skip training data 6191 due to missing loss information\n",
      "Skip training data 6192 due to missing loss information\n",
      "Skip training data 6193 due to missing loss information\n",
      "Skip training data 6194 due to missing loss information\n",
      "Skip training data 6195 due to missing loss information\n",
      "Skip training data 6196 due to missing loss information\n",
      "Skip training data 6197 due to missing loss information\n",
      "Skip training data 6198 due to missing loss information\n",
      "Skip training data 6199 due to missing loss information\n",
      "Skip training data 6200 due to missing loss information\n",
      "Skip training data 6201 due to missing loss information\n",
      "Skip training data 6202 due to missing loss information\n",
      "Skip training data 6203 due to missing loss information\n",
      "Skip training data 6204 due to missing loss information\n",
      "Skip training data 6205 due to missing loss information\n",
      "Skip training data 6206 due to missing loss information\n",
      "Skip training data 6207 due to missing loss information\n",
      "Skip training data 6208 due to missing loss information\n",
      "Skip training data 6209 due to missing loss information\n",
      "Skip training data 6210 due to missing loss information\n",
      "Skip training data 6211 due to missing loss information\n",
      "Skip training data 6212 due to missing loss information\n",
      "Skip training data 6213 due to missing loss information\n",
      "Skip training data 6214 due to missing loss information\n",
      "Skip training data 6215 due to missing loss information\n",
      "Skip training data 6216 due to missing loss information\n",
      "Skip training data 6217 due to missing loss information\n",
      "Skip training data 6218 due to missing loss information\n",
      "Skip training data 6219 due to missing loss information\n",
      "Skip training data 6220 due to missing loss information\n",
      "Skip training data 6221 due to missing loss information\n",
      "Skip training data 6222 due to missing loss information\n",
      "Skip training data 6223 due to missing loss information\n",
      "Skip training data 6224 due to missing loss information\n",
      "Skip training data 6225 due to missing loss information\n",
      "Skip training data 6226 due to missing loss information\n",
      "Skip training data 6227 due to missing loss information\n",
      "Skip training data 6228 due to missing loss information\n",
      "Skip training data 6229 due to missing loss information\n",
      "Skip training data 6230 due to missing loss information\n",
      "Skip training data 6231 due to missing loss information\n",
      "Skip training data 6232 due to missing loss information\n",
      "Skip training data 6233 due to missing loss information\n",
      "Skip training data 6234 due to missing loss information\n",
      "Skip training data 6235 due to missing loss information\n",
      "Skip training data 6236 due to missing loss information\n",
      "Skip training data 6237 due to missing loss information\n",
      "Skip training data 6238 due to missing loss information\n",
      "Skip training data 6239 due to missing loss information\n",
      "Skip training data 6240 due to missing loss information\n",
      "Skip training data 6241 due to missing loss information\n",
      "Skip training data 6242 due to missing loss information\n",
      "Skip training data 6243 due to missing loss information\n",
      "Skip training data 6244 due to missing loss information\n",
      "Skip training data 6245 due to missing loss information\n",
      "Skip training data 6246 due to missing loss information\n",
      "Skip training data 6247 due to missing loss information\n",
      "Skip training data 6248 due to missing loss information\n",
      "Skip training data 6249 due to missing loss information\n",
      "Skip training data 6250 due to missing loss information\n",
      "Skip training data 6251 due to missing loss information\n",
      "Skip training data 6252 due to missing loss information\n",
      "Skip training data 6253 due to missing loss information\n",
      "Skip training data 6254 due to missing loss information\n",
      "Skip training data 6255 due to missing loss information\n",
      "Skip training data 6256 due to missing loss information\n",
      "Skip training data 6257 due to missing loss information\n",
      "Skip training data 6258 due to missing loss information\n",
      "Skip training data 6259 due to missing loss information\n",
      "Skip training data 6260 due to missing loss information\n",
      "Skip training data 6261 due to missing loss information\n",
      "Skip training data 6262 due to missing loss information\n",
      "Skip training data 6263 due to missing loss information\n",
      "Skip training data 6264 due to missing loss information\n",
      "Skip training data 6265 due to missing loss information\n",
      "Skip training data 6266 due to missing loss information\n",
      "Skip training data 6267 due to missing loss information\n",
      "Skip training data 6268 due to missing loss information\n",
      "Skip training data 6269 due to missing loss information\n",
      "Skip training data 6270 due to missing loss information\n",
      "Skip training data 6271 due to missing loss information\n",
      "Skip training data 6272 due to missing loss information\n",
      "Skip training data 6273 due to missing loss information\n",
      "Skip training data 6274 due to missing loss information\n",
      "Skip training data 6275 due to missing loss information\n",
      "Skip training data 6276 due to missing loss information\n",
      "Skip training data 6277 due to missing loss information\n",
      "Skip training data 6278 due to missing loss information\n",
      "Skip training data 6279 due to missing loss information\n",
      "Skip training data 6280 due to missing loss information\n",
      "Skip training data 6281 due to missing loss information\n",
      "Skip training data 6282 due to missing loss information\n",
      "Skip training data 6283 due to missing loss information\n",
      "Skip training data 6284 due to missing loss information\n",
      "Skip training data 6285 due to missing loss information\n",
      "Skip training data 6286 due to missing loss information\n",
      "Skip training data 6287 due to missing loss information\n",
      "Skip training data 6288 due to missing loss information\n",
      "Skip training data 6289 due to missing loss information\n",
      "Skip training data 6290 due to missing loss information\n",
      "Skip training data 6291 due to missing loss information\n",
      "Skip training data 6292 due to missing loss information\n",
      "Skip training data 6293 due to missing loss information\n",
      "Skip training data 6294 due to missing loss information\n",
      "Skip training data 6295 due to missing loss information\n",
      "Skip training data 6296 due to missing loss information\n",
      "Skip training data 6297 due to missing loss information\n",
      "Skip training data 6298 due to missing loss information\n",
      "Skip training data 6299 due to missing loss information\n",
      "Skip training data 6300 due to missing loss information\n",
      "Skip training data 6301 due to missing loss information\n",
      "Skip training data 6302 due to missing loss information\n",
      "Skip training data 6303 due to missing loss information\n",
      "Skip training data 6304 due to missing loss information\n",
      "Skip training data 6305 due to missing loss information\n",
      "Skip training data 6306 due to missing loss information\n",
      "Skip training data 6307 due to missing loss information\n",
      "Skip training data 6308 due to missing loss information\n",
      "Skip training data 6309 due to missing loss information\n",
      "Skip training data 6310 due to missing loss information\n",
      "Skip training data 6311 due to missing loss information\n",
      "Skip training data 6312 due to missing loss information\n",
      "Skip training data 6313 due to missing loss information\n",
      "Skip training data 6314 due to missing loss information\n",
      "Skip training data 6315 due to missing loss information\n",
      "Skip training data 6316 due to missing loss information\n",
      "Skip training data 6317 due to missing loss information\n",
      "Skip training data 6318 due to missing loss information\n",
      "Skip training data 6319 due to missing loss information\n",
      "Skip training data 6320 due to missing loss information\n",
      "Skip training data 6321 due to missing loss information\n",
      "Skip training data 6322 due to missing loss information\n",
      "Skip training data 6323 due to missing loss information\n",
      "Skip training data 6324 due to missing loss information\n",
      "Skip training data 6325 due to missing loss information\n",
      "Skip training data 6326 due to missing loss information\n",
      "Skip training data 6327 due to missing loss information\n",
      "Skip training data 6328 due to missing loss information\n",
      "Skip training data 6329 due to missing loss information\n",
      "Skip training data 6330 due to missing loss information\n",
      "Skip training data 6331 due to missing loss information\n",
      "Skip training data 6332 due to missing loss information\n",
      "Skip training data 6333 due to missing loss information\n",
      "Skip training data 6334 due to missing loss information\n",
      "Skip training data 6335 due to missing loss information\n",
      "Skip training data 6336 due to missing loss information\n",
      "Skip training data 6337 due to missing loss information\n",
      "Skip training data 6338 due to missing loss information\n",
      "Skip training data 6339 due to missing loss information\n",
      "Skip training data 6340 due to missing loss information\n",
      "Skip training data 6341 due to missing loss information\n",
      "Skip training data 6342 due to missing loss information\n",
      "Skip training data 6343 due to missing loss information\n",
      "Skip training data 6344 due to missing loss information\n",
      "Skip training data 6345 due to missing loss information\n",
      "Skip training data 6346 due to missing loss information\n",
      "Skip training data 6347 due to missing loss information\n",
      "Skip training data 6348 due to missing loss information\n",
      "Skip training data 6349 due to missing loss information\n",
      "Skip training data 6350 due to missing loss information\n",
      "Skip training data 6351 due to missing loss information\n",
      "Skip training data 6352 due to missing loss information\n",
      "Skip training data 6353 due to missing loss information\n",
      "Skip training data 6354 due to missing loss information\n",
      "Skip training data 6355 due to missing loss information\n",
      "Skip training data 6356 due to missing loss information\n",
      "Skip training data 6357 due to missing loss information\n",
      "Skip training data 6358 due to missing loss information\n",
      "Skip training data 6359 due to missing loss information\n",
      "Skip training data 6360 due to missing loss information\n",
      "Skip training data 6361 due to missing loss information\n",
      "Skip training data 6362 due to missing loss information\n",
      "Skip training data 6363 due to missing loss information\n",
      "Skip training data 6364 due to missing loss information\n",
      "Skip training data 6365 due to missing loss information\n",
      "Skip training data 6366 due to missing loss information\n",
      "Skip training data 6367 due to missing loss information\n",
      "Skip training data 6368 due to missing loss information\n",
      "Skip training data 6369 due to missing loss information\n",
      "Skip training data 6370 due to missing loss information\n",
      "Skip training data 6371 due to missing loss information\n",
      "Skip training data 6372 due to missing loss information\n",
      "Skip training data 6373 due to missing loss information\n",
      "Skip training data 6374 due to missing loss information\n",
      "Skip training data 6375 due to missing loss information\n",
      "Skip training data 6376 due to missing loss information\n",
      "Skip training data 6377 due to missing loss information\n",
      "Skip training data 6378 due to missing loss information\n",
      "Skip training data 6379 due to missing loss information\n",
      "Skip training data 6380 due to missing loss information\n",
      "Skip training data 6381 due to missing loss information\n",
      "Skip training data 6382 due to missing loss information\n",
      "Skip training data 6383 due to missing loss information\n",
      "Skip training data 6384 due to missing loss information\n",
      "Skip training data 6385 due to missing loss information\n",
      "Skip training data 6386 due to missing loss information\n",
      "Skip training data 6387 due to missing loss information\n",
      "Skip training data 6388 due to missing loss information\n",
      "Skip training data 6389 due to missing loss information\n",
      "Skip training data 6390 due to missing loss information\n",
      "Skip training data 6391 due to missing loss information\n",
      "Skip training data 6392 due to missing loss information\n",
      "Skip training data 6393 due to missing loss information\n",
      "Skip training data 6394 due to missing loss information\n",
      "Skip training data 6395 due to missing loss information\n",
      "Skip training data 6396 due to missing loss information\n",
      "Skip training data 6397 due to missing loss information\n",
      "Skip training data 6398 due to missing loss information\n",
      "Skip training data 6399 due to missing loss information\n",
      "Skip training data 6400 due to missing loss information\n",
      "Skip training data 6401 due to missing loss information\n",
      "Skip training data 6402 due to missing loss information\n",
      "Skip training data 6403 due to missing loss information\n",
      "Skip training data 6404 due to missing loss information\n",
      "Skip training data 6405 due to missing loss information\n",
      "Skip training data 6406 due to missing loss information\n",
      "Skip training data 6407 due to missing loss information\n",
      "Skip training data 6408 due to missing loss information\n",
      "Skip training data 6409 due to missing loss information\n",
      "Skip training data 6410 due to missing loss information\n",
      "Skip training data 6411 due to missing loss information\n",
      "Skip training data 6412 due to missing loss information\n",
      "Skip training data 6413 due to missing loss information\n",
      "Skip training data 6414 due to missing loss information\n",
      "Skip training data 6415 due to missing loss information\n",
      "Skip training data 6416 due to missing loss information\n",
      "Skip training data 6417 due to missing loss information\n",
      "Skip training data 6418 due to missing loss information\n",
      "Skip training data 6419 due to missing loss information\n",
      "Skip training data 6420 due to missing loss information\n",
      "Skip training data 6421 due to missing loss information\n",
      "Skip training data 6422 due to missing loss information\n",
      "Skip training data 6423 due to missing loss information\n",
      "Skip training data 6424 due to missing loss information\n",
      "Skip training data 6425 due to missing loss information\n",
      "Skip training data 6426 due to missing loss information\n",
      "Skip training data 6427 due to missing loss information\n",
      "Skip training data 6428 due to missing loss information\n",
      "Skip training data 6429 due to missing loss information\n",
      "Skip training data 6430 due to missing loss information\n",
      "Skip training data 6431 due to missing loss information\n",
      "Skip training data 6432 due to missing loss information\n",
      "Skip training data 6433 due to missing loss information\n",
      "Skip training data 6434 due to missing loss information\n",
      "Skip training data 6435 due to missing loss information\n",
      "Skip training data 6436 due to missing loss information\n",
      "Skip training data 6437 due to missing loss information\n",
      "Skip training data 6438 due to missing loss information\n",
      "Skip training data 6439 due to missing loss information\n",
      "Skip training data 6440 due to missing loss information\n",
      "Skip training data 6441 due to missing loss information\n",
      "Skip training data 6442 due to missing loss information\n",
      "Skip training data 6443 due to missing loss information\n",
      "Skip training data 6444 due to missing loss information\n",
      "Skip training data 6445 due to missing loss information\n",
      "Skip training data 6446 due to missing loss information\n",
      "Skip training data 6447 due to missing loss information\n",
      "Skip training data 6448 due to missing loss information\n",
      "Skip training data 6449 due to missing loss information\n",
      "Skip training data 6450 due to missing loss information\n",
      "Skip training data 6451 due to missing loss information\n",
      "Skip training data 6452 due to missing loss information\n",
      "Skip training data 6453 due to missing loss information\n",
      "Skip training data 6454 due to missing loss information\n",
      "Skip training data 6455 due to missing loss information\n",
      "Skip training data 6456 due to missing loss information\n",
      "Skip training data 6457 due to missing loss information\n",
      "Skip training data 6458 due to missing loss information\n",
      "Skip training data 6459 due to missing loss information\n",
      "Skip training data 6460 due to missing loss information\n",
      "Skip training data 6461 due to missing loss information\n",
      "Skip training data 6462 due to missing loss information\n",
      "Skip training data 6463 due to missing loss information\n",
      "Skip training data 6464 due to missing loss information\n",
      "Skip training data 6465 due to missing loss information\n",
      "Skip training data 6466 due to missing loss information\n",
      "Skip training data 6467 due to missing loss information\n",
      "Skip training data 6468 due to missing loss information\n",
      "Skip training data 6469 due to missing loss information\n",
      "Skip training data 6470 due to missing loss information\n",
      "Skip training data 6471 due to missing loss information\n",
      "Skip training data 6472 due to missing loss information\n",
      "Skip training data 6473 due to missing loss information\n",
      "Skip training data 6474 due to missing loss information\n",
      "Skip training data 6475 due to missing loss information\n",
      "Skip training data 6476 due to missing loss information\n",
      "Skip training data 6477 due to missing loss information\n",
      "Skip training data 6478 due to missing loss information\n",
      "Skip training data 6479 due to missing loss information\n",
      "Skip training data 6480 due to missing loss information\n",
      "Skip training data 6481 due to missing loss information\n",
      "Skip training data 6482 due to missing loss information\n",
      "Skip training data 6483 due to missing loss information\n",
      "Skip training data 6484 due to missing loss information\n",
      "Skip training data 6485 due to missing loss information\n",
      "Skip training data 6486 due to missing loss information\n",
      "Skip training data 6487 due to missing loss information\n",
      "Skip training data 6488 due to missing loss information\n",
      "Skip training data 6489 due to missing loss information\n",
      "Skip training data 6490 due to missing loss information\n",
      "Skip training data 6491 due to missing loss information\n",
      "Skip training data 6492 due to missing loss information\n",
      "Skip training data 6493 due to missing loss information\n",
      "Skip training data 6494 due to missing loss information\n",
      "Skip training data 6495 due to missing loss information\n",
      "Skip training data 6496 due to missing loss information\n",
      "Skip training data 6497 due to missing loss information\n",
      "Skip training data 6498 due to missing loss information\n",
      "Skip training data 6499 due to missing loss information\n",
      "Skip training data 6500 due to missing loss information\n",
      "Skip training data 6501 due to missing loss information\n",
      "Skip training data 6502 due to missing loss information\n",
      "Skip training data 6503 due to missing loss information\n",
      "Skip training data 6504 due to missing loss information\n",
      "Skip training data 6505 due to missing loss information\n",
      "Skip training data 6506 due to missing loss information\n",
      "Skip training data 6507 due to missing loss information\n",
      "Skip training data 6508 due to missing loss information\n",
      "Skip training data 6509 due to missing loss information\n",
      "Skip training data 6510 due to missing loss information\n",
      "Skip training data 6511 due to missing loss information\n",
      "Skip training data 6512 due to missing loss information\n",
      "Skip training data 6513 due to missing loss information\n",
      "Skip training data 6514 due to missing loss information\n",
      "Skip training data 6515 due to missing loss information\n",
      "Skip training data 6516 due to missing loss information\n",
      "Skip training data 6517 due to missing loss information\n",
      "Skip training data 6518 due to missing loss information\n",
      "Skip training data 6519 due to missing loss information\n",
      "Skip training data 6520 due to missing loss information\n",
      "Skip training data 6521 due to missing loss information\n",
      "Skip training data 6522 due to missing loss information\n",
      "Skip training data 6523 due to missing loss information\n",
      "Skip training data 6524 due to missing loss information\n",
      "Skip training data 6525 due to missing loss information\n",
      "Skip training data 6526 due to missing loss information\n",
      "Skip training data 6527 due to missing loss information\n",
      "Skip training data 6528 due to missing loss information\n",
      "Skip training data 6529 due to missing loss information\n",
      "Skip training data 6530 due to missing loss information\n",
      "Skip training data 6531 due to missing loss information\n",
      "Skip training data 6532 due to missing loss information\n",
      "Skip training data 6533 due to missing loss information\n",
      "Skip training data 6534 due to missing loss information\n",
      "Skip training data 6535 due to missing loss information\n",
      "Skip training data 6536 due to missing loss information\n",
      "Skip training data 6537 due to missing loss information\n",
      "Skip training data 6538 due to missing loss information\n",
      "Skip training data 6539 due to missing loss information\n",
      "Skip training data 6540 due to missing loss information\n",
      "Skip training data 6541 due to missing loss information\n",
      "Skip training data 6542 due to missing loss information\n",
      "Skip training data 6543 due to missing loss information\n",
      "Skip training data 6544 due to missing loss information\n",
      "Skip training data 6545 due to missing loss information\n",
      "Skip training data 6546 due to missing loss information\n",
      "Skip training data 6547 due to missing loss information\n",
      "Skip training data 6548 due to missing loss information\n",
      "Skip training data 6549 due to missing loss information\n",
      "Skip training data 6550 due to missing loss information\n",
      "Skip training data 6551 due to missing loss information\n",
      "Skip training data 6552 due to missing loss information\n",
      "Skip training data 6553 due to missing loss information\n",
      "Skip training data 6554 due to missing loss information\n",
      "Skip training data 6555 due to missing loss information\n",
      "Skip training data 6556 due to missing loss information\n",
      "Skip training data 6557 due to missing loss information\n",
      "Skip training data 6558 due to missing loss information\n",
      "Skip training data 6559 due to missing loss information\n",
      "Skip training data 6560 due to missing loss information\n",
      "Skip training data 6561 due to missing loss information\n",
      "Skip training data 6562 due to missing loss information\n",
      "Skip training data 6563 due to missing loss information\n",
      "Skip training data 6564 due to missing loss information\n",
      "Skip training data 6565 due to missing loss information\n",
      "Skip training data 6566 due to missing loss information\n",
      "Skip training data 6567 due to missing loss information\n",
      "Skip training data 6568 due to missing loss information\n",
      "Skip training data 6569 due to missing loss information\n",
      "Skip training data 6570 due to missing loss information\n",
      "Skip training data 6571 due to missing loss information\n",
      "Skip training data 6572 due to missing loss information\n",
      "Skip training data 6573 due to missing loss information\n",
      "Skip training data 6574 due to missing loss information\n",
      "Skip training data 6575 due to missing loss information\n",
      "Skip training data 6576 due to missing loss information\n",
      "Skip training data 6577 due to missing loss information\n",
      "Skip training data 6578 due to missing loss information\n",
      "Skip training data 6579 due to missing loss information\n",
      "Skip training data 6580 due to missing loss information\n",
      "Skip training data 6581 due to missing loss information\n",
      "Skip training data 6582 due to missing loss information\n",
      "Skip training data 6583 due to missing loss information\n",
      "Skip training data 6584 due to missing loss information\n",
      "Skip training data 6585 due to missing loss information\n",
      "Skip training data 6586 due to missing loss information\n",
      "Skip training data 6587 due to missing loss information\n",
      "Skip training data 6588 due to missing loss information\n",
      "Skip training data 6589 due to missing loss information\n",
      "Skip training data 6590 due to missing loss information\n",
      "Skip training data 6591 due to missing loss information\n",
      "Skip training data 6592 due to missing loss information\n",
      "Skip training data 6593 due to missing loss information\n",
      "Skip training data 6594 due to missing loss information\n",
      "Skip training data 6595 due to missing loss information\n",
      "Skip training data 6596 due to missing loss information\n",
      "Skip training data 6597 due to missing loss information\n",
      "Skip training data 6598 due to missing loss information\n",
      "Skip training data 6599 due to missing loss information\n",
      "Skip training data 6600 due to missing loss information\n",
      "Skip training data 6601 due to missing loss information\n",
      "Skip training data 6602 due to missing loss information\n",
      "Skip training data 6603 due to missing loss information\n",
      "Skip training data 6604 due to missing loss information\n",
      "Skip training data 6605 due to missing loss information\n",
      "Skip training data 6606 due to missing loss information\n",
      "Skip training data 6607 due to missing loss information\n",
      "Skip training data 6608 due to missing loss information\n",
      "Skip training data 6609 due to missing loss information\n",
      "Skip training data 6610 due to missing loss information\n",
      "Skip training data 6611 due to missing loss information\n",
      "Skip training data 6612 due to missing loss information\n",
      "Skip training data 6613 due to missing loss information\n",
      "Skip training data 6614 due to missing loss information\n",
      "Skip training data 6615 due to missing loss information\n",
      "Skip training data 6616 due to missing loss information\n",
      "Skip training data 6617 due to missing loss information\n",
      "Skip training data 6618 due to missing loss information\n",
      "Skip training data 6619 due to missing loss information\n",
      "Skip training data 6620 due to missing loss information\n",
      "Skip training data 6621 due to missing loss information\n",
      "Skip training data 6622 due to missing loss information\n",
      "Skip training data 6623 due to missing loss information\n",
      "Skip training data 6624 due to missing loss information\n",
      "Skip training data 6625 due to missing loss information\n",
      "Skip training data 6626 due to missing loss information\n",
      "Skip training data 6627 due to missing loss information\n",
      "Skip training data 6628 due to missing loss information\n",
      "Skip training data 6629 due to missing loss information\n",
      "Skip training data 6630 due to missing loss information\n",
      "Skip training data 6631 due to missing loss information\n",
      "Skip training data 6632 due to missing loss information\n",
      "Skip training data 6633 due to missing loss information\n",
      "Skip training data 6634 due to missing loss information\n",
      "Skip training data 6635 due to missing loss information\n",
      "Skip training data 6636 due to missing loss information\n",
      "Skip training data 6637 due to missing loss information\n",
      "Skip training data 6638 due to missing loss information\n",
      "Skip training data 6639 due to missing loss information\n",
      "Skip training data 6640 due to missing loss information\n",
      "Skip training data 6641 due to missing loss information\n",
      "Skip training data 6642 due to missing loss information\n",
      "Skip training data 6643 due to missing loss information\n",
      "Skip training data 6644 due to missing loss information\n",
      "Skip training data 6645 due to missing loss information\n",
      "Skip training data 6646 due to missing loss information\n",
      "Skip training data 6647 due to missing loss information\n",
      "Skip training data 6648 due to missing loss information\n",
      "Skip training data 6649 due to missing loss information\n",
      "Skip training data 6650 due to missing loss information\n",
      "Skip training data 6651 due to missing loss information\n",
      "Skip training data 6652 due to missing loss information\n",
      "Skip training data 6653 due to missing loss information\n",
      "Skip training data 6654 due to missing loss information\n",
      "Skip training data 6655 due to missing loss information\n",
      "Skip training data 6656 due to missing loss information\n",
      "Skip training data 6657 due to missing loss information\n",
      "Skip training data 6658 due to missing loss information\n",
      "Skip training data 6659 due to missing loss information\n",
      "Skip training data 6660 due to missing loss information\n",
      "Skip training data 6661 due to missing loss information\n",
      "Skip training data 6662 due to missing loss information\n",
      "Skip training data 6663 due to missing loss information\n",
      "Skip training data 6664 due to missing loss information\n",
      "Skip training data 6665 due to missing loss information\n",
      "Skip training data 6666 due to missing loss information\n",
      "Skip training data 6667 due to missing loss information\n",
      "Skip training data 6668 due to missing loss information\n",
      "Skip training data 6669 due to missing loss information\n",
      "Skip training data 6670 due to missing loss information\n",
      "Skip training data 6671 due to missing loss information\n",
      "Skip training data 6672 due to missing loss information\n",
      "Skip training data 6673 due to missing loss information\n",
      "Skip training data 6674 due to missing loss information\n",
      "Skip training data 6675 due to missing loss information\n",
      "Skip training data 6676 due to missing loss information\n",
      "Skip training data 6677 due to missing loss information\n",
      "Skip training data 6678 due to missing loss information\n",
      "Skip training data 6679 due to missing loss information\n",
      "Skip training data 6680 due to missing loss information\n",
      "Skip training data 6681 due to missing loss information\n",
      "Skip training data 6682 due to missing loss information\n",
      "Skip training data 6683 due to missing loss information\n",
      "Skip training data 6684 due to missing loss information\n",
      "Skip training data 6685 due to missing loss information\n",
      "Skip training data 6686 due to missing loss information\n",
      "Skip training data 6687 due to missing loss information\n",
      "Skip training data 6688 due to missing loss information\n",
      "Skip training data 6689 due to missing loss information\n",
      "Skip training data 6690 due to missing loss information\n",
      "Skip training data 6691 due to missing loss information\n",
      "Skip training data 6692 due to missing loss information\n",
      "Skip training data 6693 due to missing loss information\n",
      "Skip training data 6694 due to missing loss information\n",
      "Skip training data 6695 due to missing loss information\n",
      "Skip training data 6696 due to missing loss information\n",
      "Skip training data 6697 due to missing loss information\n",
      "Skip training data 6698 due to missing loss information\n",
      "Skip training data 6699 due to missing loss information\n",
      "Skip training data 6700 due to missing loss information\n",
      "Skip training data 6701 due to missing loss information\n",
      "Skip training data 6702 due to missing loss information\n",
      "Skip training data 6703 due to missing loss information\n",
      "Skip training data 6704 due to missing loss information\n",
      "Skip training data 6705 due to missing loss information\n",
      "Skip training data 6706 due to missing loss information\n",
      "Skip training data 6707 due to missing loss information\n",
      "Skip training data 6708 due to missing loss information\n",
      "Skip training data 6709 due to missing loss information\n",
      "Skip training data 6710 due to missing loss information\n",
      "Skip training data 6711 due to missing loss information\n",
      "Skip training data 6712 due to missing loss information\n",
      "Skip training data 6713 due to missing loss information\n",
      "Skip training data 6714 due to missing loss information\n",
      "Skip training data 6715 due to missing loss information\n",
      "Skip training data 6716 due to missing loss information\n",
      "Skip training data 6717 due to missing loss information\n",
      "Skip training data 6718 due to missing loss information\n",
      "Skip training data 6719 due to missing loss information\n",
      "Skip training data 6720 due to missing loss information\n",
      "Skip training data 6721 due to missing loss information\n",
      "Skip training data 6722 due to missing loss information\n",
      "Skip training data 6723 due to missing loss information\n",
      "Skip training data 6724 due to missing loss information\n",
      "Skip training data 6725 due to missing loss information\n",
      "Skip training data 6726 due to missing loss information\n",
      "Skip training data 6727 due to missing loss information\n",
      "Skip training data 6728 due to missing loss information\n",
      "Skip training data 6729 due to missing loss information\n",
      "Skip training data 6730 due to missing loss information\n",
      "Skip training data 6731 due to missing loss information\n",
      "Skip training data 6732 due to missing loss information\n",
      "Skip training data 6733 due to missing loss information\n",
      "Skip training data 6734 due to missing loss information\n",
      "Skip training data 6735 due to missing loss information\n",
      "Skip training data 6736 due to missing loss information\n",
      "Skip training data 6737 due to missing loss information\n",
      "Skip training data 6738 due to missing loss information\n",
      "Skip training data 6739 due to missing loss information\n",
      "Skip training data 6740 due to missing loss information\n",
      "Skip training data 6741 due to missing loss information\n",
      "Skip training data 6742 due to missing loss information\n",
      "Skip training data 6743 due to missing loss information\n",
      "Skip training data 6744 due to missing loss information\n",
      "Skip training data 6745 due to missing loss information\n",
      "Skip training data 6746 due to missing loss information\n",
      "Skip training data 6747 due to missing loss information\n",
      "Skip training data 6748 due to missing loss information\n",
      "Skip training data 6749 due to missing loss information\n",
      "Skip training data 6750 due to missing loss information\n",
      "Skip training data 6751 due to missing loss information\n",
      "Skip training data 6752 due to missing loss information\n",
      "Skip training data 6753 due to missing loss information\n",
      "Skip training data 6754 due to missing loss information\n",
      "Skip training data 6755 due to missing loss information\n",
      "Skip training data 6756 due to missing loss information\n",
      "Skip training data 6757 due to missing loss information\n",
      "Skip training data 6758 due to missing loss information\n",
      "Skip training data 6759 due to missing loss information\n",
      "Skip training data 6760 due to missing loss information\n",
      "Skip training data 6761 due to missing loss information\n",
      "Skip training data 6762 due to missing loss information\n",
      "Skip training data 6763 due to missing loss information\n",
      "Skip training data 6764 due to missing loss information\n",
      "Skip training data 6765 due to missing loss information\n",
      "Skip training data 6766 due to missing loss information\n",
      "Skip training data 6767 due to missing loss information\n",
      "Skip training data 6768 due to missing loss information\n",
      "Skip training data 6769 due to missing loss information\n",
      "Skip training data 6770 due to missing loss information\n",
      "Skip training data 6771 due to missing loss information\n",
      "Skip training data 6772 due to missing loss information\n",
      "Skip training data 6773 due to missing loss information\n",
      "Skip training data 6774 due to missing loss information\n",
      "Skip training data 6775 due to missing loss information\n",
      "Skip training data 6776 due to missing loss information\n",
      "Skip training data 6777 due to missing loss information\n",
      "Skip training data 6778 due to missing loss information\n",
      "Skip training data 6779 due to missing loss information\n",
      "Skip training data 6780 due to missing loss information\n",
      "Skip training data 6781 due to missing loss information\n",
      "Skip training data 6782 due to missing loss information\n",
      "Skip training data 6783 due to missing loss information\n",
      "Skip training data 6784 due to missing loss information\n",
      "Skip training data 6785 due to missing loss information\n",
      "Skip training data 6786 due to missing loss information\n",
      "Skip training data 6787 due to missing loss information\n",
      "Skip training data 6788 due to missing loss information\n",
      "Skip training data 6789 due to missing loss information\n",
      "Skip training data 6790 due to missing loss information\n",
      "Skip training data 6791 due to missing loss information\n",
      "Skip training data 6792 due to missing loss information\n",
      "Skip training data 6793 due to missing loss information\n",
      "Skip training data 6794 due to missing loss information\n",
      "Skip training data 6795 due to missing loss information\n",
      "Skip training data 6796 due to missing loss information\n",
      "Skip training data 6797 due to missing loss information\n",
      "Skip training data 6798 due to missing loss information\n",
      "Skip training data 6799 due to missing loss information\n",
      "Skip training data 6800 due to missing loss information\n",
      "Skip training data 6801 due to missing loss information\n",
      "Skip training data 6802 due to missing loss information\n",
      "Skip training data 6803 due to missing loss information\n",
      "Skip training data 6804 due to missing loss information\n",
      "Skip training data 6805 due to missing loss information\n",
      "Skip training data 6806 due to missing loss information\n",
      "Skip training data 6807 due to missing loss information\n",
      "Skip training data 6808 due to missing loss information\n",
      "Skip training data 6809 due to missing loss information\n",
      "Skip training data 6810 due to missing loss information\n",
      "Skip training data 6811 due to missing loss information\n",
      "Skip training data 6812 due to missing loss information\n",
      "Skip training data 6813 due to missing loss information\n",
      "Skip training data 6814 due to missing loss information\n",
      "Skip training data 6815 due to missing loss information\n",
      "Skip training data 6816 due to missing loss information\n",
      "Skip training data 6817 due to missing loss information\n",
      "Skip training data 6818 due to missing loss information\n",
      "Skip training data 6819 due to missing loss information\n",
      "Skip training data 6820 due to missing loss information\n",
      "Skip training data 6821 due to missing loss information\n",
      "Skip training data 6822 due to missing loss information\n",
      "Skip training data 6823 due to missing loss information\n",
      "Skip training data 6824 due to missing loss information\n",
      "Skip training data 6825 due to missing loss information\n",
      "Skip training data 6826 due to missing loss information\n",
      "Skip training data 6827 due to missing loss information\n",
      "Skip training data 6828 due to missing loss information\n",
      "Skip training data 6829 due to missing loss information\n",
      "Skip training data 6830 due to missing loss information\n",
      "Skip training data 6831 due to missing loss information\n",
      "Skip training data 6832 due to missing loss information\n",
      "Skip training data 6833 due to missing loss information\n",
      "Skip training data 6834 due to missing loss information\n",
      "Skip training data 6835 due to missing loss information\n",
      "Skip training data 6836 due to missing loss information\n",
      "Skip training data 6837 due to missing loss information\n",
      "Skip training data 6838 due to missing loss information\n",
      "Skip training data 6839 due to missing loss information\n",
      "Skip training data 6840 due to missing loss information\n",
      "Skip training data 6841 due to missing loss information\n",
      "Skip training data 6842 due to missing loss information\n",
      "Skip training data 6843 due to missing loss information\n",
      "Skip training data 6844 due to missing loss information\n",
      "Skip training data 6845 due to missing loss information\n",
      "Skip training data 6846 due to missing loss information\n",
      "Skip training data 6847 due to missing loss information\n",
      "Skip training data 6848 due to missing loss information\n",
      "Skip training data 6849 due to missing loss information\n",
      "Skip training data 6850 due to missing loss information\n",
      "Skip training data 6851 due to missing loss information\n",
      "Skip training data 6852 due to missing loss information\n",
      "Skip training data 6853 due to missing loss information\n",
      "Skip training data 6854 due to missing loss information\n",
      "Skip training data 6855 due to missing loss information\n",
      "Skip training data 6856 due to missing loss information\n",
      "Skip training data 6857 due to missing loss information\n",
      "Skip training data 6858 due to missing loss information\n",
      "Skip training data 6859 due to missing loss information\n",
      "Skip training data 6860 due to missing loss information\n",
      "Skip training data 6861 due to missing loss information\n",
      "Skip training data 6862 due to missing loss information\n",
      "Skip training data 6863 due to missing loss information\n",
      "Skip training data 6864 due to missing loss information\n",
      "Skip training data 6865 due to missing loss information\n",
      "Skip training data 6866 due to missing loss information\n",
      "Skip training data 6867 due to missing loss information\n",
      "Skip training data 6868 due to missing loss information\n",
      "Skip training data 6869 due to missing loss information\n",
      "Skip training data 6870 due to missing loss information\n",
      "Skip training data 6871 due to missing loss information\n",
      "Skip training data 6872 due to missing loss information\n",
      "Skip training data 6873 due to missing loss information\n",
      "Skip training data 6874 due to missing loss information\n",
      "Skip training data 6875 due to missing loss information\n",
      "Skip training data 6876 due to missing loss information\n",
      "Skip training data 6877 due to missing loss information\n",
      "Skip training data 6878 due to missing loss information\n",
      "Skip training data 6879 due to missing loss information\n",
      "Skip training data 6880 due to missing loss information\n",
      "Skip training data 6881 due to missing loss information\n",
      "Skip training data 6882 due to missing loss information\n",
      "Skip training data 6883 due to missing loss information\n",
      "Skip training data 6884 due to missing loss information\n",
      "Skip training data 6885 due to missing loss information\n",
      "Skip training data 6886 due to missing loss information\n",
      "Skip training data 6887 due to missing loss information\n",
      "Skip training data 6888 due to missing loss information\n",
      "Skip training data 6889 due to missing loss information\n",
      "Skip training data 6890 due to missing loss information\n",
      "Skip training data 6891 due to missing loss information\n",
      "Skip training data 6892 due to missing loss information\n",
      "Skip training data 6893 due to missing loss information\n",
      "Skip training data 6894 due to missing loss information\n",
      "Skip training data 6895 due to missing loss information\n",
      "Skip training data 6896 due to missing loss information\n",
      "Skip training data 6897 due to missing loss information\n",
      "Skip training data 6898 due to missing loss information\n",
      "Skip training data 6899 due to missing loss information\n",
      "Skip training data 6900 due to missing loss information\n",
      "Skip training data 6901 due to missing loss information\n",
      "Skip training data 6902 due to missing loss information\n",
      "Skip training data 6903 due to missing loss information\n",
      "Skip training data 6904 due to missing loss information\n",
      "Skip training data 6905 due to missing loss information\n",
      "Skip training data 6906 due to missing loss information\n",
      "Skip training data 6907 due to missing loss information\n",
      "Skip training data 6908 due to missing loss information\n",
      "Skip training data 6909 due to missing loss information\n",
      "Skip training data 6910 due to missing loss information\n",
      "Skip training data 6911 due to missing loss information\n",
      "Skip training data 6912 due to missing loss information\n",
      "Skip training data 6913 due to missing loss information\n",
      "Skip training data 6914 due to missing loss information\n",
      "Skip training data 6915 due to missing loss information\n",
      "Skip training data 6916 due to missing loss information\n",
      "Skip training data 6917 due to missing loss information\n",
      "Skip training data 6918 due to missing loss information\n",
      "Skip training data 6919 due to missing loss information\n",
      "Skip training data 6920 due to missing loss information\n",
      "Skip training data 6921 due to missing loss information\n",
      "Skip training data 6922 due to missing loss information\n",
      "Skip training data 6923 due to missing loss information\n",
      "Skip training data 6924 due to missing loss information\n",
      "Skip training data 6925 due to missing loss information\n",
      "Skip training data 6926 due to missing loss information\n",
      "Skip training data 6927 due to missing loss information\n",
      "Skip training data 6928 due to missing loss information\n",
      "Skip training data 6929 due to missing loss information\n",
      "Skip training data 6930 due to missing loss information\n",
      "Skip training data 6931 due to missing loss information\n",
      "Skip training data 6932 due to missing loss information\n",
      "Skip training data 6933 due to missing loss information\n",
      "Skip training data 6934 due to missing loss information\n",
      "Skip training data 6935 due to missing loss information\n",
      "Skip training data 6936 due to missing loss information\n",
      "Skip training data 6937 due to missing loss information\n",
      "Skip training data 6938 due to missing loss information\n",
      "Skip training data 6939 due to missing loss information\n",
      "Skip training data 6940 due to missing loss information\n",
      "Skip training data 6941 due to missing loss information\n",
      "Skip training data 6942 due to missing loss information\n",
      "Skip training data 6943 due to missing loss information\n",
      "Skip training data 6944 due to missing loss information\n",
      "Skip training data 6945 due to missing loss information\n",
      "Skip training data 6946 due to missing loss information\n",
      "Skip training data 6947 due to missing loss information\n",
      "Skip training data 6948 due to missing loss information\n",
      "Skip training data 6949 due to missing loss information\n",
      "Skip training data 6950 due to missing loss information\n",
      "Skip training data 6951 due to missing loss information\n",
      "Skip training data 6952 due to missing loss information\n",
      "Skip training data 6953 due to missing loss information\n",
      "Skip training data 6954 due to missing loss information\n",
      "Skip training data 6955 due to missing loss information\n",
      "Skip training data 6956 due to missing loss information\n",
      "Skip training data 6957 due to missing loss information\n",
      "Skip training data 6958 due to missing loss information\n",
      "Skip training data 6959 due to missing loss information\n",
      "Skip training data 6960 due to missing loss information\n",
      "Skip training data 6961 due to missing loss information\n",
      "Skip training data 6962 due to missing loss information\n",
      "Skip training data 6963 due to missing loss information\n",
      "Skip training data 6964 due to missing loss information\n",
      "Skip training data 6965 due to missing loss information\n",
      "Skip training data 6966 due to missing loss information\n",
      "Skip training data 6967 due to missing loss information\n",
      "Skip training data 6968 due to missing loss information\n",
      "Skip training data 6969 due to missing loss information\n",
      "Skip training data 6970 due to missing loss information\n",
      "Skip training data 6971 due to missing loss information\n",
      "Skip training data 6972 due to missing loss information\n",
      "Skip training data 6973 due to missing loss information\n",
      "Skip training data 6974 due to missing loss information\n",
      "Skip training data 6975 due to missing loss information\n",
      "Skip training data 6976 due to missing loss information\n",
      "Skip training data 6977 due to missing loss information\n",
      "Skip training data 6978 due to missing loss information\n",
      "Skip training data 6979 due to missing loss information\n",
      "Skip training data 6980 due to missing loss information\n",
      "Skip training data 6981 due to missing loss information\n",
      "Skip training data 6982 due to missing loss information\n",
      "Skip training data 6983 due to missing loss information\n",
      "Skip training data 6984 due to missing loss information\n",
      "Skip training data 6985 due to missing loss information\n",
      "Skip training data 6986 due to missing loss information\n",
      "Skip training data 6987 due to missing loss information\n",
      "Skip training data 6988 due to missing loss information\n",
      "Skip training data 6989 due to missing loss information\n",
      "Skip training data 6990 due to missing loss information\n",
      "Skip training data 6991 due to missing loss information\n",
      "Skip training data 6992 due to missing loss information\n",
      "Skip training data 6993 due to missing loss information\n",
      "Skip training data 6994 due to missing loss information\n",
      "Skip training data 6995 due to missing loss information\n",
      "Skip training data 6996 due to missing loss information\n",
      "Skip training data 6997 due to missing loss information\n",
      "Skip training data 6998 due to missing loss information\n",
      "Skip training data 6999 due to missing loss information\n",
      "Skip training data 7000 due to missing loss information\n",
      "Skip training data 7001 due to missing loss information\n",
      "Skip training data 7002 due to missing loss information\n",
      "Skip training data 7003 due to missing loss information\n",
      "Skip training data 7004 due to missing loss information\n",
      "Skip training data 7005 due to missing loss information\n",
      "Skip training data 7006 due to missing loss information\n",
      "Skip training data 7007 due to missing loss information\n",
      "Skip training data 7008 due to missing loss information\n",
      "Skip training data 7009 due to missing loss information\n",
      "Skip training data 7010 due to missing loss information\n",
      "Skip training data 7011 due to missing loss information\n",
      "Skip training data 7012 due to missing loss information\n",
      "Skip training data 7013 due to missing loss information\n",
      "Skip training data 7014 due to missing loss information\n",
      "Skip training data 7015 due to missing loss information\n",
      "Skip training data 7016 due to missing loss information\n",
      "Skip training data 7017 due to missing loss information\n",
      "Skip training data 7018 due to missing loss information\n",
      "Skip training data 7019 due to missing loss information\n",
      "Skip training data 7020 due to missing loss information\n",
      "Skip training data 7021 due to missing loss information\n",
      "Skip training data 7022 due to missing loss information\n",
      "Skip training data 7023 due to missing loss information\n",
      "Skip training data 7024 due to missing loss information\n",
      "Skip training data 7025 due to missing loss information\n",
      "Skip training data 7026 due to missing loss information\n",
      "Skip training data 7027 due to missing loss information\n",
      "Skip training data 7028 due to missing loss information\n",
      "Skip training data 7029 due to missing loss information\n",
      "Skip training data 7030 due to missing loss information\n",
      "Skip training data 7031 due to missing loss information\n",
      "Skip training data 7032 due to missing loss information\n",
      "Skip training data 7033 due to missing loss information\n",
      "Skip training data 7034 due to missing loss information\n",
      "Skip training data 7035 due to missing loss information\n",
      "Skip training data 7036 due to missing loss information\n",
      "Skip training data 7037 due to missing loss information\n",
      "Skip training data 7038 due to missing loss information\n",
      "Skip training data 7039 due to missing loss information\n",
      "Skip training data 7040 due to missing loss information\n",
      "Skip training data 7041 due to missing loss information\n",
      "Skip training data 7042 due to missing loss information\n",
      "Skip training data 7043 due to missing loss information\n",
      "Skip training data 7044 due to missing loss information\n",
      "Skip training data 7045 due to missing loss information\n",
      "Skip training data 7046 due to missing loss information\n",
      "Skip training data 7047 due to missing loss information\n",
      "Skip training data 7048 due to missing loss information\n",
      "Skip training data 7049 due to missing loss information\n",
      "Skip training data 7050 due to missing loss information\n",
      "Skip training data 7051 due to missing loss information\n",
      "Skip training data 7052 due to missing loss information\n",
      "Skip training data 7053 due to missing loss information\n",
      "Skip training data 7054 due to missing loss information\n",
      "Skip training data 7055 due to missing loss information\n",
      "Skip training data 7056 due to missing loss information\n",
      "Skip training data 7057 due to missing loss information\n",
      "Skip training data 7058 due to missing loss information\n",
      "Skip training data 7059 due to missing loss information\n",
      "Skip training data 7060 due to missing loss information\n",
      "Skip training data 7061 due to missing loss information\n",
      "Skip training data 7062 due to missing loss information\n",
      "Skip training data 7063 due to missing loss information\n",
      "Skip training data 7064 due to missing loss information\n",
      "Skip training data 7065 due to missing loss information\n",
      "Skip training data 7066 due to missing loss information\n",
      "Skip training data 7067 due to missing loss information\n",
      "Skip training data 7068 due to missing loss information\n",
      "Skip training data 7069 due to missing loss information\n",
      "Skip training data 7070 due to missing loss information\n",
      "Skip training data 7071 due to missing loss information\n",
      "Skip training data 7072 due to missing loss information\n",
      "Skip training data 7073 due to missing loss information\n",
      "Skip training data 7074 due to missing loss information\n",
      "Skip training data 7075 due to missing loss information\n",
      "Skip training data 7076 due to missing loss information\n",
      "Skip training data 7077 due to missing loss information\n",
      "Skip training data 7078 due to missing loss information\n",
      "Skip training data 7079 due to missing loss information\n",
      "Skip training data 7080 due to missing loss information\n",
      "Skip training data 7081 due to missing loss information\n",
      "Skip training data 7082 due to missing loss information\n",
      "Skip training data 7083 due to missing loss information\n",
      "Skip training data 7084 due to missing loss information\n",
      "Skip training data 7085 due to missing loss information\n",
      "Skip training data 7086 due to missing loss information\n",
      "Skip training data 7087 due to missing loss information\n",
      "Skip training data 7088 due to missing loss information\n",
      "Skip training data 7089 due to missing loss information\n",
      "Skip training data 7090 due to missing loss information\n",
      "Skip training data 7091 due to missing loss information\n",
      "Skip training data 7092 due to missing loss information\n",
      "Skip training data 7093 due to missing loss information\n",
      "Skip training data 7094 due to missing loss information\n",
      "Skip training data 7095 due to missing loss information\n",
      "Skip training data 7096 due to missing loss information\n",
      "Skip training data 7097 due to missing loss information\n",
      "Skip training data 7098 due to missing loss information\n",
      "Skip training data 7099 due to missing loss information\n",
      "Skip training data 7100 due to missing loss information\n",
      "Skip training data 7101 due to missing loss information\n",
      "Skip training data 7102 due to missing loss information\n",
      "Skip training data 7103 due to missing loss information\n",
      "Skip training data 7104 due to missing loss information\n",
      "Skip training data 7105 due to missing loss information\n",
      "Skip training data 7106 due to missing loss information\n",
      "Skip training data 7107 due to missing loss information\n",
      "Skip training data 7108 due to missing loss information\n",
      "Skip training data 7109 due to missing loss information\n",
      "Skip training data 7110 due to missing loss information\n",
      "Skip training data 7111 due to missing loss information\n",
      "Skip training data 7112 due to missing loss information\n",
      "Skip training data 7113 due to missing loss information\n",
      "Skip training data 7114 due to missing loss information\n",
      "Skip training data 7115 due to missing loss information\n",
      "Skip training data 7116 due to missing loss information\n",
      "Skip training data 7117 due to missing loss information\n",
      "Skip training data 7118 due to missing loss information\n",
      "Skip training data 7119 due to missing loss information\n",
      "Skip training data 7120 due to missing loss information\n",
      "Skip training data 7121 due to missing loss information\n",
      "Skip training data 7122 due to missing loss information\n",
      "Skip training data 7123 due to missing loss information\n",
      "Skip training data 7124 due to missing loss information\n",
      "Skip training data 7125 due to missing loss information\n",
      "Skip training data 7126 due to missing loss information\n",
      "Skip training data 7127 due to missing loss information\n",
      "Skip training data 7128 due to missing loss information\n",
      "Skip training data 7129 due to missing loss information\n",
      "Skip training data 7130 due to missing loss information\n",
      "Skip training data 7131 due to missing loss information\n",
      "Skip training data 7132 due to missing loss information\n",
      "Skip training data 7133 due to missing loss information\n",
      "Skip training data 7134 due to missing loss information\n",
      "Skip training data 7135 due to missing loss information\n",
      "Skip training data 7136 due to missing loss information\n",
      "Skip training data 7137 due to missing loss information\n",
      "Skip training data 7138 due to missing loss information\n",
      "Skip training data 7139 due to missing loss information\n",
      "Skip training data 7140 due to missing loss information\n",
      "Skip training data 7141 due to missing loss information\n",
      "Skip training data 7142 due to missing loss information\n",
      "Skip training data 7143 due to missing loss information\n",
      "Skip training data 7144 due to missing loss information\n",
      "Skip training data 7145 due to missing loss information\n",
      "Skip training data 7146 due to missing loss information\n",
      "Skip training data 7147 due to missing loss information\n",
      "Skip training data 7148 due to missing loss information\n",
      "Skip training data 7149 due to missing loss information\n",
      "Skip training data 7150 due to missing loss information\n",
      "Skip training data 7151 due to missing loss information\n",
      "Skip training data 7152 due to missing loss information\n",
      "Skip training data 7153 due to missing loss information\n",
      "Skip training data 7154 due to missing loss information\n",
      "Skip training data 7155 due to missing loss information\n",
      "Skip training data 7156 due to missing loss information\n",
      "Skip training data 7157 due to missing loss information\n",
      "Skip training data 7158 due to missing loss information\n",
      "Skip training data 7159 due to missing loss information\n",
      "Skip training data 7160 due to missing loss information\n",
      "Skip training data 7161 due to missing loss information\n",
      "Skip training data 7162 due to missing loss information\n",
      "Skip training data 7163 due to missing loss information\n",
      "Skip training data 7164 due to missing loss information\n",
      "Skip training data 7165 due to missing loss information\n",
      "Skip training data 7166 due to missing loss information\n",
      "Skip training data 7167 due to missing loss information\n",
      "Skip training data 7168 due to missing loss information\n",
      "Skip training data 7169 due to missing loss information\n",
      "Skip training data 7170 due to missing loss information\n",
      "Skip training data 7171 due to missing loss information\n",
      "Skip training data 7172 due to missing loss information\n",
      "Skip training data 7173 due to missing loss information\n",
      "Skip training data 7174 due to missing loss information\n",
      "Skip training data 7175 due to missing loss information\n",
      "Skip training data 7176 due to missing loss information\n",
      "Skip training data 7177 due to missing loss information\n",
      "Skip training data 7178 due to missing loss information\n",
      "Skip training data 7179 due to missing loss information\n",
      "Skip training data 7180 due to missing loss information\n",
      "Skip training data 7181 due to missing loss information\n",
      "Skip training data 7182 due to missing loss information\n",
      "Skip training data 7183 due to missing loss information\n",
      "Skip training data 7184 due to missing loss information\n",
      "Skip training data 7185 due to missing loss information\n",
      "Skip training data 7186 due to missing loss information\n",
      "Skip training data 7187 due to missing loss information\n",
      "Skip training data 7188 due to missing loss information\n",
      "Skip training data 7189 due to missing loss information\n",
      "Skip training data 7190 due to missing loss information\n",
      "Skip training data 7191 due to missing loss information\n",
      "Skip training data 7192 due to missing loss information\n",
      "Skip training data 7193 due to missing loss information\n",
      "Skip training data 7194 due to missing loss information\n",
      "Skip training data 7195 due to missing loss information\n",
      "Skip training data 7196 due to missing loss information\n",
      "Skip training data 7197 due to missing loss information\n",
      "Skip training data 7198 due to missing loss information\n",
      "Skip training data 7199 due to missing loss information\n",
      "Skip training data 7200 due to missing loss information\n",
      "Skip training data 7201 due to missing loss information\n",
      "Skip training data 7202 due to missing loss information\n",
      "Skip training data 7203 due to missing loss information\n",
      "Skip training data 7204 due to missing loss information\n",
      "Skip training data 7205 due to missing loss information\n",
      "Skip training data 7206 due to missing loss information\n",
      "Skip training data 7207 due to missing loss information\n",
      "Skip training data 7208 due to missing loss information\n",
      "Skip training data 7209 due to missing loss information\n",
      "Skip training data 7210 due to missing loss information\n",
      "Skip training data 7211 due to missing loss information\n",
      "Skip training data 7212 due to missing loss information\n",
      "Skip training data 7213 due to missing loss information\n",
      "Skip training data 7214 due to missing loss information\n",
      "Skip training data 7215 due to missing loss information\n",
      "Skip training data 7216 due to missing loss information\n",
      "Skip training data 7217 due to missing loss information\n",
      "Skip training data 7218 due to missing loss information\n",
      "Skip training data 7219 due to missing loss information\n",
      "Skip training data 7220 due to missing loss information\n",
      "Skip training data 7221 due to missing loss information\n",
      "Skip training data 7222 due to missing loss information\n",
      "Skip training data 7223 due to missing loss information\n",
      "Skip training data 7224 due to missing loss information\n",
      "Skip training data 7225 due to missing loss information\n",
      "Skip training data 7226 due to missing loss information\n",
      "Skip training data 7227 due to missing loss information\n",
      "Skip training data 7228 due to missing loss information\n",
      "Skip training data 7229 due to missing loss information\n",
      "Skip training data 7230 due to missing loss information\n",
      "Skip training data 7231 due to missing loss information\n",
      "Skip training data 7232 due to missing loss information\n",
      "Skip training data 7233 due to missing loss information\n",
      "Skip training data 7234 due to missing loss information\n",
      "Skip training data 7235 due to missing loss information\n",
      "Skip training data 7236 due to missing loss information\n",
      "Skip training data 7237 due to missing loss information\n",
      "Skip training data 7238 due to missing loss information\n",
      "Skip training data 7239 due to missing loss information\n",
      "Skip training data 7240 due to missing loss information\n",
      "Skip training data 7241 due to missing loss information\n",
      "Skip training data 7242 due to missing loss information\n",
      "Skip training data 7243 due to missing loss information\n",
      "Skip training data 7244 due to missing loss information\n",
      "Skip training data 7245 due to missing loss information\n",
      "Skip training data 7246 due to missing loss information\n",
      "Skip training data 7247 due to missing loss information\n",
      "Skip training data 7248 due to missing loss information\n",
      "Skip training data 7249 due to missing loss information\n",
      "Skip training data 7250 due to missing loss information\n",
      "Skip training data 7251 due to missing loss information\n",
      "Skip training data 7252 due to missing loss information\n",
      "Skip training data 7253 due to missing loss information\n",
      "Skip training data 7254 due to missing loss information\n",
      "Skip training data 7255 due to missing loss information\n",
      "Skip training data 7256 due to missing loss information\n",
      "Skip training data 7257 due to missing loss information\n",
      "Skip training data 7258 due to missing loss information\n",
      "Skip training data 7259 due to missing loss information\n",
      "Skip training data 7260 due to missing loss information\n",
      "Skip training data 7261 due to missing loss information\n",
      "Skip training data 7262 due to missing loss information\n",
      "Skip training data 7263 due to missing loss information\n",
      "Skip training data 7264 due to missing loss information\n",
      "Skip training data 7265 due to missing loss information\n",
      "Skip training data 7266 due to missing loss information\n",
      "Skip training data 7267 due to missing loss information\n",
      "Skip training data 7268 due to missing loss information\n",
      "Skip training data 7269 due to missing loss information\n",
      "Skip training data 7270 due to missing loss information\n",
      "Skip training data 7271 due to missing loss information\n",
      "Skip training data 7272 due to missing loss information\n",
      "Skip training data 7273 due to missing loss information\n",
      "Skip training data 7274 due to missing loss information\n",
      "Skip training data 7275 due to missing loss information\n",
      "Skip training data 7276 due to missing loss information\n",
      "Skip training data 7277 due to missing loss information\n",
      "Skip training data 7278 due to missing loss information\n",
      "Skip training data 7279 due to missing loss information\n",
      "Skip training data 7280 due to missing loss information\n",
      "Skip training data 7281 due to missing loss information\n",
      "Skip training data 7282 due to missing loss information\n",
      "Skip training data 7283 due to missing loss information\n",
      "Skip training data 7284 due to missing loss information\n",
      "Skip training data 7285 due to missing loss information\n",
      "Skip training data 7286 due to missing loss information\n",
      "Skip training data 7287 due to missing loss information\n",
      "Skip training data 7288 due to missing loss information\n",
      "Skip training data 7289 due to missing loss information\n",
      "Skip training data 7290 due to missing loss information\n",
      "Skip training data 7291 due to missing loss information\n",
      "Skip training data 7292 due to missing loss information\n",
      "Skip training data 7293 due to missing loss information\n",
      "Skip training data 7294 due to missing loss information\n",
      "Skip training data 7295 due to missing loss information\n",
      "Skip training data 7296 due to missing loss information\n",
      "Skip training data 7297 due to missing loss information\n",
      "Skip training data 7298 due to missing loss information\n",
      "Skip training data 7299 due to missing loss information\n",
      "Skip training data 7300 due to missing loss information\n",
      "Skip training data 7301 due to missing loss information\n",
      "Skip training data 7302 due to missing loss information\n",
      "Skip training data 7303 due to missing loss information\n",
      "Skip training data 7304 due to missing loss information\n",
      "Skip training data 7305 due to missing loss information\n",
      "Skip training data 7306 due to missing loss information\n",
      "Skip training data 7307 due to missing loss information\n",
      "Skip training data 7308 due to missing loss information\n",
      "Skip training data 7309 due to missing loss information\n",
      "Skip training data 7310 due to missing loss information\n",
      "Skip training data 7311 due to missing loss information\n",
      "Skip training data 7312 due to missing loss information\n",
      "Skip training data 7313 due to missing loss information\n",
      "Skip training data 7314 due to missing loss information\n",
      "Skip training data 7315 due to missing loss information\n",
      "Skip training data 7316 due to missing loss information\n",
      "Skip training data 7317 due to missing loss information\n",
      "Skip training data 7318 due to missing loss information\n",
      "Skip training data 7319 due to missing loss information\n",
      "Skip training data 7320 due to missing loss information\n",
      "Skip training data 7321 due to missing loss information\n",
      "Skip training data 7322 due to missing loss information\n",
      "Skip training data 7323 due to missing loss information\n",
      "Skip training data 7324 due to missing loss information\n",
      "Skip training data 7325 due to missing loss information\n",
      "Skip training data 7326 due to missing loss information\n",
      "Skip training data 7327 due to missing loss information\n",
      "Skip training data 7328 due to missing loss information\n",
      "Skip training data 7329 due to missing loss information\n",
      "Skip training data 7330 due to missing loss information\n",
      "Skip training data 7331 due to missing loss information\n",
      "Skip training data 7332 due to missing loss information\n",
      "Skip training data 7333 due to missing loss information\n",
      "Skip training data 7334 due to missing loss information\n",
      "Skip training data 7335 due to missing loss information\n",
      "Skip training data 7336 due to missing loss information\n",
      "Skip training data 7337 due to missing loss information\n",
      "Skip training data 7338 due to missing loss information\n",
      "Skip training data 7339 due to missing loss information\n",
      "Skip training data 7340 due to missing loss information\n",
      "Skip training data 7341 due to missing loss information\n",
      "Skip training data 7342 due to missing loss information\n",
      "Skip training data 7343 due to missing loss information\n",
      "Skip training data 7344 due to missing loss information\n",
      "Skip training data 7345 due to missing loss information\n",
      "Skip training data 7346 due to missing loss information\n",
      "Skip training data 7347 due to missing loss information\n",
      "Skip training data 7348 due to missing loss information\n",
      "Skip training data 7349 due to missing loss information\n",
      "Skip training data 7350 due to missing loss information\n",
      "Skip training data 7351 due to missing loss information\n",
      "Skip training data 7352 due to missing loss information\n",
      "Skip training data 7353 due to missing loss information\n",
      "Skip training data 7354 due to missing loss information\n",
      "Skip training data 7355 due to missing loss information\n",
      "Skip training data 7356 due to missing loss information\n",
      "Skip training data 7357 due to missing loss information\n",
      "Skip training data 7358 due to missing loss information\n",
      "Skip training data 7359 due to missing loss information\n",
      "Skip training data 7360 due to missing loss information\n",
      "Skip training data 7361 due to missing loss information\n",
      "Skip training data 7362 due to missing loss information\n",
      "Skip training data 7363 due to missing loss information\n",
      "Skip training data 7364 due to missing loss information\n",
      "Skip training data 7365 due to missing loss information\n",
      "Skip training data 7366 due to missing loss information\n",
      "Skip training data 7367 due to missing loss information\n",
      "Skip training data 7368 due to missing loss information\n",
      "Skip training data 7369 due to missing loss information\n",
      "Skip training data 7370 due to missing loss information\n",
      "Skip training data 7371 due to missing loss information\n",
      "Skip training data 7372 due to missing loss information\n",
      "Skip training data 7373 due to missing loss information\n",
      "Skip training data 7374 due to missing loss information\n",
      "Skip training data 7375 due to missing loss information\n",
      "Skip training data 7376 due to missing loss information\n",
      "Skip training data 7377 due to missing loss information\n",
      "Skip training data 7378 due to missing loss information\n",
      "Skip training data 7379 due to missing loss information\n",
      "Skip training data 7380 due to missing loss information\n",
      "Skip training data 7381 due to missing loss information\n",
      "Skip training data 7382 due to missing loss information\n",
      "Skip training data 7383 due to missing loss information\n",
      "Skip training data 7384 due to missing loss information\n",
      "Skip training data 7385 due to missing loss information\n",
      "Skip training data 7386 due to missing loss information\n",
      "Skip training data 7387 due to missing loss information\n",
      "Skip training data 7388 due to missing loss information\n",
      "Skip training data 7389 due to missing loss information\n",
      "Skip training data 7390 due to missing loss information\n",
      "Skip training data 7391 due to missing loss information\n",
      "Skip training data 7392 due to missing loss information\n",
      "Skip training data 7393 due to missing loss information\n",
      "Skip training data 7394 due to missing loss information\n",
      "Skip training data 7395 due to missing loss information\n",
      "Skip training data 7396 due to missing loss information\n",
      "Skip training data 7397 due to missing loss information\n",
      "Skip training data 7398 due to missing loss information\n",
      "Skip training data 7399 due to missing loss information\n",
      "Skip training data 7400 due to missing loss information\n",
      "Skip training data 7401 due to missing loss information\n",
      "Skip training data 7402 due to missing loss information\n",
      "Skip training data 7403 due to missing loss information\n",
      "Skip training data 7404 due to missing loss information\n",
      "Skip training data 7405 due to missing loss information\n",
      "Skip training data 7406 due to missing loss information\n",
      "Skip training data 7407 due to missing loss information\n",
      "Skip training data 7408 due to missing loss information\n",
      "Skip training data 7409 due to missing loss information\n",
      "Skip training data 7410 due to missing loss information\n",
      "Skip training data 7411 due to missing loss information\n",
      "Skip training data 7412 due to missing loss information\n",
      "Skip training data 7413 due to missing loss information\n",
      "Skip training data 7414 due to missing loss information\n",
      "Skip training data 7415 due to missing loss information\n",
      "Skip training data 7416 due to missing loss information\n",
      "Skip training data 7417 due to missing loss information\n",
      "Skip training data 7418 due to missing loss information\n",
      "Skip training data 7419 due to missing loss information\n",
      "Skip training data 7420 due to missing loss information\n",
      "Skip training data 7421 due to missing loss information\n",
      "Skip training data 7422 due to missing loss information\n",
      "Skip training data 7423 due to missing loss information\n",
      "Skip training data 7424 due to missing loss information\n",
      "Skip training data 7425 due to missing loss information\n",
      "Skip training data 7426 due to missing loss information\n",
      "Skip training data 7427 due to missing loss information\n",
      "Skip training data 7428 due to missing loss information\n",
      "Skip training data 7429 due to missing loss information\n",
      "Skip training data 7430 due to missing loss information\n",
      "Skip training data 7431 due to missing loss information\n",
      "Skip training data 7432 due to missing loss information\n",
      "Skip training data 7433 due to missing loss information\n",
      "Skip training data 7434 due to missing loss information\n",
      "Skip training data 7435 due to missing loss information\n",
      "Skip training data 7436 due to missing loss information\n",
      "Skip training data 7437 due to missing loss information\n",
      "Skip training data 7438 due to missing loss information\n",
      "Skip training data 7439 due to missing loss information\n",
      "Skip training data 7440 due to missing loss information\n",
      "Skip training data 7441 due to missing loss information\n",
      "Skip training data 7442 due to missing loss information\n",
      "Skip training data 7443 due to missing loss information\n",
      "Skip training data 7444 due to missing loss information\n",
      "Skip training data 7445 due to missing loss information\n",
      "Skip training data 7446 due to missing loss information\n",
      "Skip training data 7447 due to missing loss information\n",
      "Skip training data 7448 due to missing loss information\n",
      "Skip training data 7449 due to missing loss information\n",
      "Skip training data 7450 due to missing loss information\n",
      "Skip training data 7451 due to missing loss information\n",
      "Skip training data 7452 due to missing loss information\n",
      "Skip training data 7453 due to missing loss information\n",
      "Skip training data 7454 due to missing loss information\n",
      "Skip training data 7455 due to missing loss information\n",
      "Skip training data 7456 due to missing loss information\n",
      "Skip training data 7457 due to missing loss information\n",
      "Skip training data 7458 due to missing loss information\n",
      "Skip training data 7459 due to missing loss information\n",
      "Skip training data 7460 due to missing loss information\n",
      "Skip training data 7461 due to missing loss information\n",
      "Skip training data 7462 due to missing loss information\n",
      "Skip training data 7463 due to missing loss information\n",
      "Skip training data 7464 due to missing loss information\n",
      "Skip training data 7465 due to missing loss information\n",
      "Skip training data 7466 due to missing loss information\n",
      "Skip training data 7467 due to missing loss information\n",
      "Skip training data 7468 due to missing loss information\n",
      "Skip training data 7469 due to missing loss information\n",
      "Skip training data 7470 due to missing loss information\n",
      "Skip training data 7471 due to missing loss information\n",
      "Skip training data 7472 due to missing loss information\n",
      "Skip training data 7473 due to missing loss information\n",
      "Skip training data 7474 due to missing loss information\n",
      "Skip training data 7475 due to missing loss information\n",
      "Skip training data 7476 due to missing loss information\n",
      "Skip training data 7477 due to missing loss information\n",
      "Skip training data 7478 due to missing loss information\n",
      "Skip training data 7479 due to missing loss information\n",
      "Skip training data 7480 due to missing loss information\n",
      "Skip training data 7481 due to missing loss information\n",
      "Skip training data 7482 due to missing loss information\n",
      "Skip training data 7483 due to missing loss information\n",
      "Skip training data 7484 due to missing loss information\n",
      "Skip training data 7485 due to missing loss information\n",
      "Skip training data 7486 due to missing loss information\n",
      "Skip training data 7487 due to missing loss information\n",
      "Skip training data 7488 due to missing loss information\n",
      "Skip training data 7489 due to missing loss information\n",
      "Skip training data 7490 due to missing loss information\n",
      "Skip training data 7491 due to missing loss information\n",
      "Skip training data 7492 due to missing loss information\n",
      "Skip training data 7493 due to missing loss information\n",
      "Skip training data 7494 due to missing loss information\n",
      "Skip training data 7495 due to missing loss information\n",
      "Skip training data 7496 due to missing loss information\n",
      "Skip training data 7497 due to missing loss information\n",
      "Skip training data 7498 due to missing loss information\n",
      "Skip training data 7499 due to missing loss information\n",
      "Skip training data 7500 due to missing loss information\n",
      "Skip training data 7501 due to missing loss information\n",
      "Skip training data 7502 due to missing loss information\n",
      "Skip training data 7503 due to missing loss information\n",
      "Skip training data 7504 due to missing loss information\n",
      "Skip training data 7505 due to missing loss information\n",
      "Skip training data 7506 due to missing loss information\n",
      "Skip training data 7507 due to missing loss information\n",
      "Skip training data 7508 due to missing loss information\n",
      "Skip training data 7509 due to missing loss information\n",
      "Skip training data 7510 due to missing loss information\n",
      "Skip training data 7511 due to missing loss information\n",
      "Skip training data 7512 due to missing loss information\n",
      "Skip training data 7513 due to missing loss information\n",
      "Skip training data 7514 due to missing loss information\n",
      "Skip training data 7515 due to missing loss information\n",
      "Skip training data 7516 due to missing loss information\n",
      "Skip training data 7517 due to missing loss information\n",
      "Skip training data 7518 due to missing loss information\n",
      "Skip training data 7519 due to missing loss information\n",
      "Skip training data 7520 due to missing loss information\n",
      "Skip training data 7521 due to missing loss information\n",
      "Skip training data 7522 due to missing loss information\n",
      "Skip training data 7523 due to missing loss information\n",
      "Skip training data 7524 due to missing loss information\n",
      "Skip training data 7525 due to missing loss information\n",
      "Skip training data 7526 due to missing loss information\n",
      "Skip training data 7527 due to missing loss information\n",
      "Skip training data 7528 due to missing loss information\n",
      "Skip training data 7529 due to missing loss information\n",
      "Skip training data 7530 due to missing loss information\n",
      "Skip training data 7531 due to missing loss information\n",
      "Skip training data 7532 due to missing loss information\n",
      "Skip training data 7533 due to missing loss information\n",
      "Skip training data 7534 due to missing loss information\n",
      "Skip training data 7535 due to missing loss information\n",
      "Skip training data 7536 due to missing loss information\n",
      "Skip training data 7537 due to missing loss information\n",
      "Skip training data 7538 due to missing loss information\n",
      "Skip training data 7539 due to missing loss information\n",
      "Skip training data 7540 due to missing loss information\n",
      "Skip training data 7541 due to missing loss information\n",
      "Skip training data 7542 due to missing loss information\n",
      "Skip training data 7543 due to missing loss information\n",
      "Skip training data 7544 due to missing loss information\n",
      "Skip training data 7545 due to missing loss information\n",
      "Skip training data 7546 due to missing loss information\n",
      "Skip training data 7547 due to missing loss information\n",
      "Skip training data 7548 due to missing loss information\n",
      "Skip training data 7549 due to missing loss information\n",
      "Skip training data 7550 due to missing loss information\n",
      "Skip training data 7551 due to missing loss information\n",
      "Skip training data 7552 due to missing loss information\n",
      "Skip training data 7553 due to missing loss information\n",
      "Skip training data 7554 due to missing loss information\n",
      "Skip training data 7555 due to missing loss information\n",
      "Skip training data 7556 due to missing loss information\n",
      "Skip training data 7557 due to missing loss information\n",
      "Skip training data 7558 due to missing loss information\n",
      "Skip training data 7559 due to missing loss information\n",
      "Skip training data 7560 due to missing loss information\n",
      "Skip training data 7561 due to missing loss information\n",
      "Skip training data 7562 due to missing loss information\n",
      "Skip training data 7563 due to missing loss information\n",
      "Skip training data 7564 due to missing loss information\n",
      "Skip training data 7565 due to missing loss information\n",
      "Skip training data 7566 due to missing loss information\n",
      "Skip training data 7567 due to missing loss information\n",
      "Skip training data 7568 due to missing loss information\n",
      "Skip training data 7569 due to missing loss information\n",
      "Skip training data 7570 due to missing loss information\n",
      "Skip training data 7571 due to missing loss information\n",
      "Skip training data 7572 due to missing loss information\n",
      "Skip training data 7573 due to missing loss information\n",
      "Skip training data 7574 due to missing loss information\n",
      "Skip training data 7575 due to missing loss information\n",
      "Skip training data 7576 due to missing loss information\n",
      "Skip training data 7577 due to missing loss information\n",
      "Skip training data 7578 due to missing loss information\n",
      "Skip training data 7579 due to missing loss information\n",
      "Skip training data 7580 due to missing loss information\n",
      "Skip training data 7581 due to missing loss information\n",
      "Skip training data 7582 due to missing loss information\n",
      "Skip training data 7583 due to missing loss information\n",
      "Skip training data 7584 due to missing loss information\n",
      "Skip training data 7585 due to missing loss information\n",
      "Skip training data 7586 due to missing loss information\n",
      "Skip training data 7587 due to missing loss information\n",
      "Skip training data 7588 due to missing loss information\n",
      "Skip training data 7589 due to missing loss information\n",
      "Skip training data 7590 due to missing loss information\n",
      "Skip training data 7591 due to missing loss information\n",
      "Skip training data 7592 due to missing loss information\n",
      "Skip training data 7593 due to missing loss information\n",
      "Skip training data 7594 due to missing loss information\n",
      "Skip training data 7595 due to missing loss information\n",
      "Skip training data 7596 due to missing loss information\n",
      "Skip training data 7597 due to missing loss information\n",
      "Skip training data 7598 due to missing loss information\n",
      "Skip training data 7599 due to missing loss information\n",
      "Skip training data 7600 due to missing loss information\n",
      "Skip training data 7601 due to missing loss information\n",
      "Skip training data 7602 due to missing loss information\n",
      "Skip training data 7603 due to missing loss information\n",
      "Skip training data 7604 due to missing loss information\n",
      "Skip training data 7605 due to missing loss information\n",
      "Skip training data 7606 due to missing loss information\n",
      "Skip training data 7607 due to missing loss information\n",
      "Skip training data 7608 due to missing loss information\n",
      "Skip training data 7609 due to missing loss information\n",
      "Skip training data 7610 due to missing loss information\n",
      "Skip training data 7611 due to missing loss information\n",
      "Skip training data 7612 due to missing loss information\n",
      "Skip training data 7613 due to missing loss information\n",
      "Skip training data 7614 due to missing loss information\n",
      "Skip training data 7615 due to missing loss information\n",
      "Skip training data 7616 due to missing loss information\n",
      "Skip training data 7617 due to missing loss information\n",
      "Skip training data 7618 due to missing loss information\n",
      "Skip training data 7619 due to missing loss information\n",
      "Skip training data 7620 due to missing loss information\n",
      "Skip training data 7621 due to missing loss information\n",
      "Skip training data 7622 due to missing loss information\n",
      "Skip training data 7623 due to missing loss information\n",
      "Skip training data 7624 due to missing loss information\n",
      "Skip training data 7625 due to missing loss information\n",
      "Skip training data 7626 due to missing loss information\n",
      "Skip training data 7627 due to missing loss information\n",
      "Skip training data 7628 due to missing loss information\n",
      "Skip training data 7629 due to missing loss information\n",
      "Skip training data 7630 due to missing loss information\n",
      "Skip training data 7631 due to missing loss information\n",
      "Skip training data 7632 due to missing loss information\n",
      "Skip training data 7633 due to missing loss information\n",
      "Skip training data 7634 due to missing loss information\n",
      "Skip training data 7635 due to missing loss information\n",
      "Skip training data 7636 due to missing loss information\n",
      "Skip training data 7637 due to missing loss information\n",
      "Skip training data 7638 due to missing loss information\n",
      "Skip training data 7639 due to missing loss information\n",
      "Skip training data 7640 due to missing loss information\n",
      "Skip training data 7641 due to missing loss information\n",
      "Skip training data 7642 due to missing loss information\n",
      "Skip training data 7643 due to missing loss information\n",
      "Skip training data 7644 due to missing loss information\n",
      "Skip training data 7645 due to missing loss information\n",
      "Skip training data 7646 due to missing loss information\n",
      "Skip training data 7647 due to missing loss information\n",
      "Skip training data 7648 due to missing loss information\n",
      "Skip training data 7649 due to missing loss information\n",
      "Skip training data 7650 due to missing loss information\n",
      "Skip training data 7651 due to missing loss information\n",
      "Skip training data 7652 due to missing loss information\n",
      "Skip training data 7653 due to missing loss information\n",
      "Skip training data 7654 due to missing loss information\n",
      "Skip training data 7655 due to missing loss information\n",
      "Skip training data 7656 due to missing loss information\n",
      "Skip training data 7657 due to missing loss information\n",
      "Skip training data 7658 due to missing loss information\n",
      "Skip training data 7659 due to missing loss information\n",
      "Skip training data 7660 due to missing loss information\n",
      "Skip training data 7661 due to missing loss information\n",
      "Skip training data 7662 due to missing loss information\n",
      "Skip training data 7663 due to missing loss information\n",
      "Skip training data 7664 due to missing loss information\n",
      "Skip training data 7665 due to missing loss information\n",
      "Skip training data 7666 due to missing loss information\n",
      "Skip training data 7667 due to missing loss information\n",
      "Skip training data 7668 due to missing loss information\n",
      "Skip training data 7669 due to missing loss information\n",
      "Skip training data 7670 due to missing loss information\n",
      "Skip training data 7671 due to missing loss information\n",
      "Skip training data 7672 due to missing loss information\n",
      "Skip training data 7673 due to missing loss information\n",
      "Skip training data 7674 due to missing loss information\n",
      "Skip training data 7675 due to missing loss information\n",
      "Skip training data 7676 due to missing loss information\n",
      "Skip training data 7677 due to missing loss information\n",
      "Skip training data 7678 due to missing loss information\n",
      "Skip training data 7679 due to missing loss information\n",
      "Skip training data 7680 due to missing loss information\n",
      "Skip training data 7681 due to missing loss information\n",
      "Skip training data 7682 due to missing loss information\n",
      "Skip training data 7683 due to missing loss information\n",
      "Skip training data 7684 due to missing loss information\n",
      "Skip training data 7685 due to missing loss information\n",
      "Skip training data 7686 due to missing loss information\n",
      "Skip training data 7687 due to missing loss information\n",
      "Skip training data 7688 due to missing loss information\n",
      "Skip training data 7689 due to missing loss information\n",
      "Skip training data 7690 due to missing loss information\n",
      "Skip training data 7691 due to missing loss information\n",
      "Skip training data 7692 due to missing loss information\n",
      "Skip training data 7693 due to missing loss information\n",
      "Skip training data 7694 due to missing loss information\n",
      "Skip training data 7695 due to missing loss information\n",
      "Skip training data 7696 due to missing loss information\n",
      "Skip training data 7697 due to missing loss information\n",
      "Skip training data 7698 due to missing loss information\n",
      "Skip training data 7699 due to missing loss information\n",
      "Skip training data 7700 due to missing loss information\n",
      "Skip training data 7701 due to missing loss information\n",
      "Skip training data 7702 due to missing loss information\n",
      "Skip training data 7703 due to missing loss information\n",
      "Skip training data 7704 due to missing loss information\n",
      "Skip training data 7705 due to missing loss information\n",
      "Skip training data 7706 due to missing loss information\n",
      "Skip training data 7707 due to missing loss information\n",
      "Skip training data 7708 due to missing loss information\n",
      "Skip training data 7709 due to missing loss information\n",
      "Skip training data 7710 due to missing loss information\n",
      "Skip training data 7711 due to missing loss information\n",
      "Skip training data 7712 due to missing loss information\n",
      "Skip training data 7713 due to missing loss information\n",
      "Skip training data 7714 due to missing loss information\n",
      "Skip training data 7715 due to missing loss information\n",
      "Skip training data 7716 due to missing loss information\n",
      "Skip training data 7717 due to missing loss information\n",
      "Skip training data 7718 due to missing loss information\n",
      "Skip training data 7719 due to missing loss information\n",
      "Skip training data 7720 due to missing loss information\n",
      "Skip training data 7721 due to missing loss information\n",
      "Skip training data 7722 due to missing loss information\n",
      "Skip training data 7723 due to missing loss information\n",
      "Skip training data 7724 due to missing loss information\n",
      "Skip training data 7725 due to missing loss information\n",
      "Skip training data 7726 due to missing loss information\n",
      "Skip training data 7727 due to missing loss information\n",
      "Skip training data 7728 due to missing loss information\n",
      "Skip training data 7729 due to missing loss information\n",
      "Skip training data 7730 due to missing loss information\n",
      "Skip training data 7731 due to missing loss information\n",
      "Skip training data 7732 due to missing loss information\n",
      "Skip training data 7733 due to missing loss information\n",
      "Skip training data 7734 due to missing loss information\n",
      "Skip training data 7735 due to missing loss information\n",
      "Skip training data 7736 due to missing loss information\n",
      "Skip training data 7737 due to missing loss information\n",
      "Skip training data 7738 due to missing loss information\n",
      "Skip training data 7739 due to missing loss information\n",
      "Skip training data 7740 due to missing loss information\n",
      "Skip training data 7741 due to missing loss information\n",
      "Skip training data 7742 due to missing loss information\n",
      "Skip training data 7743 due to missing loss information\n",
      "Skip training data 7744 due to missing loss information\n",
      "Skip training data 7745 due to missing loss information\n",
      "Skip training data 7746 due to missing loss information\n",
      "Skip training data 7747 due to missing loss information\n",
      "Skip training data 7748 due to missing loss information\n",
      "Skip training data 7749 due to missing loss information\n",
      "Skip training data 7750 due to missing loss information\n",
      "Skip training data 7751 due to missing loss information\n",
      "Skip training data 7752 due to missing loss information\n",
      "Skip training data 7753 due to missing loss information\n",
      "Skip training data 7754 due to missing loss information\n",
      "Skip training data 7755 due to missing loss information\n",
      "Skip training data 7756 due to missing loss information\n",
      "Skip training data 7757 due to missing loss information\n",
      "Skip training data 7758 due to missing loss information\n",
      "Skip training data 7759 due to missing loss information\n",
      "Skip training data 7760 due to missing loss information\n",
      "Skip training data 7761 due to missing loss information\n",
      "Skip training data 7762 due to missing loss information\n",
      "Skip training data 7763 due to missing loss information\n",
      "Skip training data 7764 due to missing loss information\n",
      "Skip training data 7765 due to missing loss information\n",
      "Skip training data 7766 due to missing loss information\n",
      "Skip training data 7767 due to missing loss information\n",
      "Skip training data 7768 due to missing loss information\n",
      "Skip training data 7769 due to missing loss information\n",
      "Skip training data 7770 due to missing loss information\n",
      "Skip training data 7771 due to missing loss information\n",
      "Skip training data 7772 due to missing loss information\n",
      "Skip training data 7773 due to missing loss information\n",
      "Skip training data 7774 due to missing loss information\n",
      "Skip training data 7775 due to missing loss information\n",
      "Skip training data 7776 due to missing loss information\n",
      "Skip training data 7777 due to missing loss information\n",
      "Skip training data 7778 due to missing loss information\n",
      "Skip training data 7779 due to missing loss information\n",
      "Skip training data 7780 due to missing loss information\n",
      "Skip training data 7781 due to missing loss information\n",
      "Skip training data 7782 due to missing loss information\n",
      "Skip training data 7783 due to missing loss information\n",
      "Skip training data 7784 due to missing loss information\n",
      "Skip training data 7785 due to missing loss information\n",
      "Skip training data 7786 due to missing loss information\n",
      "Skip training data 7787 due to missing loss information\n",
      "Skip training data 7788 due to missing loss information\n",
      "Skip training data 7789 due to missing loss information\n",
      "Skip training data 7790 due to missing loss information\n",
      "Skip training data 7791 due to missing loss information\n",
      "Skip training data 7792 due to missing loss information\n",
      "Skip training data 7793 due to missing loss information\n",
      "Skip training data 7794 due to missing loss information\n",
      "Skip training data 7795 due to missing loss information\n",
      "Skip training data 7796 due to missing loss information\n",
      "Skip training data 7797 due to missing loss information\n",
      "Skip training data 7798 due to missing loss information\n",
      "Skip training data 7799 due to missing loss information\n",
      "Skip training data 7800 due to missing loss information\n",
      "Skip training data 7801 due to missing loss information\n",
      "Skip training data 7802 due to missing loss information\n",
      "Skip training data 7803 due to missing loss information\n",
      "Skip training data 7804 due to missing loss information\n",
      "Skip training data 7805 due to missing loss information\n",
      "Skip training data 7806 due to missing loss information\n",
      "Skip training data 7807 due to missing loss information\n",
      "Skip training data 7808 due to missing loss information\n",
      "Skip training data 7809 due to missing loss information\n",
      "Skip training data 7810 due to missing loss information\n",
      "Skip training data 7811 due to missing loss information\n",
      "Skip training data 7812 due to missing loss information\n",
      "Skip training data 7813 due to missing loss information\n",
      "Skip training data 7814 due to missing loss information\n",
      "Skip training data 7815 due to missing loss information\n",
      "Skip training data 7816 due to missing loss information\n",
      "Skip training data 7817 due to missing loss information\n",
      "Skip training data 7818 due to missing loss information\n",
      "Skip training data 7819 due to missing loss information\n",
      "Skip training data 7820 due to missing loss information\n",
      "Skip training data 7821 due to missing loss information\n",
      "Skip training data 7822 due to missing loss information\n",
      "Skip training data 7823 due to missing loss information\n",
      "Skip training data 7824 due to missing loss information\n",
      "Skip training data 7825 due to missing loss information\n",
      "Skip training data 7826 due to missing loss information\n",
      "Skip training data 7827 due to missing loss information\n",
      "Skip training data 7828 due to missing loss information\n",
      "Skip training data 7829 due to missing loss information\n",
      "Skip training data 7830 due to missing loss information\n",
      "Skip training data 7831 due to missing loss information\n",
      "Skip training data 7832 due to missing loss information\n",
      "Skip training data 7833 due to missing loss information\n",
      "Skip training data 7834 due to missing loss information\n",
      "Skip training data 7835 due to missing loss information\n",
      "Skip training data 7836 due to missing loss information\n",
      "Skip training data 7837 due to missing loss information\n",
      "Skip training data 7838 due to missing loss information\n",
      "Skip training data 7839 due to missing loss information\n",
      "Skip training data 7840 due to missing loss information\n",
      "Skip training data 7841 due to missing loss information\n",
      "Skip training data 7842 due to missing loss information\n",
      "Skip training data 7843 due to missing loss information\n",
      "Skip training data 7844 due to missing loss information\n",
      "Skip training data 7845 due to missing loss information\n",
      "Skip training data 7846 due to missing loss information\n",
      "Skip training data 7847 due to missing loss information\n",
      "Skip training data 7848 due to missing loss information\n",
      "Skip training data 7849 due to missing loss information\n",
      "Skip training data 7850 due to missing loss information\n",
      "Skip training data 7851 due to missing loss information\n",
      "Skip training data 7852 due to missing loss information\n",
      "Skip training data 7853 due to missing loss information\n",
      "Skip training data 7854 due to missing loss information\n",
      "Skip training data 7855 due to missing loss information\n",
      "Skip training data 7856 due to missing loss information\n",
      "Skip training data 7857 due to missing loss information\n",
      "Skip training data 7858 due to missing loss information\n",
      "Skip training data 7859 due to missing loss information\n",
      "Skip training data 7860 due to missing loss information\n",
      "Skip training data 7861 due to missing loss information\n",
      "Skip training data 7862 due to missing loss information\n",
      "Skip training data 7863 due to missing loss information\n",
      "Skip training data 7864 due to missing loss information\n",
      "Skip training data 7865 due to missing loss information\n",
      "Skip training data 7866 due to missing loss information\n",
      "Skip training data 7867 due to missing loss information\n",
      "Skip training data 7868 due to missing loss information\n",
      "Skip training data 7869 due to missing loss information\n",
      "Skip training data 7870 due to missing loss information\n",
      "Skip training data 7871 due to missing loss information\n",
      "Skip training data 7872 due to missing loss information\n",
      "Skip training data 7873 due to missing loss information\n",
      "Skip training data 7874 due to missing loss information\n",
      "Skip training data 7875 due to missing loss information\n",
      "Skip training data 7876 due to missing loss information\n",
      "Skip training data 7877 due to missing loss information\n",
      "Skip training data 7878 due to missing loss information\n",
      "Skip training data 7879 due to missing loss information\n",
      "Skip training data 7880 due to missing loss information\n",
      "Skip training data 7881 due to missing loss information\n",
      "Skip training data 7882 due to missing loss information\n",
      "Skip training data 7883 due to missing loss information\n",
      "Skip training data 7884 due to missing loss information\n",
      "Skip training data 7885 due to missing loss information\n",
      "Skip training data 7886 due to missing loss information\n",
      "Skip training data 7887 due to missing loss information\n",
      "Skip training data 7888 due to missing loss information\n",
      "Skip training data 7889 due to missing loss information\n",
      "Skip training data 7890 due to missing loss information\n",
      "Skip training data 7891 due to missing loss information\n",
      "Skip training data 7892 due to missing loss information\n",
      "Skip training data 7893 due to missing loss information\n",
      "Skip training data 7894 due to missing loss information\n",
      "Skip training data 7895 due to missing loss information\n",
      "Skip training data 7896 due to missing loss information\n",
      "Skip training data 7897 due to missing loss information\n",
      "Skip training data 7898 due to missing loss information\n",
      "Skip training data 7899 due to missing loss information\n",
      "Skip training data 7900 due to missing loss information\n",
      "Skip training data 7901 due to missing loss information\n",
      "Skip training data 7902 due to missing loss information\n",
      "Skip training data 7903 due to missing loss information\n",
      "Skip training data 7904 due to missing loss information\n",
      "Skip training data 7905 due to missing loss information\n",
      "Skip training data 7906 due to missing loss information\n",
      "Skip training data 7907 due to missing loss information\n",
      "Skip training data 7908 due to missing loss information\n",
      "Skip training data 7909 due to missing loss information\n",
      "Skip training data 7910 due to missing loss information\n",
      "Skip training data 7911 due to missing loss information\n",
      "Skip training data 7912 due to missing loss information\n",
      "Skip training data 7913 due to missing loss information\n",
      "Skip training data 7914 due to missing loss information\n",
      "Skip training data 7915 due to missing loss information\n",
      "Skip training data 7916 due to missing loss information\n",
      "Skip training data 7917 due to missing loss information\n",
      "Skip training data 7918 due to missing loss information\n",
      "Skip training data 7919 due to missing loss information\n",
      "Skip training data 7920 due to missing loss information\n",
      "Skip training data 7921 due to missing loss information\n",
      "Skip training data 7922 due to missing loss information\n",
      "Skip training data 7923 due to missing loss information\n",
      "Skip training data 7924 due to missing loss information\n",
      "Skip training data 7925 due to missing loss information\n",
      "Skip training data 7926 due to missing loss information\n",
      "Skip training data 7927 due to missing loss information\n",
      "Skip training data 7928 due to missing loss information\n",
      "Skip training data 7929 due to missing loss information\n",
      "Skip training data 7930 due to missing loss information\n",
      "Skip training data 7931 due to missing loss information\n",
      "Skip training data 7932 due to missing loss information\n",
      "Skip training data 7933 due to missing loss information\n",
      "Skip training data 7934 due to missing loss information\n",
      "Skip training data 7935 due to missing loss information\n",
      "Skip training data 7936 due to missing loss information\n",
      "Skip training data 7937 due to missing loss information\n",
      "Skip training data 7938 due to missing loss information\n",
      "Skip training data 7939 due to missing loss information\n",
      "Skip training data 7940 due to missing loss information\n",
      "Skip training data 7941 due to missing loss information\n",
      "Skip training data 7942 due to missing loss information\n",
      "Skip training data 7943 due to missing loss information\n",
      "Skip training data 7944 due to missing loss information\n",
      "Skip training data 7945 due to missing loss information\n",
      "Skip training data 7946 due to missing loss information\n",
      "Skip training data 7947 due to missing loss information\n",
      "Skip training data 7948 due to missing loss information\n",
      "Skip training data 7949 due to missing loss information\n",
      "Skip training data 7950 due to missing loss information\n",
      "Skip training data 7951 due to missing loss information\n",
      "Skip training data 7952 due to missing loss information\n",
      "Skip training data 7953 due to missing loss information\n",
      "Skip training data 7954 due to missing loss information\n",
      "Skip training data 7955 due to missing loss information\n",
      "Skip training data 7956 due to missing loss information\n",
      "Skip training data 7957 due to missing loss information\n",
      "Skip training data 7958 due to missing loss information\n",
      "Skip training data 7959 due to missing loss information\n",
      "Skip training data 7960 due to missing loss information\n",
      "Skip training data 7961 due to missing loss information\n",
      "Skip training data 7962 due to missing loss information\n",
      "Skip training data 7963 due to missing loss information\n",
      "Skip training data 7964 due to missing loss information\n",
      "Skip training data 7965 due to missing loss information\n",
      "Skip training data 7966 due to missing loss information\n",
      "Skip training data 7967 due to missing loss information\n",
      "Skip training data 7968 due to missing loss information\n",
      "Skip training data 7969 due to missing loss information\n",
      "Skip training data 7970 due to missing loss information\n",
      "Skip training data 7971 due to missing loss information\n",
      "Skip training data 7972 due to missing loss information\n",
      "Skip training data 7973 due to missing loss information\n",
      "Skip training data 7974 due to missing loss information\n",
      "Skip training data 7975 due to missing loss information\n",
      "Skip training data 7976 due to missing loss information\n",
      "Skip training data 7977 due to missing loss information\n",
      "Skip training data 7978 due to missing loss information\n",
      "Skip training data 7979 due to missing loss information\n",
      "Skip training data 7980 due to missing loss information\n",
      "Skip training data 7981 due to missing loss information\n",
      "Skip training data 7982 due to missing loss information\n",
      "Skip training data 7983 due to missing loss information\n",
      "Skip training data 7984 due to missing loss information\n",
      "Skip training data 7985 due to missing loss information\n",
      "Skip training data 7986 due to missing loss information\n",
      "Skip training data 7987 due to missing loss information\n",
      "Skip training data 7988 due to missing loss information\n",
      "Skip training data 7989 due to missing loss information\n",
      "Skip training data 7990 due to missing loss information\n",
      "Skip training data 7991 due to missing loss information\n",
      "Skip training data 7992 due to missing loss information\n",
      "Skip training data 7993 due to missing loss information\n",
      "Skip training data 7994 due to missing loss information\n",
      "Skip training data 7995 due to missing loss information\n",
      "Skip training data 7996 due to missing loss information\n",
      "Skip training data 7997 due to missing loss information\n",
      "Skip training data 7998 due to missing loss information\n",
      "Skip training data 7999 due to missing loss information\n",
      "Skip training data 8000 due to missing loss information\n",
      "Skip training data 8001 due to missing loss information\n",
      "Skip training data 8002 due to missing loss information\n",
      "Skip training data 8003 due to missing loss information\n",
      "Skip training data 8004 due to missing loss information\n",
      "Skip training data 8005 due to missing loss information\n",
      "Skip training data 8006 due to missing loss information\n",
      "Skip training data 8007 due to missing loss information\n",
      "Skip training data 8008 due to missing loss information\n",
      "Skip training data 8009 due to missing loss information\n",
      "Skip training data 8010 due to missing loss information\n",
      "Skip training data 8011 due to missing loss information\n",
      "Skip training data 8012 due to missing loss information\n",
      "Skip training data 8013 due to missing loss information\n",
      "Skip training data 8014 due to missing loss information\n",
      "Skip training data 8015 due to missing loss information\n",
      "Skip training data 8016 due to missing loss information\n",
      "Skip training data 8017 due to missing loss information\n",
      "Skip training data 8018 due to missing loss information\n",
      "Skip training data 8019 due to missing loss information\n",
      "Skip training data 8020 due to missing loss information\n",
      "Skip training data 8021 due to missing loss information\n",
      "Skip training data 8022 due to missing loss information\n",
      "Skip training data 8023 due to missing loss information\n",
      "Skip training data 8024 due to missing loss information\n",
      "Skip training data 8025 due to missing loss information\n",
      "Skip training data 8026 due to missing loss information\n",
      "Skip training data 8027 due to missing loss information\n",
      "Skip training data 8028 due to missing loss information\n",
      "Skip training data 8029 due to missing loss information\n",
      "Skip training data 8030 due to missing loss information\n",
      "Skip training data 8031 due to missing loss information\n",
      "Skip training data 8032 due to missing loss information\n",
      "Skip training data 8033 due to missing loss information\n",
      "Skip training data 8034 due to missing loss information\n",
      "Skip training data 8035 due to missing loss information\n",
      "Skip training data 8036 due to missing loss information\n",
      "Skip training data 8037 due to missing loss information\n",
      "Skip training data 8038 due to missing loss information\n",
      "Skip training data 8039 due to missing loss information\n",
      "Skip training data 8040 due to missing loss information\n",
      "Skip training data 8041 due to missing loss information\n",
      "Skip training data 8042 due to missing loss information\n",
      "Skip training data 8043 due to missing loss information\n",
      "Skip training data 8044 due to missing loss information\n",
      "Skip training data 8045 due to missing loss information\n",
      "Skip training data 8046 due to missing loss information\n",
      "Skip training data 8047 due to missing loss information\n",
      "Skip training data 8048 due to missing loss information\n",
      "Skip training data 8049 due to missing loss information\n",
      "Skip training data 8050 due to missing loss information\n",
      "Skip training data 8051 due to missing loss information\n",
      "Skip training data 8052 due to missing loss information\n",
      "Skip training data 8053 due to missing loss information\n",
      "Skip training data 8054 due to missing loss information\n",
      "Skip training data 8055 due to missing loss information\n",
      "Skip training data 8056 due to missing loss information\n",
      "Skip training data 8057 due to missing loss information\n",
      "Skip training data 8058 due to missing loss information\n",
      "Skip training data 8059 due to missing loss information\n",
      "Skip training data 8060 due to missing loss information\n",
      "Skip training data 8061 due to missing loss information\n",
      "Skip training data 8062 due to missing loss information\n",
      "Skip training data 8063 due to missing loss information\n",
      "Skip training data 8064 due to missing loss information\n",
      "Skip training data 8065 due to missing loss information\n",
      "Skip training data 8066 due to missing loss information\n",
      "Skip training data 8067 due to missing loss information\n",
      "Skip training data 8068 due to missing loss information\n",
      "Skip training data 8069 due to missing loss information\n",
      "Skip training data 8070 due to missing loss information\n",
      "Skip training data 8071 due to missing loss information\n",
      "Skip training data 8072 due to missing loss information\n",
      "Skip training data 8073 due to missing loss information\n",
      "Skip training data 8074 due to missing loss information\n",
      "Skip training data 8075 due to missing loss information\n",
      "Skip training data 8076 due to missing loss information\n",
      "Skip training data 8077 due to missing loss information\n",
      "Skip training data 8078 due to missing loss information\n",
      "Skip training data 8079 due to missing loss information\n",
      "Skip training data 8080 due to missing loss information\n",
      "Skip training data 8081 due to missing loss information\n",
      "Skip training data 8082 due to missing loss information\n",
      "Skip training data 8083 due to missing loss information\n",
      "Skip training data 8084 due to missing loss information\n",
      "Skip training data 8085 due to missing loss information\n",
      "Skip training data 8086 due to missing loss information\n",
      "Skip training data 8087 due to missing loss information\n",
      "Skip training data 8088 due to missing loss information\n",
      "Skip training data 8089 due to missing loss information\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip training data 8090 due to missing loss information\n",
      "Skip training data 8091 due to missing loss information\n",
      "Skip training data 8092 due to missing loss information\n",
      "Skip training data 8093 due to missing loss information\n",
      "Skip training data 8094 due to missing loss information\n",
      "Skip training data 8095 due to missing loss information\n",
      "Skip training data 8096 due to missing loss information\n",
      "Skip training data 8097 due to missing loss information\n",
      "Skip training data 8098 due to missing loss information\n",
      "Skip training data 8099 due to missing loss information\n",
      "Skip training data 8100 due to missing loss information\n",
      "Skip training data 8101 due to missing loss information\n",
      "Skip training data 8102 due to missing loss information\n",
      "Skip training data 8103 due to missing loss information\n",
      "Skip training data 8104 due to missing loss information\n",
      "Skip training data 8105 due to missing loss information\n",
      "Skip training data 8106 due to missing loss information\n",
      "Skip training data 8107 due to missing loss information\n",
      "Skip training data 8108 due to missing loss information\n",
      "Skip training data 8109 due to missing loss information\n",
      "Skip training data 8110 due to missing loss information\n",
      "Skip training data 8111 due to missing loss information\n",
      "Skip training data 8112 due to missing loss information\n",
      "Skip training data 8113 due to missing loss information\n",
      "Skip training data 8114 due to missing loss information\n",
      "Skip training data 8115 due to missing loss information\n",
      "Skip training data 8116 due to missing loss information\n",
      "Skip training data 8117 due to missing loss information\n",
      "Skip training data 8118 due to missing loss information\n",
      "Skip training data 8119 due to missing loss information\n",
      "Skip training data 8120 due to missing loss information\n",
      "Skip training data 8121 due to missing loss information\n",
      "Skip training data 8122 due to missing loss information\n",
      "Skip training data 8123 due to missing loss information\n",
      "Skip training data 8124 due to missing loss information\n",
      "Skip training data 8125 due to missing loss information\n",
      "Skip training data 8126 due to missing loss information\n",
      "Skip training data 8127 due to missing loss information\n",
      "Skip training data 8128 due to missing loss information\n",
      "Skip training data 8129 due to missing loss information\n",
      "Skip training data 8130 due to missing loss information\n",
      "Skip training data 8131 due to missing loss information\n",
      "Skip training data 8132 due to missing loss information\n",
      "Skip training data 8133 due to missing loss information\n",
      "Skip training data 8134 due to missing loss information\n",
      "Skip training data 8135 due to missing loss information\n",
      "Skip training data 8136 due to missing loss information\n",
      "Skip training data 8137 due to missing loss information\n",
      "Skip training data 8138 due to missing loss information\n",
      "Skip training data 8139 due to missing loss information\n",
      "Skip training data 8140 due to missing loss information\n",
      "Skip training data 8141 due to missing loss information\n",
      "Skip training data 8142 due to missing loss information\n",
      "Skip training data 8143 due to missing loss information\n",
      "Skip training data 8144 due to missing loss information\n",
      "Skip training data 8145 due to missing loss information\n",
      "Skip training data 8146 due to missing loss information\n",
      "Skip training data 8147 due to missing loss information\n",
      "Skip training data 8148 due to missing loss information\n",
      "Skip training data 8149 due to missing loss information\n",
      "Skip training data 8150 due to missing loss information\n",
      "Skip training data 8151 due to missing loss information\n",
      "Skip training data 8152 due to missing loss information\n",
      "Skip training data 8153 due to missing loss information\n",
      "Skip training data 8154 due to missing loss information\n",
      "Skip training data 8155 due to missing loss information\n",
      "Skip training data 8156 due to missing loss information\n",
      "Skip training data 8157 due to missing loss information\n",
      "Skip training data 8158 due to missing loss information\n",
      "Skip training data 8159 due to missing loss information\n",
      "Skip training data 8160 due to missing loss information\n",
      "Skip training data 8161 due to missing loss information\n",
      "Skip training data 8162 due to missing loss information\n",
      "Skip training data 8163 due to missing loss information\n",
      "Skip training data 8164 due to missing loss information\n",
      "Skip training data 8165 due to missing loss information\n",
      "Skip training data 8166 due to missing loss information\n",
      "Skip training data 8167 due to missing loss information\n",
      "Skip training data 8168 due to missing loss information\n",
      "Skip training data 8169 due to missing loss information\n",
      "Skip training data 8170 due to missing loss information\n",
      "Skip training data 8171 due to missing loss information\n",
      "Skip training data 8172 due to missing loss information\n",
      "Skip training data 8173 due to missing loss information\n",
      "Skip training data 8174 due to missing loss information\n",
      "Skip training data 8175 due to missing loss information\n",
      "Skip training data 8176 due to missing loss information\n",
      "Skip training data 8177 due to missing loss information\n",
      "Skip training data 8178 due to missing loss information\n",
      "Skip training data 8179 due to missing loss information\n",
      "Skip training data 8180 due to missing loss information\n",
      "Skip training data 8181 due to missing loss information\n",
      "Skip training data 8182 due to missing loss information\n",
      "Skip training data 8183 due to missing loss information\n",
      "Skip training data 8184 due to missing loss information\n",
      "Skip training data 8185 due to missing loss information\n",
      "Skip training data 8186 due to missing loss information\n",
      "Skip training data 8187 due to missing loss information\n",
      "Skip training data 8188 due to missing loss information\n",
      "Skip training data 8189 due to missing loss information\n",
      "Skip training data 8190 due to missing loss information\n",
      "Skip training data 8191 due to missing loss information\n",
      "Skip training data 8192 due to missing loss information\n",
      "Skip training data 8193 due to missing loss information\n",
      "Skip training data 8194 due to missing loss information\n",
      "Skip training data 8195 due to missing loss information\n",
      "Skip training data 8196 due to missing loss information\n",
      "Skip training data 8197 due to missing loss information\n",
      "Skip training data 8198 due to missing loss information\n",
      "Skip training data 8199 due to missing loss information\n",
      "Skip training data 8200 due to missing loss information\n",
      "Skip training data 8201 due to missing loss information\n",
      "Skip training data 8202 due to missing loss information\n",
      "Skip training data 8203 due to missing loss information\n",
      "Skip training data 8204 due to missing loss information\n",
      "Skip training data 8205 due to missing loss information\n",
      "Skip training data 8206 due to missing loss information\n",
      "Skip training data 8207 due to missing loss information\n",
      "Skip training data 8208 due to missing loss information\n",
      "Skip training data 8209 due to missing loss information\n",
      "Skip training data 8210 due to missing loss information\n",
      "Skip training data 8211 due to missing loss information\n",
      "Skip training data 8212 due to missing loss information\n",
      "Skip training data 8213 due to missing loss information\n",
      "Skip training data 8214 due to missing loss information\n",
      "Skip training data 8215 due to missing loss information\n",
      "Skip training data 8216 due to missing loss information\n",
      "Skip training data 8217 due to missing loss information\n",
      "Skip training data 8218 due to missing loss information\n",
      "Skip training data 8219 due to missing loss information\n",
      "Skip training data 8220 due to missing loss information\n",
      "Skip training data 8221 due to missing loss information\n",
      "Skip training data 8222 due to missing loss information\n",
      "Skip training data 8223 due to missing loss information\n",
      "Skip training data 8224 due to missing loss information\n",
      "Skip training data 8225 due to missing loss information\n",
      "Skip training data 8226 due to missing loss information\n",
      "Skip training data 8227 due to missing loss information\n",
      "Skip training data 8228 due to missing loss information\n",
      "Skip training data 8229 due to missing loss information\n",
      "Skip training data 8230 due to missing loss information\n",
      "Skip training data 8231 due to missing loss information\n",
      "Skip training data 8232 due to missing loss information\n",
      "Skip training data 8233 due to missing loss information\n",
      "Skip training data 8234 due to missing loss information\n",
      "Skip training data 8235 due to missing loss information\n",
      "Skip training data 8236 due to missing loss information\n",
      "Skip training data 8237 due to missing loss information\n",
      "Skip training data 8238 due to missing loss information\n",
      "Skip training data 8239 due to missing loss information\n",
      "Skip training data 8240 due to missing loss information\n",
      "Skip training data 8241 due to missing loss information\n",
      "Skip training data 8242 due to missing loss information\n",
      "Skip training data 8243 due to missing loss information\n",
      "Skip training data 8244 due to missing loss information\n",
      "Skip training data 8245 due to missing loss information\n",
      "Skip training data 8246 due to missing loss information\n",
      "Skip training data 8247 due to missing loss information\n",
      "Skip training data 8248 due to missing loss information\n",
      "Skip training data 8249 due to missing loss information\n",
      "Skip training data 8250 due to missing loss information\n",
      "Skip training data 8251 due to missing loss information\n",
      "Skip training data 8252 due to missing loss information\n",
      "Skip training data 8253 due to missing loss information\n",
      "Skip training data 8254 due to missing loss information\n",
      "Skip training data 8255 due to missing loss information\n",
      "Skip training data 8256 due to missing loss information\n",
      "Skip training data 8257 due to missing loss information\n",
      "Skip training data 8258 due to missing loss information\n",
      "Skip training data 8259 due to missing loss information\n",
      "Skip training data 8260 due to missing loss information\n",
      "Skip training data 8261 due to missing loss information\n",
      "Skip training data 8262 due to missing loss information\n",
      "Skip training data 8263 due to missing loss information\n",
      "Skip training data 8264 due to missing loss information\n",
      "Skip training data 8265 due to missing loss information\n",
      "Skip training data 8266 due to missing loss information\n",
      "Skip training data 8267 due to missing loss information\n",
      "Skip training data 8268 due to missing loss information\n",
      "Skip training data 8269 due to missing loss information\n",
      "Skip training data 8270 due to missing loss information\n",
      "Skip training data 8271 due to missing loss information\n",
      "Skip training data 8272 due to missing loss information\n",
      "Skip training data 8273 due to missing loss information\n",
      "Skip training data 8274 due to missing loss information\n",
      "Skip training data 8275 due to missing loss information\n",
      "Skip training data 8276 due to missing loss information\n",
      "Skip training data 8277 due to missing loss information\n",
      "Skip training data 8278 due to missing loss information\n",
      "Skip training data 8279 due to missing loss information\n",
      "Skip training data 8280 due to missing loss information\n",
      "Skip training data 8281 due to missing loss information\n",
      "Skip training data 8282 due to missing loss information\n",
      "Skip training data 8283 due to missing loss information\n",
      "Skip training data 8284 due to missing loss information\n",
      "Skip training data 8285 due to missing loss information\n",
      "Skip training data 8286 due to missing loss information\n",
      "Skip training data 8287 due to missing loss information\n",
      "Skip training data 8288 due to missing loss information\n",
      "Skip training data 8289 due to missing loss information\n",
      "Skip training data 8290 due to missing loss information\n",
      "Skip training data 8291 due to missing loss information\n",
      "Skip training data 8292 due to missing loss information\n",
      "Skip training data 8293 due to missing loss information\n",
      "Skip training data 8294 due to missing loss information\n",
      "Skip training data 8295 due to missing loss information\n",
      "Skip training data 8296 due to missing loss information\n",
      "Skip training data 8297 due to missing loss information\n",
      "Skip training data 8298 due to missing loss information\n",
      "Skip training data 8299 due to missing loss information\n",
      "Skip training data 8300 due to missing loss information\n",
      "Skip training data 8301 due to missing loss information\n",
      "Skip training data 8302 due to missing loss information\n",
      "Skip training data 8303 due to missing loss information\n",
      "Skip training data 8304 due to missing loss information\n",
      "Skip training data 8305 due to missing loss information\n",
      "Skip training data 8306 due to missing loss information\n",
      "Skip training data 8307 due to missing loss information\n",
      "Skip training data 8308 due to missing loss information\n",
      "Skip training data 8309 due to missing loss information\n",
      "Skip training data 8310 due to missing loss information\n",
      "Skip training data 8311 due to missing loss information\n",
      "Skip training data 8312 due to missing loss information\n",
      "Skip training data 8313 due to missing loss information\n",
      "Skip training data 8314 due to missing loss information\n",
      "Skip training data 8315 due to missing loss information\n",
      "Skip training data 8316 due to missing loss information\n",
      "Skip training data 8317 due to missing loss information\n",
      "Skip training data 8318 due to missing loss information\n",
      "Skip training data 8319 due to missing loss information\n",
      "Skip training data 8320 due to missing loss information\n",
      "Skip training data 8321 due to missing loss information\n",
      "Skip training data 8322 due to missing loss information\n",
      "Skip training data 8323 due to missing loss information\n",
      "Skip training data 8324 due to missing loss information\n",
      "Skip training data 8325 due to missing loss information\n",
      "Skip training data 8326 due to missing loss information\n",
      "Skip training data 8327 due to missing loss information\n",
      "Skip training data 8328 due to missing loss information\n",
      "Skip training data 8329 due to missing loss information\n",
      "Skip training data 8330 due to missing loss information\n",
      "Skip training data 8331 due to missing loss information\n",
      "Skip training data 8332 due to missing loss information\n",
      "Skip training data 8333 due to missing loss information\n",
      "Skip training data 8334 due to missing loss information\n",
      "Skip training data 8335 due to missing loss information\n",
      "Skip training data 8336 due to missing loss information\n",
      "Skip training data 8337 due to missing loss information\n",
      "Skip training data 8338 due to missing loss information\n",
      "Skip training data 8339 due to missing loss information\n",
      "Skip training data 8340 due to missing loss information\n",
      "Skip training data 8341 due to missing loss information\n",
      "Skip training data 8342 due to missing loss information\n",
      "Skip training data 8343 due to missing loss information\n",
      "Skip training data 8344 due to missing loss information\n",
      "Skip training data 8345 due to missing loss information\n",
      "Skip training data 8346 due to missing loss information\n",
      "Skip training data 8347 due to missing loss information\n",
      "Skip training data 8348 due to missing loss information\n",
      "Skip training data 8349 due to missing loss information\n",
      "Skip training data 8350 due to missing loss information\n",
      "Skip training data 8351 due to missing loss information\n",
      "Skip training data 8352 due to missing loss information\n",
      "Skip training data 8353 due to missing loss information\n",
      "Skip training data 8354 due to missing loss information\n",
      "Skip training data 8355 due to missing loss information\n",
      "Skip training data 8356 due to missing loss information\n",
      "Skip training data 8357 due to missing loss information\n",
      "Skip training data 8358 due to missing loss information\n",
      "Skip training data 8359 due to missing loss information\n",
      "Skip training data 8360 due to missing loss information\n",
      "Skip training data 8361 due to missing loss information\n",
      "Skip training data 8362 due to missing loss information\n",
      "Skip training data 8363 due to missing loss information\n",
      "Skip training data 8364 due to missing loss information\n",
      "Skip training data 8365 due to missing loss information\n",
      "Skip training data 8366 due to missing loss information\n",
      "Skip training data 8367 due to missing loss information\n",
      "Skip training data 8368 due to missing loss information\n",
      "Skip training data 8369 due to missing loss information\n",
      "Skip training data 8370 due to missing loss information\n",
      "Skip training data 8371 due to missing loss information\n",
      "Skip training data 8372 due to missing loss information\n",
      "Skip training data 8373 due to missing loss information\n",
      "Skip training data 8374 due to missing loss information\n",
      "Skip training data 8375 due to missing loss information\n",
      "Skip training data 8376 due to missing loss information\n",
      "Skip training data 8377 due to missing loss information\n",
      "Skip training data 8378 due to missing loss information\n",
      "Skip training data 8379 due to missing loss information\n",
      "Skip training data 8380 due to missing loss information\n",
      "Skip training data 8381 due to missing loss information\n",
      "Skip training data 8382 due to missing loss information\n",
      "Skip training data 8383 due to missing loss information\n",
      "Skip training data 8384 due to missing loss information\n",
      "Skip training data 8385 due to missing loss information\n",
      "Skip training data 8386 due to missing loss information\n",
      "Skip training data 8387 due to missing loss information\n",
      "Skip training data 8388 due to missing loss information\n",
      "Skip training data 8389 due to missing loss information\n",
      "Skip training data 8390 due to missing loss information\n",
      "Skip training data 8391 due to missing loss information\n",
      "Skip training data 8392 due to missing loss information\n",
      "Skip training data 8393 due to missing loss information\n",
      "Skip training data 8394 due to missing loss information\n",
      "Skip training data 8395 due to missing loss information\n",
      "Skip training data 8396 due to missing loss information\n",
      "Skip training data 8397 due to missing loss information\n",
      "Skip training data 8398 due to missing loss information\n",
      "Skip training data 8399 due to missing loss information\n",
      "Skip training data 8400 due to missing loss information\n",
      "Skip training data 8401 due to missing loss information\n",
      "Skip training data 8402 due to missing loss information\n",
      "Skip training data 8403 due to missing loss information\n",
      "Skip training data 8404 due to missing loss information\n",
      "Skip training data 8405 due to missing loss information\n",
      "Skip training data 8406 due to missing loss information\n",
      "Skip training data 8407 due to missing loss information\n",
      "Skip training data 8408 due to missing loss information\n",
      "Skip training data 8409 due to missing loss information\n",
      "Skip training data 8410 due to missing loss information\n",
      "Skip training data 8411 due to missing loss information\n",
      "Skip training data 8412 due to missing loss information\n",
      "Skip training data 8413 due to missing loss information\n",
      "Skip training data 8414 due to missing loss information\n",
      "Skip training data 8415 due to missing loss information\n",
      "Skip training data 8416 due to missing loss information\n",
      "Skip training data 8417 due to missing loss information\n",
      "Skip training data 8418 due to missing loss information\n",
      "Skip training data 8419 due to missing loss information\n",
      "Skip training data 8420 due to missing loss information\n",
      "Skip training data 8421 due to missing loss information\n",
      "Skip training data 8422 due to missing loss information\n",
      "Skip training data 8423 due to missing loss information\n",
      "Skip training data 8424 due to missing loss information\n",
      "Skip training data 8425 due to missing loss information\n",
      "Skip training data 8426 due to missing loss information\n",
      "Skip training data 8427 due to missing loss information\n",
      "Skip training data 8428 due to missing loss information\n",
      "Skip training data 8429 due to missing loss information\n",
      "Skip training data 8430 due to missing loss information\n",
      "Skip training data 8431 due to missing loss information\n",
      "Skip training data 8432 due to missing loss information\n",
      "Skip training data 8433 due to missing loss information\n",
      "Skip training data 8434 due to missing loss information\n",
      "Skip training data 8435 due to missing loss information\n",
      "Skip training data 8436 due to missing loss information\n",
      "Skip training data 8437 due to missing loss information\n",
      "Skip training data 8438 due to missing loss information\n",
      "Skip training data 8439 due to missing loss information\n",
      "Skip training data 8440 due to missing loss information\n",
      "Skip training data 8441 due to missing loss information\n",
      "Skip training data 8442 due to missing loss information\n",
      "Skip training data 8443 due to missing loss information\n",
      "Skip training data 8444 due to missing loss information\n",
      "Skip training data 8445 due to missing loss information\n",
      "Skip training data 8446 due to missing loss information\n",
      "Skip training data 8447 due to missing loss information\n",
      "Skip training data 8448 due to missing loss information\n",
      "Skip training data 8449 due to missing loss information\n",
      "Skip training data 8450 due to missing loss information\n",
      "Skip training data 8451 due to missing loss information\n",
      "Skip training data 8452 due to missing loss information\n",
      "Skip training data 8453 due to missing loss information\n",
      "Skip training data 8454 due to missing loss information\n",
      "Skip training data 8455 due to missing loss information\n",
      "Skip training data 8456 due to missing loss information\n",
      "Skip training data 8457 due to missing loss information\n",
      "Skip training data 8458 due to missing loss information\n",
      "Skip training data 8459 due to missing loss information\n",
      "Skip training data 8460 due to missing loss information\n",
      "Skip training data 8461 due to missing loss information\n",
      "Skip training data 8462 due to missing loss information\n",
      "Skip training data 8463 due to missing loss information\n",
      "Skip training data 8464 due to missing loss information\n",
      "Skip training data 8465 due to missing loss information\n",
      "Skip training data 8466 due to missing loss information\n",
      "Skip training data 8467 due to missing loss information\n",
      "Skip training data 8468 due to missing loss information\n",
      "Skip training data 8469 due to missing loss information\n",
      "Skip training data 8470 due to missing loss information\n",
      "Skip training data 8471 due to missing loss information\n",
      "Skip training data 8472 due to missing loss information\n",
      "Skip training data 8473 due to missing loss information\n",
      "Skip training data 8474 due to missing loss information\n",
      "Skip training data 8475 due to missing loss information\n",
      "Skip training data 8476 due to missing loss information\n",
      "Skip training data 8477 due to missing loss information\n",
      "Skip training data 8478 due to missing loss information\n",
      "Skip training data 8479 due to missing loss information\n",
      "Skip training data 8480 due to missing loss information\n",
      "Skip training data 8481 due to missing loss information\n",
      "Skip training data 8482 due to missing loss information\n",
      "Skip training data 8483 due to missing loss information\n",
      "Skip training data 8484 due to missing loss information\n",
      "Skip training data 8485 due to missing loss information\n",
      "Skip training data 8486 due to missing loss information\n",
      "Skip training data 8487 due to missing loss information\n",
      "Skip training data 8488 due to missing loss information\n",
      "Skip training data 8489 due to missing loss information\n",
      "Skip training data 8490 due to missing loss information\n",
      "Skip training data 8491 due to missing loss information\n",
      "Skip training data 8492 due to missing loss information\n",
      "Skip training data 8493 due to missing loss information\n",
      "Skip training data 8494 due to missing loss information\n",
      "Skip training data 8495 due to missing loss information\n",
      "Skip training data 8496 due to missing loss information\n",
      "Skip training data 8497 due to missing loss information\n",
      "Skip training data 8498 due to missing loss information\n",
      "Skip training data 8499 due to missing loss information\n",
      "Skip training data 8500 due to missing loss information\n",
      "Skip training data 8501 due to missing loss information\n",
      "Skip training data 8502 due to missing loss information\n",
      "Skip training data 8503 due to missing loss information\n",
      "Skip training data 8504 due to missing loss information\n",
      "Skip training data 8505 due to missing loss information\n",
      "Skip training data 8506 due to missing loss information\n",
      "Skip training data 8507 due to missing loss information\n",
      "Skip training data 8508 due to missing loss information\n",
      "Skip training data 8509 due to missing loss information\n",
      "Skip training data 8510 due to missing loss information\n",
      "Skip training data 8511 due to missing loss information\n",
      "Skip training data 8512 due to missing loss information\n",
      "Skip training data 8513 due to missing loss information\n",
      "Skip training data 8514 due to missing loss information\n",
      "Skip training data 8515 due to missing loss information\n",
      "Skip training data 8516 due to missing loss information\n",
      "Skip training data 8517 due to missing loss information\n",
      "Skip training data 8518 due to missing loss information\n",
      "Skip training data 8519 due to missing loss information\n",
      "Skip training data 8520 due to missing loss information\n",
      "Skip training data 8521 due to missing loss information\n",
      "Skip training data 8522 due to missing loss information\n",
      "Skip training data 8523 due to missing loss information\n",
      "Skip training data 8524 due to missing loss information\n",
      "Skip training data 8525 due to missing loss information\n",
      "Skip training data 8526 due to missing loss information\n",
      "Skip training data 8527 due to missing loss information\n",
      "Skip training data 8528 due to missing loss information\n",
      "Skip training data 8529 due to missing loss information\n",
      "Skip training data 8530 due to missing loss information\n",
      "Skip training data 8531 due to missing loss information\n",
      "Skip training data 8532 due to missing loss information\n",
      "Skip training data 8533 due to missing loss information\n",
      "Skip training data 8534 due to missing loss information\n",
      "Skip training data 8535 due to missing loss information\n",
      "Skip training data 8536 due to missing loss information\n",
      "Skip training data 8537 due to missing loss information\n",
      "Skip training data 8538 due to missing loss information\n",
      "Skip training data 8539 due to missing loss information\n",
      "Skip training data 8540 due to missing loss information\n",
      "Skip training data 8541 due to missing loss information\n",
      "Skip training data 8542 due to missing loss information\n",
      "Skip training data 8543 due to missing loss information\n",
      "Skip training data 8544 due to missing loss information\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def mean(l):\n",
    "    return sum(l) * 1.0 / max(len(l), 1e-10)\n",
    "\n",
    "def plot_training_loss_over_height(file_name=None):\n",
    "    trans_length_dict = dict()\n",
    "    for data in train_data:\n",
    "        trans_len = len(data.tokens)\n",
    "        if not trans_len in trans_length_dict:\n",
    "            trans_length_dict[trans_len] = list()\n",
    "        if mean(data.loss) == 0:\n",
    "            print(\"Skip training data \" + str(data.index) + \" due to missing loss information\")\n",
    "        trans_length_dict[trans_len].append(mean(data.loss))\n",
    "    bar_vals = list()\n",
    "    max_trans = max(trans_length_dict.keys()) + 1\n",
    "    for trans_len in range(max_trans):\n",
    "        if trans_len in trans_length_dict:\n",
    "            bar_vals.append(mean(trans_length_dict[trans_len]))\n",
    "        else:\n",
    "            bar_vals.append(0)\n",
    "    plt.bar(range(max_trans), bar_vals)\n",
    "    plt.title(\"Train loss over token length\")\n",
    "    plt.ylabel(\"Average loss\")\n",
    "    plt.xlabel(\"Token length\")\n",
    "    if file_name is not None:\n",
    "        with open(file_name, \"w\") as f:\n",
    "            f.write(\" \".join([str(x) for x in bar_vals]))\n",
    "\n",
    "plot_training_loss_over_height(\"tree_ration_binary.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Class distribution over datasets\n",
    "dev_data_labels = [data.label for data in dev_data]\n",
    "train_data_labels = [data.label for data in train_data]\n",
    "test_data_labels = [data.label for data in test_data]\n",
    "\n",
    "c = [(0.6, 0.2, 0.2), (0.8, 0.5, 0.5), (0.6, 0.6, 0.6), (0.5, 0.8, 0.5), (0.2, 0.6, 0.2)]\n",
    "\n",
    "fig, ax = plt.subplots(1,3,figsize=(20,5))\n",
    "ax[0].bar(range(5), [sum([d==i for d in dev_data_labels]) for i in range(5)], color=c)\n",
    "ax[0].set_title(\"Evaluation dataset\")\n",
    "ax[1].bar(range(5), [sum([d==i for d in train_data_labels]) for i in range(5)], color=c)\n",
    "ax[1].set_title(\"Training dataset\")\n",
    "ax[2].bar(range(5), [sum([d==i for d in test_data_labels]) for i in range(5)], color=c)\n",
    "ax[2].set_title(\"Test dataset\")\n",
    "ax[0].set_xticklabels((\"\", \"Very negative\", \"Negative\", \"Neutral\", \"Positive\", \"Very positive\"), rotation=45)\n",
    "ax[1].set_xticklabels((\"\", \"Very negative\", \"Negative\", \"Neutral\", \"Positive\", \"Very positive\"), rotation=45)\n",
    "ax[2].set_xticklabels((\"\", \"Very negative\", \"Negative\", \"Neutral\", \"Positive\", \"Very positive\"), rotation=45)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1584x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluate_preds_over_token_len(dataset, preds):\n",
    "    acc_over_len = dict()\n",
    "    acc_over_classes = dict()\n",
    "    preds_over_classes = {label:{l:0 for l in range(5)} for label in range(5)}\n",
    "    for ex, pred in zip(dataset, preds):\n",
    "        token_len = len(ex.tokens)\n",
    "        if not token_len in acc_over_len:\n",
    "            acc_over_len[token_len] = list()\n",
    "        if not ex.label in acc_over_classes:\n",
    "            acc_over_classes[ex.label] = list()\n",
    "        acc_over_len[token_len].append(pred[-1] == ex.label)\n",
    "        acc_over_classes[ex.label].append(pred[-1] == ex.label)\n",
    "        preds_over_classes[ex.label][pred[-1]] += 1\n",
    "    \n",
    "    bar_vals_acc = list()\n",
    "    bar_vals_wrong = list()\n",
    "    max_token_len = max(acc_over_len.keys()) + 1\n",
    "    for token_len in range(max_token_len):\n",
    "        if token_len in acc_over_len:\n",
    "            bar_vals_acc.append(mean(acc_over_len[token_len]))\n",
    "            bar_vals_wrong.append(len(acc_over_len[token_len]) - sum(acc_over_len[token_len]))\n",
    "        else:\n",
    "            bar_vals_acc.append(0)\n",
    "            bar_vals_wrong.append(0)\n",
    "            \n",
    "    bar_vals_classes = list()\n",
    "    max_label = max(acc_over_classes.keys()) + 1\n",
    "    for label in range(max_label):\n",
    "        if label in acc_over_classes:\n",
    "            bar_vals_classes.append(mean(acc_over_classes[label]))\n",
    "        else:\n",
    "            bar_vals_classes.append(0)\n",
    "            \n",
    "    fig, ax = plt.subplots(1, 4, figsize=(22,4))\n",
    "    ax[0].bar(range(max_token_len), bar_vals_acc)\n",
    "    ax[0].set_title(\"Evaluation accuracy over token length\")\n",
    "    ax[0].set_ylabel(\"Accuracy\")\n",
    "    ax[0].set_xlabel(\"Token length\")\n",
    "    ax[1].bar(range(max_token_len), bar_vals_wrong)\n",
    "    ax[1].set_title(\"Number of wrong predictions over token length\")\n",
    "    ax[1].set_ylabel(\"Wrong predictions\")\n",
    "    ax[1].set_xlabel(\"Token length\")\n",
    "    ax[2].bar(range(max_label), bar_vals_classes, color=c)\n",
    "    ax[2].set_title(\"Evaluation accuracy over classes\")\n",
    "    ax[2].set_ylabel(\"Accuracy\")\n",
    "    ax[2].set_xticklabels((\"\", \"Very negative\", \"Negative\", \"Neutral\", \"Positive\", \"Very positive\"), rotation=45)\n",
    "    bottom_pos = [0 for i in range(5)]\n",
    "    norm_consts = [sum([preds_over_classes[label][l] for l in range(5)]) for label in range(5)]\n",
    "    for label in range(5):\n",
    "        bar_heights = [preds_over_classes[l][label]/max(norm_consts[l],1e-10) for l in range(5)]\n",
    "        ax[3].bar(range(max_label), bar_heights, color=c[label], bottom=bottom_pos)\n",
    "        bottom_pos = [bottom_pos[i] + bar_heights[i] for i in range(5)]\n",
    "    ax[3].set_title(\"Predictons over classes\")\n",
    "    ax[3].set_ylabel(\"Predictions\")\n",
    "    ax[3].set_xticklabels((\"\", \"Very negative\", \"Negative\", \"Neutral\", \"Positive\", \"Very positive\"), rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "evaluate_preds_over_token_len(dev_data, [[random.randint(0,4)] for d in dev_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "DEBUG_TREE_PRINT = True\n",
    "\n",
    "def pred_to_tree(pred, default_tree, pred_vals=None, index=0, loss_factors=None, gt_labels=None):\n",
    "    if isinstance(default_tree[0], Tree):\n",
    "        for subtree in default_tree:\n",
    "            _, index = pred_to_tree(pred, subtree, pred_vals=pred_vals, index=index, loss_factors=loss_factors, gt_labels=gt_labels)\n",
    "    label = str(pred[index])# str(pred[index].item())\n",
    "    if pred_vals is not None:\n",
    "        label += \" (%.1f\" % (100.0 * pred_vals[index].item()) + \"%)\"\n",
    "    if loss_factors is not None:\n",
    "        label += \"-%.3f\" % loss_factors[index].item()\n",
    "    if gt_labels is not None:\n",
    "        label += \" [\" + str(gt_labels[index]) + \"]\"\n",
    "    default_tree.set_label(label)\n",
    "    index += 1\n",
    "    return default_tree, index\n",
    "\n",
    "def evaluate(model, data, \n",
    "             batch_fn=get_minibatch, prep_fn=prepare_minibatch,\n",
    "             batch_size=16):\n",
    "    \"\"\"Accuracy of a model on given data set (using minibatches)\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()  # disable dropout\n",
    "    pred_list = list()\n",
    "\n",
    "    for mb in batch_fn(data, batch_size=batch_size, shuffle=False, is_eval=True):\n",
    "        x, targets = prep_fn(mb, model.vocab, is_eval=True)\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)\n",
    "            \n",
    "        input_tokens, input_transitions = x\n",
    "        targets, _, _ = targets\n",
    "        # input_transitions = torch.LongTensor(input_transitions)\n",
    "        # input_transitions = input_transitions.to(device)\n",
    "        predictions = logits.argmax(dim=-1)\n",
    "        # print(input_transitions.size())\n",
    "        # print(predictions.size())\n",
    "        \n",
    "        # Use only valid last prediction to get final prediction\n",
    "        predictions = predictions.transpose(0, 1).contiguous()  # [B, T]\n",
    "        targets = targets.transpose(0, 1).contiguous()  # [B, T]\n",
    "        \n",
    "        batch_preds = predictions.data.cpu().numpy()\n",
    "\n",
    "        # to be super-sure we're not accidentally indexing the wrong state\n",
    "        # we zero out positions that are invalid\n",
    "        pad_positions = (targets == -1)\n",
    "\n",
    "        predictions = predictions.contiguous()      \n",
    "        predictions = predictions.masked_fill_(pad_positions, 0.)\n",
    "\n",
    "        mask = (targets != -1)  # true for valid positions [B, T]\n",
    "        lengths = mask.sum(dim=1)                  # [B, 1]\n",
    "\n",
    "        B = targets.size(0)\n",
    "        T = targets.size(1)\n",
    "        \n",
    "        indexes = (lengths - 1) + torch.arange(B, device=targets.device, dtype=targets.dtype) * T\n",
    "        predictions = predictions.view(-1)[indexes]  # [B]\n",
    "        targets = targets.view(-1)[indexes]\n",
    "\n",
    "        # add the number of correct predictions to the total correct\n",
    "        correct += (predictions == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "        \n",
    "        for ex_index, ex in enumerate(mb):\n",
    "            pred_list.append(batch_preds[ex_index][:len(ex.transitions)])\n",
    "        \n",
    "        del x\n",
    "        del targets\n",
    "        del predictions\n",
    "        \n",
    "    print(\"Evaluation on \" + str(total) + \" elements. Correct: \" + str(correct))\n",
    "    \"\"\"\n",
    "    ex_ind = 3\n",
    "    # single_pred = logits.argmax(dim=-1)[:,0]\n",
    "    softmax = nn.Softmax(dim=-1)\n",
    "    pred_vals, single_pred = torch.exp(logits).max(dim=-1)\n",
    "    pred_vals = pred_vals[:,ex_ind]\n",
    "    single_pred = single_pred[:,ex_ind]\n",
    "    if DEBUG_TREE_PRINT:\n",
    "        print(TreePrettyPrinter(mb[ex_ind].tree))\n",
    "        print(TreePrettyPrinter(pred_to_tree(single_pred, copy.deepcopy(mb[ex_ind].tree), pred_vals=pred_vals)[0]))\n",
    "    \"\"\"\n",
    "    del logits\n",
    "    return correct, total, correct / float(total), pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_out_evals(dev_data, preds, sort_by_class = True, filename=\"eval_output.txt\"):\n",
    "    pred_trees = list()\n",
    "    for pred, data in zip(preds, dev_data):\n",
    "        sublabels = get_sublabel_of_tree(data.tree)[0]\n",
    "        pred_trees.append(pred_to_tree(pred, copy.deepcopy(data.tree), gt_labels = sublabels)[0])\n",
    "    if sort_by_class:\n",
    "        pred_trees_by_class = {l: list() for l in range(5)}\n",
    "        for pred_tree, data in zip(pred_trees, dev_data):\n",
    "            pred_trees_by_class[data.label].append(pred_tree)\n",
    "        pred_trees = list()\n",
    "        for l in range(5):\n",
    "            pred_trees += pred_trees_by_class[l]\n",
    "    output_string = \"=\"*50 + \"\\n\\tEvaluation\\n\" + \"=\"*50 + \"\\n\\n\"\n",
    "    for tree in pred_trees:\n",
    "        output_string += \"\\n\" + str(TreePrettyPrinter(tree)) + \"\\n\" + \"=\"*100 + \"\\n\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(output_string)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plain hierarchical loss with loss at every node and simple decay factor over depth\n",
    "class HierarchicalLoss(nn.Module):\n",
    "    \"\"\"Hierarchical loss function\"\"\"\n",
    "\n",
    "    def __init__(self, decay_factor = 0.25):\n",
    "        super(HierarchicalLoss, self).__init__()\n",
    "        self.loss_func = nn.NLLLoss(reduction='none')\n",
    "        self.decay_factor = decay_factor\n",
    "        self.range_tensor = torch.arange(5).to(device)\n",
    "\n",
    "    def forward(self, pred, targets, depth):\n",
    "        T = pred.size(0)\n",
    "        B = pred.size(1)\n",
    "        \n",
    "        reshaped_targets = targets.view(B*T)\n",
    "        valid_targets = torch.max(reshaped_targets, reshaped_targets.new_zeros(B*T))\n",
    "        reshaped_preds = pred.view([B * T, -1])\n",
    "        \n",
    "        # Introduce weighting of single classes: if the network is far off (very positive instead of very negative) => higher loss\n",
    "        # Or extra ML loss between prediction and target (does that work out?)\n",
    "        \n",
    "        \n",
    "        anti_prob = torch.log(1 - reshaped_preds)\n",
    "        a = torch.pow(valid_targets.unsqueeze(-1) - self.range_tensor.unsqueeze(0), 2)\n",
    "        add_loss = torch.sum(anti_prob * torch.div(a, 16).type_as(anti_prob),dim=-1)\n",
    "        \n",
    "        loss_terms = self.loss_func(reshaped_preds, valid_targets)\n",
    "        # loss_terms += add_loss\n",
    "        loss_terms = loss_terms.view(T, B)\n",
    "        loss_mask = torch.min(targets+1, targets.new_zeros(T,B)+1)\n",
    "        loss_weight = (torch.pow(targets.new_zeros(1)+self.decay_factor, depth) * loss_mask).float()\n",
    "        weighted_loss = loss_terms * loss_weight\n",
    "        loss_norm = torch.sum(weighted_loss, dim=0) / torch.sum(loss_weight, dim=0)\n",
    "        mean_batch_loss = torch.mean(loss_norm)\n",
    "        return mean_batch_loss, loss_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_hardest_words(depth=0):\n",
    "    hard_words = sorted([(ex.tokens, sum(ex.loss)/len(ex.loss)) if len(ex.loss) > 0 else (ex.tokens,-1) for ex in all_subtrees[depth]], key=lambda x:-x[1])\n",
    "    s = \"Hardest words: \"\n",
    "    for tok, loss in hard_words[:5]:\n",
    "        s += \"'\" + \" \".join(tok) + \"' (%.2f)\" % (loss) + \", \"\n",
    "    print(s)\n",
    "    s = \"Easiest words: \"\n",
    "    for tok, loss in hard_words[-5:]:\n",
    "        s += \"'\" + \" \".join(tok) + \"' (%.2f)\" % (loss) + \", \"\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "HEURISTIC_BINARY = 0\n",
    "HEURISTIC_1_DISTANCE = 1\n",
    "HEURISTIC_2ND_HIGHEST = 2\n",
    "HEURISTIC_TAI_ET_AL = 3\n",
    "\n",
    "def train_tree_model(model, optimizer, num_iterations=10000, \n",
    "                print_every=1000, eval_every=1000,\n",
    "                batch_fn=get_examples, \n",
    "                prep_fn=prepare_example,\n",
    "                eval_fn=simple_evaluate,\n",
    "                batch_size=1, eval_batch_size=None,\n",
    "                dataset_train=None, dataset_eval=None, dataset_test=None, file_prefix=None,\n",
    "                training_setting = None):\n",
    "    \"\"\"Train a model.\"\"\"  \n",
    "    iter_i = 0\n",
    "    train_loss = 0.\n",
    "    print_num = 0\n",
    "    start = time.time()\n",
    "    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=80, gamma=0.1) # Learning rate scheduler. Not used right now\n",
    "    \n",
    "    # Determine loss function\n",
    "    if training_setting is None:\n",
    "        criterion = HierarchicalLossV2()\n",
    "    else:\n",
    "        if training_setting == HEURISTIC_BINARY:\n",
    "            criterion = HierarchicalLossV2(allowed_dist=0, stochastic_choice=False)\n",
    "        elif training_setting == HEURISTIC_BINARY:\n",
    "            criterion = HierarchicalLossV2(allowed_dist=1, stochastic_choice=False)\n",
    "        elif training_setting == HEURISTIC_BINARY:\n",
    "            criterion = HierarchicalLossV2(allowed_dist=1, stochastic_choice=True)\n",
    "        elif training_setting == HEURISTIC_BINARY:\n",
    "            criterion = HierarchicalLoss(decay_factor=1.0)\n",
    "    \n",
    "    best_eval = 0.\n",
    "    best_test_acc = 0.\n",
    "    best_iter = 0\n",
    "    i = datetime.datetime.now()\n",
    "    filename = \"eval_output_%s_%s_%s-%s_%s_%s.txt\" % (i.day, i.month, i.year, i.hour, i.minute, i.second)\n",
    "\n",
    "    if not dataset_train:\n",
    "        dataset_train = train_data\n",
    "    if not dataset_eval:\n",
    "        dataset_eval = dev_data\n",
    "    if not dataset_test:\n",
    "        dataset_test = test_data\n",
    "    \n",
    "    if not file_prefix:\n",
    "        checkpoint_path = CHECKPOINT_DIR + \"{}.pt\".format(model.__class__.__name__)\n",
    "    else:\n",
    "        checkpoint_path = CHECKPOINT_DIR + file_prefix + \".pt\"\n",
    "    \n",
    "    # store train loss and validation accuracy during training\n",
    "    # so we can plot them afterwards\n",
    "    losses = []\n",
    "    accuracies = []  \n",
    "\n",
    "    if eval_batch_size is None:\n",
    "        eval_batch_size = batch_size\n",
    "\n",
    "    while True:  # when we run out of examples, shuffle and continue\n",
    "        # if iter_i > 0:\n",
    "        #     scheduler.step()\n",
    "        #     for param_group in optimizer.param_groups:\n",
    "        #         print(\"Scheduler step. New learning rate: \" + str(param_group['lr']))\n",
    "        # if iter_i > 5000:\n",
    "        #     model.embed.weight.requires_grad = True\n",
    "        for batch in batch_fn(dataset_train, batch_size=batch_size, is_eval=False, iteration_num=iter_i):\n",
    "\n",
    "            # forward pass\n",
    "            model.train()\n",
    "            x, y = prep_fn(batch, model.vocab)\n",
    "            \n",
    "            targets, label_depth, transition_matrices = y\n",
    "            input_tokens, input_transitions = x\n",
    "            \n",
    "            logits = model(x) # From now on, this must already be with softmax!\n",
    "\n",
    "            T = targets.size(0)\n",
    "            B = targets.size(1)  # later we will use B examples per update\n",
    "\n",
    "            # compute cross-entropy loss (our criterion)\n",
    "            # note that the cross entropy loss function computes the softmax for us\n",
    "            loss, elementwise_loss, loss_factors, tree_exploration = criterion(logits, targets, transition_matrices, label_depth)\n",
    "            train_loss += loss.item()\n",
    "            for ex_index, ex in enumerate(batch):\n",
    "                ex.loss.append(tree_exploration[ex_index].item())\n",
    "                if len(ex.loss) > 8: # Keep record of last losses fixed to 5\n",
    "                    del ex.loss[0]\n",
    "\n",
    "            # backward pass\n",
    "            # Tip: check the Introduction to PyTorch notebook.\n",
    "\n",
    "            # erase previous gradients\n",
    "            model.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "            # update weights - take a small step in the opposite dir of the gradient\n",
    "            optimizer.step()\n",
    "\n",
    "            print_num += 1\n",
    "            iter_i += 1\n",
    "\n",
    "            # print info\n",
    "            if iter_i % print_every == 0:\n",
    "                print(\"Iter %r: loss=%.4f, time=%.2fs\" % \n",
    "                (iter_i, train_loss, time.time()-start))\n",
    "                # print_hardest_words()\n",
    "                smallest_transition=100\n",
    "                min_ex_ind=-1\n",
    "                for ex_ind, ex in enumerate(batch):\n",
    "                    if len(ex.transitions) < smallest_transition:\n",
    "                        smallest_transition = len(ex.transitions)\n",
    "                        min_ex_ind = ex_ind\n",
    "                # min_ex_ind = elementwise_loss.argmax(dim=-1).item()\n",
    "                pred_vals, single_pred = torch.exp(logits[:,min_ex_ind,:]).max(dim=-1)\n",
    "                if DEBUG_TREE_PRINT:\n",
    "                    print(TreePrettyPrinter(batch[min_ex_ind].tree))\n",
    "                    print(TreePrettyPrinter(pred_to_tree(single_pred, copy.deepcopy(batch[min_ex_ind].tree), pred_vals=pred_vals, loss_factors=loss_factors[:,min_ex_ind])[0]))\n",
    "                losses.append(train_loss)\n",
    "                print_num = 0        \n",
    "                train_loss = 0.\n",
    "\n",
    "            # evaluate\n",
    "            if iter_i % eval_every == 0:\n",
    "                _, _, accuracy, dev_preds = eval_fn(model, dataset_eval, batch_size=eval_batch_size,\n",
    "                         batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "                accuracies.append(accuracy)\n",
    "                print(\"iter %r: dev acc=%.4f\" % (iter_i, accuracy)) \n",
    "                _, _, test_acc, test_preds = eval_fn(model, dataset_test, batch_size=eval_batch_size, \n",
    "                         batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "                print(\"iter %r: test acc=%.4f\" % (iter_i, test_acc)) \n",
    "\n",
    "                # save best model parameters\n",
    "                if accuracy > best_eval:\n",
    "                    print(\"new highscore\")\n",
    "                    evaluate_preds_over_token_len(dataset_eval, dev_preds)\n",
    "                    write_out_evals(dataset_eval, dev_preds, filename=filename)\n",
    "                    best_eval = accuracy\n",
    "                    best_iter = iter_i\n",
    "                    # path = \"{}.pt\".format(model.__class__.__name__)\n",
    "                    ckpt = {\n",
    "                    \"state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"best_eval\": best_eval,\n",
    "                    \"best_iter\": best_iter\n",
    "                    }\n",
    "                    torch.save(ckpt, checkpoint_path)\n",
    "                \n",
    "                if test_acc > best_test_acc:\n",
    "                    best_test_acc = test_acc\n",
    "                    \n",
    "            del targets\n",
    "            del transition_matrices\n",
    "            del label_depth\n",
    "            del x\n",
    "            del logits\n",
    "            del loss\n",
    "            del elementwise_loss\n",
    "            del loss_factors\n",
    "            torch.cuda.empty_cache()\n",
    "                    \n",
    "            # done training\n",
    "            if iter_i == num_iterations:\n",
    "                print(\"Done training\")\n",
    "\n",
    "                # evaluate on train, dev, and test with best model\n",
    "                print(\"Loading best model\")\n",
    "                # path = \"{}_1.pt\".format(model.__class__.__name__)        \n",
    "                ckpt = torch.load(checkpoint_path)\n",
    "                model.load_state_dict(ckpt[\"state_dict\"])\n",
    "\n",
    "                _, _, train_acc, train_preds = eval_fn(\n",
    "                model, dataset_train, batch_size=eval_batch_size, \n",
    "                batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "                _, _, dev_acc, dev_preds = eval_fn(\n",
    "                model, dataset_eval, batch_size=eval_batch_size,\n",
    "                batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "                _, _, test_acc, test_preds = eval_fn(\n",
    "                model, dataset_test, batch_size=eval_batch_size, \n",
    "                batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "\n",
    "                print(\"best model iter {:d}: \"\n",
    "                \"train acc={:.4f}, dev acc={:.4f}, test acc={:.4f}\".format(\n",
    "                best_iter, train_acc, dev_acc, test_acc))\n",
    "                print(\"Best test accuracy would have been %.4f\" % (best_test_acc))\n",
    "\n",
    "                return losses, accuracies, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention module is not used within the tree! Implementation just for testing the approach\n",
    "class AttentionModule(nn.Module):\n",
    "    \"\"\"Realizes a very simple attention module. Takes last state and word into account with two-layer feedforward\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, hidden_dim):\n",
    "        super(AttentionModule, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.state_dim = state_dim\n",
    "        self.output_layer = nn.Sequential(OrderedDict([\n",
    "            # ('fc1', nn.Linear(2*state_dim, hidden_dim)),\n",
    "            # ('tanh', nn.Tanh()),\n",
    "            # ('dropout', nn.Dropout(0.5)),\n",
    "            ('fc2', nn.Linear(2*state_dim, 2)),\n",
    "            # ('fc', nn.Linear(embedding_dim+state_dim, 1)),\n",
    "            ('sigmoid', nn.Sigmoid())\n",
    "        ]))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"This is PyTorch's default initialization method\"\"\"\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_dim)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)  \n",
    "    \n",
    "    def forward(self, child_l, child_r):\n",
    "        \n",
    "        input_tensor = torch.cat([child_l, child_r], dim=1)\n",
    "        \n",
    "        for layer in self.output_layer:\n",
    "            input_tensor = layer(input_tensor)\n",
    "        \n",
    "        return input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeLSTMCell(nn.Module):\n",
    "    \"\"\"A Binary Tree LSTM cell\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        \"\"\"Creates the weights for this LSTM\"\"\"\n",
    "        super(TreeLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "        \n",
    "        self.attent_module = AttentionModule(self.hidden_size, self.hidden_size)\n",
    "\n",
    "        self.reduce_layer = nn.Linear(2 * hidden_size, 5 * hidden_size)\n",
    "        self.tanh_act = nn.Tanh()\n",
    "        self.sigmoid_act = nn.Sigmoid()\n",
    "        self.dropout_layer = nn.Dropout(p=0.1)\n",
    "        self.counter = 0\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"This is PyTorch's default initialization method\"\"\"\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)  \n",
    "\n",
    "    def forward(self, hx_l, hx_r, mask=None):\n",
    "        \"\"\"\n",
    "        hx_l is ((batch, hidden_size), (batch, hidden_size))\n",
    "        hx_r is ((batch, hidden_size), (batch, hidden_size))    \n",
    "        \"\"\"\n",
    "        prev_h_l, prev_c_l = hx_l  # left child\n",
    "        prev_h_r, prev_c_r = hx_r  # right child\n",
    "\n",
    "        B = prev_h_l.size(0)\n",
    "\n",
    "        # we concatenate the left and right children\n",
    "        # you can also project from them separately and then sum\n",
    "        children = torch.cat([prev_h_l, prev_h_r], dim=1)\n",
    "\n",
    "        # project the combined children into a 5D tensor for i,fl,fr,g,o\n",
    "        # this is done for speed, and you could also do it separately\n",
    "        proj = self.reduce_layer(children)  # shape: B x 5D\n",
    "\n",
    "        # each shape: B x D\n",
    "        i, f_l, f_r, g, o = torch.chunk(proj, 5, dim=-1)\n",
    "\n",
    "        # main Tree LSTM computation\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # You only need to complete the commented lines below.\n",
    "\n",
    "        # The shape of each of these is [batch_size, hidden_size]\n",
    "\n",
    "        i = self.sigmoid_act(i)\n",
    "        f_l = self.sigmoid_act(f_l)    \n",
    "        f_r = self.sigmoid_act(f_r)\n",
    "        g = self.tanh_act(g)\n",
    "        o = self.sigmoid_act(o)\n",
    "\n",
    "        c = self.dropout_layer(i * g) + f_l * prev_c_l + f_r * prev_c_r\n",
    "        h = o * self.tanh_act(c)\n",
    "        \n",
    "        # attent_weights = self.attent_module(prev_h_l, prev_h_r)\n",
    "        # h_children = attent_weights[:,0:1] * prev_h_l + (1 - attent_weights[:,0:1]) * prev_h_r\n",
    "        # h = attent_weights[:,1:2] * h + (1 - attent_weights[:,1:2]) * h_children\n",
    "        # c_children = attent_weights[:,0:1] * prev_c_l + (1 - attent_weights[:,0:1]) * prev_c_r\n",
    "        # c = attent_weights[:,1:2] * c + (1 - attent_weights[:,1:2]) * c_children\n",
    "\n",
    "        return h, c\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"{}({:d}, {:d})\".format(\n",
    "            self.__class__.__name__, self.input_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeLSTM(nn.Module):\n",
    "    \"\"\"Encodes a sentence using a TreeLSTMCell\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        \"\"\"Creates the weights for this LSTM\"\"\"\n",
    "        super(TreeLSTM, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "        self.reduce = TreeLSTMCell(input_size, hidden_size)\n",
    "\n",
    "        # project word to initial c\n",
    "        self.pre_layer = nn.Linear(input_size, input_size)\n",
    "        self.dropout_layer = nn.Dropout(p=0.25)\n",
    "        self.relu_act = nn.ReLU()\n",
    "        self.proj_x = nn.Linear(input_size, hidden_size)\n",
    "        self.proj_x_gate = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        self.buffers_dropout = nn.Dropout(p=0.1)\n",
    "        self.counter = 0\n",
    "\n",
    "    def forward(self, x, transitions):\n",
    "        \"\"\"\n",
    "        WARNING: assuming x is reversed!\n",
    "        :param x: word embeddings [B, T, E]\n",
    "        :param transitions: [2T-1, B]\n",
    "        :return: root states\n",
    "        \"\"\"\n",
    "\n",
    "        B = x.size(0)  # batch size\n",
    "        T = x.size(1)  # time\n",
    "\n",
    "        # compute an initial c and h for each word\n",
    "        # Note: this corresponds to input x in the Tai et al. Tree LSTM paper.\n",
    "        # We do not handle input x in the TreeLSTMCell itself.\n",
    "        x = self.dropout_layer(self.relu_act(self.pre_layer(x))) # Fine-tuning layer\n",
    "        buffers_c = self.proj_x(x)\n",
    "        buffers_h = buffers_c.tanh()\n",
    "        buffers_h_gate = self.proj_x_gate(x).sigmoid()\n",
    "        buffers_h = buffers_h_gate * buffers_h\n",
    "\n",
    "        # concatenate h and c for each word\n",
    "        buffers = torch.cat([buffers_h, buffers_c], dim=-1)\n",
    "\n",
    "        D = buffers.size(-1) // 2\n",
    "\n",
    "        # we turn buffers into a list of stacks (1 stack for each sentence)\n",
    "        # first we split buffers so that it is a list of sentences (length B)\n",
    "        # then we split each sentence to be a list of word vectors\n",
    "        buffers = buffers.split(1, dim=0)  # Bx[T, 2D]\n",
    "        buffers = [list(b.squeeze(0).split(1, dim=0)) for b in buffers]  # BxTx[2D]\n",
    "\n",
    "        # create B empty stacks\n",
    "        stacks = [[] for _ in buffers]\n",
    "        \n",
    "        tree_embeddings = []\n",
    "        \n",
    "\n",
    "        # t_batch holds 1 transition for each sentence\n",
    "        for t_batch in transitions:\n",
    "            loc_embedding = list()\n",
    "            child_l = []  # contains the left child for each sentence with reduce action\n",
    "            child_r = []  # contains the corresponding right child\n",
    "\n",
    "            # iterate over sentences in the batch\n",
    "            # each has a transition t, a buffer and a stack\n",
    "            for transition, buffer, stack in zip(t_batch, buffers, stacks):\n",
    "                if transition == SHIFT:\n",
    "                    stack.append(buffer.pop())\n",
    "                elif transition == REDUCE:\n",
    "                    assert len(stack) >= 2, \\\n",
    "                        \"Stack too small! Should not happen with valid transition sequences\"\n",
    "                    child_r.append(stack.pop())  # right child is on top\n",
    "                    child_l.append(stack.pop())\n",
    "\n",
    "            # if there are sentences with reduce transition, perform them batched\n",
    "            if child_l:\n",
    "                reduced = iter(unbatch(self.reduce(batch(child_l), batch(child_r))))\n",
    "                for transition, stack in zip(t_batch, stacks):\n",
    "                    if transition == REDUCE:\n",
    "                        stack.append(next(reduced))\n",
    "            \n",
    "            tree_embeddings.append([stack[-1].chunk(2, -1)[0] for stack in stacks])\n",
    "\n",
    "        # final = [stack.pop().chunk(2, -1)[0] for stack in stacks]\n",
    "        final = torch.stack([torch.cat(emb, dim=0) for emb in tree_embeddings], dim=0)\n",
    "        # final = torch.cat(final, dim=0)  # tensor [T, B, D]\n",
    "        # print(final.shape)\n",
    "        # del tree_embeddings\n",
    "        \n",
    "        return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeLSTMClassifier(nn.Module):\n",
    "    \"\"\"Encodes sentence with a TreeLSTM and projects final hidden state\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, vocab):\n",
    "        super(TreeLSTMClassifier, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim, padding_idx=1)\n",
    "        self.treelstm = TreeLSTM(embedding_dim, hidden_dim)\n",
    "        self.embed_dropout = nn.Dropout(p=0.25)\n",
    "        self.output_dim = output_dim\n",
    "        self.output_layer = nn.Sequential(     \n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.Linear(hidden_dim, output_dim, bias=True),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # x is a pair here of words and transitions; we unpack it here.\n",
    "        # x is batch-major: [B, T], transitions is time major [2T-1, B]\n",
    "        x, transitions = x\n",
    "        emb = self.embed(x)\n",
    "\n",
    "        # we use the root/top state of the Tree LSTM to classify the sentence\n",
    "        root_states = self.treelstm(emb, transitions)\n",
    "\n",
    "        # we use the last hidden state to classify the sentence\n",
    "        T = root_states.size(0)\n",
    "        B = root_states.size(1)\n",
    "        \n",
    "        # print(root_states.view(T * B, self.hidden_dim).shape)\n",
    "        logits = self.output_layer(root_states.view(T * B, self.hidden_dim)).view(T, B, self.output_dim)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalLossV2(nn.Module):\n",
    "    \"\"\"Hierarchical loss function with stopping at wrong predictions\"\"\"\n",
    "\n",
    "    def __init__(self, decay_factor = 0.8, allowed_dist = 1, stochastic_choice=True):\n",
    "        super(HierarchicalLossV2, self).__init__()\n",
    "        self.loss_func = nn.NLLLoss(reduction='none')\n",
    "        self.decay_factor = decay_factor\n",
    "        self.range_tensor = torch.arange(5).to(device)\n",
    "        self.allowed_dist = allowed_dist\n",
    "        self.stochastic_choice = stochastic_choice\n",
    "        self.stochastic_approach = 2\n",
    "        self.counter = 0\n",
    "\n",
    "    def forward(self, pred, targets, transition_matrices, depth):\n",
    "        T = pred.size(0)\n",
    "        B = pred.size(1)\n",
    "        \n",
    "        reshaped_targets = targets.contiguous().view(B*T)\n",
    "        valid_targets = torch.max(reshaped_targets, reshaped_targets.new_zeros(B*T))\n",
    "        reshaped_preds = pred.view([B * T, -1])\n",
    "        \n",
    "        # Calculate standard loss for every node\n",
    "        loss_mask = (targets != -1).float()\n",
    "        loss_terms = self.loss_func(reshaped_preds, valid_targets)\n",
    "        loss_terms = loss_terms.view(T, B)\n",
    "        \n",
    "        # Determine the mask/weights that should be applied on the loss\n",
    "        wrong_prediction_mask = loss_mask * self.determine_wrong_predictions(targets, pred, depth)\n",
    "        loss_weight_matrix = wrong_prediction_mask[:,None,:].float() * transition_matrices\n",
    "        parent_weight_matrix = wrong_prediction_mask[None,:,:].float()*(loss_weight_matrix.transpose(0,1) == 0).float()\n",
    "        valid_parent_weights = (parent_weight_matrix.sum(dim=1) == 0).float()\n",
    "        valid_parent_loss_matrix = loss_weight_matrix * valid_parent_weights[:,None,:]\n",
    "        final_loss_weights = ((valid_parent_loss_matrix != -1).float() * valid_parent_loss_matrix).sum(dim=0)\n",
    "        ignored_nodes = valid_parent_weights * (final_loss_weights == 0).float()\n",
    "        ignored_nodes_weights = ignored_nodes * torch.min(final_loss_weights.sum(dim=0) / (ignored_nodes.sum(dim=0) + 1e-5), ignored_nodes.new_zeros(B) + self.decay_factor**2)[None,:]\n",
    "        \n",
    "        # Some metrics to measure tree exploration ratio\n",
    "        min_depth = torch.min((valid_parent_weights == 1).float() * (wrong_prediction_mask == 1).float() * depth.float() + ((valid_parent_weights == 0).float() + (wrong_prediction_mask == 0).float())*100., dim=0)[0] # Shape: [B]  \n",
    "        max_punished_depth = torch.max((valid_parent_weights == 1).float() * (wrong_prediction_mask == 1).float() * depth.float(), dim=0)[0] # Shape: [B]  \n",
    "        max_depth = torch.max(depth.float(), dim=0)[0]\n",
    "        tree_exploration_ratio = 1 - max_punished_depth / (max_depth + 1e-8)\n",
    "        \n",
    "        # Debug printouts if necessary\n",
    "        \"\"\"\n",
    "        print(\"Predictions: \" + str(pred.size()))\n",
    "        print(\"Targets: \" + str(targets.size()))\n",
    "        print(\"Transition matrices: \" + str(transition_matrices.size()))\n",
    "        print(\"Targets: \" + str(targets[:,0]))\n",
    "        print(\"Predictions: \" + str(pred.argmax(dim=-1)[:,0]))\n",
    "        print(\"Wrong predictions: \" + str(wrong_prediction_mask[:,0]))\n",
    "        print(\"Wrong prediction mask: \" + str(wrong_prediction_mask.size()))\n",
    "        print(\"Loss weight matrix: \" + str(loss_weight_matrix[:,:,0]))\n",
    "        print(\"loss_weight_matrix: \" + str(loss_weight_matrix.size()))\n",
    "        print(\"Parent weight matrix: \" + str(parent_weight_matrix[:,:,0]))\n",
    "        print(\"parent_weight_matrix: \" + str(parent_weight_matrix.size()))\n",
    "        print(\"Valid parent weights: \" + str(valid_parent_weights[:,0]))\n",
    "        print(\"valid_parent_weights: \" + str(valid_parent_weights.size()))\n",
    "        print(\"Valid parent loss matrix: \" + str(valid_parent_loss_matrix[:,:,0]))\n",
    "        print(\"valid_parent_loss_matrix: \" + str(valid_parent_loss_matrix.size()))\n",
    "        print(\"final_loss_weights: \" + str(final_loss_weights.size()))\n",
    "        print(\"Final loss weights: \" + str(final_loss_weights[:,0]))\n",
    "        sys.exit(1)\n",
    "        \"\"\"\n",
    "        # Check whether every node was correctly classified => apply loss weights on highest node with decay factor\n",
    "        is_final_loss_zero = (torch.sum(final_loss_weights, dim=0)==0).float()\n",
    "        default_loss_weights = (torch.pow(targets.new_zeros(1).float()+self.decay_factor, depth.float()) * loss_mask).float()\n",
    "        final_loss_weights = final_loss_weights * (1 - is_final_loss_zero) + is_final_loss_zero * default_loss_weights\n",
    "        \n",
    "        # Combine losses\n",
    "        weighted_loss = loss_terms * final_loss_weights\n",
    "        loss_norm = torch.sum(weighted_loss, dim=0) / (torch.sum(final_loss_weights, dim=0)+1e-10)\n",
    "        mean_batch_loss = torch.mean(loss_norm) # Either mean over trees or mean over nodes: torch.sum(weighted_loss) / torch.sum(final_loss_weights). In our experiments, there was no significant difference\n",
    "        \n",
    "        return mean_batch_loss, loss_norm, final_loss_weights, tree_exploration_ratio\n",
    "    \n",
    "    def determine_wrong_predictions(self, targets, pred, depth):\n",
    "        if not self.stochastic_choice:\n",
    "            # n-distance heuristic\n",
    "            return (torch.abs(targets - pred.argmax(dim=-1)) > self.allowed_dist).float()\n",
    "        else:\n",
    "            pred_probs = torch.exp(pred)\n",
    "            if self.stochastic_approach == 0: # Probabilities based on distance\n",
    "                range_val = torch.arange(start=-2,end=3,step=1).float()\n",
    "                mu = torch.sum(pred_probs * range_val[None,None,:], dim=-1)\n",
    "                sigma = torch.sum(pred_probs * torch.pow(torch.abs(range_val[None,None,:] - mu[:,:,None]), pred_probs.new_zeros(pred_probs.size()) + 2), dim=-1)\n",
    "                stop_probabilities = 1 - torch.exp(-torch.abs(mu - (targets.float() - 2))**2) * torch.exp(-torch.max(sigma-0.5, sigma.new_zeros(sigma.size())))\n",
    "                \n",
    "                \n",
    "            elif self.stochastic_approach == 1: # Probabilities based on the difference of the approximated Gaussian of our predictions and an \"optimal\" distribution\n",
    "                alpha = 0.75\n",
    "                targets_one_hot = (torch.arange(0,5)[None,None,:] == targets[:,:,None]).float()\n",
    "                targets_dist = torch.abs(torch.arange(0,5)[None,None,:] - targets[:,:,None]).float()\n",
    "                correct_probs = torch.sum(pred_probs * targets_one_hot, dim=-1)\n",
    "                probs_without_correct = pred_probs * (1 - targets_one_hot)\n",
    "                second_highest_prob = torch.max(probs_without_correct, dim=-1)[0]\n",
    "                sum_other_probabilities = torch.sum(probs_without_correct * targets_dist, dim=-1) * 0.75\n",
    "                prob_ratio = correct_probs / (correct_probs + alpha * second_highest_prob + (1 - alpha) * sum_other_probabilities)\n",
    "                stop_probabilities = 1 - torch.sqrt(torch.min(torch.max(prob_ratio*2-0.25, prob_ratio.new_zeros(prob_ratio.size())), prob_ratio.new_zeros(prob_ratio.size()) + 1))\n",
    "                \n",
    "                \n",
    "            elif self.stochastic_approach == 2: # 2nd highest heuristic as proposed in the paper\n",
    "                targets_one_hot = (torch.arange(0,5)[None,None,:].to(device).float() == targets[:,:,None].float()).float()\n",
    "                correct_probs = torch.sum(pred_probs * targets_one_hot, dim=-1)\n",
    "                continue_probabilities = (torch.abs(targets.float() - pred.argmax(dim=-1).float()) == 0.).float() # Correct predictions continue\n",
    "                default_prob_one_dist = 1.0 - torch.pow(2, -depth.float())\n",
    "                continue_probabilities += (torch.abs(targets.float() - pred.argmax(dim=-1).float()) == 1.).float() * default_prob_one_dist * torch.abs(correct_probs / pred_probs.max(dim=-1)[0]) #  * (torch.abs(1 - (pred_probs.max(dim=-1)[0] - correct_probs)))\n",
    "                stop_probabilities = 1 - continue_probabilities\n",
    "            \n",
    "            wrong_predictions = torch.bernoulli(stop_probabilities.cpu()).to(device) # Sampling of stops\n",
    "            return wrong_predictions\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start creating transition matrices...\n",
      "Convert them into a dict...\n",
      "Finished\n",
      "(73, 73)\n"
     ]
    }
   ],
   "source": [
    "# Create transition matrices for loss weighting.\n",
    "DEBUG_TREE_PRINT = False\n",
    "TRANSITION_DECAY_FACTOR = 0.5 # parameter delta in the paper\n",
    "create_transition_matrices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_train(batch_size=64, dataset_train=None, num_iterations=50000, lr=2e-4, hidden_units=150, eval_every=200, seed=123, file_prefix=None, heuristic=None):\n",
    "    for _, depth_dict in all_subtrees.items():\n",
    "        for subtree in depth_dict:\n",
    "            subtree.loss.clear()\n",
    "    for data in dataset_train:\n",
    "        data.loss.clear()\n",
    "        \n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    tree_model = TreeLSTMClassifier(\n",
    "        len(v.w2i), 300, hidden_units, len(t2i), v)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
    "        tree_model.embed.weight.requires_grad = False\n",
    "        # tree_model.embed.weight.register_hook(lambda grad: grad * 0.01)\n",
    "    print(tree_model)\n",
    "    print_parameters(tree_model)\n",
    "\n",
    "    tree_model = tree_model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(tree_model.parameters(), lr=lr)\n",
    "\n",
    "    return train_tree_model(\n",
    "        tree_model, optimizer, num_iterations=num_iterations, \n",
    "        print_every=50, eval_every=eval_every,\n",
    "        prep_fn=prepare_treelstm_minibatch,\n",
    "        eval_fn=evaluate,\n",
    "        batch_fn=get_minibatch,\n",
    "        batch_size=batch_size, eval_batch_size=64,\n",
    "        dataset_train=dataset_train, file_prefix=file_prefix,\n",
    "        training_setting=heuristic)\n",
    "\n",
    "\n",
    "\n",
    "def split_dataset(dataset, division_factor):\n",
    "    data_by_label = {i:list() for i in range(5)}\n",
    "    for d_index, d in enumerate(dataset):\n",
    "        data_by_label[d.label].append(d)\n",
    "    new_dataset = list()\n",
    "    for i in range(5):\n",
    "        random.shuffle(data_by_label[i])\n",
    "        no_examples = int(len(data_by_label[i])/division_factor)\n",
    "        new_dataset += data_by_label[i][:no_examples]\n",
    "    random.shuffle(new_dataset) # Shuffle dataset just to be sure\n",
    "    return new_dataset\n",
    "\n",
    "def test_different_seeds(number_seeds, batch_size=64, dataset_train=None, num_iterations=50000, hidden_units=150, eval_every=200, lr=2e-4, file_prefix=None, dataset_division=None, heuristic=None):\n",
    "    test_accuracies = list()\n",
    "    val_accuracies = list()\n",
    "    if not dataset_train:\n",
    "        dataset_train = train_data\n",
    "    for i in range(number_seeds):\n",
    "        if file_prefix is not None:\n",
    "            check_path = file_prefix + \"_\" + str(i).zfill(2)\n",
    "        else:\n",
    "            check_path = file_prefix\n",
    "        if dataset_division is not None:\n",
    "            random.seed(i)\n",
    "            dataset_train = split_dataset(train_data, dataset_division)\n",
    "        losses, acc, test_acc = do_train(batch_size=batch_size, dataset_train=dataset_train, num_iterations=num_iterations, hidden_units=hidden_units, lr=lr, eval_every=eval_every, seed=(123 + i), file_prefix=check_path, heuristic=heuristic)\n",
    "        test_accuracies.append(test_acc)\n",
    "        val_accuracies.append(max(acc))\n",
    "    print((\"=\"*100+\"\\n\")*4)\n",
    "    print(\"Eval for batch size \" + str(batch_size)+\"; dataset len \" + str(len(dataset_train))+ \"; \"+str(num_iterations) + \" iterations for \" + str(number_seeds) + \" seeds\")\n",
    "    print(\"Validation accuracy: %.2f\" % (100.0*mean(val_accuracies))+\"%\")\n",
    "    print(\"Test accuracy: %.2f\" % (100.0*mean(test_accuracies))+\"%\")\n",
    "    return val_accuracies, test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TreeLSTMClassifier(\n",
      "  (embed): Embedding(20727, 300, padding_idx=1)\n",
      "  (treelstm): TreeLSTM(\n",
      "    (reduce): TreeLSTMCell(300, 150)\n",
      "    (pre_layer): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (dropout_layer): Dropout(p=0.25)\n",
      "    (relu_act): ReLU()\n",
      "    (proj_x): Linear(in_features=300, out_features=150, bias=True)\n",
      "    (proj_x_gate): Linear(in_features=300, out_features=150, bias=True)\n",
      "    (buffers_dropout): Dropout(p=0.1)\n",
      "  )\n",
      "  (embed_dropout): Dropout(p=0.25)\n",
      "  (output_layer): Sequential(\n",
      "    (0): Dropout(p=0.25)\n",
      "    (1): Linear(in_features=150, out_features=5, bias=True)\n",
      "    (2): LogSoftmax()\n",
      "  )\n",
      ")\n",
      "embed.weight             [20727, 300] requires_grad=False\n",
      "treelstm.reduce.attent_module.output_layer.fc2.weight [2, 300]     requires_grad=True\n",
      "treelstm.reduce.attent_module.output_layer.fc2.bias [2]          requires_grad=True\n",
      "treelstm.reduce.reduce_layer.weight [750, 300]   requires_grad=True\n",
      "treelstm.reduce.reduce_layer.bias [750]        requires_grad=True\n",
      "treelstm.pre_layer.weight [300, 300]   requires_grad=True\n",
      "treelstm.pre_layer.bias  [300]        requires_grad=True\n",
      "treelstm.proj_x.weight   [150, 300]   requires_grad=True\n",
      "treelstm.proj_x.bias     [150]        requires_grad=True\n",
      "treelstm.proj_x_gate.weight [150, 300]   requires_grad=True\n",
      "treelstm.proj_x_gate.bias [150]        requires_grad=True\n",
      "output_layer.1.weight    [5, 150]     requires_grad=True\n",
      "output_layer.1.bias      [5]          requires_grad=True\n",
      "\n",
      "Total parameters: 6625807\n",
      "\n",
      "Shuffling training data\n",
      "Iter 50: loss=75.5112, time=8.44s\n",
      "Iter 100: loss=66.2201, time=18.47s\n",
      "Iter 150: loss=58.4247, time=26.64s\n",
      "Iter 200: loss=54.4334, time=35.19s\n",
      "Evaluation on 1101 elements. Correct: 374\n",
      "iter 200: dev acc=0.3397\n",
      "Evaluation on 2210 elements. Correct: 697\n",
      "iter 200: test acc=0.3154\n",
      "new highscore\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1584x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-586f4001f81a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train all models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mval_accs_binary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accs_binary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_different_seeds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"OurModel_binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheuristic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHEURISTIC_BINARY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-98-39d5819378fd>\u001b[0m in \u001b[0;36mtest_different_seeds\u001b[0;34m(number_seeds, batch_size, dataset_train, num_iterations, hidden_units, eval_every, lr, file_prefix, dataset_division, heuristic)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mdataset_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_division\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m123\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheuristic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheuristic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mtest_accuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mval_accuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-98-39d5819378fd>\u001b[0m in \u001b[0;36mdo_train\u001b[0;34m(batch_size, dataset_train, num_iterations, lr, hidden_units, eval_every, seed, file_prefix, heuristic)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mdataset_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         training_setting=heuristic)\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-91-257feeb14978>\u001b[0m in \u001b[0;36mtrain_tree_model\u001b[0;34m(model, optimizer, num_iterations, print_every, eval_every, batch_fn, prep_fn, eval_fn, batch_size, eval_batch_size, dataset_train, dataset_eval, dataset_test, file_prefix, training_setting)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0;31m# update weights - take a small step in the opposite dir of the gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train all models\n",
    "val_accs_binary, test_accs_binary = test_different_seeds(3, batch_size=32, hidden_units=150, num_iterations=30000, file_prefix=\"OurModel_binary\", eval_every=200, heuristic=HEURISTIC_BINARY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train heuristic 1 distance\n",
    "val_accs_1_distance, test_accs_1_distance = test_different_seeds(3, batch_size=32, hidden_units=150, num_iterations=20000, file_prefix=\"OurModel_1_distance\", eval_every=200, heuristic=HEURISTIC_1_DISTANCE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 2nd highest\n",
    "val_accs_2nd_highest, test_accs_2nd_highest = test_different_seeds(3, batch_size=32, hidden_units=150, num_iterations=50000, file_prefix=\"OurModel_2nd_highest\", eval_every=200, heuristic=HEURISTIC_2ND_HIGHEST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all nodes equally\n",
    "val_accs_tai, test_accs_tai = test_different_seeds(3, batch_size=32, hidden_units=150, num_iterations=20000, file_prefix=\"Model_Tai_et_al\", eval_every=200, heuristic=HEURISTIC_TAI_ET_AL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_predictions(model, \n",
    "                batch_fn=get_minibatch, \n",
    "                prep_fn=prepare_treelstm_minibatch,\n",
    "                eval_fn=evaluate,\n",
    "                eval_batch_size=32,\n",
    "                file_prefix=None):\n",
    "    \"\"\"Evaluate a model.\"\"\"  \n",
    "    \n",
    "    if not file_prefix:\n",
    "        checkpoint_path = CHECKPOINT_DIR + \"{}.pt\".format(model.__class__.__name__)\n",
    "    else:\n",
    "        checkpoint_path = CHECKPOINT_DIR + file_prefix + \".pt\"\n",
    "\n",
    "    # evaluate on train, dev, and test with best model\n",
    "    print(\"Loading model\")    \n",
    "    ckpt = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(ckpt[\"state_dict\"])\n",
    "\n",
    "    _, _, _, preds = eval_fn(\n",
    "    model, test_data, batch_size=eval_batch_size, \n",
    "    batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_our_model():\n",
    "    tree_model = TreeLSTMClassifier(\n",
    "            len(v.w2i), 300, 150, len(t2i), v)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
    "        tree_model.embed.weight.requires_grad = False\n",
    "    tree_model.to(device)\n",
    "    return tree_model\n",
    "\n",
    "preds = list()\n",
    "for i in range(3):\n",
    "    tree_preds = get_test_predictions(get_our_model(), file_prefix=\"OurModel_1distance_\" + str(i).zfill(2))\n",
    "    preds.append([p[-1] for p in tree_preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(preds)\n",
    "# print([mean([d.label == p for p, d in zip(preds[i], test_data)]) for i in range(len(preds))])\n",
    "def list_to_str(l):\n",
    "    return \" \".join([str(x) for x in l])\n",
    "    \n",
    "s = \"#\".join([list_to_str(p) for p in preds])\n",
    "with open(\"preds_1distance.txt\", \"w\") as f:\n",
    "    f.write(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f7QZZH86eHqu"
   },
   "source": [
    "# Further experiments and report\n",
    "\n",
    "For your report, you are expected to answer research questions by doing further experiments.\n",
    "\n",
    "## Research Questions\n",
    "\n",
    "Make sure you cover at least the following:\n",
    "\n",
    "- How important is word order for this task?\n",
    "- Does the tree structure help to get a better accuracy?\n",
    "- How does performance depend on the sentence length? Compare the various models. Is there a model that does better on longer sentences? If so, why?\n",
    "- Do you get better performance if you supervise the sentiment **at each node in the tree**? You can extract more training examples by treating every node in each tree as a separate tree. You will need to write a function that extracts all subtrees given a treestring. \n",
    "    - Warning: NLTK's Tree function seems to result in invalid trees in some cases, so be careful if you want to parse the string to a tree structure before extraction the phrases.\n",
    "\n",
    "Optionally, you can also investigate the following:\n",
    "\n",
    "- When making a wrong prediction, can you figure out at what point in the tree (sentence) the model fails? You can make a prediction at each node to investigate.\n",
    "- How does N-ary Tree LSTM compare to the Child-Sum Tree LSTM? \n",
    "- How do the Tai et al. Tree LSTMs compare to Le & Zuidema's formulation?\n",
    "- Or.. your own research question!\n",
    "\n",
    "In general:\n",
    "\n",
    "- ***When you report numbers, please report the mean accuracy across 3 (or more) runs with different random seed, together with the standard deviation.*** This is because the final performance may vary per random seed.\n",
    "\n",
    "## Report instructions\n",
    "\n",
    "Your report needs to be written in LaTeX. You are required to use the ACL 2018 template [(zip)](https://acl2018.org/downloads/acl18-latex.zip) which you can edit directly on [Overleaf](https://www.overleaf.com/latex/templates/instructions-for-acl-2018-proceedings/xzmhqgnmkppc). Make sure your names and student numbers are visible at the top. (Tip: you need to uncomment `/aclfinalcopy`).\n",
    "You can find some general tips about writing a research paper [here](https://www.microsoft.com/en-us/research/academic-program/write-great-research-paper/), but note that you need to make your own judgment about what is appropriate for this project. \n",
    "\n",
    "We expect you to use the following structure:\n",
    "1. Introduction (~1 page) - describe the problem, your research questions and goals, a summary of your findings and contributions. Please cite related work (models, data set) as part of your introduction here, since this is a short paper.\n",
    "    - Introduce the task and the main goal\n",
    "    - Clear research questions\n",
    "    - Motivating the importance of the questions and explaining the expectations\n",
    "    - How are these addressed or not addressed in the literature\n",
    "    - What is your approach\n",
    "    - Short summary of your findings\n",
    "2. Background (~1/2-1 page) -\n",
    "cover the main techniques (\"building blocks\") used in your project (e.g. word embeddings, LSTM, Tree LSTM) and intuitions behind them. Be accurate and concise.\n",
    "    - How each technique that you use works (don't just copy the formulas)\n",
    "    - The relation between the techniques\n",
    "3. Models (~1/2 page) - Cover the models that you used.\n",
    "    - The architecture of the final models (how do you use LSTM or Tree LSTM for the sentiment classification task. what layers you have, how do you do classification? What is your loss function?)\n",
    "4. Experiments (~1/2 page) - Describe your experimental setup. The information here should allow someone else to re-create your experiments. Describe how you evaluate the models.\n",
    "    - Explain the task and the data\n",
    "    - Training the models (model, data, parameters and hyper parameters if the models, training algorithms, what supervision signals you use, etc.)\n",
    "    - Evaluation (e.g. metrics)\n",
    "5. Results and Analysis (~1 page). Go over the results and analyse your findings.\n",
    "    - Answer each of the research questions you raised in the introduction.\n",
    "    - Plots and figures highlighting interesting patterns\n",
    "    - What are the factors that makes model A  better than model B in task C? investigate to prove their effect!\n",
    "6. Conclusion (~1/4 page). The main conclusions of your experiments.\n",
    "    - What you learned from you experiments? how does it relate to what is already known in the literature?\n",
    "    - Where the results as expected ? any surprising results? why?\n",
    "    - Based on what you learned what would you suggest to do next?\n",
    "\n",
    "\n",
    "General Tips:\n",
    "\n",
    "- Math notation  define each variable (either in running text, or in a pseudo-legenda after or before the equation)\n",
    "- Define technical terminology you need\n",
    "- Avoid colloquial language  everything can be said in a scientific-sounding way\n",
    "- Avoid lengthy sentences, stay to the point!\n",
    "- Do not spend space on \"obvious\" things!\n",
    "\n",
    "\n",
    "An ideal report:\n",
    "- Precise, scientific-sounding, technical, to the point \n",
    "  - Little general waffle/chit-chat\n",
    "- Not boring  because you dont explain obvious things too much\n",
    "- Efficient delivery of (only) the facts that we need to know to understand/reimplement\n",
    "- Results visually well-presented and described with the correct priority of importance of sub-results\n",
    "- Insightful analysis  speculation should connect to something interesting and not be too much; the reader learns something new\n",
    "- No typos, no colloquialisms  well-considered language\n",
    "- This normally means several re-draftings (re-orderings of information)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLP1 Practical II (student version)",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
