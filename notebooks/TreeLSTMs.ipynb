{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jILqpPLlE9r0"
   },
   "source": [
    "# Practical 2: Encoding Sentences with Neural Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8JXOZ5uhQ8Qq"
   },
   "source": [
    "In this practical we will train neural models to encode sentences, after which we can use our sentence representation for a downstream task such as sentiment classification. Rather than simply answering questions, like in the previous lab, **this time you are expected to write a four-page scientific report with your findings**. You will be judged by the quality of your report (see below). In this notebook, we will help you to develop models for your experiments. You do not have to hand in your answers, but please do include a link to a private Github gist with your code in your report as a footnote. \n",
    "\n",
    "### Data set\n",
    "We will make use of the [Stanford Sentiment Treebank](https://nlp.stanford.edu/sentiment/) (SST), which provides sentences, their tree structure, and (fine-grained) sentiment scores.\n",
    "This dataset is different from the one we used in the first practical. \n",
    "Before, a review consisted of several sentences, and we would have one sentiment score for it. Now, a review is a single sentence, for which we get a sentiment score. The special thing about our new data set is that we get a binary parse tree for each sentence, and a sentiment score has been assigned to each node in the tree. (We will look at an example below.)\n",
    "\n",
    "For the first part of this practical we will only make use of the tokens, but in the end we will also exploit the tree-structure that is provided!\n",
    "\n",
    "We will cover the following approaches:\n",
    "\n",
    "- Bag-of-words (BOW)\n",
    "- Continuous bag-of-words (CBOW)\n",
    "- Deep continuous bag-of-words (Deep CBOW)\n",
    "- LSTM\n",
    "- TreeLSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bSmmXQoC8ebA"
   },
   "source": [
    "### Important: Report \n",
    "\n",
    "The main purpose of this lab is for you to learn how to answer research questions by experimenting and then writing a scientific report.\n",
    "Your grade will be based on the quality of your report, not on the exercises in this notebook.\n",
    "You can find the requirements for the report at the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YbNKef3lymaj"
   },
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9jxTkpg59FlU"
   },
   "source": [
    "Let's first download the data set and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WZp53HmMP3F2"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TovFkDTgE_d6"
   },
   "outputs": [],
   "source": [
    "# !wget http://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip\n",
    "# !unzip trainDevTestTrees_PTB.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0IpAphkBO5eW"
   },
   "outputs": [],
   "source": [
    "# this function reads in a textfile and fixes an issue with \\\\\n",
    "def filereader(path): \n",
    "  with open(path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "      yield line.strip().replace(\"\\\\\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ylkIopm0QJML"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3 (2 It) (4 (4 (2 's) (4 (3 (2 a) (4 (3 lovely) (2 film))) (3 (2 with) (4 (3 (3 lovely) (2 performances)) (2 (2 by) (2 (2 (2 Buy) (2 and)) (2 Accorsi))))))) (2 .)))\n"
     ]
    }
   ],
   "source": [
    "# Let's look at a data point.\n",
    "# If you look closely you will find that this is a flattened tree, \n",
    "# with sentiment scores at every node, and words as the leaves.\n",
    "s = next(filereader(\"trees/dev.txt\"))\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7_U7HTFwdrWt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              3                                                                     \n",
      "  ____________|____________________                                                  \n",
      " |                                 4                                                \n",
      " |        _________________________|______________________________________________   \n",
      " |       4                                                                        | \n",
      " |    ___|______________                                                          |  \n",
      " |   |                  4                                                         | \n",
      " |   |         _________|__________                                               |  \n",
      " |   |        |                    3                                              | \n",
      " |   |        |               _____|______________________                        |  \n",
      " |   |        |              |                            4                       | \n",
      " |   |        |              |            ________________|_______                |  \n",
      " |   |        |              |           |                        2               | \n",
      " |   |        |              |           |                 _______|___            |  \n",
      " |   |        3              |           |                |           2           | \n",
      " |   |    ____|_____         |           |                |        ___|_____      |  \n",
      " |   |   |          4        |           3                |       2         |     | \n",
      " |   |   |     _____|___     |      _____|_______         |    ___|___      |     |  \n",
      " 2   2   2    3         2    2     3             2        2   2       2     2     2 \n",
      " |   |   |    |         |    |     |             |        |   |       |     |     |  \n",
      " It  's  a  lovely     film with lovely     performances  by Buy     and Accorsi  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can use NLTK to print the tree structure more nicely\n",
    "from nltk import Tree\n",
    "from nltk.treeprettyprinter import TreePrettyPrinter\n",
    "tree = Tree.fromstring(s)\n",
    "print(TreePrettyPrinter(tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ekAWKsji9t93"
   },
   "source": [
    "You can see the sentiment annotations here **at every node**! 0 is very negative, 4 is very positive. For now, we will only use the sentiment score at the **root node**; this is the score for the complete sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DKynLm0xPKr2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', \"'s\", 'a', 'lovely', 'film', 'with', 'lovely', 'performances', 'by', 'Buy', 'and', 'Accorsi', '.']\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "# Let's first make a function that extracts the tokens (the leaves).\n",
    "\n",
    "def tokens_from_treestring(s):\n",
    "  \"\"\"extract the tokens from a sentiment tree\"\"\"\n",
    "  return re.sub(r\"\\([0-9] |\\)\", \"\", s).split()\n",
    " \n",
    "# let's try it on our example tree\n",
    "tokens = tokens_from_treestring(s)\n",
    "print(tokens)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B8vFkeqN-NLP"
   },
   "source": [
    "> *Warning: you could also parse a treestring using NLTK and ask it to return the leaves, but there seems to be an issue with NLTK not always correctly parsing the input, so do not rely on it.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Akr9K_Mv4dym"
   },
   "outputs": [],
   "source": [
    "# We will also need the following function, but you can ignore this for now.\n",
    "# It is explained later on.\n",
    "\n",
    "SHIFT = 0\n",
    "REDUCE = 1\n",
    "\n",
    "\n",
    "def transitions_from_treestring(s):\n",
    "  s = re.sub(\"\\([0-5] ([^)]+)\\)\", \"0\", s)\n",
    "  s = re.sub(\"\\)\", \" )\", s)\n",
    "  s = re.sub(\"\\([0-4] \", \"\", s)\n",
    "  s = re.sub(\"\\([0-4] \", \"\", s)\n",
    "  s = re.sub(\"\\)\", \"1\", s)\n",
    "  return list(map(int, s.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mNtPdlwPgRat"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trees/train.txt  8544\n",
      "trees/dev.txt    1101\n",
      "trees/test.txt   2210\n"
     ]
    }
   ],
   "source": [
    "# Now let's first see how large our data sets are.\n",
    "for path in (\"trees/train.txt\", \"trees/dev.txt\", \"trees/test.txt\"):\n",
    "  print(\"{:16s} {:4d}\".format(path, sum(1 for _ in filereader(path))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HexlSqTR_UrY"
   },
   "source": [
    "You can see that this data set is not very large! That's probably because it required so much manual annotation. However, it's large enough to train a neural network on.\n",
    "\n",
    "It will be useful to store each data example in an `Example` object,\n",
    "containing everything that we may need for each data point.\n",
    "It will contain the tokens, the tree, the top-level sentiment label, and \n",
    "the transitions (explained later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4I07Hb_-q8wg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 8544\n",
      "dev 1101\n",
      "test 2210\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "from nltk import Tree\n",
    "\n",
    "# A simple way to define a class is using namedtuple.\n",
    "Example = namedtuple(\"Example\", [\"tokens\", \"tree\", \"label\", \"transitions\", \"index\", \"loss\", \"transition_matrix\"])\n",
    "\n",
    "   \n",
    "def examplereader(path, lower=False):\n",
    "    \"\"\"Returns all examples in a file one by one.\"\"\"\n",
    "    index=0\n",
    "    for line in filereader(path):\n",
    "        line = line.lower() if lower else line\n",
    "        tokens = tokens_from_treestring(line)\n",
    "        tree = Tree.fromstring(line)  # use NLTK's Tree\n",
    "        label = int(line[1])\n",
    "        trans = transitions_from_treestring(line)\n",
    "        index += 1\n",
    "        yield Example(tokens=tokens, tree=tree, label=label, transitions=trans, index=index, loss=list(), transition_matrix=None)\n",
    "  \n",
    "\n",
    "# Let's load the data into memory.\n",
    "LOWER = False  # we will keep the original casing\n",
    "train_data = list(examplereader(\"trees/train.txt\", lower=LOWER))\n",
    "dev_data = list(examplereader(\"trees/dev.txt\", lower=LOWER))\n",
    "test_data = list(examplereader(\"trees/test.txt\", lower=LOWER))\n",
    "\n",
    "print(\"train\", len(train_data))\n",
    "print(\"dev\", len(dev_data))\n",
    "print(\"test\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6KM0bDyeVZtP"
   },
   "source": [
    "Let's check out an example object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J8mwcaZwxP1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First train example: Example(tokens=['It', \"'s\", 'a', 'lovely', 'film', 'with', 'lovely', 'performances', 'by', 'Buy', 'and', 'Accorsi', '.'], tree=Tree('3', [Tree('2', ['It']), Tree('4', [Tree('4', [Tree('2', [\"'s\"]), Tree('4', [Tree('3', [Tree('2', ['a']), Tree('4', [Tree('3', ['lovely']), Tree('2', ['film'])])]), Tree('3', [Tree('2', ['with']), Tree('4', [Tree('3', [Tree('3', ['lovely']), Tree('2', ['performances'])]), Tree('2', [Tree('2', ['by']), Tree('2', [Tree('2', [Tree('2', ['Buy']), Tree('2', ['and'])]), Tree('2', ['Accorsi'])])])])])])]), Tree('2', ['.'])])]), label=3, transitions=[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1], index=1, loss=[], transition_matrix=None)\n",
      "First train example tokens: ['It', \"'s\", 'a', 'lovely', 'film', 'with', 'lovely', 'performances', 'by', 'Buy', 'and', 'Accorsi', '.']\n",
      "First train example label: 3\n"
     ]
    }
   ],
   "source": [
    "example = dev_data[0]\n",
    "print(\"First train example:\", example)\n",
    "print(\"First train example tokens:\", example.tokens)\n",
    "print(\"First train example label:\",  example.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-WDSprDBVcr-"
   },
   "source": [
    "#### Vocabulary \n",
    "To work with this data we will need a vocabulary.\n",
    "The job of the vocabulary is to map each word to a unique ID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VvNgKx7usRSt"
   },
   "outputs": [],
   "source": [
    "# Here we first define a class that can map a word to an ID (w2i)\n",
    "# and back (i2w).\n",
    "\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "\n",
    "\n",
    "class OrderedCounter(Counter, OrderedDict):\n",
    "  \"\"\"Counter that remembers the order elements are first seen\"\"\"\n",
    "  def __repr__(self):\n",
    "    return '%s(%r)' % (self.__class__.__name__,\n",
    "                      OrderedDict(self))\n",
    "  def __reduce__(self):\n",
    "    return self.__class__, (OrderedDict(self),)\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "  \"\"\"A vocabulary, assigns IDs to tokens\"\"\"\n",
    "  \n",
    "  def __init__(self):\n",
    "    self.freqs = OrderedCounter()\n",
    "    self.w2i = {}\n",
    "    self.i2w = []\n",
    "\n",
    "  def count_token(self, t):\n",
    "    self.freqs[t] += 1\n",
    "    \n",
    "  def add_token(self, t):\n",
    "    self.w2i[t] = len(self.w2i)\n",
    "    self.i2w.append(t)    \n",
    "    \n",
    "  def build(self, min_freq=0):\n",
    "    self.add_token(\"<unk>\")  # reserve 0 for <unk> (unknown words)\n",
    "    self.add_token(\"<pad>\")  # reserve 1 for <pad> (discussed later)   \n",
    "    \n",
    "    tok_freq = list(self.freqs.items())\n",
    "    tok_freq.sort(key=lambda x: x[1], reverse=True)\n",
    "    for tok, freq in tok_freq:\n",
    "      if freq >= min_freq:\n",
    "        self.add_token(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kOvkH_llVsoW"
   },
   "source": [
    "Let's build the token vocabulary!\n",
    "\n",
    "When randomly initializing word vectors, we take the words in our training\n",
    "set and assign them unique IDs, and assign all other words to <unk>, \n",
    "because those we cannot learn a vector for based on our training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GwGQgQQBNUSq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 18280\n"
     ]
    }
   ],
   "source": [
    "# This process should be deterministic and should have the same result \n",
    "# if run multiple times on the same data set.\n",
    "\n",
    "v = Vocabulary()\n",
    "for data_set in (train_data,):\n",
    "  for ex in data_set:\n",
    "    for token in ex.tokens:\n",
    "      v.count_token(token)\n",
    "\n",
    "v.build()\n",
    "print(\"Vocabulary size:\", len(v.w2i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-UNIedPrPdCw"
   },
   "source": [
    "Let's have a closer look at the properties of our vocabulary. Having a good idea of what it is like can facilitate data analysis and debugging later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oJyuogmh0CA7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1973\n"
     ]
    }
   ],
   "source": [
    "# What is the ID for \"century?\"\n",
    "print(v.w2i[\"century\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O8OkPQ8Zv-rI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 0: <unk>\n",
      "Word 1: <pad>\n",
      "Word 2: .\n",
      "Word 3: ,\n",
      "Word 4: the\n",
      "Word 5: and\n",
      "Word 6: a\n",
      "Word 7: of\n",
      "Word 8: to\n",
      "Word 9: 's\n"
     ]
    }
   ],
   "source": [
    "# What are the first 10 words in the vocabulary?\n",
    "for i in range(10):\n",
    "    print(\"Word \" + str(i) + \": \" + v.i2w[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kmXwu02lOLWI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency top  1 (8024): .\n",
      "Frequency top  2 (7131): ,\n",
      "Frequency top  3 (6037): the\n",
      "Frequency top  4 (4431): and\n",
      "Frequency top  5 (4403): a\n",
      "Frequency top  6 (4386): of\n",
      "Frequency top  7 (2995): to\n",
      "Frequency top  8 (2544): 's\n",
      "Frequency top  9 (2536): is\n",
      "Frequency top 10 (1915): that\n"
     ]
    }
   ],
   "source": [
    "# What are the 10 most common words?\n",
    "common_indices_list = sorted([(i, v.freqs[v.i2w[i]]) for i in range(len(v.freqs))], key=lambda x: -x[1])\n",
    "for i in range(10):\n",
    "    print(\"Frequency top \" + str(i+1).rjust(2) + \" (\" + str(common_indices_list[i][1]) + \"): \" + v.i2w[common_indices_list[i][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "__NDPaCeOT_m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words with frequency 1: 9541\n"
     ]
    }
   ],
   "source": [
    "# And how many words are there with frequency 1?\n",
    "no_freq_1 = sum([v.freqs[v.i2w[i]]==1 for i in range(len(v.freqs))])\n",
    "print(\"Words with frequency 1: \" + str(no_freq_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xKHocugctZGM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jarecki\n",
      "uplifter\n",
      "Queen\n",
      "varies\n",
      "ease\n",
      "Philippe\n",
      "whiff\n",
      "Important\n",
      "element\n",
      "Morton\n",
      "cars\n",
      "honoring\n",
      "having\n",
      "pot\n",
      "Ribisi\n",
      "independent-community\n",
      "self-assured\n",
      "ill-conceived\n",
      "blended\n",
      "juice\n"
     ]
    }
   ],
   "source": [
    "# Finally 20 random words from the vocabulary.\n",
    "# This is a simple way to get a feeling for the data.\n",
    "for i in range(20):\n",
    "    word_index = random.randint(0,len(v.freqs)-1)\n",
    "    print(v.i2w[word_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nGWaZahKV_dH"
   },
   "source": [
    "#### Sentiment label vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AmTC-rvQelpl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['very negative', 'negative', 'neutral', 'positive', 'very positive']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's map the sentiment labels 0-4 to a more readable form\n",
    "i2t = [\"very negative\", \"negative\", \"neutral\", \"positive\", \"very positive\"]\n",
    "i2t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D7UI26DP2dr2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('very negative', 0),\n",
       "             ('negative', 1),\n",
       "             ('neutral', 2),\n",
       "             ('positive', 3),\n",
       "             ('very positive', 4)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And let's also create the opposite mapping.\n",
    "# We won't use a Vocabulary for this (although we could), since the labels\n",
    "# are already numeric.\n",
    "t2i = OrderedDict({p : i for p, i in zip(i2t, range(len(i2t)))})\n",
    "t2i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y0067ax54-rd"
   },
   "source": [
    "## Installing PyTorch\n",
    "\n",
    "We are going to need PyTorch and Google Colab does not have it installed by default. Run the cell below to install it.\n",
    "\n",
    "*For installing PyTorch in your own computer, follow the instructions on [pytorch.org](pytorch.org) instead. This is for Google Colab only.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qKQMGtkR5KWr"
   },
   "outputs": [],
   "source": [
    "# http://pytorch.org/\n",
    "from os.path import exists\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BhiRqhTM5V4c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.1.post2'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this should result in the PyTorch version\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BYt8uTyGCKc7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyTorch can run on CPU or on Nvidia GPU (video card) using CUDA\n",
    "# This cell selects the GPU if one is available.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2d1VMOOYx1Bw"
   },
   "outputs": [],
   "source": [
    "# Seed manually to make runs reproducible\n",
    "# You need to set this again if you do multiple runs of the same model\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# When running on the CuDNN backend two further options must be set for reproducibility\n",
    "if torch.cuda.is_available():\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uWBTzkuE3CtZ"
   },
   "source": [
    "# BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TBAjYYySOA5W"
   },
   "source": [
    "Our first model is a super simple neural **bag-of-words (BOW) model**.\n",
    "Unlike the bag-of-words model that you used in the previous lab, here we associate each word with a vector of size 5, exactly our number of sentiment classes. \n",
    "\n",
    "To make a classification, we simply **sum** the vectors of the words in our sentence and a bias vector. Because we sum the vectors, we lose word order: that's why we call this a neural bag-of-words model.\n",
    "\n",
    "```\n",
    "this   [0.0, 0.1, 0.1, 0.1, 0.0]\n",
    "movie  [0.1, 0.1, 0.1, 0.2, 0.1]\n",
    "is     [0.0, 0.1, 0.0, 0.0, 0.0]\n",
    "stupid [0.9, 0.5, 0.1, 0.0, 0.0]\n",
    "\n",
    "bias   [0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "--------------------------------\n",
    "sum    [0.9, 0.8, 0.3, 0.3, 0.1]\n",
    "\n",
    "argmax: 0 (very negative)\n",
    "```\n",
    "\n",
    "Now, the **argmax** of this sum is our predicted label.\n",
    "\n",
    "We initialize all vectors *randomly* and train them using cross-entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rLtBAIQGynkB"
   },
   "source": [
    "#### Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QZfNklWf3tvs"
   },
   "outputs": [],
   "source": [
    "class BOW(nn.Module):\n",
    "  \"\"\"A simple bag-of-words model\"\"\"\n",
    "\n",
    "  def __init__(self, vocab_size, embedding_dim, vocab):\n",
    "    super(BOW, self).__init__()\n",
    "    self.vocab = vocab\n",
    "    \n",
    "    # this is a trained look-up table with word embeddings\n",
    "    self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "    \n",
    "    # this is a trained bias term\n",
    "    self.bias = nn.Parameter(torch.zeros(embedding_dim), requires_grad=True)        \n",
    "\n",
    "  def forward(self, inputs):\n",
    "    # this is the forward pass of the neural network\n",
    "    # given inputs, it computes the output\n",
    "\n",
    "    # this looks up the embeddings for each word ID in inputs\n",
    "    # the result is a sequence of word embeddings\n",
    "    embeds = self.embed(inputs)\n",
    "    \n",
    "    # the output is the sum across the time dimension (1)\n",
    "    # with the bias term added\n",
    "    logits = embeds.sum(1) + self.bias\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eKHvBnoBAr6z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW(\n",
      "  (embed): Embedding(18280, 5)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Let's create a model.\n",
    "vocab_size = len(v.w2i)\n",
    "n_classes = len(t2i)\n",
    "bow_model = BOW(vocab_size, n_classes, v)\n",
    "print(bow_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vfCx-HvMH1qQ"
   },
   "source": [
    "> **Hey, wait, where is the bias vector?**\n",
    "> PyTorch does not print Parameters, only Modules!\n",
    "\n",
    "> We can print it ourselves though, to check that it is there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fhvk5HenAroT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias                     [5]          requires_grad=True\n",
      "embed.weight             [18280, 5]   requires_grad=True\n",
      "\n",
      "Total parameters: 91405\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Here we print each parameter name, shape, and if it is trainable.\n",
    "def print_parameters(model):\n",
    "  total = 0\n",
    "  for name, p in model.named_parameters():\n",
    "    total += np.prod(p.shape)\n",
    "    print(\"{:24s} {:12s} requires_grad={}\".format(name, str(list(p.shape)), p.requires_grad))\n",
    "  print(\"\\nTotal parameters: {}\\n\".format(total))\n",
    "    \n",
    "\n",
    "print_parameters(bow_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WSAw292WxuP4"
   },
   "source": [
    "#### Preparing an example for input\n",
    "\n",
    "To feed sentences to our PyTorch model, we need to convert a sequence of tokens to a sequence of IDs. The `prepare_example` function below takes care of this for us. With these IDs we index the word embedding table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YWeGTC_OGReV"
   },
   "outputs": [],
   "source": [
    "def prepare_example(example, vocab):\n",
    "  \"\"\"\n",
    "  Map tokens to their IDs for 1 example\n",
    "  \"\"\"\n",
    "  \n",
    "  # vocab returns 0 if the word is not there\n",
    "  x = [vocab.w2i.get(t, 0) for t in example.tokens]\n",
    "  \n",
    "  x = torch.LongTensor([x])\n",
    "  x = x.to(device)\n",
    "  \n",
    "  y = torch.LongTensor([example.label])\n",
    "  y = y.to(device)\n",
    "  \n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oKNQjEc0yXnJ"
   },
   "source": [
    "#### Evaluation\n",
    "We will need one more thing: an evaluation metric.\n",
    "How many predictions do we get right? The accuracy will tell us.\n",
    "Make sure that you understand this code block.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yGmQLcVYKZsh"
   },
   "outputs": [],
   "source": [
    "\n",
    "def simple_evaluate(model, data, prep_fn=prepare_example, **kwargs):\n",
    "  \"\"\"Accuracy of a model on given data set.\"\"\"\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  model.eval()  # disable dropout (explained later)\n",
    "\n",
    "  for example in data:\n",
    "    \n",
    "    # convert the example input and label to PyTorch tensors\n",
    "    x, target = prep_fn(example, model.vocab)\n",
    "\n",
    "    # forward pass\n",
    "    # get the output from the neural network for input x\n",
    "    with torch.no_grad():\n",
    "      logits = model(x)\n",
    "    \n",
    "    # get out the prediction\n",
    "    prediction = logits.argmax(dim=-1)\n",
    "    \n",
    "    # add the number of correct predictions to the total correct\n",
    "    correct += (prediction == target).sum().item()\n",
    "    total += 1\n",
    "\n",
    "  return correct, total, correct / float(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dIk6OtSdzGRP"
   },
   "source": [
    "#### Example feed\n",
    "For stochastic gradient descent (SGD) we will need a random training example for every update.\n",
    "We implement this by shuffling the training data and returning examples one by one using `yield`.\n",
    "\n",
    "Shuffling is optional so that we get to use this to get validation and test examples, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dxDFOZLfCXvJ"
   },
   "outputs": [],
   "source": [
    "def get_examples(data, shuffle=True, **kwargs):\n",
    "  \"\"\"Shuffle data set and return 1 example at a time (until nothing left)\"\"\"\n",
    "  if shuffle:\n",
    "    print(\"Shuffling training data\")\n",
    "    random.shuffle(data)  # shuffle training data each epoch\n",
    "  for example in data:\n",
    "    yield example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g09SM8yb2cjx"
   },
   "source": [
    "#### Exercise: Training function\n",
    "\n",
    "Your task is now to complete the training loop below.\n",
    "Before you do so, please read the section about optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TVfUukVdM_1c"
   },
   "source": [
    "**Optimization**\n",
    "\n",
    "As mentioned in the \"Introduction to PyTorch\" notebook, one of the perks of using PyTorch are its automatic differentiation abilities. We will use these to train our BOW model. \n",
    "\n",
    "We train our model by feeding it an input and performing **forward** pass, which results in an output for which we obtain a **loss** with our loss function.\n",
    "After the gradients are calculated in the **backward** pass, we can take a step on the loss surface towards more optimal parameter settings (e.g. gradient descent). \n",
    "\n",
    "The package we will use to do this optimization is [torch.optim](https://pytorch.org/docs/stable/optim.html). Besides implementations of stochastic gradient descent (SGD), this package also implements the optimization algorithm Adam, which we'll be using in this practical. \n",
    "For the purposes of this assignment you do not need to know what Adam does besides that it uses gradient information to update our model parameters by calling: \n",
    "\n",
    "```\n",
    "optimizer.step()\n",
    "```\n",
    "Remember when we updated our parameters in the PyTorch tutorial in a loop?\n",
    "\n",
    "\n",
    "```python\n",
    "# update weights\n",
    "learning_rate = 0.5\n",
    "for f in net.parameters():\n",
    "    # for each parameter, take a small step in the opposite dir of the gradient\n",
    "    p.data = p.data - p.grad.data * learning_rate\n",
    "\n",
    "```\n",
    "The function call optimizer.step() does effectively the same thing.\n",
    "\n",
    "*(If you want to know more about optimization algorithms using gradient information [this blog](http://ruder.io/optimizing-gradient-descent/.) gives a nice intuitive overview.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ktFnKBux25lD"
   },
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, num_iterations=10000, \n",
    "                print_every=1000, eval_every=1000,\n",
    "                batch_fn=get_examples, \n",
    "                prep_fn=prepare_example,\n",
    "                eval_fn=simple_evaluate,\n",
    "                batch_size=1, eval_batch_size=None):\n",
    "    \"\"\"Train a model.\"\"\"  \n",
    "    iter_i = 0\n",
    "    train_loss = 0.\n",
    "    print_num = 0\n",
    "    start = time.time()\n",
    "    criterion = nn.CrossEntropyLoss() # loss function\n",
    "    best_eval = 0.\n",
    "    best_iter = 0\n",
    "\n",
    "    # store train loss and validation accuracy during training\n",
    "    # so we can plot them afterwards\n",
    "    losses = []\n",
    "    accuracies = []  \n",
    "\n",
    "    if eval_batch_size is None:\n",
    "        eval_batch_size = batch_size\n",
    "\n",
    "    while True:  # when we run out of examples, shuffle and continue\n",
    "        for batch in batch_fn(train_data, batch_size=batch_size):\n",
    "\n",
    "            # forward pass\n",
    "            model.train()\n",
    "            x, targets = prep_fn(batch, model.vocab)\n",
    "            logits = model(x)\n",
    "\n",
    "            B = targets.size(0)  # later we will use B examples per update\n",
    "\n",
    "            # compute cross-entropy loss (our criterion)\n",
    "            # note that the cross entropy loss function computes the softmax for us\n",
    "            loss = criterion(logits.view([B, -1]), targets.view(-1))\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # backward pass\n",
    "            # Tip: check the Introduction to PyTorch notebook.\n",
    "\n",
    "            # erase previous gradients\n",
    "            model.zero_grad()\n",
    "            \n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            # update weights - take a small step in the opposite dir of the gradient\n",
    "            optimizer.step()\n",
    "\n",
    "            print_num += 1\n",
    "            iter_i += 1\n",
    "\n",
    "            # print info\n",
    "            if iter_i % print_every == 0:\n",
    "                print(\"Iter %r: loss=%.4f, time=%.2fs\" % \n",
    "                (iter_i, train_loss, time.time()-start))\n",
    "                losses.append(train_loss)\n",
    "                print_num = 0        \n",
    "                train_loss = 0.\n",
    "\n",
    "            # evaluate\n",
    "            if iter_i % eval_every == 0:\n",
    "                _, _, accuracy = eval_fn(model, dev_data, batch_size=eval_batch_size,\n",
    "                         batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "                accuracies.append(accuracy)\n",
    "                print(\"iter %r: dev acc=%.4f\" % (iter_i, accuracy))       \n",
    "\n",
    "                # save best model parameters\n",
    "                if accuracy > best_eval:\n",
    "                    print(\"new highscore\")\n",
    "                    best_eval = accuracy\n",
    "                    best_iter = iter_i\n",
    "                    path = \"{}.pt\".format(model.__class__.__name__)\n",
    "                    ckpt = {\n",
    "                    \"state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"best_eval\": best_eval,\n",
    "                    \"best_iter\": best_iter\n",
    "                    }\n",
    "                    torch.save(ckpt, path)\n",
    "\n",
    "            # done training\n",
    "            if iter_i == num_iterations:\n",
    "                print(\"Done training\")\n",
    "\n",
    "                # evaluate on train, dev, and test with best model\n",
    "                print(\"Loading best model\")\n",
    "                path = \"{}.pt\".format(model.__class__.__name__)        \n",
    "                ckpt = torch.load(path)\n",
    "                model.load_state_dict(ckpt[\"state_dict\"])\n",
    "\n",
    "                _, _, train_acc = eval_fn(\n",
    "                model, train_data, batch_size=eval_batch_size, \n",
    "                batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "                _, _, dev_acc = eval_fn(\n",
    "                model, dev_data, batch_size=eval_batch_size,\n",
    "                batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "                _, _, test_acc = eval_fn(\n",
    "                model, test_data, batch_size=eval_batch_size, \n",
    "                batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "\n",
    "                print(\"best model iter {:d}: \"\n",
    "                \"train acc={:.4f}, dev acc={:.4f}, test acc={:.4f}\".format(\n",
    "                best_iter, train_acc, dev_acc, test_acc))\n",
    "\n",
    "                return losses, accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XEPsLvI-3D5b"
   },
   "source": [
    "### Training the BOW model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E9mB1_XhMPNN"
   },
   "source": [
    "# CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pWk78FvNMw4o"
   },
   "source": [
    "We now continue with a **continuous bag-of-words (CBOW)** model.\n",
    "It is similar to the BOW model, but now embeddings can have a dimension of *arbitrary size*. \n",
    "This means that we can make them bigger and learn more aspects of each word. We will still sum the vectors to get a sentence representation, but now the result is no longer conveniently the number of sentiment classes. \n",
    "\n",
    "*Note that this is not the same as word2vec CBOW.* Both models take a few words as input, sum and make a prediction. However, in word2vec the prediction is the word in the middle of the input words. Here, the prediction is a sentiment class (1 out of 5 possible numbers 0-4).\n",
    "\n",
    "So what can we do? We can *learn* a parameter matrix $W$ that turns our summed vector into the number of output classes, by matrix-multiplying it: $$Wx$$\n",
    "If the size of $W$ is `5 x d`, and $x$ is `d x 1` and , then the result will be `5x1`, and then we can again find our prediction using an argmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gIjrCPfCwsXI"
   },
   "source": [
    "## Exercise: write CBOW class & train it\n",
    "\n",
    "Write a class `CBOW` that:\n",
    "\n",
    "- has word embeddings with size 300\n",
    "- sums the word vectors for the input words (just like in `BOW`)\n",
    "- projects the resulting vector down to 5 units using a linear layer (including a bias term)\n",
    "\n",
    "Train your CBOW model and plot the validation accuracy and training loss over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PEV22aR2MP0Q"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "class CBOW(nn.Module):\n",
    "    \"\"\"A continuos bag-of-words model\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim, vocab):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.vocab = vocab\n",
    "\n",
    "        # this is a trained look-up table with word embeddings\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.applyW = nn.Linear(embedding_dim, output_dim)\n",
    "\n",
    "        # this is a trained bias term\n",
    "        self.bias = nn.Parameter(torch.zeros(output_dim), requires_grad=True)        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # this is the forward pass of the neural network\n",
    "        # given inputs, it computes the output\n",
    "\n",
    "        # this looks up the embeddings for each word ID in inputs\n",
    "        # the result is a sequence of word embeddings\n",
    "        embeds = self.embed(inputs)\n",
    "        trans_embeds = self.applyW(embeds)\n",
    "\n",
    "        # the output is the sum across the time dimension (1)\n",
    "        # with the bias term added\n",
    "        logits = trans_embeds.sum(1) + self.bias\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cbow_model = CBOW(len(v.w2i), 300, len(t2i), vocab=v)\n",
    "# print(cbow_model)\n",
    "\n",
    "# cbow_model = cbow_model.to(device)\n",
    "\n",
    "# optimizer = optim.Adam(cbow_model.parameters(), lr=0.0005)\n",
    "# cbow_losses, cbow_accuracies = train_model(\n",
    "#     cbow_model, optimizer, num_iterations=300000, \n",
    "#     print_every=10000, eval_every=10000, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(cbow_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(cbow_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zpFt_Fo2TdN0"
   },
   "source": [
    "# Deep CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iZanOMesTfEZ"
   },
   "source": [
    "To see if we can squeeze some more performance out of the CBOW model, we can make it deeper and non-linear: add more layers and tanh-activations.\n",
    "By using more parameters we can learn more aspects of the data, and by using more layers and non-linearities, we can try to learn a more complex function. \n",
    "This is not something that always works. If the input-output mapping of your data is simple, then a complicated function could easily overfit on your training set, which will lead to poor generalization. \n",
    "\n",
    "We will use E for the size of the word embeddings (use: 300) and D for the size of a hidden layer (use: 100).\n",
    "\n",
    "#### Exercise: write Deep CBOW class and train it\n",
    "\n",
    "Write a class `DeepCBOW`.\n",
    "\n",
    "In your code, make sure that your `output_layer` consists of the following:\n",
    "- A linear transformation from E units to D units.\n",
    "- A Tanh activation\n",
    "- A linear transformation from D units to D units\n",
    "- A Tanh activation\n",
    "- A linear transformation from D units to 5 units (our output classes).\n",
    "\n",
    "We recommend using [nn.Sequential](https://pytorch.org/docs/stable/nn.html?highlight=sequential#torch.nn.Sequential) to implement this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l8Z1igvpTrZq"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "class DeepCBOW(nn.Module):\n",
    "    \"\"\"A continuos bag-of-words model\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, E, D, output_dim, vocab):\n",
    "        super(DeepCBOW, self).__init__()\n",
    "        self.vocab = vocab\n",
    "\n",
    "        # this is a trained look-up table with word embeddings\n",
    "        self.embed = nn.Embedding(vocab_size, E)\n",
    "        self.output_layer = nn.Sequential(OrderedDict([\n",
    "          ('linear1', nn.Linear(E, D)),\n",
    "          ('tanh1', nn.Tanh()),\n",
    "          ('dropout1', nn.Dropout(0.1)),\n",
    "          ('linear2', nn.Linear(D,D)),\n",
    "          ('relu2', nn.Tanh()),\n",
    "          ('dropout2', nn.Dropout(0.1)),\n",
    "          ('linear3', nn.Linear(D,output_dim))\n",
    "        ]))\n",
    "\n",
    "        # this is a trained bias term\n",
    "        self.bias = nn.Parameter(torch.zeros(output_dim), requires_grad=True)        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # this is the forward pass of the neural network\n",
    "        # given inputs, it computes the output\n",
    "\n",
    "        # this looks up the embeddings for each word ID in inputs\n",
    "        # the result is a sequence of word embeddings\n",
    "        embeds = self.embed(inputs)\n",
    "        for layer in self.output_layer:\n",
    "            embeds = layer(embeds)\n",
    "\n",
    "        # the output is the sum across the time dimension (1)\n",
    "        # with the bias term added\n",
    "        logits = embeds.sum(1) + self.bias\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep_cbow_model = DeepCBOW(len(v.w2i), 300, 100, len(t2i), vocab=v)\n",
    "# print(deep_cbow_model)\n",
    "\n",
    "# deep_cbow_model = deep_cbow_model.to(device)\n",
    "\n",
    "# optimizer = optim.Adam(deep_cbow_model.parameters(), lr=0.002)\n",
    "# deep_cbow_losses, deep_cbow_accuracies = train_model(\n",
    "#     deep_cbow_model, optimizer, num_iterations=300000, \n",
    "#     print_every=1000, eval_every=1000, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(deep_cbow_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(deep_cbow_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MQZ5flHwiiHY"
   },
   "source": [
    "# Pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9NX35vecmHy6"
   },
   "source": [
    "The Stanford sentiment treebank is a very small data set, since it was manually annotated. This makes it difficult for the model to learn good word embeddings, to learn the \"meaning\" of the words in our vocabulary.\n",
    "\n",
    "Think for a moment about the fact that the only error signal that the network receives is from predicting the sentiment!\n",
    "\n",
    "To mitigate our data sparsity, we can download **pre-trained word embeddings**. \n",
    "You can choose which pre-trained word embeddings to use:\n",
    "\n",
    "- **Glove**. The \"original\" Stanford Sentiment classification [paper](http://aclweb.org/anthology/P/P15/P15-1150.pdf) used Glove embeddings, which are just another method (like *word2vec*) to get word embeddings from unannotated text. Glove is described in the following paper which you should cite if you use them:\n",
    "> Jeffrey Pennington, Richard Socher, and Christopher Manning. [\"Glove: Global vectors for word representation.\"](https://nlp.stanford.edu/pubs/glove.pdf) EMNLP 2014. \n",
    "\n",
    "- **word2vec**. This is the method that you learned about in class, described in:\n",
    "> Mikolov, Tomas, et al. [\"Distributed representations of words and phrases and their compositionality.\"](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) Advances in neural information processing systems. 2013.\n",
    "\n",
    "With these pre-trained word embeddings, we can initialize our word embedding lookup table and start form a point where similar words are already close to one another. \n",
    "\n",
    "You can choose to keep the word embeddings **fixed** or to train them further.\n",
    "We will keep them fixed for now.\n",
    "\n",
    "For the purposes of this lab, it is enough if you understand how word2vec works (whichever vectors you use). If you are interested you may also check out the Glove paper.\n",
    "\n",
    "You can either download the word2vec vectors, or the Glove vectors.\n",
    "If you want to compare your results to the Stanford paper later on, then you should use Glove. \n",
    "**At the end of this lab you have the option to compare which vectors give you the best performance. For now, simply choose one of them and continue with that.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lGYr02WWO993"
   },
   "outputs": [],
   "source": [
    "# This downloads the Glove 840B 300d embeddings.\n",
    "# The original file is at http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "# Since that file is 2GB, we provide you with a *filtered version*\n",
    "# which contains all the words you need for this data set.\n",
    "\n",
    "# You only need to do this once.\n",
    "# Please comment this cell out after downloading.\n",
    "\n",
    "# !wget https://gist.githubusercontent.com/bastings/b094de2813da58056a05e8e7950d4ad1/raw/3fbd3976199c2b88de2ae62afc0ecc6f15e6f7ce/glove.840B.300d.sst.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6NLsgFGiTjmI"
   },
   "outputs": [],
   "source": [
    "# This downloads the word2vec 300D Google News vectors \n",
    "# The file has been truncated to only contain words that appear in our data set.\n",
    "# You can find the original file here: https://code.google.com/archive/p/word2vec/\n",
    "\n",
    "# You only need to do this once.\n",
    "# Please comment this out after downloading.\n",
    "# !wget https://gist.githubusercontent.com/bastings/4d1c346c68969b95f2c34cfbc00ba0a0/raw/76b4fefc9ef635a79d0d8002522543bc53ca2683/googlenews.word2vec.300d.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GXBITzPRQUQb"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive (to save the downloaded files)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uFvzPuiKSCbl"
   },
   "outputs": [],
   "source": [
    "# Copy word vectors *to* Google Drive\n",
    "\n",
    "# You only need to do this once.\n",
    "# Please comment this out after running it. \n",
    "# !cp \"glove.840B.300d.sst.txt\" \"/gdrive/My Drive/\"\n",
    "# !cp \"googlenews.word2vec.300d.txt\" \"/gdrive/My Drive/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kUMH0bM6BuY9"
   },
   "outputs": [],
   "source": [
    "# If you copied the word vectors to your Drive before,\n",
    "# here is where you copy them back to the Colab notebook.\n",
    "\n",
    "# Copy Glove vectors *from* Google Drive\n",
    "# !cp \"/gdrive/My Drive/glove.840B.300d.sst.txt\" .\n",
    "# !cp \"/gdrive/My Drive/googlenews.word2vec.300d.txt\" ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MX2GJVHILM8n"
   },
   "source": [
    "At this point you have the pre-trained word embedding files, but what do they look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ChsChH14Ruxn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n"
     ]
    }
   ],
   "source": [
    "# Exercise: Print the first 4 lines of the files that you downloaded.\n",
    "# What do you see?\n",
    "with open(\"glove.840B.300d.sst.txt\", \"r\") as f:\n",
    "    glove_init_lines = f.readlines()\n",
    "    print(glove_init_lines[2].split(\" \")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WIVCkUkE_IjR"
   },
   "source": [
    "#### Exercise: New Vocabulary\n",
    "\n",
    "Since we now use pre-trained word embeddings, we need to create a new vocabulary. \n",
    "This is because of two reasons:\n",
    "\n",
    "1. Not all words in our training set are in the set of pre-trained word embeddings. We do not want words in our vocabulary that are not in that set.\n",
    "2. We get to look up the pre-trained word embedding for words in the validation and test set, even if we don't know them from training. \n",
    "\n",
    "Now, create a new vocabulary object `v` and load the pre-trained vectors into a `vectors`.\n",
    "\n",
    "The vocabulary `v` should consist of:\n",
    " - a  `<unk>` token at position 0,\n",
    " - a  `<pad>` token at position 1, \n",
    " - and then all words in the pre-trained vector set.\n",
    " \n",
    "\n",
    "After storing each vector in a list `vectors`, turn in into a numpy matrix like this:\n",
    "```python\n",
    " vectors = np.stack(vectors, axis=0)\n",
    "```\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ITyyCvDnCL4U"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "v = Vocabulary()\n",
    "vectors = list()\n",
    "vectors.append(np.zeros(300, dtype=np.float32)) # <unk>\n",
    "vectors.append(np.zeros(300, dtype=np.float32)) # <pad>\n",
    "for line in glove_init_lines:\n",
    "    v.count_token(line.split(\" \")[0])\n",
    "    vectors.append(np.array([float(x) for x in line.split(\" \")[1:]], dtype=np.float32))\n",
    "vectors = np.stack(vectors, axis=0)\n",
    "v.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xC-7mRyYNG9b"
   },
   "source": [
    "#### Exercise: words not in our pre-trained set\n",
    "\n",
    "How many words in the training, dev, and test set are also in your vector set?\n",
    "How many words are not there?\n",
    "\n",
    "Store the words that are not in the word vector set in the set below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K6MA3-wF_X5M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'church-wary', 'Equlibrium', 'step-printing', 'so-five-minutes-ago', 'super-dooper-adorability', 'hotsies', 'oh-those-wacky-Brits', 'Achero', 'Cineasts', 'long-on-the-shelf', 'Insufferably', 'flck', 'warm-milk', 'Director-chef', 'ego-destroying', 'lovable-loser', 'power-lunchers', 'semi-throwback', 'yawn-provoking', 'teen-gang', 'Oscar-sweeping', 'cold-fish', 'often-hilarious', 'fizzability', 'Janklowicz-Mann', 'not-very-funny', 'non-firsthand', 'deadeningly', 'sub-music', 'East-vs', 'Unambitious', 'Spy-vs', 'two-hour-and-fifteen-minute', 'bare-midriff', 'nonethnic', 'Talancn', 'dark-as-pitch', 'auto-critique', 'too-frosty', 'dog-paddle', 'un-bear-able', 'singer-turned', 'Priggish', 'handbag-clutching', 'pro-Serb', 'jazz-playing', 'community-therapy', 'bore-athon', 'rape-payback', 'not-quite-dead', 'Nietzsche-referencing', 'image-mongering', 'voices-from-the-other-side', 'bang-the-drum', 'razor-sided', 'Efteriades', 'surfacey', 'Dognini', 'estrogen-free', 'lip-non-synching', 'ennui-hobbled', 'Birot', 'already-shallow', 'out-depress', 'Romething', 'Hitler-study', 'unemotive', 'Phoned-in', 'Talkiness', 'dungpile', 'Kubrick-meets-Spielberg', 'stuporously', 'Zishe', 'gender-war', 'grade-grubbers', 'McBeal-style', 'sense-spinning', 'show-stoppingly', 'splendid-looking', 'slasher-movie', 'gender-provoking', 'Komediant', 'Formuliac', 'cat-and-mouser', 'Heremakono', 'Japanimator', 'Steinis', 'daytime-drama', 'seventy-minute', 'Je-Gyu', 'Hollywood-predictable', 'Tiresomely', 'wide-smiling', 'pathos-filled', 'still-inestimable', 'Punch-and-Judy', 'MIBII', 'out-outrage', 'valley-girl', 'qatsi', 'Cosby-Seinfeld', 'Phonce', 'Driver-esque', 'soon-to-be-forgettable', 'give-me-an-Oscar', 'moldy-oldie', 'kids-and-family-oriented', 'Kahlories', 'oh-so-Hollywood', 'Poke-mania', 'under-7', 'white-empowered', 'a-bornin', 'unhibited', 'sports-movie', 'clich-riddled', 'teen-exploitation', 'skyscraper-trapeze', 'video-shot', 'neo-Hitchcockianism', 'hippie-turned-yuppie', 'guilt-suffused', 'prefeminist', 'big-budget/all-star', 'movie-specific', 'under-inspired', 'still-raw', 'audience-pleaser', 'groan-to-guffaw', 'artsploitation', 'Glizty', 'Nickelodeon-esque', 'direct-to-video/DVD', 'huge-screen', 'Banderas-Lucy', 'indieflick', 'Wewannour', 'message-movie', 'actorliness', 'tryingly', 'shlockmeister', 'talking-animal', 'Marcken', 'identity-seeking', 'redneck-versus-blueblood', 'Puportedly', 'every-joke-has', 'teen-speak', 'quick-cuts', 'overcoming-obstacles', 'superficiale', 'flakeball', 'ultra-cheesy', 'thinks-it-is', 'bump-in', '24-and-unders', 'sub-formulaic', 'snazziness', 'often-cute', 'anti-adult', '102-minute', 'yahoo-ing', 'stiletto-stomps', 'drama/action', 'Vulakoro', 'Frankenstein-monster', 'brain-deadening', 'Wollter', 'ballistic-pyrotechnic', 'kids-cute', 'near-hypnotic', 'psychodramatics', 'actress-producer', 'been-told-a', 'hotter-two-years-ago', 'life-embracing', 'phoney-feeling', 'Kosashvili', 'gut-clutching', 'monster/science', 'clone-gag', 'old-fashioned-movie', 'Lynch-like', 'not-nearly', 'young-guns', 'Silbersteins', 'marveilleux', 'Butterfingered', 'toilet-humor', 'from-television', 'jump-in-your-seat', 'bio-doc', 'college-friends', 'unembarrassing', 'big-bug', 'Chabrolian', 'revigorates', 'too-hot-for-TV', 'self-glorified', 'Stortelling', 'kiddie-oriented', 'outgag', 'docu-Dogma', 'adventues', 'Watstein', 'Juliet/West', 'tries-so-hard-to-be-cool', 'road-and-buddy', 'modern-office', 'water-camera', 'smarter-than-thou', 'triple-crosses', 'stunt-hungry', 'soul-stripping', 'farewell-to-innocence', 'Holofcenter', 'Bond-inspired', 'gangster/crime', 'war-movie', 'disease-of', 'hayseeds-vs', 'sillified', 'trance-noir', 'Schneidermeister', 'non-mystery', 'Journalistically', 'kid-empowerment', 'pseudo-witty', 'wise-cracker', 'movie-biz', 'Potty-mouthed', 'wannabe-hip', 'Brother-Man', 'unfakable', 'II-Birkenau', 'trouble-in-the-ghetto', 'Unspools', 'Margolo', 'even-flowing', 'ultra-loud', 'cellophane-pop', 'eardrum-dicing', 'Co-writer/director', 'hit-hungry', 'achronological', 'Clarke-Williams', 'Debrauwer', 'raunch-fests', 'not-so-Divine', 'unfussily', 'remain-nameless', 'hammily', 'nerve-raked', 'Laissez-passer', 'genial-rogue', 'girl-meets-girl', 'video-game-based', 'spaniel-eyed', 'social/economic/urban', 'Oscar-size', 'boom-bam', '103-minute', 'lack-of-attention', 'media-soaked', 'Copmovieland', 'ultra-provincial', 'lascivious-minded', 'bottom-of-the-bill', 'spook-a-rama', 'actorish', '129-minute', 'truncheoning', 'zinger-filled', 'as-it', 'overmanipulative', 'gabbiest', 'unsalvageability', 'Globetrotters-Generals', 'out-to-change-the-world', 'Devolves', 'kinetically-charged', 'fustily', 'Ki-Deok', 'brain-slappingly', 'dust-caked', 'ever-ruminating', 'like-themed', 'groupie/scholar', 'girl-buddy', 'Self-congratulatory', 'wonder-what', 'late-twenty-somethings', 'renegade-cop', 'prechewed', 'self-amused', 'non-Britney', 'RunTelDat', 'derisions', 'Witch-style', 'headline-fresh', 'Reeboir', 'movie-movie', 'Bazadona', 'I-2-spoofing', 'quasi-Shakespearean', 'materalism', 'anti-Kieslowski', 'song-and-dance-man', 'reality-snubbing', 'flatula', 'Brothers-style', 'tub-thumpingly', 'Seldahl', 'musclefest', 'Hellstenius', 'big-fisted', 'anti-erotic', 'Pie-like', 'spy-action', 'raw-nerved', 'feardotcom', '112-minute', 'gore-free', 'gay-niche', 'sub-Tarantino', 'female-bonding', 'the-cash', 'drama/character', 'Choquart', 'ricture', 'TV-insider', 'Rashomon-for-dipsticks', 'indie-heads', 'girls-behaving-badly', 'reel/real', 'inside-show-biz', 'truck-loving', 'I-heard-a-joke', 'oft-brilliant', '-RRB-', 'something-borrowed', 'Felinni', 'Whiffle-Ball', 'Bottom-rung', 'sequel-for-the-sake', 'rubber-face', 'stomach-knotting', 'wind-in-the-hair', 'better-focused', 'autocritique', 'coming-of-age/coming-out', 'therapy-dependent', 'hyper-artificiality', 'kids-in-peril', 'and-miss', 'Jaglomized', 'overstylized', 'of-a-sequel', 'guy-in-a-dress', 'slash-fest', 'fun-for-fun', 'pop-cyber', 'screeching-metal', 'FearDotCom', 'Venice/Venice', 'Denlopp', 'semi-amusing', 'style-free', 'overly-familiar', 'none-too-funny', 'pyro-correctly', 'Brothers/Abrahams', 'snow-and-stuntwork', 'industrial-model', 'male-ridden', 'direct-to-void', 'LePlouff', 'cameo-packed', 'S1M0NE', 'wild-and-woolly', 'Sandlerian', 'well-contructed', 'fang-baring', 'digital-effects-heavy', 'By-the-numbers', 'who-wrote-Shakespeare', 'independent-community', 'queasy-stomached', 'cheatfully', 'democracie', 'college-spawned', 'Star/producer', 'German-Expressionist', 'demented-funny', 'Rintar', 'goth-vampire', 'Piercingly', 'thinly-conceived', 'action-and-popcorn', 'retro-refitting', 'unslick', '168-minute', 'boundary-hopping', 'bottomlessly', 'as-nasty', 'best-foreign-film', 'hidden-agenda', 'badly-rendered', 'kid-movie', 'dolphin-gasm', 'surface-effect', 'Singer/composer', 'Ill-considered', 'the-loose', 'run-of-the-filth', 'anti-Harry', 'buzz-obsessed', 'Mushes', 'Cliff-Notes', 'Eckstraordinarily', 'Age-inspired', 'half-dimensional', 'over-amorous', 'ill-wrought', 'now-cliched', 'barn-burningly', 'ANTWONE', 'out-bad-act', 'hour-and-a-half-long', 'Univac-like', 'beloved-major', 'revenge-of-the-nerds', 'post-Tarantino', 'high-buffed', 'special-effects-laden', 'super-stupid', 'Marine/legal', 'repellantly', 'shoe-loving', 'Nebrida', 'out-shock', 'skit-com', 'disappearing/reappearing', 'Minac', 'Bjorkness', 'two-drink-minimum', 'surehanded', 'pseudo-serious', 'underdramatized', 'Idemoto', 'happily-ever', 'celeb-strewn', 'Hollywood-itis', 'made-for-home-video', 'Re-Fried', 'sub-sophomoric', 'pro-Serbian', 'pantomimesque', 'she-cute', 'preachy-keen', 'junk-calorie', 'uncinematic', 'double-pistoled', 'achival', 'Verete', 'cipherlike', 'Stevenon', 'Director-writer', 'sex-in-the-city', 'Audacious-impossible', 'often-funny', 'pee-related', 'splatterfests', 'Oprahfication', 'barn-side', 'smashups', 'Sychowski', 'clung-to', 'sitcom-worthy', 'Kafka-inspired', 'talk-heavy', 'tuba-playing', 'foul-natured', 'Norrington-directed', 'Ear-splitting', 'Pasach', 'thrill-kill', 'self-defeatingly', 'cable-sports', 'Jean-Claud', 'crummy-looking', 'shock-you-into-laughter', 'gone-to-seed', 'Live-style', 'doofus-on', 'Auteil', 'Ourside', 'spy-savvy', 'the-week', 'Less-than-compelling', 'humor-seeking', 'time-it-is', 'Truckzilla', 'stand-up-comedy', 'police-oriented', 'makeup-deep', 'landbound', 'pokepie', 'not-at-all-good', 'propriety-obsessed', 'natural-seeming', 'sitcomishly', 'diciness', 'gunfest', 'TV-cops', 'crash-and-bash', 'Mordantly', 'Bruckheimeresque', 'so-bad-they', 'too-conscientious', 'rough-trade', 'techno-tripe', 'based-on-truth', 'Asiaphiles', 'matinee-style', 'monster-in-the', 'family-film', 'uncharismatically', 'pro-wildlife', 'stadium-seat', 'dirgelike', 'well-lensed', '91-minute', 'kid-vid', 'underrehearsed', 'faux-urban', 'message-mongering', 'teen-driven', 'egocentricities', 'character-who-shall', 'sense-of-humour', 'Volletta', 'flex-a-thon', 'waydowntown', 'title-bout', 'none-too-original', 'two-wrongs-make-a-right', 'nonchallenging', 'quasi-improvised', 'Bettany/McDowell', 'disease-of-the-week', 'bibbidy-bobbidi-bland', 'video-cam', 'surface-obsession', 'lower-wit', 'bad-movie', 'Gator-bashing', 'O2-tank', 'Jeong-Hyang', 'underventilated', 'best-sustained', 'Imaxy', 'travel-agency', 'acting-workshop', 'scary-funny', 'pseudo-educational', 'banter-filled', 'dudsville', 'romantic/comedy', 'dullingly', 'Damon/Bourne', 'cinemantic', 'art-conscious', 'Hjelje', 'feardotcom.com', 'French-produced', 'media-constructed', 'less-compelling', 'cheese-laced', 'pasta-fagioli', 'hell-jaunt', 'damaged-goods', 'pop-induced', 'Short-story', 'semimusical', 'pseudo-rock-video', 'kibbitzes', 'Brit-com', 'pseudo-philosophic', 'life-at-arm', 'unconned', 'food-spittingly', 'creepy-scary', 'imponderably', 'Hollywood-action', 'blood-splattering', 'non-Bondish', 'pooper-scoopers', 'affirmational', 'slummer', 'cor-blimey-luv-a-duck', 'goose-pimple', 'just-above-average', 'sleep-inducingly', 'telanovela', 'crime-land', 'techno-sex', 'tech-geeks', 'Georgian-Israeli', 'zombie-land', 'Qutting', 'disaffected-indie-film', 'Hopkins/Rock', 'Well-meant', 'stagecrafts', 'smile-button', 'Underachieves', 'yarn-spinner', 'unencouraging', 'dictator-madman', 'Rocky-like', 'pulpiness', 'Feardotcom', 'genre-curling', 'tardier', 'murder-on-campus', 'Warmed-over', 'pseudo-bio', 'barking-mad', 'non-exploitive', 'Otto-Sallies', 'Annie-Mary', 'Djeinaba', 'show-don', 'Brazil-like', 'Altman-esque', 'action-thriller/dark', 'follow-your-dream', 'pre-fils', 'Waters-like', 'unlaughable', 'film-culture', 'tolerable-to-adults', 'Fulford-Wierzbicki', 'merchandised-to-the-max', 'flick-knife', 'in-jokey', 'cop-flick', 'trash-cinema', 'eroti-comedy', 'at-a-frat-party', 'stable-full', 'than-likely', 'dead-undead', 'affectation-free', 'animated-movie', 'thousand-times', 'dirty-joke', 'e-graveyard', '-LRB-', 'artnering', 'Seldhal', 'Cool-J', 'mush-hearted', 'Good-naturedly', 'heart-rate-raising', 'Interminably', 'rock-n-rolling', 'doing-it-for', 'clich-laden', 'Recoing', 'Punitively', 'videologue', 'shuck-and-jive', 'Veret', 'Wisegirls', 'made-for-movie', 'teen-sleaze', 'moral-condundrum', 'Ga', 'grunge-pirate', 'beast-within', 'Jae-eun', 'bio-drama', 'neo-Augustinian', 'time-switching', 'Channel-style', 'close-to-solid', 'too-extreme-for-TV', 'no-surprise', 'fast-edit', 'tough-man', 'semi-surrealist', 'Koshashvili', 'whoopee-cushion', 'tear-drenched', 'beyond-lame', 'too-spectacular', 'meets-John'}\n",
      "699\n"
     ]
    }
   ],
   "source": [
    "words_not_found = set()\n",
    "# YOUR CODE HERE\n",
    "for data_set in (train_data,):\n",
    "    for ex in data_set:\n",
    "        for token in ex.tokens:\n",
    "            if v.w2i.get(token, 0) == 0:\n",
    "                words_not_found.update([token])\n",
    "print(words_not_found)\n",
    "print(len(words_not_found))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BfEd38W0NnAI"
   },
   "source": [
    "#### Exercise: train Deep CBOW with (fixed) pre-trained embeddings\n",
    "\n",
    "Now train Deep CBOW again using the pre-trained word vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z_6ooqgEsB20"
   },
   "outputs": [],
   "source": [
    "# We define a dummy class so that we save the model to a different file.\n",
    "class PTDeepCBOW(DeepCBOW):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, vocab):\n",
    "        super(PTDeepCBOW, self).__init__(\n",
    "            vocab_size, embedding_dim, hidden_dim, output_dim, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JfIh4Ni6yuAh"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# pt_deep_cbow_model = PTDeepCBOW(len(v.w2i), 300, 100, 5, v)\n",
    "\n",
    "# copy pre-trained word vectors into embeddings table\n",
    "# pt_deep_cbow_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
    "\n",
    "# disable training the pre-trained embeddings\n",
    "# pt_deep_cbow_model.embed.weight.requires_grad = False\n",
    "\n",
    "# move model to specified device\n",
    "# pt_deep_cbow_model = pt_deep_cbow_model.to(device)\n",
    "\n",
    "# train the model\n",
    "# pt_deep_cbow_model = pt_deep_cbow_model.to(device)\n",
    "\n",
    "# optimizer = optim.Adam(pt_deep_cbow_model.parameters(), lr=0.0001)\n",
    "# pt_deep_cbow_losses, pt_deep_cbow_accuracies = train_model(\n",
    "#     pt_deep_cbow_model, optimizer, num_iterations=30000, \n",
    "#    print_every=1000, eval_every=1000, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ufujv3x31ufD"
   },
   "outputs": [],
   "source": [
    "# plot dev accuracies\n",
    "# plt.plot(pt_deep_cbow_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YTJtKBzd7Qjr"
   },
   "outputs": [],
   "source": [
    "# plot train loss\n",
    "# plt.plot(pt_deep_cbow_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yFu8xzCy9XDW"
   },
   "source": [
    "**It looks like we've hit what is possible with just using words.**\n",
    "Let's move on by incorporating word order!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g41yW4PL9jG0"
   },
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ODzXEH0MaGpa"
   },
   "source": [
    "It is time to get more serious. Even with pre-trained word embeddings and multiple layers, we seem to do pretty badly at sentiment classification here. \n",
    "The next step we can take is to introduce word order again, and to get a representation of the sentence as a whole, without independence assumptions.\n",
    "\n",
    "We will get this representation using an **Long Short-Term Memory** (LSTM). As an exercise, we will code our own LSTM cell, so that we get comfortable with its inner workings.\n",
    "Once we have an LSTM cell, we can call it repeatedly, updating its hidden state one word at a time:\n",
    "\n",
    "```python\n",
    "rnn = MyLSTMCell(input_size, hidden_size)\n",
    "\n",
    "hx = torch.zeros(1, hidden_size)  # initial state\n",
    "cx = torch.zeros(1, hidden_size)  # initial memory cell\n",
    "output = []                       # to save intermediate LSTM states\n",
    "\n",
    "# feed one word at a time\n",
    "for i in range(n_timesteps):\n",
    "  hx, cx = rnn(input[i], (hx, cx))\n",
    "  output.append(hx)\n",
    "```\n",
    "\n",
    "If you need some more help understanding LSTMs, then check out these resources:\n",
    "- Blog post (highly recommended): http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- Paper covering LSTM formulas in detail: https://arxiv.org/abs/1503.04069 \n",
    "\n",
    "#### Exercise: Finish the LSTM cell below. \n",
    "You will need to implement the LSTM formulas:\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "        i = \\sigma(W_{ii} x + b_{ii} + W_{hi} h + b_{hi}) \\\\\n",
    "        f = \\sigma(W_{if} x + b_{if} + W_{hf} h + b_{hf}) \\\\\n",
    "        g = \\tanh(W_{ig} x + b_{ig} + W_{hg} h + b_{hg}) \\\\\n",
    "        o = \\sigma(W_{io} x + b_{io} + W_{ho} h + b_{ho}) \\\\\n",
    "        c' = f * c + i * g \\\\\n",
    "        h' = o \\tanh(c') \\\\\n",
    "\\end{array}\n",
    " $$\n",
    "\n",
    "where $\\sigma$ is the sigmoid function.\n",
    "\n",
    "*Note that the LSTM formulas can differ slightly between different papers. We use the PyTorch LSTM formulation here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zJ9m5kLMd7-v"
   },
   "outputs": [],
   "source": [
    "class MyLSTMCell(nn.Module):\n",
    "    \"\"\"Our own LSTM cell\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        \"\"\"Creates the weights for this LSTM\"\"\"\n",
    "        super(MyLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "\n",
    "        self.tanh_act = nn.Tanh()\n",
    "        self.sigmoid_act = nn.Sigmoid()\n",
    "        # self.input_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        # self.forget_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        # self.candidate_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        # self.output_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.combined_gate = nn.Linear(input_size + hidden_size, 4 * hidden_size)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"This is PyTorch's default initialization method\"\"\"\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)  \n",
    "\n",
    "    def forward(self, input_, hx, mask=None):\n",
    "        \"\"\"\n",
    "        input is (batch, input_size)\n",
    "        hx is ((batch, hidden_size), (batch, hidden_size))\n",
    "        \"\"\"\n",
    "        prev_h, prev_c = hx\n",
    "\n",
    "        # project input and prev state\n",
    "        cat_input = torch.cat([input_,prev_h], dim=1)\n",
    "\n",
    "        # main LSTM computation    \n",
    "\n",
    "        # i = self.sigmoid_act(self.input_gate(cat_input))\n",
    "        # f = self.sigmoid_act(self.forget_gate(cat_input))\n",
    "        # g = self.tanh_act(self.candidate_gate(cat_input))\n",
    "        # o = self.sigmoid_act(self.output_gate(cat_input))\n",
    "        combined_output = self.combined_gate(cat_input)\n",
    "        i = self.sigmoid_act(combined_output[:,0 * self.hidden_size:1 * self.hidden_size])\n",
    "        f = self.sigmoid_act(combined_output[:,1 * self.hidden_size:2 * self.hidden_size])\n",
    "        g = self.tanh_act(combined_output[:,2 * self.hidden_size:3 * self.hidden_size])\n",
    "        o = self.sigmoid_act(combined_output[:,3 * self.hidden_size:4 * self.hidden_size])\n",
    "\n",
    "        c = f * prev_c + i * g\n",
    "        h = o * self.tanh_act(c)\n",
    "\n",
    "        return h, c\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"{}({:d}, {:d})\".format(\n",
    "            self.__class__.__name__, self.input_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4JM7xPhkQeE5"
   },
   "source": [
    "#### Optional: Efficient Matrix Multiplication\n",
    "\n",
    "It is more efficient to do a few big matrix multiplications than to do many smaller ones.\n",
    "\n",
    "It is possible to implement the above cell using just **two** linear layers.\n",
    "\n",
    "This is because the eight linear transformations from one forward pass through an LSTM cell can be done in just two:\n",
    "$$W_h h + b_h$$\n",
    "$$W_i x + b_i $$ \n",
    "\n",
    "with $h = $ `prev_h` and $x = $ `input_`.\n",
    "\n",
    "and where: \n",
    "\n",
    "$W_h =  \\begin{pmatrix}\n",
    "W_{hi}\\\\ \n",
    "W_{hf}\\\\ \n",
    "W_{hg}\\\\ \n",
    "W_{ho}\n",
    "\\end{pmatrix}$, $b_h = \\begin{pmatrix}\n",
    "b_{hi}\\\\ \n",
    "b_{hf}\\\\ \n",
    "b_{hg}\\\\ \n",
    "b_{ho}\n",
    "\\end{pmatrix}$,  $W_i = \\begin{pmatrix}\n",
    "W_{ii}\\\\ \n",
    "W_{if}\\\\ \n",
    "W_{ig}\\\\ \n",
    "W_{io}\n",
    "\\end{pmatrix}$ and $b_i = \\begin{pmatrix}\n",
    "b_{ii}\\\\ \n",
    "b_{if}\\\\ \n",
    "b_{ig}\\\\ \n",
    "b_{io}\n",
    "\\end{pmatrix}$.\n",
    "\n",
    "Convince yourself that, after chunking with [torch.chunk](https://pytorch.org/docs/stable/torch.html?highlight=chunk#torch.chunk), the output of those two linear transformations is equivalent to the output of the eight linear transformations in the LSTM cell calculations above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X9gA-UcqSBe0"
   },
   "source": [
    "#### LSTM Classifier\n",
    "\n",
    "Having an LSTM cell is not enough: we still need some code that calls it repeatedly, and then makes a prediction from the final hidden state. \n",
    "You will find that code below. Make sure that you understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3iuYZm5poEn5"
   },
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"Encodes sentence with an LSTM and projects final hidden state\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, vocab):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim, padding_idx=1)\n",
    "        self.rnn = MyLSTMCell(embedding_dim, hidden_dim)\n",
    "\n",
    "        self.output_layer = nn.Sequential(     \n",
    "            nn.Dropout(p=0.1),  # explained later\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        B = x.size(0)  # batch size (this is 1 for now, i.e. 1 single example)\n",
    "        T = x.size(1)  # time (the number of words in the sentence)\n",
    "        \n",
    "        input_ = self.embed(x)\n",
    "        \n",
    "\n",
    "        # here we create initial hidden states containing zeros\n",
    "        # we use a trick here so that, if input is on the GPU, then so are hx and cx\n",
    "        hx = input_.new_zeros(B, self.rnn.hidden_size)\n",
    "        cx = input_.new_zeros(B, self.rnn.hidden_size)\n",
    "\n",
    "        # process input sentences one word/timestep at a time\n",
    "        # input is batch-major, so the first word(s) is/are input_[:, 0]\n",
    "        outputs = []   \n",
    "        for i in range(T):\n",
    "            hx, cx = self.rnn(input_[:, i], (hx, cx))\n",
    "            outputs.append(hx)\n",
    "\n",
    "        # if we have a single example, our final LSTM state is the last hx\n",
    "        if B == 1:\n",
    "            final = hx\n",
    "        else:\n",
    "            #\n",
    "            # This part is explained in next section, ignore this else-block for now.\n",
    "            #\n",
    "            # we processed sentences with different lengths, so some of the sentences\n",
    "            # had already finished and we have been adding padding inputs to hx\n",
    "            # we select the final state based on the length of each sentence\n",
    "\n",
    "            # two lines below not needed if using LSTM form pytorch\n",
    "            outputs = torch.stack(outputs, dim=0)          # [T, B, D]\n",
    "            outputs = outputs.transpose(0, 1).contiguous()  # [B, T, D]\n",
    "\n",
    "            # to be super-sure we're not accidentally indexing the wrong state\n",
    "            # we zero out positions that are invalid\n",
    "            pad_positions = (x == 1).unsqueeze(-1)\n",
    "\n",
    "            outputs = outputs.contiguous()      \n",
    "            outputs = outputs.masked_fill_(pad_positions, 0.)\n",
    "\n",
    "            mask = (x != 1)  # true for valid positions [B, T]\n",
    "            lengths = mask.sum(dim=1)                  # [B, 1]\n",
    "\n",
    "            indexes = (lengths - 1) + torch.arange(B, device=x.device, dtype=x.dtype) * T\n",
    "            final = outputs.view(-1, self.hidden_dim)[indexes]  # [B, D]\n",
    "\n",
    "        # we use the last hidden state to classify the sentence\n",
    "        logits = self.output_layer(final)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FxFoVpvMPB6g"
   },
   "source": [
    "#### Dropout\n",
    "\n",
    "Besides not being able to learn meaningful word embeddings, there is another negative effect that can follow from data sparsity and a small data set: *overfitting*. This is a phenomenom that is very likely to occur when fitting strong and expressive models, like LSTMs, to small data. In practice, if your model overfits, this means that it will be very good at predicting (or: 'remembering') the sentiment of the training set, but unable to generalize to new, unseen data in the test set. This is undesirable and one technique to mitigate it is *dropout*. \n",
    "\n",
    "A dropout layer is defined by the following formula: $\\mathbf{d} \\in \\{0, 1\\}^n$, with $d_j \\sim \\text{Bernoulli}(p)$, and can be applied to for example a linear layer:\n",
    "\n",
    "$$\\text{tanh}(W(\\mathbf{h}\\odot \\mathbf{d}) + \\mathbf{b})$$\n",
    "\n",
    "\n",
    "These formulas simply mean that we *drop* certain parameters during training (by setting them to zero). Which parameters we drop is stochastically determined by a Bernoulli distribution and the probability of each parameter being dropped is set to $p = 0.5$ in our experiments (see the previous cell of code where we define our output layer). A dropout layer can be applied at many different places in our models. This technique helps against the undesirable effect where our model relies on single parameters for  prediction (e.g. if $h^{\\prime}_j$ is large, always predict positive). If we use dropout, the model needs to learn to rely on different parameters, which is desirable when generalizing to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XQjEjLt9z0XW"
   },
   "source": [
    "**Let's train our LSTM! ** Note that is will be a lot slower, because we need to do many more computations per sentence!\n",
    "\n",
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LgZoSPD4fsf_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlstm_model = LSTMClassifier(len(v.w2i), 300, 168, len(t2i), v)\\n\\n# copy pre-trained word vectors into embeddings table\\nwith torch.no_grad():\\n    lstm_model.embed.weight.data.copy_(torch.from_numpy(vectors))\\n    lstm_model.embed.weight.requires_grad = False\\n\\nprint(lstm_model)\\nprint_parameters(lstm_model)\\n\\nlstm_model = lstm_model.to(device)\\noptimizer = optim.Adam(lstm_model.parameters(), lr=3e-4)\\n\\nlstm_losses, lstm_accuracies = train_model(\\n    lstm_model, optimizer, num_iterations=25000, \\n    print_every=250, eval_every=1000)\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "lstm_model = LSTMClassifier(len(v.w2i), 300, 168, len(t2i), v)\n",
    "\n",
    "# copy pre-trained word vectors into embeddings table\n",
    "with torch.no_grad():\n",
    "    lstm_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
    "    lstm_model.embed.weight.requires_grad = False\n",
    "\n",
    "print(lstm_model)\n",
    "print_parameters(lstm_model)\n",
    "\n",
    "lstm_model = lstm_model.to(device)\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=3e-4)\n",
    "\n",
    "lstm_losses, lstm_accuracies = train_model(\n",
    "    lstm_model, optimizer, num_iterations=25000, \n",
    "    print_every=250, eval_every=1000)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2BKVnyg0Hq5E"
   },
   "outputs": [],
   "source": [
    "# plot validation accuracy\n",
    "# plt.plot(lstm_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZowTV0EBTb3z"
   },
   "outputs": [],
   "source": [
    "# plot training loss\n",
    "# plt.plot(lstm_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YEw6XHQY_AAQ"
   },
   "source": [
    "# Mini-batching\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FPf96wGzBTQJ"
   },
   "source": [
    "**Why is the LSTM so slow?** Despite our best efforts, we still need to make a lot of matrix multiplications per example (linear in the length of the example) just to get a single classification, and we can only process the 2nd word once we have computed the hidden state for the 1st word (sequential computation).\n",
    "\n",
    "GPUs are more efficient if we do a few big matrix multiplications, rather than lots of small ones. If we could process multiple examples at the same time, then we could exploit that. We still process the input sequentially, but now we can do so for multiple sentences at the same time.\n",
    "\n",
    "Up to now our \"minibatch\" consisted of a single example. This was for a reason: the sentences in our data sets have **different lengths**, and this makes it difficult to process them at the same time.\n",
    "\n",
    "Consider a batch of 2 sentences:\n",
    "\n",
    "```\n",
    "this movie is bad\n",
    "this movie is super cool !\n",
    "```\n",
    "\n",
    "Let's say the IDs for these sentences are:\n",
    "\n",
    "```\n",
    "2 3 4 5\n",
    "2 3 4 6 7 8\n",
    "```\n",
    "\n",
    "We cannot feed PyTorch an object with variable length rows! We need to turn this into a matrix.\n",
    "\n",
    "The solution is to add **padding values** to our mini-batch:\n",
    "\n",
    "```\n",
    "2 3 4 5 1 1\n",
    "2 3 4 6 7 8\n",
    "```\n",
    "\n",
    "Whenever a sentence is shorter than the longest sentence in a mini-batch, we just use a padding value (here: 1) to fill the matrix.\n",
    "\n",
    "In our computation, we should **ignore** the padding positions (e.g. mask them out). Paddings should not contribute to the loss.\n",
    "\n",
    "#### Mini-batch feed\n",
    "We will now code a `get_minibatch` function that will replace our `get_example` function, and returns a mini-batch of the requested size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IoAE2JBiXJ3P"
   },
   "outputs": [],
   "source": [
    "def get_minibatch(data, batch_size=25, shuffle=True):\n",
    "    \"\"\"Return minibatches, optional shuffling\"\"\"\n",
    "\n",
    "    if shuffle:\n",
    "        # print(\"Shuffling training data\")\n",
    "        random.shuffle(data)  # shuffle training data each epoch\n",
    "\n",
    "    batch = []\n",
    "\n",
    "    # yield minibatches\n",
    "    for example in data:\n",
    "        batch.append(example)\n",
    "\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "\n",
    "    # in case there is something left\n",
    "    if len(batch) > 0:\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DwZM-XYkT8Zx"
   },
   "source": [
    "#### Pad function\n",
    "We will need a function that adds padding 1s to a sequence of IDs so that\n",
    "it becomes as long as the longest sequencen in the minibatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sp0sK1ghw4Ft"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 1, 1]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pad(tokens, length, pad_value=1):\n",
    "    \"\"\"add padding 1s to a sequence to that it has the desired length\"\"\"\n",
    "    return tokens + [pad_value] * (length - len(tokens))\n",
    "\n",
    "# example\n",
    "tokens = [2, 3, 4]\n",
    "pad(tokens, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SL2iixMYUgfh"
   },
   "source": [
    "#### New prepare function\n",
    "\n",
    "We will also need a new function that turns a mini-batch into PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZID0cqozWks8"
   },
   "outputs": [],
   "source": [
    "def prepare_minibatch(mb, vocab):\n",
    "    \"\"\"\n",
    "    Minibatch is a list of examples.\n",
    "    This function converts words to IDs and returns\n",
    "    torch tensors to be used as input/targets.\n",
    "    \"\"\"\n",
    "    batch_size = len(mb)\n",
    "    maxlen = max([len(ex.tokens) for ex in mb])\n",
    "\n",
    "    # vocab returns 0 if the word is not there\n",
    "    x = [pad([vocab.w2i.get(t, 0) for t in ex.tokens], maxlen) for ex in mb]\n",
    "\n",
    "    x = torch.LongTensor(x)\n",
    "    x = x.to(device)\n",
    "\n",
    "    y = [ex.label for ex in mb]\n",
    "    y = torch.LongTensor(y)\n",
    "    y = y.to(device)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OwDAtCv1x2hB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Example(tokens=['The', 'Rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'Century', \"'s\", 'new', '``', 'Conan', \"''\", 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'Arnold', 'Schwarzenegger', ',', 'Jean-Claud', 'Van', 'Damme', 'or', 'Steven', 'Segal', '.'], tree=Tree('3', [Tree('2', [Tree('2', ['The']), Tree('2', ['Rock'])]), Tree('4', [Tree('3', [Tree('2', ['is']), Tree('4', [Tree('2', ['destined']), Tree('2', [Tree('2', [Tree('2', [Tree('2', [Tree('2', ['to']), Tree('2', [Tree('2', ['be']), Tree('2', [Tree('2', ['the']), Tree('2', [Tree('2', ['21st']), Tree('2', [Tree('2', [Tree('2', ['Century']), Tree('2', [\"'s\"])]), Tree('2', [Tree('3', ['new']), Tree('2', [Tree('2', ['``']), Tree('2', ['Conan'])])])])])])])]), Tree('2', [\"''\"])]), Tree('2', ['and'])]), Tree('3', [Tree('2', ['that']), Tree('3', [Tree('2', ['he']), Tree('3', [Tree('2', [\"'s\"]), Tree('3', [Tree('2', ['going']), Tree('3', [Tree('2', ['to']), Tree('4', [Tree('3', [Tree('2', ['make']), Tree('3', [Tree('3', [Tree('2', ['a']), Tree('3', ['splash'])]), Tree('2', [Tree('2', ['even']), Tree('3', ['greater'])])])]), Tree('2', [Tree('2', ['than']), Tree('2', [Tree('2', [Tree('2', [Tree('2', [Tree('1', [Tree('2', ['Arnold']), Tree('2', ['Schwarzenegger'])]), Tree('2', [','])]), Tree('2', [Tree('2', ['Jean-Claud']), Tree('2', [Tree('2', ['Van']), Tree('2', ['Damme'])])])]), Tree('2', ['or'])]), Tree('2', [Tree('2', ['Steven']), Tree('2', ['Segal'])])])])])])])])])])])])]), Tree('2', ['.'])])]), label=3, transitions=[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], index=1, loss=[], transition_matrix=None)\n",
      "Example(tokens=['The', 'gorgeously', 'elaborate', 'continuation', 'of', '``', 'The', 'Lord', 'of', 'the', 'Rings', \"''\", 'trilogy', 'is', 'so', 'huge', 'that', 'a', 'column', 'of', 'words', 'can', 'not', 'adequately', 'describe', 'co-writer/director', 'Peter', 'Jackson', \"'s\", 'expanded', 'vision', 'of', 'J.R.R.', 'Tolkien', \"'s\", 'Middle-earth', '.'], tree=Tree('4', [Tree('4', [Tree('4', [Tree('2', ['The']), Tree('4', [Tree('3', ['gorgeously']), Tree('3', [Tree('2', ['elaborate']), Tree('2', ['continuation'])])])]), Tree('2', [Tree('2', [Tree('2', ['of']), Tree('2', ['``'])]), Tree('2', [Tree('2', ['The']), Tree('2', [Tree('2', [Tree('2', ['Lord']), Tree('2', [Tree('2', ['of']), Tree('2', [Tree('2', ['the']), Tree('2', ['Rings'])])])]), Tree('2', [Tree('2', [\"''\"]), Tree('2', ['trilogy'])])])])])]), Tree('2', [Tree('3', [Tree('2', [Tree('2', ['is']), Tree('2', [Tree('2', ['so']), Tree('2', ['huge'])])]), Tree('2', [Tree('2', ['that']), Tree('3', [Tree('2', [Tree('2', [Tree('2', ['a']), Tree('2', ['column'])]), Tree('2', [Tree('2', ['of']), Tree('2', ['words'])])]), Tree('2', [Tree('2', [Tree('2', [Tree('2', ['can']), Tree('1', ['not'])]), Tree('3', ['adequately'])]), Tree('2', [Tree('2', ['describe']), Tree('2', [Tree('3', [Tree('2', [Tree('2', ['co-writer/director']), Tree('2', [Tree('2', ['Peter']), Tree('3', [Tree('2', ['Jackson']), Tree('2', [\"'s\"])])])]), Tree('3', [Tree('2', ['expanded']), Tree('2', ['vision'])])]), Tree('2', [Tree('2', ['of']), Tree('2', [Tree('2', [Tree('2', ['J.R.R.']), Tree('2', [Tree('2', ['Tolkien']), Tree('2', [\"'s\"])])]), Tree('2', ['Middle-earth'])])])])])])])])]), Tree('2', ['.'])])]), label=4, transitions=[0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], index=2, loss=[], transition_matrix=None)\n",
      "Example(tokens=['Singer/composer', 'Bryan', 'Adams', 'contributes', 'a', 'slew', 'of', 'songs', '--', 'a', 'few', 'potential', 'hits', ',', 'a', 'few', 'more', 'simply', 'intrusive', 'to', 'the', 'story', '--', 'but', 'the', 'whole', 'package', 'certainly', 'captures', 'the', 'intended', ',', 'er', ',', 'spirit', 'of', 'the', 'piece', '.'], tree=Tree('3', [Tree('3', [Tree('2', [Tree('2', [Tree('2', [Tree('2', [Tree('2', ['Singer/composer']), Tree('2', [Tree('2', ['Bryan']), Tree('2', ['Adams'])])]), Tree('2', [Tree('2', ['contributes']), Tree('2', [Tree('2', [Tree('2', ['a']), Tree('2', ['slew'])]), Tree('2', [Tree('2', ['of']), Tree('2', ['songs'])])])])]), Tree('2', [Tree('2', ['--']), Tree('2', [Tree('2', [Tree('2', [Tree('2', ['a']), Tree('2', [Tree('2', ['few']), Tree('3', ['potential'])])]), Tree('2', [Tree('2', [Tree('2', ['hits']), Tree('2', [','])]), Tree('2', [Tree('2', [Tree('2', ['a']), Tree('2', ['few'])]), Tree('1', [Tree('1', [Tree('2', ['more']), Tree('1', [Tree('2', ['simply']), Tree('2', ['intrusive'])])]), Tree('2', [Tree('2', ['to']), Tree('2', [Tree('2', ['the']), Tree('2', ['story'])])])])])])]), Tree('2', ['--'])])])]), Tree('2', ['but'])]), Tree('3', [Tree('4', [Tree('2', ['the']), Tree('3', [Tree('2', ['whole']), Tree('2', ['package'])])]), Tree('2', [Tree('3', ['certainly']), Tree('3', [Tree('2', ['captures']), Tree('2', [Tree('1', [Tree('2', ['the']), Tree('2', [Tree('2', [Tree('2', ['intended']), Tree('2', [Tree('2', [',']), Tree('2', [Tree('2', ['er']), Tree('2', [','])])])]), Tree('3', ['spirit'])])]), Tree('2', [Tree('2', ['of']), Tree('2', [Tree('2', ['the']), Tree('2', ['piece'])])])])])])])]), Tree('2', ['.'])]), label=3, transitions=[0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1], index=3, loss=[], transition_matrix=None)\n"
     ]
    }
   ],
   "source": [
    "# Let's test our new function.\n",
    "# This should give us 3 examples.\n",
    "mb = next(get_minibatch(train_data, batch_size=3, shuffle=False))\n",
    "print(len(mb))\n",
    "for ex in mb:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dg8zEK8zyUCH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x tensor([[   23,  1414,    11,  8222,     6,    27,     4,  2826,  3764,    21,\n",
      "            92,  5233,  8558, 15010,     5,    16,    53,    21,   183,     6,\n",
      "           107,     8,  7409,   148,  1552,    97,  6000, 10464,     2,     0,\n",
      "          2740, 15451,    33,  4008, 12925,     3,     1,     1,     1],\n",
      "        [   23, 15300,  6665,  8307,     7,  5233,    23,  1383,     7,     4,\n",
      "          4884, 15010,  8681,    11,    59,   933,    16,     8,  3045,     7,\n",
      "           566,    42,    36,  7800,  2886, 20302,  1433,  1977,    21,  4360,\n",
      "          2274,     7, 15256, 10099,    21, 15561,     3,     1,     1],\n",
      "        [    0,  5314,  3878,  7735,     8,  9726,     7,  1261,   158,     8,\n",
      "           219,  1060,  2187,     2,     8,   219,    50,   688, 11605,     6,\n",
      "             4,   414,   158,    43,     4,   494,  1568,  1214,  6985,     4,\n",
      "          2243,     2,  5986,     2,  2320,     7,     4,   982,     3]],\n",
      "       device='cuda:0')\n",
      "y tensor([3, 4, 3], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# We should find 1s at the end where padding is.\n",
    "x, y = prepare_minibatch(mb, v)\n",
    "print(\"x\", x)\n",
    "print(\"y\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xYBJEoSNUwI0"
   },
   "source": [
    "#### Evaluate (mini-batch version)\n",
    "\n",
    "We can now update our evaluation function to use minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eiZZpEghzqou"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data, \n",
    "             batch_fn=get_minibatch, prep_fn=prepare_minibatch,\n",
    "             batch_size=16):\n",
    "    \"\"\"Accuracy of a model on given data set (using minibatches)\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()  # disable dropout\n",
    "\n",
    "    for mb in batch_fn(data, batch_size=batch_size, shuffle=False):\n",
    "        x, targets = prep_fn(mb, model.vocab)\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)\n",
    "\n",
    "        predictions = logits.argmax(dim=-1).view(-1)\n",
    "\n",
    "        # add the number of correct predictions to the total correct\n",
    "        correct += (predictions == targets.view(-1)).sum().item()\n",
    "        total += targets.size(0)\n",
    "    print(\"Evaluation on \" + str(total) + \" elements. Correct: \" + str(correct)) \n",
    "\n",
    "    return correct, total, correct / float(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "23wAZomozh_2"
   },
   "source": [
    "# LSTM (Mini-batched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B-gkPU7jzBe2"
   },
   "source": [
    "With this, let's run the LSTM again but now using minibatches!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "226Xg9OPzFbA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlstm_model = LSTMClassifier(\\n    len(v.w2i), 300, 168, len(t2i), v)\\n\\n# copy pre-trained vectors into embeddings table\\nwith torch.no_grad():\\n    lstm_model.embed.weight.data.copy_(torch.from_numpy(vectors))\\n    lstm_model.embed.weight.requires_grad = False\\n\\nprint(lstm_model)\\nprint_parameters(lstm_model)  \\n  \\nlstm_model = lstm_model.to(device)\\n\\nbatch_size = 512\\noptimizer = optim.Adam(lstm_model.parameters(), lr=2e-4)\\n\\nlstm_losses, lstm_accuracies = train_model(\\n    lstm_model, optimizer, num_iterations=30000, \\n    print_every=250, eval_every=250,\\n    batch_size=batch_size,\\n    batch_fn=get_minibatch, \\n    prep_fn=prepare_minibatch,\\n    eval_fn=evaluate)\\n'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "lstm_model = LSTMClassifier(\n",
    "    len(v.w2i), 300, 168, len(t2i), v)\n",
    "\n",
    "# copy pre-trained vectors into embeddings table\n",
    "with torch.no_grad():\n",
    "    lstm_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
    "    lstm_model.embed.weight.requires_grad = False\n",
    "\n",
    "print(lstm_model)\n",
    "print_parameters(lstm_model)  \n",
    "  \n",
    "lstm_model = lstm_model.to(device)\n",
    "\n",
    "batch_size = 512\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=2e-4)\n",
    "\n",
    "lstm_losses, lstm_accuracies = train_model(\n",
    "    lstm_model, optimizer, num_iterations=30000, \n",
    "    print_every=250, eval_every=250,\n",
    "    batch_size=batch_size,\n",
    "    batch_fn=get_minibatch, \n",
    "    prep_fn=prepare_minibatch,\n",
    "    eval_fn=evaluate)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ymj1rLDMvyhp"
   },
   "outputs": [],
   "source": [
    "# plot validation accuracy\n",
    "# plt.plot(lstm_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1je5S1RHVC5R"
   },
   "outputs": [],
   "source": [
    "# plot training loss\n",
    "# plt.plot(lstm_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q7WjcxXntMi5"
   },
   "source": [
    "# Tree LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jyj_UD6GtO5M"
   },
   "source": [
    "In the final part of this lab we will exploit the tree-structure of our data. \n",
    "Until now we only used the surface tokens, but remember that our data examples include trees with a sentiment score at every node.\n",
    "\n",
    "In particular, we will implement **N-ary Tree-LSTMs** which are described in:\n",
    "\n",
    "> Kai Sheng Tai, Richard Socher, and Christopher D. Manning. [Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks](http://aclweb.org/anthology/P/P15/P15-1150.pdf) ACL 2015.\n",
    "\n",
    "Since our trees are binary, N=2, and we can refer to these as *Binary Tree-LSTMs*.\n",
    "\n",
    "You should read this paper carefully and make sure that you understand the approach. You will also find our LSTM baseline there.\n",
    "Note however that Tree LSTMs were also invented around the same time by two other groups:\n",
    "\n",
    "> Phong Le and Willem Zuidema. [Compositional distributional semantics with long short term memory](http://anthology.aclweb.org/S/S15/S15-1002.pdf). *SEM 2015.\n",
    "\n",
    "> Xiaodan Zhu, Parinaz Sobihani,  and Hongyu Guo. [Long short-term memory over recursive structures](http://proceedings.mlr.press/v37/zhub15.pdf). ICML 2015.\n",
    "\n",
    "It is good scientific practice to cite all three papers in your report.\n",
    "\n",
    "If you study equations (9) to (14) in the paper, you will find that they are not all too different from the original LSTM that you already have.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1rDzvSos3JFp"
   },
   "source": [
    "## Computation\n",
    "\n",
    "Do you remember the `transitions_from_treestring` function all the way in the beginning of this lab? Every example contains a **transition sequence** made by this function. Let's look at it again:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5pg0Xumc3ZUS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              3                                                                     \n",
      "  ____________|____________________                                                  \n",
      " |                                 4                                                \n",
      " |        _________________________|______________________________________________   \n",
      " |       4                                                                        | \n",
      " |    ___|______________                                                          |  \n",
      " |   |                  4                                                         | \n",
      " |   |         _________|__________                                               |  \n",
      " |   |        |                    3                                              | \n",
      " |   |        |               _____|______________________                        |  \n",
      " |   |        |              |                            4                       | \n",
      " |   |        |              |            ________________|_______                |  \n",
      " |   |        |              |           |                        2               | \n",
      " |   |        |              |           |                 _______|___            |  \n",
      " |   |        3              |           |                |           2           | \n",
      " |   |    ____|_____         |           |                |        ___|_____      |  \n",
      " |   |   |          4        |           3                |       2         |     | \n",
      " |   |   |     _____|___     |      _____|_______         |    ___|___      |     |  \n",
      " 2   2   2    3         2    2     3             2        2   2       2     2     2 \n",
      " |   |   |    |         |    |     |             |        |   |       |     |     |  \n",
      " It  's  a  lovely     film with lovely     performances  by Buy     and Accorsi  . \n",
      "\n",
      "Transitions:\n",
      "[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "ex = next(examplereader(\"trees/dev.txt\"))\n",
    "print(TreePrettyPrinter(ex.tree))\n",
    "print(\"Transitions:\")\n",
    "print(ex.transitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ceBFe9fU4BI_"
   },
   "source": [
    "Note that the tree is **binary**. Every node has two children, except for pre-terminal nodes.\n",
    "\n",
    "A tree like this can be described by a sequence of **SHIFT (0)** and **REDUCE (1)** actions.\n",
    "\n",
    "We can use the transitions like this to construct the tree:\n",
    "- **reverse** the sentence (a list of tokens) and call this the **buffer**\n",
    "   - the first word is now on top (last in the list), and we would get it when calling pop() on the buffer\n",
    "- create an empty list and call it the **stack**\n",
    "- iterate through the transition sequence:\n",
    "  - if it says SHIFT(0), we pop a word from the buffer, and push it to the stack\n",
    "  - if it says REDUCE(1), we pop the **top two items** from the stack, and combine them (e.g. with a tree LSTM!), creating a new node that we push back on the stack\n",
    "  \n",
    "Convince yourself that going through the transition sequence above will result in the tree that you see.\n",
    "For example, we would start by putting the following words on the stack (by shifting 5 times, starting with `It`):\n",
    "\n",
    "```\n",
    "Top of the stack:\n",
    "-----------------\n",
    "film\n",
    "lovely\n",
    "a \n",
    "'s  \n",
    "It\n",
    "```\n",
    "Now we find a REDUCE in the transition sequence, so we get the top two words (film and lovely), and combine them, so our new stack becomes:\n",
    "```\n",
    "Top of the stack:\n",
    "-----------------\n",
    "lovely film\n",
    "a \n",
    "'s  \n",
    "It\n",
    "```\n",
    "\n",
    "We will use this approach when encoding sentences with our Tree LSTM.\n",
    "Now, our sentence is a (reversed) list of word embeddings.\n",
    "When we shift, we move a word embedding to the stack.\n",
    "When we reduce, we apply a Tree LSTM to the top two vectors, and the result is a single vector that we put back on the stack.\n",
    "After going through the whole transition sequence, we will have the root node on our stack! We can use that to classify the sentence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pDWKShm1AfmR"
   },
   "source": [
    "## Obtaining the transition sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fO7VKWVpAbWj"
   },
   "source": [
    "\n",
    "So what goes on in the `transitions_from_treestring` function?\n",
    "\n",
    "The idea ([explained in this blog post](https://devblogs.nvidia.com/recursive-neural-networks-pytorch/)) is that, if we had a tree, we could traverse through the tree, and every time that we find a node containing only a word, we output a SHIFT.\n",
    "Every time **after** we have finished visiting the children of a node, we output a REDUCE.\n",
    "(What is this tree traversal called?)\n",
    "\n",
    "However, our `transitions_from_treestring` function operates directly on the string representation. It works as follows.\n",
    "\n",
    "We start with the representation:\n",
    "\n",
    "```\n",
    "(3 (2 It) (4 (4 (2 's) (4 (3 (2 a) (4 (3 lovely) (2 film))) (3 (2 with) (4 (3 (3 lovely) (2 performances)) (2 (2 by) (2 (2 (2 Buy) (2 and)) (2 Accorsi))))))) (2 .)))\n",
    "```\n",
    "\n",
    "First we remove pre-terminal nodes (and add spaces before closing brackets):\n",
    "\n",
    "```\n",
    "(3 It (4 (4 's (4 (3 a (4 lovely film ) ) (3 with (4 (3 lovely performances ) (2 by (2 (2 Buy and )  Accorsi ) ) ) ) ) ) . ) )\n",
    "```\n",
    "\n",
    "Then we remove node labels:\n",
    "\n",
    "```\n",
    "( It ( ( 's ( ( a ( lovely film ) ) ( with ( ( lovely performances) ( by ( ( Buy and )  Accorsi ) ) ) ) ) ) . ) )\n",
    "```\n",
    "\n",
    "Then we remove opening brackets:\n",
    "\n",
    "```\n",
    "It 's a lovely film ) ) with lovely performances ) by Buy and ) Accorsi ) ) ) ) ) ) . ) )\n",
    "```\n",
    "\n",
    "Now we replace words by S (for SHIFT), and closing brackets by R (for REDUCE):\n",
    "\n",
    "```\n",
    "S S S S S R R S S S R S S S R S R R R R R R S R R\n",
    "0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 \n",
    "```\n",
    "\n",
    "Et voila. We just obtained the transition sequence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1y069gM4_v64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S S S S S R R S S S R S S S R S R R R R R R S R R\n",
      "0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1\n"
     ]
    }
   ],
   "source": [
    "# for comparison\n",
    "seq = ex.transitions\n",
    "s = \" \".join([\"S\" if t == 0 else \"R\" for t in seq])\n",
    "print(s)\n",
    "print(\" \".join(map(str, seq)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d-qOuKbDAiBn"
   },
   "source": [
    "## Coding the Tree LSTM\n",
    "\n",
    "The code below contains a Binary Tree LSTM cell.\n",
    "It is used in the TreeLSTM class below it, which in turn is used in the TreeLSTMClassifier.\n",
    "The job of the TreeLSTM class is to encode a complete sentence and return the root node.\n",
    "The job of the TreeLSTMCell is to return a new state when provided with two children (a reduce action). By repeatedly calling the TreeLSTMCell, the TreeLSTM will encode a sentence. This can be done for multiple sentences at the same time.\n",
    "\n",
    "\n",
    "#### Exercise \n",
    "Check the `forward` function and complete the Tree LSTM formulas.\n",
    "You can see that we defined a large linear layer for you, that projects the *concatenation* of the left and right child into the input gate, left forget gate, right forget gate, candidate, and output gate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J9b9mjMlN7Pb"
   },
   "outputs": [],
   "source": [
    "class TreeLSTMCell(nn.Module):\n",
    "    \"\"\"A Binary Tree LSTM cell\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        \"\"\"Creates the weights for this LSTM\"\"\"\n",
    "        super(TreeLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "\n",
    "        self.reduce_layer = nn.Linear(2 * hidden_size, 5 * hidden_size)\n",
    "        self.dropout_layer = nn.Dropout(p=0.25)\n",
    "        self.tanh_act = nn.Tanh()\n",
    "        self.sigmoid_act = nn.Sigmoid()\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"This is PyTorch's default initialization method\"\"\"\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)  \n",
    "\n",
    "    def forward(self, hx_l, hx_r, mask=None):\n",
    "        \"\"\"\n",
    "        hx_l is ((batch, hidden_size), (batch, hidden_size))\n",
    "        hx_r is ((batch, hidden_size), (batch, hidden_size))    \n",
    "        \"\"\"\n",
    "        prev_h_l, prev_c_l = hx_l  # left child\n",
    "        prev_h_r, prev_c_r = hx_r  # right child\n",
    "\n",
    "        B = prev_h_l.size(0)\n",
    "\n",
    "        # we concatenate the left and right children\n",
    "        # you can also project from them separately and then sum\n",
    "        children = torch.cat([prev_h_l, prev_h_r], dim=1)\n",
    "\n",
    "        # project the combined children into a 5D tensor for i,fl,fr,g,o\n",
    "        # this is done for speed, and you could also do it separately\n",
    "        proj = self.reduce_layer(children)  # shape: B x 5D\n",
    "\n",
    "        # each shape: B x D\n",
    "        i, f_l, f_r, g, o = torch.chunk(proj, 5, dim=-1)\n",
    "\n",
    "        # main Tree LSTM computation\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # You only need to complete the commented lines below.\n",
    "\n",
    "        # The shape of each of these is [batch_size, hidden_size]\n",
    "\n",
    "        i = self.sigmoid_act(i)\n",
    "        f_l = self.sigmoid_act(f_l)    \n",
    "        f_r = self.sigmoid_act(f_r)\n",
    "        g = self.tanh_act(g)\n",
    "        o = self.sigmoid_act(o)\n",
    "\n",
    "        c = i * g + f_l * prev_c_l + f_r * prev_c_r\n",
    "        h = o * self.tanh_act(c)\n",
    "\n",
    "        return h, c\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"{}({:d}, {:d})\".format(\n",
    "            self.__class__.__name__, self.input_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dj5dYSGh_643"
   },
   "source": [
    "## Explanation of the TreeLSTM class\n",
    "\n",
    "\n",
    "The code below contains the TreeLSTM class, which implements everything we need in order to encode a sentence from word embeddings. The calculations are the same as in the paper, implemented such that the class `TreeLSTMCell` above is as general as possible and only takes two children to reduce them into a parent. \n",
    "\n",
    "\n",
    "**Initialize $\\mathbf{h}$ and $\\mathbf{c}$ outside of the cell for the leaves**\n",
    "\n",
    "At the leaves of each tree the children nodes are **empty**, whereas in higher levels the nodes are binary tree nodes that *do* have a left and right child (but no input $x$). By initializing the leaf nodes outside of the cell class (`TreeLSTMCell`), we avoid if-else statements in the forward pass.\n",
    "\n",
    "The `TreeLSTM` class (among other things) pre-calculates an initial $h$ and $c$ for every word in the sentence. Since the initial left and right child are 0, the only calculations we need to do are based on $x$, and we can drop the forget gate calculation (`prev_c_l` and `prev_c_r` are zero). The calculations we do in order to initalize $h$ and $c$ are then:\n",
    "\n",
    "$$\n",
    "c_1 =  W^{(u)}x_1 \\\\\n",
    "o_1 = \\sigma (W^{(i)}x_1) \\\\\n",
    "h_1 = o_1 \\odot \\text{tanh}(c_1)$$\n",
    "*NB: note that these equations are chosen as initializations of $c$ and $h$, other initializations are possible and might work equally well.*\n",
    "\n",
    "**Sentence Representations**\n",
    "\n",
    "All our leaf nodes are now initialized, so we can start processing the sentence in its tree form. Each sentence is represented by a buffer (initially a list with a concatenation of $[h_1, c_1]$ for every word in the reversed sentence), a stack (initially an empty list) and a transition sequence. To encode our sentence, we construct the tree from its transition sequence as explained earlier. \n",
    "\n",
    "*A short example that constructs a tree:*\n",
    "\n",
    "We loop over the time dimension of the batched transition sequences (i.e. row by row), which contain values of 0's, 1's and 2's (representing SHIFT, REDUCE and padding respectively). If we have a batch of size 2 where the first example has a transition sequence given by [0, 0, 1, 0, 0, 0, 1] and the second by [0, 0, 1, 0, 0, 1], our transition batch will be given by the following two-dimensional numpy array:\n",
    "\n",
    "$$\n",
    "\\text{transitions} = \n",
    "\\begin{pmatrix}\n",
    "0 & 0\\\\ \n",
    "0 & 0\\\\ \n",
    "1 & 1\\\\ \n",
    "0 & 0\\\\ \n",
    "0 & 0\\\\ \n",
    "0 & 1\\\\ \n",
    "1 & 2\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "The inner loop (`for transition, buffer, stack in zip(t_batch, buffers, stacks)`) goes over each example in the batch and updates its buffer and stack. The nested loop for this example will then do roughy the following:\n",
    "\n",
    "```\n",
    "Time = 0:  t_batch = [0, 0], the inner loop performs 2 SHIFTs. \n",
    "\n",
    "Time = 1:  t_batch = [0, 0], \"..\"\n",
    "\n",
    "Time = 2:  t_batch = [1, 1], causing the inner loop to fill the list child_l and child_r for both examples in the batch. Now the statement if child_l will return True, triggering a REDUCE action to be performed by our Tree LSTM cell with a batch size of 2. \n",
    "\n",
    "Time = 3:  t_batch = [0, 0], \"..\".\n",
    "\n",
    "Time = 4:  t_batch = [0, 0], \"..\"\n",
    "\n",
    "Time = 5:  t_batch = [0, 1], one SHIFT will be done and another REDUCE action will be performed by our Tree LSTM, this time of batch size 1.  \n",
    "\n",
    "Time = 6:  t_batch = [1, 2], triggering another REDUCE action with batch size 1.\n",
    "```\n",
    "*NB: note that this was an artificial example for the purpose of demonstrating parts of the code, the transition sequences do not necessarily represent actual trees.*\n",
    "\n",
    "**Batching and Unbatching**\n",
    "\n",
    "Within the body of the outer loop over time, we use the functions for batching and unbatching. \n",
    "\n",
    "*Batching*\n",
    "\n",
    "Before passing two lists of children to the reduce layer (an instance of `TreeLSTMCell`), we batch the children as they are at this point a list of tensors of variable length based on how many REDUCE actions there are to perform at a certain time step across the batch (let's call the length `L`). To do an efficient forward pass we want to transform the list to a pair of tensors of shape `([L, D], [L, D])`, which the function `batch` achieves. \n",
    "\n",
    "*Unbatching*\n",
    "\n",
    "In the same line where we batched the children, we unbatch the output of the forward pass to become a list of states of length `L` again. We do this because we need to loop over each example's transition at the current time step and push the children that are reduced into a parent to the stack.\n",
    "\n",
    "*The batch and unbatch functions let us switch between the \"PyTorch world\" (Tensors) and the Python world (easy to manipulate lists).*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5PixvTd4AqsQ"
   },
   "outputs": [],
   "source": [
    "# Helper functions for batching and unbatching states\n",
    "# For speed we want to combine computations by batching, but \n",
    "# for processing logic we want to turn the output into lists again\n",
    "# to easily manipulate.\n",
    "\n",
    "def batch(states):\n",
    "    \"\"\"\n",
    "    Turns a list of states into a single tensor for fast processing. \n",
    "    This function also chunks (splits) each state into a (h, c) pair\"\"\"\n",
    "    return torch.cat(states, 0).chunk(2, 1)\n",
    "\n",
    "def unbatch(state):\n",
    "    \"\"\"\n",
    "    Turns a tensor back into a list of states.\n",
    "    First, (h, c) are merged into a single state.\n",
    "    Then the result is split into a list of sentences.\n",
    "    \"\"\"\n",
    "    return torch.split(torch.cat(state, 1), 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CynltDasaLPt"
   },
   "source": [
    "Take some time to understand the class below, having read the explanation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rQOqMXG4gX5G"
   },
   "outputs": [],
   "source": [
    "class TreeLSTM(nn.Module):\n",
    "    \"\"\"Encodes a sentence using a TreeLSTMCell\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        \"\"\"Creates the weights for this LSTM\"\"\"\n",
    "        super(TreeLSTM, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "        self.reduce = TreeLSTMCell(input_size, hidden_size)\n",
    "\n",
    "        # project word to initial c\n",
    "        self.proj_x = nn.Linear(input_size, hidden_size)\n",
    "        self.proj_x_gate = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        self.buffers_dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x, transitions):\n",
    "        \"\"\"\n",
    "        WARNING: assuming x is reversed!\n",
    "        :param x: word embeddings [B, T, E]\n",
    "        :param transitions: [2T-1, B]\n",
    "        :return: root states\n",
    "        \"\"\"\n",
    "\n",
    "        B = x.size(0)  # batch size\n",
    "        T = x.size(1)  # time\n",
    "\n",
    "        # compute an initial c and h for each word\n",
    "        # Note: this corresponds to input x in the Tai et al. Tree LSTM paper.\n",
    "        # We do not handle input x in the TreeLSTMCell itself.\n",
    "        buffers_c = self.proj_x(x)\n",
    "        buffers_h = buffers_c.tanh()\n",
    "        buffers_h_gate = self.proj_x_gate(x).sigmoid()\n",
    "        buffers_h = buffers_h_gate * buffers_h\n",
    "\n",
    "        # concatenate h and c for each word\n",
    "        buffers = torch.cat([buffers_h, buffers_c], dim=-1)\n",
    "\n",
    "        D = buffers.size(-1) // 2\n",
    "\n",
    "        # we turn buffers into a list of stacks (1 stack for each sentence)\n",
    "        # first we split buffers so that it is a list of sentences (length B)\n",
    "        # then we split each sentence to be a list of word vectors\n",
    "        buffers = buffers.split(1, dim=0)  # Bx[T, 2D]\n",
    "        buffers = [list(b.squeeze(0).split(1, dim=0)) for b in buffers]  # BxTx[2D]\n",
    "\n",
    "        # create B empty stacks\n",
    "        stacks = [[] for _ in buffers]\n",
    "\n",
    "        # t_batch holds 1 transition for each sentence\n",
    "        for t_batch in transitions:\n",
    "\n",
    "            child_l = []  # contains the left child for each sentence with reduce action\n",
    "            child_r = []  # contains the corresponding right child\n",
    "\n",
    "            # iterate over sentences in the batch\n",
    "            # each has a transition t, a buffer and a stack\n",
    "            for transition, buffer, stack in zip(t_batch, buffers, stacks):\n",
    "                if transition == SHIFT:\n",
    "                    stack.append(buffer.pop())\n",
    "                elif transition == REDUCE:\n",
    "                    assert len(stack) >= 2, \\\n",
    "                        \"Stack too small! Should not happen with valid transition sequences\"\n",
    "                    child_r.append(stack.pop())  # right child is on top\n",
    "                    child_l.append(stack.pop())\n",
    "\n",
    "            # if there are sentences with reduce transition, perform them batched\n",
    "            if child_l:\n",
    "                reduced = iter(unbatch(self.reduce(batch(child_l), batch(child_r))))\n",
    "                for transition, stack in zip(t_batch, stacks):\n",
    "                    if transition == REDUCE:\n",
    "                        stack.append(next(reduced))\n",
    "\n",
    "        final = [stack.pop().chunk(2, -1)[0] for stack in stacks]\n",
    "        final = torch.cat(final, dim=0)  # tensor [B, D]\n",
    "\n",
    "        return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s4EzbVzqaXkw"
   },
   "source": [
    "Just like the LSTM before, we will need an extra class that does the classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nLxpYRvtQKge"
   },
   "outputs": [],
   "source": [
    "class TreeLSTMClassifier(nn.Module):\n",
    "    \"\"\"Encodes sentence with a TreeLSTM and projects final hidden state\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, vocab):\n",
    "        super(TreeLSTMClassifier, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim, padding_idx=1, requires_grad=True)\n",
    "        self.treelstm = TreeLSTM(embedding_dim, hidden_dim)\n",
    "        self.output_layer = nn.Sequential(     \n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(hidden_dim, output_dim, bias=True)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # x is a pair here of words and transitions; we unpack it here.\n",
    "        # x is batch-major: [B, T], transitions is time major [2T-1, B]\n",
    "        x, transitions = x\n",
    "        emb = self.embed(x)\n",
    "\n",
    "        # we use the root/top state of the Tree LSTM to classify the sentence\n",
    "        root_states = self.treelstm(emb, transitions)\n",
    "\n",
    "        # we use the last hidden state to classify the sentence\n",
    "        logits = self.output_layer(root_states)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gh9RbhGwaiLg"
   },
   "source": [
    "## Special prepare function for Tree LSTM\n",
    "\n",
    "We need yet another prepare function. For our implementation our sentences to be *reversed*. We will do that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DiqH-_2xdm9H"
   },
   "outputs": [],
   "source": [
    "def prepare_treelstm_minibatch(mb, vocab):\n",
    "    \"\"\"\n",
    "    Returns sentences reversed (last word first)\n",
    "    Returns transitions together with the sentences.  \n",
    "    \"\"\"\n",
    "    batch_size = len(mb)\n",
    "    maxlen = max([len(ex.tokens) for ex in mb])\n",
    "\n",
    "    # vocab returns 0 if the word is not there\n",
    "    # NOTE: reversed sequence!\n",
    "    x = [pad([vocab.w2i.get(t, 0) for t in ex.tokens], maxlen)[::-1] for ex in mb]\n",
    "\n",
    "    x = torch.LongTensor(x)\n",
    "    x = x.to(device)\n",
    "\n",
    "    y = [ex.label for ex in mb]\n",
    "    y = torch.LongTensor(y)\n",
    "    y = y.to(device)\n",
    "\n",
    "    maxlen_t = max([len(ex.transitions) for ex in mb])\n",
    "    transitions = [pad(ex.transitions, maxlen_t, pad_value=2) for ex in mb]\n",
    "    transitions = np.array(transitions)\n",
    "    transitions = transitions.T  # time-major\n",
    "\n",
    "    return (x, transitions), y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IMUsrlL9ayVe"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IpOYUdg2D3v0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntree_model = TreeLSTMClassifier(\\n    len(v.w2i), 300, 150, len(t2i), v)\\n\\nwith torch.no_grad():\\n    tree_model.embed.weight.data.copy_(torch.from_numpy(vectors))\\n    tree_model.embed.weight.requires_grad = False\\n  \\ndef do_train(model):\\n\\n    print(model)\\n    print_parameters(model)\\n\\n    model = model.to(device)\\n\\n    optimizer = optim.Adam(model.parameters(), lr=2e-4)\\n\\n    return train_model(\\n        model, optimizer, num_iterations=30000, \\n        print_every=250, eval_every=250,\\n        prep_fn=prepare_treelstm_minibatch,\\n        eval_fn=evaluate,\\n        batch_fn=get_minibatch,\\n        batch_size=32, eval_batch_size=32)\\n  \\nresults = do_train(tree_model)\\n'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's train the Tree LSTM!\n",
    "\"\"\"\n",
    "tree_model = TreeLSTMClassifier(\n",
    "    len(v.w2i), 300, 150, len(t2i), v)\n",
    "\n",
    "with torch.no_grad():\n",
    "    tree_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
    "    tree_model.embed.weight.requires_grad = False\n",
    "  \n",
    "def do_train(model):\n",
    "\n",
    "    print(model)\n",
    "    print_parameters(model)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
    "\n",
    "    return train_model(\n",
    "        model, optimizer, num_iterations=30000, \n",
    "        print_every=250, eval_every=250,\n",
    "        prep_fn=prepare_treelstm_minibatch,\n",
    "        eval_fn=evaluate,\n",
    "        batch_fn=get_minibatch,\n",
    "        batch_size=32, eval_batch_size=32)\n",
    "  \n",
    "results = do_train(tree_model)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DHcHHaLtguUg"
   },
   "outputs": [],
   "source": [
    "# plot\n",
    "# plt.plot(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([2, 2, 2, 3, 2, 4, 3, 2, 3, 2, 3, 2, 2, 2, 2, 2, 2, 2, 4, 3, 4, 4, 2, 4, 3], [1, 3, 5, 6, 6, 5, 4, 5, 7, 7, 6, 7, 9, 9, 8, 8, 7, 6, 5, 4, 3, 2, 2, 1, 0])\n",
      "[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# print(dev_data[0])\n",
    "\n",
    "# s = next(filereader(\"trees/dev.txt\"))\n",
    "# tree = Tree.fromstring(s)\n",
    "# print(TreePrettyPrinter(tree))\n",
    "\n",
    "def get_sublabel_of_tree(tree, depth=0):\n",
    "    label_list = []\n",
    "    depth_list = []\n",
    "    if isinstance(tree[0], Tree):\n",
    "        for subtree in tree:\n",
    "            sublabel_list, subdepth_list = get_sublabel_of_tree(subtree, depth=depth+1)\n",
    "            label_list += sublabel_list\n",
    "            depth_list += subdepth_list\n",
    "    label_list += [int(tree.label())]\n",
    "    depth_list += [depth]\n",
    "    return label_list, depth_list\n",
    "\n",
    "print(get_sublabel_of_tree(dev_data[0].tree))\n",
    "print(dev_data[0].transitions)\n",
    "\n",
    "DEFAULT_LOSS = 50 # High so that in the training progress we first try to explore all words/subtrees\n",
    "\n",
    "min_tree_height = 100\n",
    "\n",
    "for t in train_data:\n",
    "    if len(t.tokens)==5:\n",
    "        break\n",
    "\n",
    "def augment_example(ex, iteration_num=-1, loss_to_prop=lambda x: x**2):\n",
    "    subtrees = subtrees_train_data[ex.index]\n",
    "    max_depth = max(subtrees.keys())\n",
    "    # Determine probabilities for deciding on the depth of the tree\n",
    "    probs = [4**(-i/(iteration_num/100.0+1)) for i in range(max_depth+1)]\n",
    "    probs = [int(i==0) for i in range(max_depth+1)]\n",
    "    norm_const = sum(probs)\n",
    "    probs = [x/norm_const for x in probs]\n",
    "    rand_depth = np.random.choice(range(max_depth+1), p=probs)\n",
    "    \n",
    "    # Determine probabilities for the exact element chosen for the specified depth\n",
    "    mean_losses = [sum(ex.loss)/len(ex.loss) if len(ex.loss) > 0 else DEFAULT_LOSS for ex in subtrees[rand_depth]]\n",
    "    sum_losses = sum([loss_to_prop(x) for x in mean_losses]) # We square the loss so that we even stronger focus on hard examples. Is a hyperparameter that needs to be adjusted!\n",
    "    probs = [loss_to_prop(x)/sum_losses for x in mean_losses]\n",
    "    rand_index = np.random.choice(range(len(subtrees[rand_depth])), p=probs)\n",
    "    return subtrees[rand_depth][rand_index]\n",
    "\n",
    "\n",
    "def get_minibatch(data, batch_size=25, shuffle=True, is_eval=True, iteration_num=-1):\n",
    "    \"\"\"Return minibatches, optional shuffling\"\"\"\n",
    "\n",
    "    if shuffle:\n",
    "        # print(\"Shuffling training data\")\n",
    "        random.shuffle(data)  # shuffle training data each epoch\n",
    "\n",
    "    batch = []\n",
    "\n",
    "    # yield minibatches\n",
    "    for example in data:\n",
    "        if False and not is_eval:\n",
    "            example = augment_example(example, iteration_num=iteration_num)\n",
    "        batch.append(example) # example\n",
    "\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "\n",
    "    # in case there is something left\n",
    "    if len(batch) > 0:\n",
    "        yield batch\n",
    "        \n",
    "\n",
    "def prepare_treelstm_minibatch(mb, vocab, is_eval=False):\n",
    "    \"\"\"\n",
    "    Returns sentences reversed (last word first)\n",
    "    Returns transitions together with the sentences.  \n",
    "    \"\"\"\n",
    "    batch_size = len(mb)\n",
    "    maxlen = max([len(ex.tokens) for ex in mb])\n",
    "\n",
    "    # vocab returns 0 if the word is not there\n",
    "    # NOTE: reversed sequence!\n",
    "    x = [pad([vocab.w2i.get(t, 0) for t in ex.tokens], maxlen)[::-1] for ex in mb]\n",
    "\n",
    "    x = torch.LongTensor(x)\n",
    "    x = x.to(device)\n",
    "\n",
    "    maxlen_t = max([len(ex.transitions) for ex in mb])\n",
    "    transitions = [pad(ex.transitions, maxlen_t, pad_value=2) for ex in mb]\n",
    "    transitions = np.array(transitions)\n",
    "    transitions = transitions.T  # time-major\n",
    "    \n",
    "    if is_eval:\n",
    "        transition_matrices = None\n",
    "    else:\n",
    "        \n",
    "        transition_matrices = [train_transition_matrices[ex.index] for ex in mb]\n",
    "        # print(\"Transition matrix before: \" + str(transition_matrices[0]))\n",
    "        transition_matrices = [np.pad(m, (0,maxlen_t - m.shape[0]), \"constant\", constant_values=(0,-1)) for m in transition_matrices]\n",
    "        transition_matrices = np.transpose(np.array(transition_matrices), (1,2,0))\n",
    "        transition_matrices = torch.FloatTensor(transition_matrices).to(device)\n",
    "        # print(\"Transition matrix after: \" + str(transition_matrices[:,:,0]))\n",
    "    \n",
    "    sublabels = [get_sublabel_of_tree(ex.tree) for ex in mb]\n",
    "\n",
    "    y = [pad(sublabels[i][0], maxlen_t, pad_value=-1) for i in range(len(mb))]\n",
    "    y = torch.LongTensor(np.array(y).T)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    label_depth = [pad(sublabels[i][1], maxlen_t, pad_value=0) for i in range(len(mb))]\n",
    "    label_depth = torch.LongTensor(np.array(label_depth).T)\n",
    "    label_depth = label_depth.to(device)\n",
    "\n",
    "    return (x, transitions), (y, label_depth, transition_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth 12: 1\n",
      "Depth 7: 2\n",
      "Depth 3: 4\n",
      "Depth 0: 37\n",
      "Depth 2: 7\n",
      "Depth 1: 11\n",
      "Depth 6: 2\n",
      "Depth 5: 2\n",
      "Depth 4: 3\n",
      "Depth 11: 1\n",
      "Depth 10: 1\n",
      "Depth 9: 1\n",
      "Depth 8: 1\n",
      "                                                                                                                                         4                                                                                                                                                                \n",
      "                                       __________________________________________________________________________________________________|_____________________________________________________________                                                                                                    \n",
      "                                      |                                                                                                                                                                2                                                                                                  \n",
      "                                      |                                                                                                                                                    ____________|________________________________________________________________________________________________   \n",
      "                                      |                                                                                                                                                   3                                                                                                             | \n",
      "                                      |                                                                        ___________________________________________________________________________|____________                                                                                                 |  \n",
      "                                      |                                                                       |                                                                                        2                                                                                                | \n",
      "                                      |                                                                       |             ___________________________________________________________________________|___________                                                                                     |  \n",
      "                                      |                                                                       |            |                                                                                       3                                                                                    | \n",
      "                                      |                                                                       |            |                    ___________________________________________________________________|____________________                                                                |  \n",
      "                                      4                                                                       |            |                   |                                                                                        2                                                               | \n",
      "                 _____________________|________________                                                       |            |                   |                          ______________________________________________________________|_____                                                          |  \n",
      "                |                                      2                                                      |            |                   |                         |                                                                    2                                                         | \n",
      "                |                                   ___|_______                                               |            |                   |                         |                 ___________________________________________________|___________                                              |  \n",
      "                |                                  |           2                                              |            |                   |                         |                |                                                               2                                             | \n",
      "                |                                  |        ___|____________                                  |            |                   |                         |                |                                    ___________________________|__________________                           |  \n",
      "                |                                  |       |                2                                 |            |                   |                         |                |                                   3                                              2                          | \n",
      "                |                                  |       |             ___|_________________                |            |                   |                         |                |                              _____|______________________            ____________|_____                     |  \n",
      "                4                                  |       |            2                     |               |            |                   |                         |                |                             2                            |          |                  2                    | \n",
      "  ______________|______                            |       |    ________|___                  |               |            |                   |                         |                |             ________________|_____                       |          |             _____|___________         |  \n",
      " |                     4                           |       |   |            2                 |               2            |                   2                         2                |            |                      2                      |          |            2                 |        | \n",
      " |       ______________|______                     |       |   |     _______|___              |            ___|___         |         __________|_______               ___|______          |            |            __________|_____                 |          |     _______|_____            |        |  \n",
      " |      |                     3                    2       |   |    |           2             2           |       2        |        2                  2             2          |         |            |           |                3                3          |    |             2           |        | \n",
      " |      |               ______|_______          ___|___    |   |    |        ___|____      ___|_____      |    ___|___     |     ___|____           ___|____      ___|___       |         |            |           |           _____|___       ______|____      |    |        _____|___        |        |  \n",
      " 2      3              2              2        2       2   2   2    2       2        2    2         2     2   2       2    2    2        2         2        2    2       1      3         2            2           2          2         2     2           2     2    2       2         2       2        2 \n",
      " |      |              |              |        |       |   |   |    |       |        |    |         |     |   |       |    |    |        |         |        |    |       |      |         |            |           |          |         |     |           |     |    |       |         |       |        |  \n",
      "The gorgeously     elaborate     continuation  of      `` The Lord  of     the     Rings  ''     trilogy  is  so     huge that  a      column      of     words can     not adequately describe co-writer/direct Peter     Jackson      's expanded     vision  of J.R.R. Tolkien      's Middle-earth  . \n",
      "                                                                                                                                                                                                       or                                                                                                 \n",
      "\n",
      "[0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1]\n",
      "[0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get all subtrees...\n",
      "Order all subtrees...\n",
      "Make sure that every subtree is unique...\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def get_all_subtrees(tree, depth_dict=None):\n",
    "    if depth_dict is None:\n",
    "        depth_dict = dict()\n",
    "    my_depth = tree.height() - 2\n",
    "    subtree_ex = subtree_to_example(copy.deepcopy(tree))\n",
    "    if my_depth in depth_dict:\n",
    "        depth_dict[my_depth].append(subtree_ex)\n",
    "    else:\n",
    "        depth_dict[my_depth] = [subtree_ex]\n",
    "    if isinstance(tree[0], Tree):\n",
    "        for subtree in tree:\n",
    "            get_all_subtrees(subtree, depth_dict=depth_dict)\n",
    "    return depth_dict\n",
    "\n",
    "def tree_to_token_list(tree, token_list=None):\n",
    "    if token_list is None:\n",
    "        token_list = list()\n",
    "    if isinstance(tree[0], Tree):\n",
    "        for subtree in tree:\n",
    "            tree_to_token_list(subtree, token_list=token_list)\n",
    "    else:\n",
    "        token_list.append(tree[0])\n",
    "    return token_list\n",
    "\n",
    "def tree_to_transition_list(tree, transition_list=None):\n",
    "    if transition_list is None:\n",
    "        transition_list = list()\n",
    "    if isinstance(tree[0], Tree):\n",
    "        for subtree in tree:\n",
    "            tree_to_transition_list(subtree, transition_list=transition_list)\n",
    "        transition_list.append(1)\n",
    "    else:\n",
    "        transition_list.append(0)\n",
    "    return transition_list\n",
    "\n",
    "def subtree_to_example(tree):\n",
    "    return Example(tokens=tree_to_token_list(tree), tree=tree, label=int(tree.label()), transitions=tree_to_transition_list(tree), index=-1, loss=list(), transition_matrix=None)\n",
    "\n",
    "subtree_dict = get_all_subtrees(train_data[1].tree)\n",
    "for key, val in subtree_dict.items():\n",
    "    print(\"Depth \" + str(key) + \": \" + str(len(val)))\n",
    "    \n",
    "print(TreePrettyPrinter(train_data[1].tree))\n",
    "print(train_data[1].transitions)\n",
    "print(tree_to_transition_list(train_data[1].tree))\n",
    "\n",
    "subtrees_train_data = dict()\n",
    "train_data_by_depth = dict()\n",
    "p = Pool()\n",
    "print(\"Get all subtrees...\")\n",
    "subtrees_res = p.map(get_all_subtrees, [ex.tree for ex in train_data])\n",
    "p.close()\n",
    "print(\"Order all subtrees...\")\n",
    "for ex_ind, ex in enumerate(train_data):\n",
    "    subtrees_train_data[ex.index] = subtrees_res[ex_ind]\n",
    "print(\"Make sure that every subtree is unique...\") # This shares loss between examples that have the same tree/tokens and therefore the same input\n",
    "all_subtrees = dict()\n",
    "subtrees_token_lists = dict() # List of strings representing token list (used for checking if tree is already there...)\n",
    "subtrees_frequency = dict()\n",
    "for ex_ind, ex in enumerate(train_data):\n",
    "    for depth, subtrees in subtrees_train_data[ex.index].items():\n",
    "        if depth not in all_subtrees:\n",
    "            all_subtrees[depth] = list()\n",
    "        if depth not in subtrees_token_lists:\n",
    "            subtrees_token_lists[depth] = list()\n",
    "            subtrees_frequency[depth] = list()\n",
    "        \n",
    "        replacement_list = list()\n",
    "        for subtree_index, subtree in enumerate(subtrees):\n",
    "            token_str = \" \".join(subtree.tokens)\n",
    "            if token_str in subtrees_token_lists[depth]:\n",
    "                copy_ind = subtrees_token_lists[depth].index(token_str)\n",
    "                subtrees_frequency[depth][copy_ind] += 1\n",
    "                replacement_list.append((subtree_index, copy_ind))\n",
    "            else:\n",
    "                subtrees_token_lists[depth].append(token_str)\n",
    "                subtrees_frequency[depth].append(1)\n",
    "                all_subtrees[depth].append(subtree)\n",
    "        \n",
    "        for subtree_index, copy_ind in replacement_list:\n",
    "            old_tree = subtrees[subtree_index]\n",
    "            subtrees[subtree_index] = all_subtrees[depth][copy_ind]\n",
    "            del old_tree\n",
    "        \n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start creating transition matrices...\n",
      "Convert them into a dict...\n",
      "Finished\n",
      "(73, 73)\n"
     ]
    }
   ],
   "source": [
    "# Create transition matrices based on our examples\n",
    "\n",
    "TRANSITION_DECAY_FACTOR = 0.5\n",
    "\n",
    "def get_transition_matrix_of_example(ex):\n",
    "    transitions = ex.transitions\n",
    "    sublabels = get_sublabel_of_tree(ex.tree)\n",
    "    \n",
    "    no_trans = len(transitions)\n",
    "    transition_matrix = np.zeros((no_trans, no_trans), dtype=np.float32) - 1\n",
    "    \n",
    "    for node_index in range(no_trans)[::-1]:\n",
    "        zero_counter = 2 if transitions[node_index] == 1 else 0\n",
    "        transition_matrix[node_index][node_index] = 1\n",
    "        for sub_node in range(node_index)[::-1]:\n",
    "            if zero_counter <= 0:\n",
    "                break\n",
    "            if transitions[sub_node] == 1:\n",
    "                zero_counter += 1\n",
    "            elif transitions[sub_node] == 0:\n",
    "                zero_counter -= 1\n",
    "            height_diff = sublabels[1][sub_node] - sublabels[1][node_index] \n",
    "            transition_matrix[sub_node][node_index] = 0 # Node_index is a parent of sub_node\n",
    "            transition_matrix[node_index][sub_node] = TRANSITION_DECAY_FACTOR**height_diff # Sub_node is a child of node_index\n",
    "        # print(transition_matrix)\n",
    "    return transition_matrix\n",
    "\n",
    "if False:\n",
    "    test_example = dev_data[smallest_tree]# Example(tokens=['Does','this','work'], tree=Tree('2', [Tree('2', [Tree('2', ['Does']), Tree('2', ['this'])]), Tree('2', ['work'])]), label=2, transitions=[0,0,1,0,1], index=-1, loss=list(), transition_matrix=None)\n",
    "    print(TreePrettyPrinter(test_example.tree))\n",
    "    trans_matrix = get_transition_matrix_of_example(test_example)\n",
    "    print(trans_matrix)\n",
    "\n",
    "train_transition_matrices = dict()\n",
    "def create_transition_matrices():\n",
    "    global train_transition_matrices\n",
    "    p = Pool()\n",
    "    print(\"Start creating transition matrices...\")\n",
    "    list_transition_matrices = p.map(get_transition_matrix_of_example, train_data)\n",
    "    p.close()\n",
    "    print(\"Convert them into a dict...\")\n",
    "    train_transition_matrices = dict()\n",
    "    for d_index, d in enumerate(train_data):\n",
    "        train_transition_matrices[d.index] = list_transition_matrices[d_index]\n",
    "    train_transition_matrices[-1] = np.array([[-1]], dtype=np.float32) # Default transition matrix where every node is independent\n",
    "    print(\"Finished\")\n",
    "    print(train_transition_matrices[train_data[1].index].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function\n",
    "for d_index, d in enumerate(train_data):\n",
    "    assert train_transition_matrices[d.index].shape[0] == len(d.transitions) and train_transition_matrices[d.index].shape[1] == len(d.transitions)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0: 1\n",
      "Index 1: 2\n",
      "                                                                                               3                                                                                                                                         \n",
      "      _________________________________________________________________________________________|____                                                                                                                                      \n",
      "     |                                                                                              4                                                                                                                                    \n",
      "     |                      ________________________________________________________________________|__________________________________________________________________________________________________________________________________   \n",
      "     |                     3                                                                                                                                                                                                           | \n",
      "     |         ____________|___________                                                                                                                                                                                                |  \n",
      "     |        |                        4                                                                                                                                                                                               | \n",
      "     |        |      __________________|_________________________________                                                                                                                                                              |  \n",
      "     |        |     |                                                    2                                                                                                                                                             | \n",
      "     |        |     |                                    ________________|_________________                                                                                                                                            |  \n",
      "     |        |     |                                   |                                  3                                                                                                                                           | \n",
      "     |        |     |                                   |                              ____|___                                                                                                                                        |  \n",
      "     |        |     |                                   |                             |        3                                                                                                                                       | \n",
      "     |        |     |                                   |                             |     ___|_________                                                                                                                              |  \n",
      "     |        |     |                                   |                             |    |             3                                                                                                                             | \n",
      "     |        |     |                                   |                             |    |    _________|_________________                                                                                                            |  \n",
      "     |        |     |                                   2                             |    |   |                           3                                                                                                           | \n",
      "     |        |     |                          _________|_________________________    |    |   |     ______________________|________________________________                                                                           |  \n",
      "     |        |     |                         2                                   |   |    |   |    |                                                       3                                                                          | \n",
      "     |        |     |           ______________|_______________________________    |   |    |   |    |     __________________________________________________|______________                                                            |  \n",
      "     |        |     |          2                                              |   |   |    |   |    |    |                                                                 4                                                           | \n",
      "     |        |     |       ___|___                                           |   |   |    |   |    |    |                  _______________________________________________|________________                                           |  \n",
      "     |        |     |      |       2                                          |   |   |    |   |    |    |                 |                                                                2                                          | \n",
      "     |        |     |      |    ___|___                                       |   |   |    |   |    |    |                 |                           _____________________________________|_______                                   |  \n",
      "     |        |     |      |   |       2                                      |   |   |    |   |    |    |                 |                          |                                             2                                  | \n",
      "     |        |     |      |   |    ___|________________                      |   |   |    |   |    |    |                 |                          |                                      _______|________________________          |  \n",
      "     |        |     |      |   |   |                    2                     |   |   |    |   |    |    |                 |                          |                                     2                                |         | \n",
      "     |        |     |      |   |   |    ________________|___                  |   |   |    |   |    |    |                 |                          |                               ______|_____________________           |         |  \n",
      "     |        |     |      |   |   |   |                    2                 |   |   |    |   |    |    |                 3                          |                              2                            |          |         | \n",
      "     |        |     |      |   |   |   |             _______|___              |   |   |    |   |    |    |    _____________|_____                     |                     _________|______________              |          |         |  \n",
      "     |        |     |      |   |   |   |            |           2             |   |   |    |   |    |    |   |                   3                    |                    2                        2             |          |         | \n",
      "     |        |     |      |   |   |   |            |        ___|___          |   |   |    |   |    |    |   |         __________|________            |            ________|_________        _______|___          |          |         |  \n",
      "     2        |     |      |   |   |   |            2       |       2         |   |   |    |   |    |    |   |        3                   2           |           1                  |      |           2         |          2         | \n",
      "  ___|___     |     |      |   |   |   |       _____|___    |    ___|____     |   |   |    |   |    |    |   |     ___|____           ____|_____      |      _____|________          |      |        ___|____     |     _____|____     |  \n",
      " 2       2    2     2      2   2   2   2      2         2   3   2        2    2   2   2    2   2    2    2   2    2        3         2          3     2     2              2         2      2       2        2    2    2          2    2 \n",
      " |       |    |     |      |   |   |   |      |         |   |   |        |    |   |   |    |   |    |    |   |    |        |         |          |     |     |              |         |      |       |        |    |    |          |    |  \n",
      "The     Rock  is destined  to  be the 21st Century      's new  ``     Conan  '' and that  he  's going  to make  a      splash     even     greater than Arnold     Schwarzenegger  ,  Jean-Claud Van     Damme  or Steven     Segal  . \n",
      "\n",
      "                                                                                                                                         4                                                                                                                                                                \n",
      "                                       __________________________________________________________________________________________________|_____________________________________________________________                                                                                                    \n",
      "                                      |                                                                                                                                                                2                                                                                                  \n",
      "                                      |                                                                                                                                                    ____________|________________________________________________________________________________________________   \n",
      "                                      |                                                                                                                                                   3                                                                                                             | \n",
      "                                      |                                                                        ___________________________________________________________________________|____________                                                                                                 |  \n",
      "                                      |                                                                       |                                                                                        2                                                                                                | \n",
      "                                      |                                                                       |             ___________________________________________________________________________|___________                                                                                     |  \n",
      "                                      |                                                                       |            |                                                                                       3                                                                                    | \n",
      "                                      |                                                                       |            |                    ___________________________________________________________________|____________________                                                                |  \n",
      "                                      4                                                                       |            |                   |                                                                                        2                                                               | \n",
      "                 _____________________|________________                                                       |            |                   |                          ______________________________________________________________|_____                                                          |  \n",
      "                |                                      2                                                      |            |                   |                         |                                                                    2                                                         | \n",
      "                |                                   ___|_______                                               |            |                   |                         |                 ___________________________________________________|___________                                              |  \n",
      "                |                                  |           2                                              |            |                   |                         |                |                                                               2                                             | \n",
      "                |                                  |        ___|____________                                  |            |                   |                         |                |                                    ___________________________|__________________                           |  \n",
      "                |                                  |       |                2                                 |            |                   |                         |                |                                   3                                              2                          | \n",
      "                |                                  |       |             ___|_________________                |            |                   |                         |                |                              _____|______________________            ____________|_____                     |  \n",
      "                4                                  |       |            2                     |               |            |                   |                         |                |                             2                            |          |                  2                    | \n",
      "  ______________|______                            |       |    ________|___                  |               |            |                   |                         |                |             ________________|_____                       |          |             _____|___________         |  \n",
      " |                     4                           |       |   |            2                 |               2            |                   2                         2                |            |                      2                      |          |            2                 |        | \n",
      " |       ______________|______                     |       |   |     _______|___              |            ___|___         |         __________|_______               ___|______          |            |            __________|_____                 |          |     _______|_____            |        |  \n",
      " |      |                     3                    2       |   |    |           2             2           |       2        |        2                  2             2          |         |            |           |                3                3          |    |             2           |        | \n",
      " |      |               ______|_______          ___|___    |   |    |        ___|____      ___|_____      |    ___|___     |     ___|____           ___|____      ___|___       |         |            |           |           _____|___       ______|____      |    |        _____|___        |        |  \n",
      " 2      3              2              2        2       2   2   2    2       2        2    2         2     2   2       2    2    2        2         2        2    2       1      3         2            2           2          2         2     2           2     2    2       2         2       2        2 \n",
      " |      |              |              |        |       |   |   |    |       |        |    |         |     |   |       |    |    |        |         |        |    |       |      |         |            |           |          |         |     |           |     |    |       |         |       |        |  \n",
      "The gorgeously     elaborate     continuation  of      `` The Lord  of     the     Rings  ''     trilogy  is  so     huge that  a      column      of     words can     not adequately describe co-writer/direct Peter     Jackson      's expanded     vision  of J.R.R. Tolkien      's Middle-earth  . \n",
      "                                                                                                                                                                                                       or                                                                                                 \n",
      "\n",
      "Last single word subtree 0: Example(tokens=['.'], tree=Tree('2', ['.']), label=2, transitions=[0], index=-1, loss=[], transition_matrix=None)\n",
      "Last single word subtree 1: Example(tokens=['.'], tree=Tree('2', ['.']), label=2, transitions=[0], index=-1, loss=[], transition_matrix=None)\n",
      "Update loss...\n",
      "Last single word subtree 0: Example(tokens=['.'], tree=Tree('2', ['.']), label=2, transitions=[0], index=-1, loss=[0.2], transition_matrix=None)\n",
      "Last single word subtree 1: Example(tokens=['.'], tree=Tree('2', ['.']), label=2, transitions=[0], index=-1, loss=[0.2], transition_matrix=None)\n",
      "Last single word subtree 0: Example(tokens=['Segal'], tree=Tree('2', ['Segal']), label=2, transitions=[0], index=-1, loss=[], transition_matrix=None)\n",
      "Last single word subtree 1: Example(tokens=['Middle-earth'], tree=Tree('2', ['Middle-earth']), label=2, transitions=[0], index=-1, loss=[], transition_matrix=None)\n",
      "Revert update...\n",
      "Last single word subtree 0: Example(tokens=['.'], tree=Tree('2', ['.']), label=2, transitions=[0], index=-1, loss=[], transition_matrix=None)\n",
      "Last single word subtree 1: Example(tokens=['.'], tree=Tree('2', ['.']), label=2, transitions=[0], index=-1, loss=[], transition_matrix=None)\n"
     ]
    }
   ],
   "source": [
    "# Test of previous function\n",
    "print(\"Index 0: \" + str(train_data[0].index))\n",
    "print(\"Index 1: \" + str(train_data[1].index))\n",
    "# print(\"Subtrees: \" + str(subtrees_train_data[train_data[1].index]))\n",
    "print(TreePrettyPrinter(train_data[0].tree))\n",
    "print(TreePrettyPrinter(train_data[1].tree))\n",
    "\n",
    "print(\"Last single word subtree 0: \" + str(subtrees_train_data[train_data[0].index][0][-1]))\n",
    "print(\"Last single word subtree 1: \" + str(subtrees_train_data[train_data[1].index][0][-1]))\n",
    "\n",
    "print(\"Update loss...\")\n",
    "subtrees_train_data[train_data[0].index][0][-1].loss.append(0.2)\n",
    "\n",
    "print(\"Last single word subtree 0: \" + str(subtrees_train_data[train_data[0].index][0][-1]))\n",
    "print(\"Last single word subtree 1: \" + str(subtrees_train_data[train_data[1].index][0][-1]))\n",
    "print(\"Last single word subtree 0: \" + str(subtrees_train_data[train_data[0].index][0][-2]))\n",
    "print(\"Last single word subtree 1: \" + str(subtrees_train_data[train_data[1].index][0][-2]))\n",
    "\n",
    "print(\"Revert update...\")\n",
    "del subtrees_train_data[train_data[1].index][0][-1].loss[0]\n",
    "\n",
    "print(\"Last single word subtree 0: \" + str(subtrees_train_data[train_data[0].index][0][-1]))\n",
    "print(\"Last single word subtree 1: \" + str(subtrees_train_data[train_data[1].index][0][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Depth 0\n",
      "==================================================\n",
      "Number of trees: 18278\n",
      "Distribution over classes:\n",
      " - Label 0: 1.21%\n",
      " - Label 1: 11.33%\n",
      " - Label 2: 74.22%\n",
      " - Label 3: 11.80%\n",
      " - Label 4: 1.44%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 1\n",
      "==================================================\n",
      "Number of trees: 26591\n",
      "Distribution over classes:\n",
      " - Label 0: 1.57%\n",
      " - Label 1: 11.17%\n",
      " - Label 2: 68.10%\n",
      " - Label 3: 16.26%\n",
      " - Label 4: 2.90%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 2\n",
      "==================================================\n",
      "Number of trees: 23109\n",
      "Distribution over classes:\n",
      " - Label 0: 2.46%\n",
      " - Label 1: 13.08%\n",
      " - Label 2: 60.47%\n",
      " - Label 3: 19.67%\n",
      " - Label 4: 4.32%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 3\n",
      "==================================================\n",
      "Number of trees: 17965\n",
      "Distribution over classes:\n",
      " - Label 0: 3.59%\n",
      " - Label 1: 15.61%\n",
      " - Label 2: 53.65%\n",
      " - Label 3: 21.76%\n",
      " - Label 4: 5.39%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 4\n",
      "==================================================\n",
      "Number of trees: 14235\n",
      "Distribution over classes:\n",
      " - Label 0: 4.24%\n",
      " - Label 1: 18.10%\n",
      " - Label 2: 47.92%\n",
      " - Label 3: 23.27%\n",
      " - Label 4: 6.48%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 5\n",
      "==================================================\n",
      "Number of trees: 11633\n",
      "Distribution over classes:\n",
      " - Label 0: 5.05%\n",
      " - Label 1: 20.85%\n",
      " - Label 2: 41.98%\n",
      " - Label 3: 24.94%\n",
      " - Label 4: 7.18%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 6\n",
      "==================================================\n",
      "Number of trees: 9756\n",
      "Distribution over classes:\n",
      " - Label 0: 6.29%\n",
      " - Label 1: 22.32%\n",
      " - Label 2: 38.28%\n",
      " - Label 3: 25.08%\n",
      " - Label 4: 8.02%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 7\n",
      "==================================================\n",
      "Number of trees: 8222\n",
      "Distribution over classes:\n",
      " - Label 0: 7.10%\n",
      " - Label 1: 24.42%\n",
      " - Label 2: 34.03%\n",
      " - Label 3: 25.63%\n",
      " - Label 4: 8.82%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 8\n",
      "==================================================\n",
      "Number of trees: 6887\n",
      "Distribution over classes:\n",
      " - Label 0: 8.29%\n",
      " - Label 1: 25.77%\n",
      " - Label 2: 30.87%\n",
      " - Label 3: 25.79%\n",
      " - Label 4: 9.28%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 9\n",
      "==================================================\n",
      "Number of trees: 5684\n",
      "Distribution over classes:\n",
      " - Label 0: 8.69%\n",
      " - Label 1: 26.14%\n",
      " - Label 2: 29.66%\n",
      " - Label 3: 25.76%\n",
      " - Label 4: 9.75%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 10\n",
      "==================================================\n",
      "Number of trees: 4532\n",
      "Distribution over classes:\n",
      " - Label 0: 9.71%\n",
      " - Label 1: 26.37%\n",
      " - Label 2: 28.24%\n",
      " - Label 3: 25.33%\n",
      " - Label 4: 10.35%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 11\n",
      "==================================================\n",
      "Number of trees: 3551\n",
      "Distribution over classes:\n",
      " - Label 0: 10.28%\n",
      " - Label 1: 26.13%\n",
      " - Label 2: 26.95%\n",
      " - Label 3: 25.91%\n",
      " - Label 4: 10.73%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 12\n",
      "==================================================\n",
      "Number of trees: 2693\n",
      "Distribution over classes:\n",
      " - Label 0: 11.10%\n",
      " - Label 1: 24.92%\n",
      " - Label 2: 26.03%\n",
      " - Label 3: 27.22%\n",
      " - Label 4: 10.73%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 13\n",
      "==================================================\n",
      "Number of trees: 1958\n",
      "Distribution over classes:\n",
      " - Label 0: 11.80%\n",
      " - Label 1: 27.78%\n",
      " - Label 2: 23.65%\n",
      " - Label 3: 25.59%\n",
      " - Label 4: 11.18%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 14\n",
      "==================================================\n",
      "Number of trees: 1412\n",
      "Distribution over classes:\n",
      " - Label 0: 13.03%\n",
      " - Label 1: 25.42%\n",
      " - Label 2: 23.37%\n",
      " - Label 3: 25.28%\n",
      " - Label 4: 12.89%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 15\n",
      "==================================================\n",
      "Number of trees: 979\n",
      "Distribution over classes:\n",
      " - Label 0: 14.81%\n",
      " - Label 1: 26.05%\n",
      " - Label 2: 21.25%\n",
      " - Label 3: 27.17%\n",
      " - Label 4: 10.73%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 16\n",
      "==================================================\n",
      "Number of trees: 642\n",
      "Distribution over classes:\n",
      " - Label 0: 13.71%\n",
      " - Label 1: 26.32%\n",
      " - Label 2: 22.12%\n",
      " - Label 3: 26.79%\n",
      " - Label 4: 11.06%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 17\n",
      "==================================================\n",
      "Number of trees: 431\n",
      "Distribution over classes:\n",
      " - Label 0: 15.55%\n",
      " - Label 1: 25.75%\n",
      " - Label 2: 21.35%\n",
      " - Label 3: 25.06%\n",
      " - Label 4: 12.30%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 18\n",
      "==================================================\n",
      "Number of trees: 275\n",
      "Distribution over classes:\n",
      " - Label 0: 13.09%\n",
      " - Label 1: 25.82%\n",
      " - Label 2: 23.64%\n",
      " - Label 3: 27.64%\n",
      " - Label 4: 9.82%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 19\n",
      "==================================================\n",
      "Number of trees: 177\n",
      "Distribution over classes:\n",
      " - Label 0: 16.95%\n",
      " - Label 1: 20.34%\n",
      " - Label 2: 17.51%\n",
      " - Label 3: 31.64%\n",
      " - Label 4: 13.56%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 20\n",
      "==================================================\n",
      "Number of trees: 109\n",
      "Distribution over classes:\n",
      " - Label 0: 14.68%\n",
      " - Label 1: 21.10%\n",
      " - Label 2: 18.35%\n",
      " - Label 3: 32.11%\n",
      " - Label 4: 13.76%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 21\n",
      "==================================================\n",
      "Number of trees: 60\n",
      "Distribution over classes:\n",
      " - Label 0: 18.33%\n",
      " - Label 1: 16.67%\n",
      " - Label 2: 25.00%\n",
      " - Label 3: 25.00%\n",
      " - Label 4: 15.00%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 22\n",
      "==================================================\n",
      "Number of trees: 42\n",
      "Distribution over classes:\n",
      " - Label 0: 21.43%\n",
      " - Label 1: 21.43%\n",
      " - Label 2: 23.81%\n",
      " - Label 3: 23.81%\n",
      " - Label 4: 9.52%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 23\n",
      "==================================================\n",
      "Number of trees: 24\n",
      "Distribution over classes:\n",
      " - Label 0: 8.33%\n",
      " - Label 1: 37.50%\n",
      " - Label 2: 16.67%\n",
      " - Label 3: 33.33%\n",
      " - Label 4: 4.17%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 24\n",
      "==================================================\n",
      "Number of trees: 16\n",
      "Distribution over classes:\n",
      " - Label 0: 12.50%\n",
      " - Label 1: 18.75%\n",
      " - Label 2: 18.75%\n",
      " - Label 3: 37.50%\n",
      " - Label 4: 12.50%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 25\n",
      "==================================================\n",
      "Number of trees: 7\n",
      "Distribution over classes:\n",
      " - Label 0: 14.29%\n",
      " - Label 1: 28.57%\n",
      " - Label 2: 14.29%\n",
      " - Label 3: 42.86%\n",
      " - Label 4: 0.00%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 26\n",
      "==================================================\n",
      "Number of trees: 5\n",
      "Distribution over classes:\n",
      " - Label 0: 0.00%\n",
      " - Label 1: 20.00%\n",
      " - Label 2: 20.00%\n",
      " - Label 3: 60.00%\n",
      " - Label 4: 0.00%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 27\n",
      "==================================================\n",
      "Number of trees: 2\n",
      "Distribution over classes:\n",
      " - Label 0: 0.00%\n",
      " - Label 1: 0.00%\n",
      " - Label 2: 50.00%\n",
      " - Label 3: 50.00%\n",
      " - Label 4: 0.00%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 28\n",
      "==================================================\n",
      "Number of trees: 1\n",
      "Distribution over classes:\n",
      " - Label 0: 0.00%\n",
      " - Label 1: 0.00%\n",
      " - Label 2: 0.00%\n",
      " - Label 3: 100.00%\n",
      " - Label 4: 0.00%\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Depth 29\n",
      "==================================================\n",
      "Number of trees: 1\n",
      "Distribution over classes:\n",
      " - Label 0: 0.00%\n",
      " - Label 1: 0.00%\n",
      " - Label 2: 0.00%\n",
      " - Label 3: 100.00%\n",
      " - Label 4: 0.00%\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEWCAYAAACufwpNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xd8HNW5//HPo24VV8kFd2zZdIyRS2IwveVCDAQChNDB3AC54ZJfAqkQSAIpQOAGCBAIkEDoxSQ0hwCmGstgTDEu2AZXWe6ybFnt+f0xI7xWVNbSrla7+r5fr31p9+yU58yu5tmZc+aMuTsiIiKxkJboAEREJHUoqYiISMwoqYiISMwoqYiISMwoqYiISMwoqYiISMwoqSQZM7vPzH6ZoHWbmf3FzDaY2buJiCEWzMzNbGSMlznJzBaa2RYzOzGWyw6Xf66ZvRHxeouZ7R4+72Zmz5rZJjN7LCz7pZmtNbPVsY4lUcxsWPjZZSQ6luaY2VIzOzJOy37VzC6Mx7JjSUmlncIvUZmZ5UWUXWhmryYwrHg5CDgKGOTu4xMdTCdzLfBHd89396fjvbJwPYvDl6cA/YA+7n6qmQ0Gvg/s5e794x1LY4na+ZnZNWb2t45ebzwkc12UVGIjA/heooPYVWaWvouzDAWWuntlPOJJckOBj9syYwx+eQ8FFrh7bcTrde6+pg2xmJml5H4hlevWqbi7Hu14AEuBq4D1QM+w7ELg1fD5MMCBjIh5XgUuDJ+fC7wJ3AxsBBYDXw3LlwFrgHMi5r0P+BMwHagAXgOGRry/R/jeemA+8M1G894BPAdUAkc2UZ/dgGnh/IuAi8LyC4AqoA7YAvyime1xPjAP2AC82BBbWKe1wODw9f5hffcIX18FfBbW6RPgpIhlxnobOTAyfJ4N/B74AigL5+sWvlcI/CNc53rgdSCtiTp/BtQD28Jtk93cdgynvwZ4HPgbsLnhu9BomX3C+TcD7wLXAW80rgPwC6AaqAnXfXEYR334+r5w+onAW2FdPgAObfR9/FW4jbeFy+0B3AOsAlYAvwTSIz6PN8LttgFYAhwXvver8DtSFa7/j03ULSes+7ownllAv4j/pyMbbau/NfpfmgqsDGP7fvjesY22wwdtqVtL3+Fmvu9nAZ+HdflJZPwEP9obvtfrgEeB3u2sy3VhXSqAl4DC1rZph+8TE7HSVHo0fImAJ4FfhmW7mlRqgfOA9PAL/gVwG8HO6ejwC5QfTn9f+Hpy+P4thDsbII9gJ3sewdHTWIId+d4R824CJoVf+Jwm6vMacHv4JR0DlANHRMT6Rgvb4kSCHeie4fp/CrwV8f6vgH8D3YC5wGUR751KsCNOA04jSHoDYr2Nwvcjk8ofCHbevYEC4Fng+vC96wmSTGb4OBiwlr4HUW7Hawh2GCeG9e3WxPIeJtgJ5QH7EOz8mqvDNYQ73vD1ocDyiNcDCXY2XwvXd1T4uiji+/gFsHf4uWUCTwN3huvvS5DYLo74PGqAi8LP4zsEO0Zr/P1uZltdHG7n3HD+A4HuzWzHL+vGjv+lv4dx7Rtu1yOb2g5trFuL3+FGy96LYKff8D27ieB72hDP5cA7wKDw/TuBv7ezLp8Bowj+h14Fbmhtm3b4PjERK02lBzuSyj4EO+widj2pLIx4b99w+n4RZeuAMeHz+4CHI97LJ/hlOJhgZ/x6o/juBK6OmPeBFuoyOFxWQUTZ9ez4tXsuLSeV54ELIl6nAVvZcbSSCcwGPgReoJkddDjtHGBKrLdR+LrhV74RJK8REdN+BVgSPr8WeIZw5x3N9yDK7XgNMKOFZaUT7LT3iCj7NW1PKlcCf220jhcJj+4Ivo/XRrzXD9hORLIDzgBeifg8FkW8lxvG07/x97uZ+p1PcNS0X0vbsXHd2PG/FLldfgvc09R2aGPdWvwON1r2zxt9z/IIjjAavgfzCH9IhK8HhJ9rRjvq8tOI15cAL7S2TTv6ofOLMeLuHxGcKrmqDbOXRTzfFi6vcVl+xOtlEevdQnCKZTeCc+kTzGxjwwM4E+jf1LxN2A1Y7+4VEWWfE/zSjcZQ4JaIda8n2HEPDGOtIdjh7wPc6OF/A4CZnW1mcyLm3Yfg9FODWG2jSEUEO8TZEet9ISwH+B3Br9aXzGyxmUX72UazHVv6HIoIdjyR03we5bqbMhQ4tdH34iCCnVxT8Qwl+AGwKmL6Owl+1Tf4sleZu28Nn0Zu/5b8lSCpPWxmK83st2aWuQv1abxdGn+uLU3fWt1a/A43shs7f88qCX7cRK7rqYhlzSP4sdGvHXWJ7M23lR3bvL3bNGaUVGLraoJTApFfwIZG7dyIsvb2yBnc8MTM8glO3awk+IK+5u49Ix757v6diHmd5q0EeptZQUTZEIJTL9FYRnAaIXL93dz9rTDWgQTb6C/AjWaWHZYPBe4GLiPowdQT+Ijgn7mtmttGkdYSJKO9I+Lt4e75AO5e4e7fd/fdgROAK8zsiCjWHc12bOlzKCc4jTI4omxIFOttzjKCI5XIzyXP3W9oJp5lBL/mCyOm7+7ue0e5vpbqhrvXuPsv3H0vgrax44Gzw7craf1/pfF2afhcm1vvrtStxe9wI6vY+XuWS9AWFrmu4xotK8fdI78Hu1qXpivY8jbtUEoqMeTui4BHgP+JKCsn2Jl828zSzex8YEQ7V/U1MzvIzLIIGu5muvsygiOlUWZ2lpllho9xZrZnlPEvIziEvt7McsxsP4IG+gejjOtPwI/MbG8AM+thZqeGz43gKOWecJmrwtghOG3gBDtTzOw8giOV9mhuG33J3esJktnNZtY3XPdAMzsmfH68mY0MY99M8CuzrrUVt3c7unsdQRvdNWaWa2Z7AedEV+0m/Q04wcyOCb+DOWZ2qJkNamb9qwgagW80s+5mlmZmI8zskCjXVwbs3tybZnaYme0b9j7cTHBKqGG7zgFOD7+7JQTdpRv7Wbhd9iZoZ3skYr3DWurhFUXdmv0ON+Fx4PiI79m17LxP/RPwq/BHE2ZWZGZTYlWXSK1s0w6lpBJ71xLsJCNdBPyA4NB4b4IdTns8RPCLfz1Bg9yZEPyyJmi0Pp3gF89q4DcEjYTROoPgfO9K4CmC9pjp0czo7k+F63vYzDYTHG0cF779PwSH/T8LT3udB5xnZge7+yfAjcDbBP9M+xL0cGmPJrdRE64kOMX1Thjzv4DR4XvF4estYWy3u/urUa6/zdsxdBnBqY3VBMn4L7sw707CJDcF+DFB4l5G8H1s6f//bCCLoCfeBoId6IAWpo90C3CKBRfJ3trE+/3D5W0mOCX0GkHiA/gZwY+uDQQ92x5qYv7XCD6zl4Hfu/tLYflj4d91ZvZeC/E1W7dWvsM7cfePgUvDGFeFy1oeMcktBJ1AXjKzCoJG+wkxrkuDlrZph2rorSGSMszsPoKG6p8mOhaRppjZMIKu2Jm+4/qilKAjFRERiRklFRERiRmd/hIRkZjRkYqIiMRMpx1COl4KCwt92LBhiQ5DRCSpzJ49e627F7U2XZdLKsOGDaO0tDTRYYiIJBUzi2pUB53+EhGRmFFSERGRmFFSERGRmFFSERGRmFFSERGRmFFSERGRmFFSERGRmFFSibHK7bU8/O4X1NVr+BsR6XqUVGLs2Q9WctWTH/Lix6tbn1hEJMUoqcTY/LLgtuQPzfwiwZGIiHQ8JZUYW1i2BYA3Fq1lydrKVqYWEUktSioxNr+sgsmjikhPM/7+ro5WRKRrUVKJoQ2V1ZRXbOfgkYUcvVc/HitdRlVNXaLDEhHpMEoqMbQgbE8p7pfPmROGsmFrDS98pAZ7Eek6lFRiaMGaoD1ldP8CvjqiD0P75KrBXkS6FCWVGFqwuoKC7Az6d88hLc341vghvLt0/ZdHMCIiqU5JJYYWlFUwqn8BZgbAKQcOIis9TUcrItJlxC2pmNlgM3vFzOaZ2cdm9r2w/BozW2Fmc8LH1yLm+ZGZLTKz+WZ2TET5sWHZIjO7KqJ8uJnNNLOFZvaImWXFqz6tcfcgqfTL/7KsT342x+3bnyfeW87W6tpEhSYi0mHieaRSC3zf3fcEJgKXmtle4Xs3u/uY8PEcQPje6cDewLHA7WaWbmbpwG3AccBewBkRy/lNuKxiYANwQRzr06K1W6rZsLWGUf0Kdir/1vghVFTV8o8PViUoMhGRjhO3pOLuq9z9vfB5BTAPGNjCLFOAh919u7svARYB48PHIndf7O7VwMPAFAvOMR0OPB7Ofz9wYnxq07qGdpPGSWX88N6M7JvPg7pmRUS6gA5pUzGzYcABwMyw6DIzm2tm95pZr7BsILAsYrblYVlz5X2Aje5e26i8qfVPNbNSMystLy+PQY3+U2R34kbr5swJQ/hg2UY+WrEpLusWEeks4p5UzCwfeAK43N03A3cAI4AxwCrgxoZJm5jd21D+n4Xud7l7ibuXFBUV7WINorOgrIJeuZkU5Wf/x3snHzCInMw0HlSDvYikuLgmFTPLJEgoD7r7kwDuXubude5eD9xNcHoLgiONwRGzDwJWtlC+FuhpZhmNyhNiQdkWivvt6PkVqUduJifstxvPzFlBRVVNAqITEekY8ez9ZcA9wDx3vymifEDEZCcBH4XPpwGnm1m2mQ0HioF3gVlAcdjTK4ugMX+auzvwCnBKOP85wDPxqk9L3J0FqysY3ag9JdK3Jgxha3UdT89JWN4TEYm7eB6pTALOAg5v1H34t2b2oZnNBQ4D/hfA3T8GHgU+AV4ALg2PaGqBy4AXCRr7Hw2nBbgSuMLMFhG0sdwTx/o0a/XmKiq21+7UnbixMYN7steA7jw08wuCfCgiknoyWp+kbdz9DZpu93iuhXl+BfyqifLnmprP3Rez4/RZwsxf3XTPr0hmxpkTh/CTpz7i/WUbGTukV7PTiogkK11RHwMN91BpKakATBkzkLysdB58Rw32IpKalFRiYEFZBUUF2fTKa/mC/vzsDKYcMJB/zF3Jxq3VHRSdiEjHUVKJgcbDs7TkW+OHsL22nifeWxHnqEREOp6SSjvV1zsL12xp9dRXg30G9mDM4J48NPNzNdiLSMpRUmmnFRu3sbW6LuqkAnDmhCF8Vl7JzCXr4xiZiEjHU1Jpp+bG/GrJ8fvtRkFOhq6wF5GUo6TSTvObGfOrJd2y0vnG2EG88NEq1m7ZHq/QREQ6nJJKOy0s28JuPXLonpO5S/OdOWEINXXOY6XL4xSZiEjHU1Jpp/mrKyjehVNfDYr7FTBmcE9e+Ej3WRGR1KGk0g519c6i8i2M7r/rSQXg0NFFzF2xiQ2VumZFRFKDkko7fL6ukuraeor7Rt+eEung4iLc4c3P1sY4MhGRxFBSaYcF4fAsbT1S2X9QDwpyMnh9gZKKiKQGJZV2aOhOPLKNRyoZ6WlMGlHIjIXluhBSRFKCkko7LCirYEjvXHKz2j7Y8+RRRazaVMVn5VtiGJmISGIoqbTDroz51ZyDiwsBmKFTYCKSApRU2qi6tp7F5ZW7dCV9Uwb3zmV4YR6vLyyPUWQiIomjpNJGS9dVUlvv7U4qAJOLC3ln8Xq219bFIDIRkcRRUmmjBW0YnqU5BxcXsa2mjtlLN7R7WSIiiaSk0kYLVleQZjCiqP1JZeKIPmSkGTMWql1FRJKbkkobLSjbwrA+eeRkprd7WfnZGYwd2kvtKiKS9JRU2ijo+dX+9pQGh4wq4uOVmymv0KjFIpK8lFTaoKqmjqXrKtvdnThSQ9fiNxfpFJiIJC8llTZYXF5JvcOoNg7P0pS9d+tBr9xMZugUmIgkMSWVNmjL3R5bk55mTBpZyOsL12rIFhFJWkoqbbCgrILMdGNYn7yYLnfyqCLKK7bz6eqKmC5XRKSjKKm0wYKyCoYX5pGVEdvN19Cuol5gIpKslFTaYEHZlpie+mowoEc3ivvm87quVxGRJKWksou2VtfyxfqtcUkqEFxdP3PJeqpqNGSLiCSfuCUVMxtsZq+Y2Twz+9jMvheW9zaz6Wa2MPzbKyw3M7vVzBaZ2VwzGxuxrHPC6Rea2TkR5Qea2YfhPLeamcWrPg0WrQmGqI9XUpk8qpDq2npmLlkfl+WLiMRTPI9UaoHvu/uewETgUjPbC7gKeNndi4GXw9cAxwHF4WMqcAcESQi4GpgAjAeubkhE4TRTI+Y7No71AWD+6oaeX7G7RiXShOF9yEpP4/UFalcRkeQTt6Ti7qvc/b3weQUwDxgITAHuDye7HzgxfD4FeMAD7wA9zWwAcAww3d3Xu/sGYDpwbPhed3d/24M+uA9ELCtuFq7ZQlZGGkNj3POrQbesdMYN76V2FRFJSq0mFTObZGZ54fNvm9lNZjZ0V1ZiZsOAA4CZQD93XwVB4gH6hpMNBJZFzLY8LGupfHkT5U2tf6qZlZpZaXl5+44A5q+uYGRRPulp8TvTdnBxEfPLKijbXBW3dYiIxEM0Ryp3AFvNbH/gh8DnBEcFUTGzfOAJ4HJ339zSpE2UeRvK/7PQ/S53L3H3kqKiotZCbtHCsgpGx/BK+qZMLg5inKFTYCKSZKJJKrXh6aUpwC3ufgsQ1V7VzDIJEsqD7v5kWFwWnroi/LsmLF8ODI6YfRCwspXyQU2Ux83mqhpWbqqKyT1UWrJH/wIK87N1CkxEkk40SaXCzH4EnAX808zSgczWZgp7Yt0DzHP3myLemgY09OA6B3gmovzssBfYRGBTeHrsReBoM+sVNtAfDbwYvldhZhPDdZ0dsay4WFgW9PwaHaeeXw3S0oyDiwt5Y9Fa6us1ZIuIJI9oksppwHbgfHdfTdBu8bso5ptEkIgON7M54eNrwA3AUWa2EDgqfA3wHLAYWATcDVwC4O7rgeuAWeHj2rAM4DvAn8N5PgOejyKuNovHmF/NmTyqkPWV1XyyqqUzhiIinUtGaxO4+2oze4Kgyy7AWuCpKOZ7g6bbPQCOaGJ6By5tZln3Avc2UV4K7NNaLLGyoKyC3Kx0BvbsFvd1TRoZDNny2oJy9hnYI+7rExGJhWh6f10EPA7cGRYNBJ6OZ1Cd1YKyCor75pMWx55fDfoW5LDngO4aB0xEkko0p78uJTiVtRnA3ReyoxtwlxKvMb+aM7m4kNmfb6Bye22HrVNEpD2iSSrb3b264YWZZdBM191UtqGymvKK7R2bVEYVUVPnzFyyrsPWKSLSHtEkldfM7MdANzM7CngMeDa+YXU+DY308e5OHOnAob3IyUxjxgJ1LRaR5BBNUrkKKAc+BC4m6KX103gG1Rk1JJV4X/gYKScznQnD++gWwyKSNFpNKu5e7+53u/up7n5K+LzLnf5aULaFguwM+nfP6dD1HlxcyOLySpZv2Nqh6xURaYtmk0o4pPzc5h4dGWRnsKCsglH9C+iA0fV3csioYMiWN3R1vYgkgZauUzm+w6JIAoX52Qzundvh6x3ZN5/+3XOYsbCc08cP6fD1i4jsimaTirt/3vDczPoT3MvEgVnhlfVdym1njm19ojgwC4ZsefHj1dTVe1xHRxYRaa9oLn68EHgXOBk4BXjHzM6Pd2Cyw6Gj+7K5qpaZi9W1WEQ6t1aHaQF+ABzg7usAzKwP8BZNDJsi8XHEnn0pyMng8dnL+Wo4fIuISGcUTZfi5UBFxOsKdr5plsRZTmY6J+y/G899tIqKqppEhyMi0qxoksoKYKaZXWNmVwPvAIvM7AozuyK+4UmDUw8cRFVNPf+cuyrRoYiINCuapPIZwQCSDdemPAOsIrhRV8ddCdjFjRnckxFFeTw2e3nrE4uIJEg0Q9//oiMCkZaZGaeWDOaG5z/ls/ItjCjquOFiRESiFU3vrxIze8rM3uvKFz92BicfMJD0NONxHa2ISCcVTe+vBwl6gH0I1Mc3HGlJ3+45HDKqiCffW87/O3q0rlkRkU4nmjaVcnef5u5L3P3zhkfcI5MmnXrgIMo2b9fNu0SkU4rmSOVqM/sz8DLBveoBcPcn4xaVNOuIPfvRKzeTx2Yv59DRXfJeaSLSiUWTVM4D9gAy2XH6ywEllQTIykhjypiBPDTzCzZuraZnblaiQxIR+VI0SWV/d9837pFI1E4tGcR9by1l2gcrOfsrwxIdjojIl6JpU3nHzPaKeyQStb1368FeA7rzWKl6gYlI5xJNUjkImGNm88PuxB+qS3HinVoyiA9XbOLT1ZsTHYqIyJeiSSrHAsXA0cAJBPdZOSGeQUnrpowZSGa66WhFRDqVaG4n3NCFeBtBA33DQxKod14WR+zRj6ffX0FNnS4fEpHOIZor6r9uZguBJcBrwFLg+TjHJVE4tWQQ6yqreeXTNYkORUQEiO7013XARGCBuw8HjgDejGtUEpVDRhVRVJCtQSZFpNOIJqnUhDfoSjOzNHd/BRgT57gkChnpaZx8wEBe+XQNa7dsb30GEZE4iyapbDSzfGAG8KCZ3QLUtjaTmd1rZmvM7KOIsmvMbIWZzQkfX4t470dmtijsZXZMRPmxYdkiM7sqony4mc00s4Vm9oiZdcmrAE8tGURtvfP0+ysSHYqISFRJZQpBI/3/Ai8Q3F8lmt5f9xH0HGvsZncfEz6eAwivgzkd2Duc53YzSzezdOA24DhgL+CMiGtmfhMuqxjYAFwQRUwpZ2TfAsYM7sljpctxV/8JEUmsaJLKUHevc/dad7/f3W8FWr3C3t1nAOujjGMK8LC7b3f3JcAiYHz4WOTui929GngYmGJmBhwOPB7Ofz9wYpTrSjmnlgxiflkFH67YlOhQRKSLiyapPGpmV1qgm5n9H3B9O9Z5WXgR5b1m1issG8jO971fHpY1V94H2OjutY3Km2RmU82s1MxKy8tTb3TfE/bfjeyMNF2zIiIJF01SmQAMBt4CZgErgUltXN8dwAiChv5VwI1heVM3BvE2lDfJ3e9y9xJ3LykqKtq1iJNA95xMjt2nP8/MWUFVTV2iwxGRLiyq3l8EbSrdgBxgibu36Wo7dy8LT6XVA3cTnN6C4EhjcMSkgwiSV3Pla4GeZpbRqLzLOvXAwWyuqmX6J2WJDkVEurBoksosgqQyjmAcsDPM7PGWZ2mamQ2IeHkS0NAzbBpwupllm9lwgmFh3g3XXRz29MoiaMyf5kGL9CvAKeH85wDPtCWmVPHVEX3YrUeOrlkRkYSKZuj7C9y9NHy+mqCh/KzWZjKzvwOHAoVmthy4GjjUzMYQnKpaClwM4O4fm9mjwCcE3ZUvdfe6cDmXAS8C6cC97v5xuIorgYfN7JfA+8A9UdQlZaWlGd84cBC3vbKI1Zuq6N8jJ9EhiUgXZNF0QzWzg4Bid/+LmRUCBWEvraRTUlLipaWlrU+YhD5fV8khv3uVHxwzmksPG5nocEQkhZjZbHcvaW26aMb+uprgqOBHYVEW8Lf2hSfxMLRPHgcXF3LPG0uoqKpJdDgi0gVF06ZyEvB1oBLA3VcCBfEMStruB8eMZn1lNXfNWJzoUESkC4omqVSHDeMOYGZ58Q1J2mO/QT05fr8B/Pn1JazZXJXocESki4n24sc7CbrwXgT8i6A7sHRSPzhmNLX19dz8r4WJDkVEuphobtL1e4LhUJ4ARgM/d/f/i3dg0nZD++Rx5oShPFq6jEVrtiQ6HBHpQqI5UsHdp7v7D9z9/7n79HgHJe333cNH0i0znd++8GmiQxGRLiSqpCLJp09+NlMn785Ln5RRujTacT1FRNpHSSWFXXjwcIoKsrn++U81LL6IdIhmk4qZvRz+/U3HhSOxlJuVweVHFjP78w28pDHBRKQDtHSkMsDMDgG+bmYHmNnYyEdHBSjtc1rJYHYvyuO3L3xKbV2bxgEVEYlaS2N//Ry4imAE4JsavecEN8mSTi4jPY0fHrMH//232Tw2ezlnjB+S6JBEJIU1m1Tc/XHgcTP7mbtf14ExSYwds3c/Dhzai5unL2DKmN3IzYpmHFERkV0XzXUq15nZ183s9+Hj+I4ITGLHzPjRcXuwpmI7976RlOOAikiSiGZAyeuB7xEMS/8J8L2wTJJIybDeHLVXP/702mLWbdme6HBEJEVF06X4v4Cj3P1ed78XODYskyRz5bGj2Vpdy//9e1GiQxGRFBXtdSo9I573iEcgEn8j+xZw2rjBPDjzc75YtzXR4YhICoomqVwPvG9m95nZ/cBs4NfxDUvi5fIjR5GeZvzupfmJDkVEUlA0DfV/ByYCT4aPr7j7w/EOTOKjX/ccLjhoOM9+sJK5yzcmOhwRSTHRDii5yt2nufsz7r463kFJfF18yAh65WZyg4ZvEZEY09hfXVD3nEz+54hi3vpsHY/NXp7ocEQkhSipdFFnf2UYX9m9Dz9/5iPmr65IdDgikiJaTCpmlmZmH3VUMNJx0tOMW84YQ352Jpc8OJvK7bWJDklEUkCLScXd64EPzEwDRqWgvgU53Hr6GBavreRnT3+k9hURabdoTn8NAD42s5fNbFrDI96BScf46shCvndEMU++v4LHStW+IiLtE83Igr+IexSSUN89vJhZS9fzs2c+Yr/BPdijf/dEhyQiSSqa61ReA5YCmeHzWcB7cY5LOlB6mvGH0w6ge7dMLnnwPbaofUVE2iiaASUvAh4H7gyLBgJPxzMo6XhFBdnccvoYlq6t5CdPfaj2FRFpk2jaVC4FJgGbAdx9IdA3nkFJYnx1RCGXHzmKZ+as5OFZyxIdjogkoWiSynZ3r254YWYZBHd+bJGZ3WtmayK7JJtZbzObbmYLw7+9wnIzs1vNbJGZzY28XbGZnRNOv9DMzokoP9DMPgznudXMLNpKS/MuPWwkBxcXcvW0j/lk5eZEhyMiSSaapPKamf0Y6GZmRwGPAc9GMd99BMPkR7oKeNndi4GXw9cAxwHF4WMqcAcESQi4GpgAjAeubkhE4TRTI+ZrvC5pg/Q04+bTxtCzWyaXPaT2FRHZNdEklauAcuBD4GLgOeCnrc3k7jOA9Y2KpwD3h8/vB06MKH/AA+8APc1sAHAMMN3d17v7BmA6cGz4Xnd3f9uDk/8PRCxL2qkwP5tbzziApesq+fGTal8Rkei12qXY3evDIe9nEpz2mu9t38v0c/dV4XJXmVlD28xAIPIk/vKwrKXy5U2US4xM3L0PVxw1it+/tIAJu/fmzAlDEx2SiCSBaHp//RfnbwKRAAAVZ0lEQVTwGXAr8EdgkZkdF+M4mmoP8TaUN71ws6lmVmpmpeXl5W0Mseu55NCgfeUXz37CRys2JTocEUkC0Zz+uhE4zN0PdfdDgMOAm9u4vrLw1BXh3zVh+XJgcMR0g4CVrZQPaqK8Se5+l7uXuHtJUVFRG0PvetLSjD+cNobeuVlc9EApKzZuS3RIItLJRZNU1rh75E3NF7MjGeyqaUBDD65zgGciys8Oe4FNBDaFp8leBI42s15hA/3RwIvhexVmNjHs9XV2xLIkhvrkZ/OX88axZXstZ90zk3Vbtic6JBHpxJpNKmZ2spmdTDDu13Nmdm7YpfdZgqvqW2RmfwfeBkab2XIzuwC4ATjKzBYCR4WvIWj8XwwsAu4GLgFw9/XAdeH6ZgHXhmUA3wH+HM7zGfD8LtVcorbngO7ce+44VmzYxrl/maUeYSLSLGuuzd3M/tLCfO7u58cnpPgqKSnx0tLSRIeRlP79aRkXPTCbCcN7c++548jJTE90SCLSQcxstruXtDpdV+suqqTSPk++t5wrHv2AY/fuz21njiU9TdecinQF0SaVVrsUm9lw4LvAsMjp3f3r7QlQktPJYwexcWsN1/7jE37y1Idcf/K+aDADEWkQzdD3TwP3ELSl1Mc3HEkG5x80nPWV1fzxlUX0zsvih8fukeiQRKSTiCapVLn7rXGPRJLK948exfqt1dz+6mf0zsviwoN3T3RIItIJRJNUbjGzq4GXgC/7k7q77qnShZkZ103Zh41bq/nlP+fRMzeLUw4c1PqMIpLSokkq+wJnAYez4/SXh6+lC2sYfHLztlKufGIuPbplctRe/RIdlogkUDQXP54E7O7uh7j7YeFDCUUAyM5I586zDmSfgT249KH3mLl4XaJDEpEEiiapfAD0jHcgkrzysjP4y7njGNyrGxfeX8rszxsPTi0iXUU0SaUf8KmZvWhm0xoe8Q5MkkvvvCz+duEE+uRncdY97/LmorWJDklEEqDVix/N7JCmyt39tbhEFGe6+DG+1lRUcfY977K4vJLbzhyrNhaRFBHtxY+tHqm4+2tNPWITpqSavgU5PDx1Invu1p3//ttsnpmzItEhiUgHiuZ+KhVmtjl8VJlZnZnp5uXSrJ65WTx44QTGDevF5Y/M4aGZXyQ6JBHpINEcqRS4e/fwkQN8g+BmXSLNys/O4L7zxnPoqCJ+/NSH3D1jcaJDEpEOEE1D/U7c/Wl0jYpEIScznTvPKuG/9h3Ar56bx03TF+h+9yIpLpoBJU+OeJkGlNDCrXtFImVlpHHrGQeQl53OrS8vZEtVLT87fk8NQimSoqK5ov6EiOe1wFJgSlyikZSUnmbccPJ+5GZlcO+bS6jcXsuvT95Xw+aLpKBWk4q7n9cRgUhqS0szrj5hL7rnZHDrvxdRWV3LTd8cQ1bGLp+BFZFOrNmkYmY/b2E+d/fr4hCPpDAz44qjR5OXncH1z3/K5qpabvvWARTkZCY6NBGJkZZ+JlY28QC4ALgyznFJCrv4kBH85hv78taitZxyx9ss37A10SGJSIw0m1Tc/caGB3AX0A04D3gY0M0zpF1OGzeE+88fz8pN2zjxtjd5/4sNiQ5JRGKgxRPaZtbbzH4JzCU4VTbW3a909zUdEp2ktEkjC3nqkq/SLSud0+96h3/OXZXokESknZpNKmb2O2AWUAHs6+7XuLt+TkpMjexbwNOXTPpy6PzbXlmka1lEklhLRyrfB3YDfgqsjBiqpULDtEgs9cnP5sELJ/D1/Xfjdy/O54ePz6W6tr71GUWk02m295e7q6+ndJiczHRuOX0MwwrzuPXlhSzbsJU/fftAeuZmJTo0EdkFShzSaZgZVxw1iptP25/3Pt/Iybe/xdK1la3PKCKdhpKKdDonHTCIv104gQ1bqznx9jd5d4nuJCmSLJRUpFMaP7w3T10yid65WZz553d4dNayRIckIlFQUpFOa1hhHk9dMokJw/vwwyfmcs20j6mtUwO+SGempCKdWo/cTO47bxznTxrOfW8t5ex732VDZXWiwxKRZiQkqZjZUjP70MzmmFlpWNbbzKab2cLwb6+w3MzsVjNbZGZzzWxsxHLOCadfaGbnJKIuEn8Z6Wn8/IS9+N0p+1G6dANTbnuT+asrEh2WiDQhkUcqh7n7GHcvCV9fBbzs7sXAy+FrgOOA4vAxFbgDgiQEXA1MAMYDVzckIklNp5YM5uGLJ7Ktpo6Tb3+Tlz5eneiQRKSRznT6awpwf/j8fuDEiPIHPPAO0NPMBgDHANPdfX14pf904NiODlo61tghvXj2soMY2TefqX+dza0vL9QV+CKdSKKSigMvmdlsM5salvVz91UA4d++YflAILLrz/KwrLny/2BmU82s1MxKy8vLY1gNSYT+PXJ45OKvcNIBA7lp+gIufeg9tlbXJjosESG6Oz/GwyR3X2lmfYHpZvZpC9M2dXtAb6H8Pwvd7yIYaZmSkhL9rE0BOZnp3PTN/dlzQAE3PP8pi8srufvsEgb3zk10aCJdWkKOVNx9Zfh3DfAUQZtIWXhai/Bvw0jIy4HBEbMPAla2UC5dhJkxdfII7j13HCs2bmPKbW/y5qK1iQ5LpEvr8KRiZnlmVtDwHDga+AiYBjT04DoHeCZ8Pg04O+wFNhHYFJ4eexE42sx6hQ30R4dl0sUcOrovT186iV65mZz555n88h+fUFVTl+iwRLqkRJz+6gc8ZWYN63/I3V8ws1nAo2Z2AfAFcGo4/XPA14BFwFaCG4Xh7uvN7DqC4fkBrnV3jefRRY0oyufZ7x7Er5+bx5/fWMKMheXcfNoY9t6tR6JDE+lSrKv1nCkpKfHS0tJEhyFx9Mr8Nfzw8bls3FrNFUeNZurk3UlPa6oJTkSiZWazIy4BaVZn6lIsEhOHje7Li5dP5sg9+/GbFz7l9Lve5ot1WxMdlkiXoKQiKal3Xha3nzmWm765P5+uquC4W2bw6KxluqZFJM6UVCRlmRknjx3E85cfzD4De/DDJ+Yy9a+zWbtle6JDE0lZSiqS8gb1yuXvF03kJ1/bk9fml3PsH2Yw/ZOyRIclkpKUVKRLSEszLpq8O9O+O4nC/GwueqCUSx6czapN2xIdmkhKUVKRLmWP/t155rJJfP+oUbw8bw1H3Pgad834jBrdp0UkJpRUpMvJzkjnu0cUM/1/D2Hi7n349XOf8l+3vq7bFovEgJKKdFlD+uRyzzkl3HXWgVRur+Obd77NFY/OUUO+SDsoqUiXZmYcvXd/pl8xme8cOoJnP1jJ4b9/lb++8zl19ep+LLKrlFREgNysDK48dg+e/95k9hnYg589/REn3f4mHyzbmOjQRJKKkopIhJF983nwwgnccvoYVm2q4sTb3+SHj3/A8g26Il8kGom6n4pIp2VmTBkzkMP26Mst/1rIX9/+nKffX8m3JgzhksNG0LcgJ9EhinRaGlBSpBUrN27j//69kEdLl5OVnsY5Xx3Gfx+yOz1zsxIdmkiHiXZASSUVkSgtWVvJH/61gGkfrCQ/K4OLJu/O+QcNJz9bB/yS+pRUmqGkIu316erN3PTSAl76pIzeeVlccugIvj1xKDmZ6YkOTSRulFSaoaQisTJn2UZufGk+ry9cS7/u2Vx22EhOHjuIPB25SApSUmmGkorE2juL1/H7F+dT+vkG8rLSOX6/3fjmuMGMHdKT8A6nIklPSaUZSioSD+7Oe19s4JFZy/jH3FVsra5jZN98TisZzEljB1KYn53oEEXaRUmlGUoqEm9bttfyz7kreWTWMt77YiMZacaRe/bjtHGDmTyqSLc2lqSkpNIMJRXpSAvLKnhk1jKefH8F6yur6d89h1MOHMSJB+zGiKJ8nR6TpKGk0gwlFUmE6tp6Xp5XxiOly5ixoJx6h+GFeRy5Z1+O2qs/Y4f0JCNdA1xI56Wk0gwlFUm01ZuqmD6vjOmflPH2Z2upqXN65WZy+B79OGqvvhxcXKQeZNLpKKk0Q0lFOpOKqhpmLFjLv+aV8e9P17BpWw1ZGWlMGtGHI/fqx5F79qNfdw0LI4mnpNIMJRXprGrq6ilduoHpn5Qxfd5qlq0PbnU8vDCPccN6MW5Yb8YP782Q3rlqi5EOp6TSDCUVSQbuzoKyLby2YA3vLtlA6efr2bi1BoC+BdmMG96b8cN6M25Yb/boX0CaepRJnEWbVHTiVqQTMjNG9y9gdP8Cpk6G+npnUfkW3l2ynneXrGfW0vX8c+4qAApyMigZ2ot9BvZgeGEeuxflM7wwjx7dMhNcC+mKlFREkkBamjGqXwGj+hXw7YlDcXeWb9jGrKXrw8cGZixcu9PdKvvkZbF7UR7DC/MYXhgkmhFFeQzpk0t2hsYpk/jQ6S+RFFFdW88X67eyZG0lS9ZuYXF5JYvXVrJkbSXlFdt3mrYwP5sBPXLo3yOHAT1yGNCj206v+/fIUeKRnXSZ019mdixwC5AO/Nndb0hwSCIJkZWRxsi++Yzsmw/02+m9zVU1LA0TzNK1W1m9eRurNlXxxbqtzFy8js1Vtf+xvD55WfTJz6Jntyx65GbSo1smPbtl0jM3kx65WTu97p6TSV52BvnZGeRkpqkjQReW1EnFzNKB24CjgOXALDOb5u6fJDYykc6le04m+w3qyX6Dejb5fuX2WlZvrmL1pipWbtwW/N1UxYbKajZtq2H5hm18vGITG7fVsLW6rsV1mUFeVga5WenkZ2eQm51OblaQcHKz0umWmU5OZjo5mWlkZwR/czLTyc5MJycjLXwvneyMNNLTjDQzMtKDv+lpRkbajudfPsxITw/+pqVBuhkZaWnB88jpzTBDSS+OkjqpAOOBRe6+GMDMHgamAEoqIrsgLzuDEUX5jCjKb3Xa6tp6Nm2rYdO2ajZurWHj1ho2batha3UtldV1bN1ey5btdWytrmXL9lq2VtdRub2WNRVVVG6vo6qm4VFPVW0diToDnxYmlzQDw8D48nnDe2aQFvE36GTX8H74Hk0nqciihufB1JGvG963L5/T6L2dl9nEenahzv/4n4Pifloz2ZPKQGBZxOvlwITGE5nZVGAqwJAhQzomMpEUlZWRRlFBNkUF7R952d2prqtne209VTV1bK+p/zLhbK+to67eqXOnvh5q6+upd6euHurq64O/7l8+r693ar+c3oN5w9d19WGZO+7Beh2oD1/XOzgOHpTVO8F0NLy/42/QFyKIyWl4zU7JMVj6ly8i/9DQjr3jdfPv7byxmiratYxsu5SC2ibZk0pTW+g/trK73wXcBUFDfbyDEpHomBnZGelkZ6TTPUddoFNBso9gtxwYHPF6ELAyQbGIiHR5yZ5UZgHFZjbczLKA04FpCY5JRKTLSurTX+5ea2aXAS8SdCm+190/TnBYIiJdVlInFQB3fw54LtFxiIhI8p/+EhGRTkRJRUREYkZJRUREYkZJRUREYqbLjVJsZuXA522cvRBYG8NwEi3V6gOpV6dUqw+kXp1SrT7QdJ2GuntRazN2uaTSHmZWGs3Qz8ki1eoDqVenVKsPpF6dUq0+0L466fSXiIjEjJKKiIjEjJLKrrkr0QHEWKrVB1KvTqlWH0i9OqVafaAddVKbioiIxIyOVEREJGaUVEREJGaUVKJgZsea2XwzW2RmVyU6nlgws6Vm9qGZzTGz0kTH0xZmdq+ZrTGzjyLKepvZdDNbGP7tlcgYd0Uz9bnGzFaEn9McM/taImPcFWY22MxeMbN5ZvaxmX0vLE/mz6i5OiXl52RmOWb2rpl9ENbnF2H5cDObGX5Gj4S3FolumWpTaZmZpQMLgKMIbgo2CzjD3T9JaGDtZGZLgRJ3T9qLtsxsMrAFeMDd9wnLfgusd/cbwh8Avdz9ykTGGa1m6nMNsMXdf5/I2NrCzAYAA9z9PTMrAGYDJwLnkryfUXN1+iZJ+DlZcNP7PHffYmaZwBvA94ArgCfd/WEz+xPwgbvfEc0ydaTSuvHAIndf7O7VwMPAlATHJIC7zwDWNyqeAtwfPr+f4B8+KTRTn6Tl7qvc/b3weQUwDxhIcn9GzdUpKXlgS/gyM3w4cDjweFi+S5+RkkrrBgLLIl4vJ4m/RBEceMnMZpvZ1EQHE0P93H0VBDsAoG+C44mFy8xsbnh6LGlOFUUys2HAAcBMUuQzalQnSNLPyczSzWwOsAaYDnwGbHT32nCSXdrnKam0zpooS4VzhpPcfSxwHHBpeOpFOp87gBHAGGAVcGNiw9l1ZpYPPAFc7u6bEx1PLDRRp6T9nNy9zt3HAIMIzszs2dRk0S5PSaV1y4HBEa8HASsTFEvMuPvK8O8a4CmCL1MqKAvPezec/16T4Hjaxd3Lwn/6euBukuxzCs/TPwE86O5PhsVJ/Rk1Vadk/5wA3H0j8CowEehpZg13Bt6lfZ6SSutmAcVhb4gs4HRgWoJjahczywsbGTGzPOBo4KOW50oa04BzwufnAM8kMJZ2a9j5hk4iiT6nsBH4HmCeu98U8VbSfkbN1SlZPyczKzKznuHzbsCRBO1ErwCnhJPt0mek3l9RCLsH/gFIB+51918lOKR2MbPdCY5OADKAh5KxTmb2d+BQgmG6y4CrgaeBR4EhwBfAqe6eFI3fzdTnUIJTKg4sBS5uaI/o7MzsIOB14EOgPiz+MUEbRLJ+Rs3V6QyS8HMys/0IGuLTCQ4yHnX3a8N9xMNAb+B94Nvuvj2qZSqpiIhIrOj0l4iIxIySioiIxIySioiIxIySioiIxIySioiIxIySikiMmVldOFLtx+Hor1eYWZv/18zsxxHPh0WOYizS2SipiMTeNncf4+57E4xu/TWCa07a6setTyLSOSipiMRROAzOVILBBi0cvO93ZjYrHHzwYgAzO9TMZpjZU2b2iZn9yczSzOwGoFt45PNguNh0M7s7PBJ6KbwSWqRTUFIRiTN3X0zwv9YXuADY5O7jgHHARWY2PJx0PPB9YF+CwQlPdver2HHkc2Y4XTFwW3gktBH4RsfVRqRlSioiHaNhtOujgbPDocZnAn0IkgTAu+F9e+qAvwMHNbOsJe4+J3w+GxgWn5BFdl1G65OISHuE4yjVEYzGa8B33f3FRtMcyn8OL97cGEqRYzDVATr9JZ2GjlRE4sjMioA/AX/0YKC9F4HvhMOnY2ajwpGiAcaHo2GnAacR3NoVoKZhepHOTkcqIrHXLTy9lQnUAn8FGoZJ/zPB6ar3wmHUy9lxq9a3gRsI2lRmsGMk6buAuWb2HvCTjqiASFtplGKRTiA8/fX/3P34RMci0h46/SUiIjGjIxUREYkZHamIiEjMKKmIiEjMKKmIiEjMKKmIiEjMKKmIiEjM/H9ov2vLIWVO7AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzsnXd4FFXbh+8njVRCSUIRJIABpCWU0KWIFDUi0pFXBQsg0kQRK6KAYnnFhmAlvioSimIB/JCqNCkKSE0Ag4aShJYO2STn+2M2axJSFpLN7oZzX9dcuztz5pzfnJ3dZ057HlFKodFoNBoNgIu9BWg0Go3GcdBGQaPRaDQWtFHQaDQajQVtFDQajUZjQRsFjUaj0VjQRkGj0Wg0FrRRKENEZIaIfGnH8jeKyMPm9yNEZE0Z5n1ARLqb35fpdYrIsyLySVnlVyDvWSJyVkTO2Cj/IutcRDqLSIyIpIpIfxGpISK/iEiKiPzXFnrsgb3v+5IQke4iEmejvINFRImImy3ytwfaKFwlInKviOwy/9BPi8hqEelib10FUUp9pZTqXVI6EYkUkVlW5NdMKbWxtLoK+4EqpV5RSj1c2rwLKasu8ATQVClVs6zzL0ghdf4y8L5SylcptQIYDZwFKiulnrC1nrzY889LRGJF5LbyLtcWVKRrKQptFK4CEZkCvA28AtQAbgQ+AO62py5b4uRPQPWAc0qphKs9sYyuux5woMDng+oaVow6+fdQLBX52pwSpZTerNgAfyAVGFxMmhnAl3k+LwXOAEnAL0CzPMfuAA4CKcBJ4Enz/gDgR+AicB74FXAporxewGFz/u8Dm4CHzcdGApvN7wWYCySY0+4DmmM8uZqATPO1/WBOHwtMM6e7DLiZ992W5zqXAVFm/b8DoXl0KeCmPJ8jgVmAD5AB5JjLSwVqF1Jv/TD+TC8CG4Gb8xyLBZ40a0sya/AspG5uK1BWpJV557vuUtT5MXPZGebyvy5Q17dhPJQ9bU57DlgCVDOfH2yux4eAv4FfzPs7AFvN+vcC3fNo2wjMBLaYv5c1QID52N/m/HLrvWMh19YO2AUkA/HAW+b93YG4Ammtuh+ALwrUw1PXeG3+wKfAaYzfyyzAtYjfhRfGPXcB4zc2Na9+jHtuOZAI/AVMLPAbvpZrecB8LWeB50qqU0fe7C7AWTagL5BFIX8UBW6ovH9uDwJ+QCWMFsaePMdOA7eY31cFWpvfvwosANzN2y2AFFJWgPlGG2RO97hZX2F/UH2A3UAVDANxM1DLfCwSmFUg71hgD1AX8MqzL++fgClP2U+af1zu5uOFGgXz++5c+QdjqTegEZCG8efrbv7hHQU88ujYYf5hVwMOAWOL+D7ylWVl3vmu+1rrvGCdFVbXwGRgO1AH4x75EPjafCzYXI//wzCmXsANGMbjDgyD0sv8OdB8zkYMA9PInH4jMKdAfsXdv9uA+8zvfYEOxXxnV3M/FKyHa7m2Feb68QGCzPfAmCKuYw7Gw1Q183e5P1e/Oe/dwHTAA2gAHAf6lPJaPjZfRyjGA8XNxdWpI2+6+8h6qgNnlVJZ1p6glPpMKZWilLqMcbOFioi/+bAJaCoilZVSF5RSv+fZXwuop5QyKaV+VeY7qgB3YHRFLFNKmTCMTlGDqSYM49QEw8AcUkqdLkH+u0qpf5RSGUUc352n7LcAT4wnvdIyFFiplPrZnPebGD+2TgW0nVJKnQd+AMLKOO+irvtq6twaxmA8VcbluUcGFehOmaGUSjPr+Q+wSim1SimVo5T6GeMp9I486RcqpaLN6Zdgfd2AcZ/cJCIBSqlUpdT2qzj3Wu4Hq65NRGoAtwOTzekTMFq+w4rIdwgwWyl1Xin1D/BunmPhGIbmZaVUplLqOMYfet68ruVaXlJKZSil9mK0ckLN+0tTp3ZBGwXrOQcEWNv/KSKuIjJHRI6JSDLGEwYYT5sAAzF+zCdEZJOIdDTvfwPj6XWNiBwXkaeLKKI28E/uB7Ph+KewhEqp9RhdHfOAeBH5SEQql3AJheZV2HGlVA4QZ9ZUWmoDJwrk/Q/Gk2Quef+I0zGewMoq7+Ku2+o6t5J6wLciclFELmK0erIxxqsK01MPGJyb3nxOF4yHiFyutW7A6M5pBBwWkZ0iEnEV517L/WDttdXDeGo/nefYhxgthsLI9z2R5zs351W7QDnPUkSdX8W1FFXvpalTu6CNgvVsAy4B/a1Mfy/GAPRtGP2hweb9AqCU2qmUuhvjxl6B8VSHuWXxhFKqAXAXMEVEehaS/2mMprGRqYjk/VwQpdS7Sqk2QDOMm3Rq7qGiTinh+vKW7YLRBXLKvCsd8M6TNu/Mn5LyPYXxw83NO/e6TpZwnjVYk3dx+q6qzq3gH+B2pVSVPJunUqooPf8AXxRI76OUmmNFWSUObiulYpRSwzHuydeAZSLig9HlZvk+RcQVCCxwenH3gzX3WHHX9g9Gl0xAnmOVlVLNisg33/eEMSEkbzl/FSjHTymVt7V1LddS+AUWXacOizYKVqKUSsLoh5xnnnPuLSLuInK7iLxeyCl+GDfyOYwf1Cu5B0TEwzyn3d/cRE3GeEJERCJE5CbzH07u/uxC8l8JNBORAebWy0Ty//laEJFwEWkvIu4YP/BLefKMx+hXvVra5Cl7svlac5vGe4B7za2lvkC3POfFA9XzdKMVZAlwp4j0NOt9wpz31mvQWNZ5W13nVrIAmC0i9QBEJFBEipvJ9iVwl4j0Mdetp3mKbx0rykrEGCQt8rsWkf+ISKD56fiieXc2EA14isid5np7HmMMJC/F3Q/W3GNFXpu5q3MN8F8RqSwiLiLSUES6FZHXEuAZEalqrpsJeY7tAJJFZJqIeJnLai4i4WV4LRaKqVOHRRuFq0Ap9RYwBeNHkYjx1DEe40m/IP/DaLaexJgBUbAv8T4g1ty1NBajTxUgBFiLMbthG/CBKmR9gFLqLDAYY1DtnPm8LUVIr4zRb3rBrOkcRn86GDM6mpqb0oVdR1F8h9FHf8F8LQPMBg5gEkYr5yIwgjz1o5Q6jDET57i5zHzNcqXUEYy6eA9jJsddwF1Kqcyr0FYopc37KuvcGt4BvsfoKkzBuEfaF1P+Pxitz2f59/6bihW/Y6VUOjAb2GKu98L6yPsCB0Qk1axtmFLqkvmBaBzwCcb9nIbRpZKX4u6HV4HnzeU+eY3Xdj/GwPBBcxnLyN9tlpeXMO7zvzCMyRd5ysnG+N7DzMfPmq8r70NKqa6lAIXWqRXn2Q0pfAxTo9Forj9EZAbGzLn/lJS2oqJbChqNRqOxoI2CRqPRaCzo7iONRqPRWNAtBY1Go9FYcDpHVAEBASo4ONjeMjQajcap2L1791mlVMH1JVfgdEYhODiYXbt22VuGRqPROBUicqLkVLr7SKPRaDR50EZBo9FoNBa0UdBoNBqNBacbU9DYH5PJRFxcHJcuOfRqfY2D4OnpSZ06dXB3d7e3FI0VaKOguWri4uLw8/MjODgYw2+fRlM4SinOnTtHXFwc9evXt7ccjRXYrPtIRD4TkQQR2V/EcRGRd0XkqIjsE5HWttKiKVsuXbpE9erVtUHQlIiIUL16dd2qdCJsOaYQieEhsChux/AyGYIRK3i+DbVoyhhtEDTWou8V58Jm3UdKqV9EJLiYJHcD/zNHr9ouIlVEpJYVYSI1Go2mQnD5+HGSf1yJtbF7fHv0wKtFC5tqsueYwg3kD5kXZ953hVEQkdEYrQluvPHGgoc11yFnzpxh8uTJ7Ny5k0qVKhEcHMzbb7+Nh4cHERER7N9faK9lqZgxYwbBwcGMHDmyzPO2FytWrKBRo0Y0bdoUgOnTp9O1a1duu+22q86re/fuREZGoj0OWM+5Tz4l6ZtvwMrWlFtQUIU2CoXVQqHmUin1EfARQNu2bbUHv+scpRT33HMPDzzwAIsXLwZgz549xMfHU7duaaJjlg9ZWVm4uTnGHI8VK1YQERFhMQovv/yynRVdX2SdOY1naEvqR0XZW4oFe65TiCN/HNW8cVA1miLZsGED7u7ujB071rIvLCyMW265JV+62NhYbrnlFlq3bk3r1q3ZutWIunn69Gm6du1KWFgYzZs359dffyU7O5uRI0fSvHlzWrRowdy5c68o19fXFy8vLw4dOkS7du3yldOyZUsAdu/eTbdu3WjTpg19+vTh9Gmj4du9e3eeffZZunXrxuzZs6lfvz4mkxHMKzk5meDgYMvnXEaOHMnEiRPp1KkTDRo0YNmyZZZjb7zxBuHh4bRs2ZIXX3zRsn/mzJk0adKEXr16MXz4cN580wiw9/HHHxMeHk5oaCgDBw4kPT2drVu38v333zN16lTCwsI4duwYI0eOZNmyZaxevZohQ4ZY8t24cSN33XUXAGvWrKFjx460bt2awYMHk5qaCkC1atVwdXW16jvUGJjiE3APqmFvGfmw5+PK98B4EVmMEYIwSY8nOB8v/XCAg6eSyzTPprUr8+JdRcVkh/3799OmTZsS8wkKCuLnn3/G09OTmJgYhg8fzq5du1i0aBF9+vThueeeIzs7m/T0dPbs2cPJkyct3U4XL168Ir8nn/w3+mJmZibHjx+nQYMGREVFMWTIEEwmExMmTOC7774jMDCQqKgonnvuOT777DNLnps2bQIMQ7Jy5Ur69+/P4sWLGThwYKHz+E+fPs3mzZs5fPgw/fr1Y9CgQaxZs4aYmBh27NiBUop+/frxyy+/4O3tzfLly/njjz/IysqidevWlnoaMGAAjzzyCADPP/88n376KRMmTKBfv35EREQwaNCgfOX26tWLMWPGkJaWho+PD1FRUQwdOpSzZ88ya9Ys1q5di4+PD6+99hpvvfUW06dP55tvvinxO9HkJ+vMGXw6drS3jHzYzCiIyNdAdyBAROKAFwF3AKXUAmAVcAdwFEgHRtlKi+b6xGQyMX78ePbs2YOrqyvR0dEAhIeH8+CDD2Iymejfvz9hYWE0aNCA48ePM2HCBO6880569+5dbN5DhgxhyZIlPP3000RFRREVFcWRI0fYv38/vXr1AiA7O5tatf4NIzx06FDL+4cffpjXX3+d/v37s3DhQj7++ONCy+nfvz8uLi40bdqU+Ph4wHhSX7NmDa1atQIgNTWVmJgYUlJSuPvuu/Hy8gKwPNmDYUiff/55Ll68SGpqKn369Cn2+tzc3Ojbty8//PADgwYNYuXKlbz++uts2rSJgwcP0rlzZ8Awjh0d7E/NWchOTSMnLQ23GkH2lpIPW84+Gl7CcQU8ZqvyNeVDcU/0tqJZs2b5ulKKYu7cudSoUYO9e/eSk5ODp6cnAF27duWXX35h5cqV3HfffUydOpX777+fvXv38n//93/MmzePJUuWWJ7wC2Po0KEMHjyYAQMGICKEhITw559/0qxZM7Zt21boOT4+Ppb3nTt3JjY2lk2bNpGdnU3z5s0LPadSpUqW97kBsZRSPPPMM4wZM+aK6y2KkSNHsmLFCkJDQ4mMjGTjxo1Fps17jfPmzaNatWqEh4fj5+eHUopevXrx9ddfl3i+pniyEgwj716zpp2V5Ef7PtI4HbfeeiuXL1/O93S9c+dOS9dMLklJSdSqVQsXFxe++OILsrOzAThx4gRBQUE88sgjPPTQQ/z++++cPXuWnJwcBg4cyMyZM/n999+L1dCwYUNcXV2ZOXOmpQXQuHFjEhMTLUbBZDJx4MCBIvO4//77GT58OKNGXV0juU+fPnz22WeWvvyTJ0+SkJBAly5d+OGHH7h06RKpqamsXLnSck5KSgq1atXCZDLx1VdfWfb7+fmRkpJSaDndu3fn999/5+OPP7ZcY4cOHdiyZQtHjx4FID093dIC01wdWeaWn5uDjSloo6BxOkSEb7/9lp9//pmGDRvSrFkzZsyYQe3atfOlGzduHJ9//jkdOnQgOjra8qS+ceNGwsLCaNWqFcuXL2fSpEmcPHmS7t27ExYWxsiRI3n11VdL1DF06FC+/PJLy4Csh4cHy5YtY9q0aYSGhhIWFmYZ3C6MESNGcOHCBYYPL7ZRfQW9e/fm3nvvpWPHjrRo0YJBgwaRkpJCeHg4/fr1IzQ0lAEDBtC2bVv8/f0BYwC6ffv29OrViyZNmljyGjZsGG+88QatWrXi2LFj+cpxdXUlIiKC1atXExERAUBgYCCRkZEMHz6cli1b0qFDBw4fPnxV+jUGJrNRcHew7iOni9Hctm1bpYPs2JdDhw5x880321uG07Ns2TK+++47vvjiizLLMzU1FV9fX9LT0+natSsfffQRrVvb34OMvmeu5OyHH5E4dy6N//gdF/M4kC0Rkd1KqbYlpXOMydIazXXGhAkTWL16NatWrSrTfEePHs3Bgwe5dOkSDzzwgEMYBE3hZMWfwaVy5XIxCFeDNgoajR147733bJLvokWLbJKvpuwxxSfgXsOxxhNAjyloNBqNXciKj8dNGwWNRqPRQK5RcKxBZtBGQaPRaModZTKRdfas7j7SaDQaDWSdPQtKOdwaBdBGQeOknDlzhmHDhtGwYUOaNm3KHXfcQXR0NLGxsUWuDi4tM2bMIDIy0iZ5F0ZkZCSnTv3rI/Lhhx/m4MGD15SXdmftWFgWrtV0PKOgZx9pnA5nd51tLZGRkTRv3tyyKO+TTz6xsyJNWWGKTwDQ3UcaTVlgb9fZYLiAmDZtGu3ataNRo0b8+uuvgOEEb+rUqRa31h9++CEAOTk5jBs3jmbNmhEREcEdd9xh8d/08ssvEx4eTvPmzRk9ejRKKZYtW8auXbsYMWIEYWFhZGRk0L17d3bt2sX8+fN56qmnLLoiIyOZMGECAF9++SXt2rUjLCyMMWPGWFx7BAYGlknda8oGS0vBAY2CbiloSsfqp+HMn2WbZ80WcPucIg87gutsMILl7Nixg1WrVvHSSy+xdu1aPv30U/z9/dm5cyeXL1+mc+fO9O7dm927dxMbG8uff/5JQkICN998Mw8++CAA48ePZ/r06QDcd999/PjjjwwaNIj333+fN998k7Zt8y9CHTRoEB07duT1118HsLjoPnToEFFRUWzZsgV3d3fGjRvHV199xf3338/OnTtLrC9N+ZGVEI+4u+Nataq9pVyBNgqaCostXWeDEaMAoE2bNsTGxgKGW+t9+/ZZWgFJSUnExMSwefNmBg8ejIuLCzVr1qRHjx6WfDZs2MDrr79Oeno658+fp1mzZvncXhckMDCQBg0asH37dkJCQjhy5AidO3dm3rx57N69m/DwcAAyMjIICnK8KY8ao/vILSgIsTIMZ3mijYKmdBTzRG8rHMF1Nvzr1trV1ZWsrCzAGO947733rohXkNdjaV4uXbrEuHHj2LVrF3Xr1mXGjBlcunSpxGsbOnQoS5YsoUmTJtxzzz2ICEopHnjgAauc+WnsS9aZMw7ZdQR6TEHjhDiC6+yi6NOnD/Pnz7eE1oyOjiYtLY0uXbqwfPlycnJyiI+Pt8QzyDUAAQEBpKam5jN2xbm1HjBgACtWrODrr7+2uLXu2bMny5YtIyHBGMQ8f/48J06cuKbr0NgWU0I87g448wh0S0HjhOS6zp48eTJz5szB09OT4OBg3n777Xzpxo0bx8CBA1m6dCk9evTI5zr7jTfewN3dHV9fX/73v/9x8uRJRo0aRU5ODsA1P20//PDDxMbG0rp1a5RSBAYGsmLFCgYOHMi6deto3rw5jRo1on379vj7+1OlShUeeeQRWrRoQXBwsKXrB4zAOGPHjsXLy+uKwD1Vq1aladOmHDx40BIvumnTpsyaNYvevXuTk5ODu7s78+bNo169etd0LRrboJQiKz4Btx632ltKoWjX2ZqrRrtBvjZy3VqfO3eOdu3asWXLFmo6WNQtW6HvmX/JTkoiun0HgqZNo/qokeVWrnadrdE4GBEREVy8eJHMzExeeOGF68YgaPLjqMF1ctFGQaMpJ6yJi6yp+DjyGgXQA80ajUZTrvxrFByzpaiNgkaj0ZQjlu6jIMdcZa6Ngkaj0ZQjWfEJuFarhnh42FtKoWijoNFoNOWIo0Zcy0UbBY1TUpFcZ7/99tukp6df9XkjR460LHbr3r27xdWGxrExJSTg7sDuR7RR0Dgdua6zu3fvzrFjxzh48CCvvPIK8ea+WmejOKOQuwpbU3HIOnMGNweejqyNgsbpcEbX2Rs3biQiIsKS1/jx44mMjOTdd9/l1KlT9OjRw+Ikz9fXl+nTp9O+fXu2bdtWqGvtglSrVg1XV9fSVKumHMjJzCT7wgWHjM2ci16noCkVr+14jcPnD5dpnk2qNWFau2lFHndG19lFMXHiRN566y02bNhAQEAAAGlpaTRv3pyXX34ZMNxXFHStXdCL6jfffFNifWjsT1aC4wbXyUUbBU2FxZFcZ3tcxUwTV1dXBg4caPl8ta61NY6LZY2CA8ZmzkUbBU2pKO6J3lY4o+vszZs3W5ztAcW6x/b09LR0BV2ra22NY/LvwjXH7T7SYwoap8MZXWfXq1ePgwcPcvnyZZKSkli3bp3lnOJcZBfnWlvjfDhybOZcbNpSEJG+wDuAK/CJUmpOgeM3Ap8DVcxpnlZKrbKlJo3z44yus+vWrcuQIUNo2bIlISEhtGrVynLO6NGjuf3226lVqxYbNmzIl19xrrU1zkfWmTOIlxculSvbW0qR2Mx1toi4AtFALyAO2AkMV0odzJPmI+APpdR8EWkKrFJKBReXr3adbX+0G2TN1aLvGYO4xx/n8sFDNPy/n8q9bGtdZ9uy+6gdcFQpdVwplQksBu4ukEYBuSbTHzhlQz0ajUZjV7LiExx6NTPY1ijcAPyT53OceV9eZgD/EZE4YBUwobCMRGS0iOwSkV2JiYm20KrRaDQ2x9FdXIBtjYIUsq9gX9VwIFIpVQe4A/hCRK7QpJT6SCnVVinVNjDQMT0LajQaTXGonByyEhIcNrhOLrY0CnFA3Tyf63Bl99BDwBIApdQ2wBMIsKEmjUajsQvZFy6gTCaHjaOQiy2Nwk4gRETqi4gHMAz4vkCav4GeACJyM4ZR0P1DGo2mwuEMaxTAhkZBKZUFjAf+DzgELFFKHRCRl0WknznZE8AjIrIX+BoYqWw1HUqj0WjsyL+xma/fMQWUUquUUo2UUg2VUrPN+6Yrpb43vz+olOqslApVSoUppdbYUo+m4lARXGefOnWKQYMGAbBnzx5Wrfp3ic7333/PnDlzijq1WGzl4ltTOrLMC9eu54FmjcYmVBTX2bVr17asUC5oFPr168fTTz9tL2kaG5CVEA8uLrgFOPawqTYKGqfDUVxnT548mU6dOtG8eXN27NgBwPnz5+nfvz8tW7akQ4cO7Nu3D4BNmzYRFhZGWFgYrVq1IiUlxdKqyczMZPr06URFRREWFkZUVBSRkZGMHz+epKQkgoODLSut09PTqVu3LiaTiWPHjtG3b1/atGnDLbfcwuHDh6/QqXEcTPHxuAUEIG6O7XLOsdVpHJ4zr7zC5UNl6zq70s1NqPnss0UedxTX2WlpaWzdupVffvmFBx98kP379/Piiy/SqlUrVqxYwfr167n//vvZs2cPb775JvPmzaNz586kpqZanPMBeHh48PLLL7Nr1y7ef/99AEv3j7+/P6GhoWzatIkePXrwww8/0KdPH9zd3Rk9ejQLFiwgJCSE3377jXHjxrF+/fordGocg6wzjr9GAbRR0FRgbO06e/jw4YDhdTU5OZmLFy+yefNmli9fDhiO+86dO0dSUhKdO3dmypQpjBgxggEDBlCnTh2rr2Po0KFERUXRo0cPFi9ezLhx40hNTWXr1q0MHjzYku7y5ctXUz2aciYrIR73evXsLaNErhujcPTCUZbFLOOp8KdwuXJ9nOYaKe6J3lY4iutsEbnic2GT50SEp59+mjvvvJNVq1bRoUMH1q5dm6+1UBz9+vXjmWee4fz58+zevZtbb72VtLQ0qlSpwp49e6zKQ2N/TPEJeIe3s7eMErlu/h1/O/MbXx36irm7r+wr1jgXjuI6OyoqCjBiJfj7++Pv70/Xrl356quvAMMba0BAAJUrV+bYsWO0aNGCadOm0bZtW0v/fy7Fuc/29fWlXbt2TJo0iYiICFxdXalcuTL169dn6dKlgDH4vnfvXitrUFPe5GRkkJOcrLuPHIl7m9xLbFIskQciucH3BoY1GWZvSZprxFFcZ1etWpVOnTqRnJxsaVXMmDGDUaNG0bJlS7y9vfn8888BePvtt9mwYQOurq40bdqU22+/ndOnT1vy6tGjB3PmzCEsLIxnnnnmirKGDh3K4MGD2bhxo2XfV199xaOPPsqsWbMwmUwMGzaM0NDQq6tMTbngLAvXwIaus21FaVxnZ+VkMXnDZH49+Svv3foeXet0LWN11wfaDbIx++jNN9+kbdsSPRFr0PdM2vbf+HvkSG6MXIhPhw520eAIrrMdDjcXN17v+jqNqzbmyU1PcvDcwZJP0mg0mlKSleD4sZlzKdEoiMjrIlJZRNxFZJ2InBWR/5SHOFvg7e7NvJ7z8K/kz/h14zmderrkkzSaAmzcuFG3EjRW86+LC8fvPrKmpdBbKZUMRGB4Pm0ETLWpKhsT6B3IBz0/ICMrg3HrxpGSWfgAn0aj0ZQFWfEJuPj64mIe13JkrDEK7ubXO4CvlVLnbain3AipGsJb3d8iNimWKRunYMox2VuSRqOpoDhDcJ1crDEKP4jIYaAtsE5EAoFLtpVVPnSs3ZHpHaez/fR2Zm6bWegcc41GoyktpoR4h/eOmkuJRkEp9TTQEWirlDIBaVwZa9lpuSfkHsa0HMO3R7/l4z8/LvkEjUajuUqcxcUFWDfQPBjIUkpli8jzwJdAbZsrK0ceC3uMiAYRvPfHe/x4/Ed7y9FYQUVwnV1edOrUCTAcBC5atMiyf9euXUycOPGa8oyMjGTGjBllIa/Co7KzyTp71inWKIB13UcvKKVSRKQL0Af4HJhvW1nli4jwUqeXaFujLdO3TGfnmZ32lqQpBmd3nZ2VlVWu5eV6hy1oFNq2bcu7775brlquR7LOnoPs7IrTfQRkm1/vBOYrpb4DPGwnyT54uHrwdo+3qeNXh8kbJvNX0l/2lqQpAnu7zj506BDt2rXLV07Lli0B2L17N926daNC/+1SAAAgAElEQVRNmzb06dPHsmq5e/fuPPvss3Tr1o3Zs2dTv359TCZjckNycjLBwcGWz7mMHDmSsWPHcsstt9CoUSN+/NFoxV66dIlRo0bRokULWrVqxYYNGwA4cOAA7dq1IywsjJYtWxITE2PRDfD000/z66+/EhYWxty5c9m4cSMRERHk5OQQHByczzPsTTfdRHx8PImJiQwcOJDw8HDCw8PZsmULAF5eXpZ8NcVjWaPgJEbBGjcXJ0XkQ+A24DURqUQFXfTmX8mfD3p+wIhVI5i6aSqL7lyEh2uFs39lyq9Lojn7T2qZ5hlQ15dbhjQq8rgjuM7OzMzk+PHjNGjQgKioKIYMGYLJZGLChAl89913BAYGEhUVxXPPPWdxgXHx4kWLf6bY2FhWrlxJ//79Wbx4MQMHDsTd3f2KMmNjY9m0aRPHjh2jR48eHD16lHnz5gHw559/cvjwYXr37k10dDQLFixg0qRJjBgxgszMTIuvp1zmzJnDm2++aTEuuS4zXFxcuPvuu/n2228ZNWoUv/32G8HBwdSoUYN7772Xxx9/nC5duvD333/Tp08fDh06xNChQ0usf43Bvy4unMMoWPPnPgQjznJfpdRFoBpOvk6hOOr41eGlTi9x5MIR5u2ZZ285mlJgMpl45JFHaNGiBYMHD+bgQWMFe3h4OAsXLmTGjBn8+eef+Pn55XOd/dNPP1G5cuVi8x4yZAhLliwBDMd4Q4cO5ciRI+zfv59evXoRFhbGrFmziIuLs5yT94/04YcfZuHChQAsXLiQUaNGFVmOi4sLISEhNGjQgMOHD7N582buu+8+AJo0aUK9evWIjo6mY8eOvPLKK7z22mucOHHiqgLt5LrnBli8eLFF69q1axk/fjxhYWH069eP5OTkIh33aQrHWWIz51JiS0EplQ58IyJBInKjeXfZRlVxMLrX7c7AkIEs3L+QrnW60qZGyU+l1yvFPdHbCkdwnZ3roG7AgAGICCEhIfz55580a9aMbdu2FXqOT56FS507d7a0ArKzs4scHLfWPTfAvffeS/v27Vm5ciV9+vThk08+4dZbby22jnLp2LEjR48eJTExkRUrVvD8888DkJOTw7Zt23Qkt1KQdSYe3N1xrVbN3lKswprZR/1EJAb4C9hkfl1ta2H25qnwp6jjV4fnNj9HambZdo9oSocjuM5u2LAhrq6uzJw50/JU3bhxYxITEy1GwWQyceDAgSLzuP/++xk+fHiRrQSApUuXkpOTw7Fjxzh+/DiNGzfO5547Ojqav//+m8aNG1u6syZOnEi/fv0soUBzKc49t4hwzz33MGXKFG6++WaqV68OQO/evS3R4AAdv+EayEqIxy0wAHFxjl53a1TOBDoA0Uqp+hhjC1tsqsoB8Hb35pUur3A67TSv7XzN3nI0ech1nf3zzz/TsGFDmjVrxowZM6hdO/9M6XHjxvH555/ToUMHoqOj87nOzo2VvHz5ciZNmsTJkyfp3r07YWFhjBw50irX2UOHDuXLL79kyJAhgBFWc9myZUybNo3Q0FDCwsIsg9uFMWLECC5cuGCJ4FYYjRs3plu3btx+++0sWLAAT09Pxo0bR3Z2Ni1atGDo0KFERkZSqVIloqKiaN68OWFhYRw+fJj7778/X14tW7bEzc2N0NDQQgfSc68nbzfXu+++y65du2jZsiVNmzZlwYIFJdaLJj+m+ATcncARngWlVLEbsMv8uhdwMb/fUdJ5ttratGmjypN3f39XNY9srtbGri3Xch2ZgwcP2ltChWDp0qXqP//5T5HHH3jgAbV06dJyVGQ7rud75mjf29U/kybbW4bK/S8vabNm9tFFEfEFfgG+EpEEoHwnWtuRsaFj2XxyMy9te4nQoFACvALsLUlTAZgwYQKrV69m1apV9paisSFKKUzx8fh2vaXkxA6CNd1HdwMZwOPAT8Ax4C5binIk3F3cebXLq6RnpfPi1he1fyRNmfDee+9x9OhRGjUqeqA+MjKSQYMGlaMqTVmTk5qKSk93ijgKuVjj+yhNKZWtlMpSSn2ulHpXKXWuPMQ5Cg2qNODxNo/zS9wvLIspedbL9YA2jhpruZ7vFWdbowDFGAURSRGR5DyvyXk/l6dIR2B4k+F0rNWRN3a+wd/Jf9tbjl3x9PTk3Llz1/WPXWMdSinOnTtnmQ58veFMwXVyKXJMQSnlV55CHB0XcWFm55kM+H4Az2x+hs/7fo6bizVDMhWPOnXqEBcXR2Jior2laJwAT09P6tSpY28ZdiErPgFwrpZCif9qItIBOKCUSjF/9gWaKaV+s7U4R6OGTw1e6PACU3+Zyqd/fsqY0DH2lmQX3N3dqV+/vr1laDQOj7P5PQLrBprnA3lXb6VTwbykXg196/fljvp3sGDvAg6cLXphkkaj0Zji43GtUgWXSpXsLcVqrDEKovJ0HiulcrDOkR4i0ldEjojIURF5uog0Q0TkoIgcEJFFhaVxNJ5t/yzVvarzzOZnyMjKsLccjUbjoDhTcJ1crDEKx0Vkooi4m7dJwPGSThIRV2AecDvQFBguIk0LpAkBngE6K6WaAZOv+grsgH8lf2Z3mc1fSX8xd/eVK0M1Go0GcmMzO88gM1hnFMYCnYCTQBzQHhhtxXntgKNKqeNKqUxgMVeG8XwEmKeUugCglEqwVri9aV+rPfc1vY+vD3/N2hNr7S1Ho9HYGNOpU6T9tuPqzklIcBrvqLlYs04hQSk1TCkVpJSqoZS618o/7xuAf/J8jjPvy0sjoJGIbBGR7SLSt7CMRGS0iOwSkV2ONONlcuvJtAxoyfNbntdBeTSaCk78q3P4+6GHyDxxwqr0ymQi+9w53GrUtLGyssWWbvukkH0FJ7a7ASFAd2A48ImIVLniJKU+Ukq1VUq1DQwMLHOh14qHqwf/7f5fPFw8mLJxCummdHtL0mg0NiDn0iVSN2+GrCwS333PqnOyEhNBqQrZfXStxAF183yuA5wqJM13SimTUuov4AiGkXAaavrU5PVur3M86Tgzts7QC7o0mgpI2rZtqIwMvFq1InnlSi4dOlTiOaYzzhVcJ5fiVjRPMr92vsa8dwIhIlJfRDyAYcD3BdKsAHqYywnA6E4qcRDb0ehQqwMTWk1gdexqFh12iglUGo3mKkhdvx4XHx/qvP8eLv7+JBTierwgzrhGAYpvKeRG/rCurVQApVQWMB4jlOchYIlS6oCIvCwi/czJ/g84JyIHgQ3AVGf1q/RQ84foUbcHb+58kz8S/rC3HI1GU0ao7GxS1m/Ap+stuFWvTsDoR0j75VfSdhQ/6GzxexRUcbqPDolILNBYRPbl2f4UkX3FnGdBKbVKKdVIKdVQKTXbvG+6Uup783ullJqilGqqlGqhlFpc6iuyEyLC7C6zqe1bmyc2PsHZjLP2lqTRaMqAjL37yD53Dr9bewJQdcQI3IKCSHxrbrHdxab4BKRSJVyrXDFM6tAUaRSUUsMxIq4dxXCVnbtFcB25zr4a/Dz8mNtjLimZKTy56UlMOSZ7S9JoNKUkdf06cHPDt1tXAFw8PQkY/xgZe/aQumFDkecZaxRqXBFn29EpdqBZKXVGKRUKnAb8zNsppZR1c7KuQxpVbcSLnV5kd/xu3tn9jr3laDSaUpKybj0+7cJxrVzZsq/KgAF4BAeTOPdtlDn2d0Gy4uNxd7KuI7Bi9pGIdANiMFYnfwBEi0hXWwtzZiIaRDCs8TA+P/g5a2LX2FuORqO5Ri4fP07mX3/ha+46ykXc3AicPInLMTEk//hjoeea4p3PxQVYNyX1LaC3UqqbUqor0AfQvh1K4Knwp2gZ2JIXtrzA8YtON6FKo9EAKevWAeDX89Yrjvn17o1ns2YkvvseOZmZ+Y4ppSzdR86GNUbBXSl1JPeDUioacLedpIqBu6s7/+32XzzdPHl84+OkmdLsLUmj0VwlqevW49m0Ke61al1xTFxcCJzyOKaTJ7kYtSTfseyLF1GZmU4VXCcXa4zCLhH5VES6m7ePgd22FlYRqOlTk9e7vk5scizTt0zXC9s0GiciKzGRjL178S2klZCLT6dOeLdvz9n588lO/ffBLyshN7iOc7m4AOuMwqPAAWAiMAk4iOEkT2MF7Wu1Z1LrSaw5sYZP939qbzkajcZKUjZsAKXw69mzyDQiQtCUx8k+f57z//vcsv/f2MzO11IoMS6CUuoyxrjCW7aXUzEZ1WwUh88f5p3f36FqpaoMbDTQ3pI0Gk0JpK5bj/sNN1CpceNi03mFhuLX6zbOf/oZVYcPx61q1TyxmSvmmIKmlIgIszvPpvMNnXl5+8t6RpJG4+DkpKWRtm0bvj1vtWqdQeCkSeRkZHDuw48AI7gOIrg5kANPa9FGoZxwd3Vnbve5tAxoybRfp7H11FZ7S9JoNEWQunkLKjPTsoq5JCrddBP+/ftzYdEiTKdOkZUQj2v16oi7883J0UahHPFy8+L9nu/TwL8BkzdMZm/iXntL0mg0hZC6fh0u/v54t21j9TmB4x8DpUicNw9TfLxTdh2BdYvXGonIxyKyRkTW527lIa4i4l/Jnw97fUiAVwDj1o4j5kKMvSVpNJo8qKwsUjZuwq97N8TNqnD0ALjXrk3Ve+8l6dsVXNp/wCnXKIB1LYWlwO/A88DUPJvmGgnwCuCjXh9RybUSY34eQ1xKnL0laTQaM+m7dpOTlHTFKmZrqD5mNC5eXmSfP++UM4/AOqOQpZSar5TaoZTanbvZXFkFp45fHT7s9SGXsy8z+ufR2quqRuMgpKxfh3h44Nvl6kPJuFWrRtVRIwFIr+JZxsrKB2uMwg8iMk5EaolItdzN5squA0KqhvDBbR9wNuMsY34eQ3Jmsr0laTTXNUopUtetx6djR1x8fK4pjwv9u7AzRNhZL6uM1ZUP1hiFBzC6i7ZirGTeDeyypajridDAUN7u8TbHk47z2NrHdJxnjcaOXD5yBNPJk8WuYi6Jo5kneWOQK3/Vcs55PCWqVkrVL2RrUB7irhc61e7Ea7e8xr6z+5iyaQqmbB2HQaOxBynr1oEIfj16XHMeuZNHEtITykpWuWLN7CN3EZkoIsvM23gRcb7Jtw5O7+DeTO8wnS0nt/Dc5ufIUTn2lqTRXHekrluPV2hoqRadRV+IBiqwUQDmA20wYil8YH4/35airlcGNhrI420eZ3Xsat7Y+YZ2oKfRlCOm06e5dPBgqbqOAGIuGi2F+PT4spBV7lgzCTfcHH0tl/Uioldd2YhRzUaRmJ7Il4e+JMg7iFHNR9lbkkZzXZCyzlh+5dfztmvOIzkzmTNpZ/By8+Jsxlmyc7JxdXEtK4nlgjUthWwRaZj7QUQaAIXHn9OUGhFhavhU+gb35a3db/Hj8cKjOmk0mrIldf06POrXp1KD+tecR+54Qrua7chW2Zy7dK6s5JUb1hiFqcAGEdkoIpuA9cATtpV1feMiLszuMpvwmuG8sOUFtp3aZm9JGk2FJjs5mbQdOwuNsHY15BqFzjcYaxyccVzBmtlH64AQjHgKE4HGSqkNthZ2vePh6sE7Pd6hvn99Jm+YzKFzh+wtSaOpsKRu+gWysq5pFXNeYi7E4OfuR8vAloBzjisUaRRE5Fbz6wDgTuAmoCFwp3mfxsb4efgxv+d8/Cv58+jaR7U7DI3GRqSsX4drQABeoS1LlU/MxRhCqoZQw9vwe1TRWgrdzK93FbJF2FiXxkwNnxosuG0BphwTY9eO5cKlC/aWpNFUKHIyM0n75Vf8enRHXK99UFgpRcwFwyhU86yGm4tbxTIKSqkXzW9fVkqNyrsBM8tHngagQZUGvN/zfc6knWH8uvF61bNGU4ak/7aDnLQ0fG8t3XjC6bTTpJpSaVS1ES7iQqBXIPFpFaj7KA/LC9m3rKyFaIqnVVArXuv6GvvP7eepX54iK8c5/apoNI5Gyrq1iLc3Ph07liqf3EHmkKohAAR5B1WsloKINBGRgYC/iAzIs40EnNP9n5PT88aePNf+OTbFbWLW9ll6cZtGU0qUyUTq+g34du6Mi2fp/tZyVzLfVOUmwDAKzjjQXNzitcYYYwdVMMYRckkBHrGlKE3RDGk8hDNpZ/j4z48J8g5iXNg4e0vSaJwSZTJx8oknyUpIwP+e/qXOL+ZCDLV9auPn4QdADe8abDm5pdT5ljdFGgWl1Hci8iMwTSn1Sjlq0pTAhFYTSMxIZP7e+QjCIy0fwc3F+ghRGs31jsrK4uTUp0hZs4agp6fhV8rxBPh35lEuNbxrkJ6VTmpmKr4evqXOv7wodkxBKZUN9ConLRorERGmd5xORIMIPtj7ASN/Gsk/yf/YW5ZG4xSorCxOPTWNlJ9+Iuipp6g+cmSp88zMziQ2KZZGVRtZ9gV5G5HXnK0LyZqB5q0i8r6I3CIirXM3azIXkb4ickREjorI08WkGyQiSkTaWq38OsfdxZ1Xb3mV1255jeMXjzPwh4Esj16uxxk0mmJQWVmcmvY0yatWETT1Sao/WDa+xf5K+osslZWvpeCsRsGaPodO5teX8+xTQLHtLRFxBeZhtDTigJ0i8r1S6mCBdH4YK6V/s1a05l/uaHAHrWu05vnNzzNj2ww2xm1kRscZVPeqbm9pGo1DobKzOfXMsySvXEnglClUf+ihMss7d5A5pEr+7iNwvgVs1ri56FHIZk0HXDvgqFLquFIqE1gM3F1IupnA68Clq1KusVDTpyYf9f6Ip8KfYuvJrQz4fgAb/taeSDSaXFR2NqeffZbkH34gcPJkAkaX7VyZmIsxuLm4Uc+/nmVfoLcRk6HCGQUR8ReRt0Rkl3n7r4j4W5H3DUDeju448768ebcC6iqlinUFKiKjc8tPTEy0oujrDxdx4b6m97E4YjGBXoFM3DCRGVtn6IVumuselZ3N6eeeJ+m77wmcNJGAsWPKvIzoC9E09G+Iu8u/8cc83TypUqlKxTMKwGcY01CHmLdkYKEV50kh+ywd3iLiAszFCo+rSqmPlFJtlVJtA0sREel6IKRqCIvuXMRDzR/im5hvGPj9QPYk7LG3LI3GLqicHE6/MJ2kFSsIGD+egEcftUk5ue4tCuKMaxWsMQoNlVIvmruBjiulXgKsidEcB9TN87kOcCrPZz+gObBRRGKBDsD3erC59Hi4ejC5zWQW9l2IQvHATw/w3h/vkZ2jw2Borh9UTg6np08n6ZtvCBg3jsDxj9mknKTLSSSkJxRtFJzM1YU1RiFDRLrkfhCRzkCGFeftBEJEpL6IeADDgO9zDyqlkpRSAUqpYKVUMLAd6KeU2nVVV6ApkjY12rDsrmXc1eAuPtr3EY+ufZSky0n2lqXR2ByVk8OZF2eQtGw51R8dS8CE8TYrK9e9Rd7pqLnU8K5RIbuPHgXmiUisiJwA3gdK7JRTSmUB44H/Aw4BS5RSB0TkZRHpVxrRGuvx9fBlVpdZzOg4g13xuxj24zCOnD9ib1kazdVzORXmd4bo/ys2WU5GBqemPc3FpUupPno0gRMnIlJYb3bZUNjMo1yCvIM4f+k8phyTzcova6yZfbTHHKO5JdBCKdVKKbXPmsyVUquUUo2UUg2VUrPN+6Yrpb4vJG133UqwHQMbDWRh34Vczr7Mfavv46fYn+wtSaO5OqJ/gvj9sGdRkUky//6b2GHDSf7xRwInTyLw8ck2NQhgzDyq7FHZsi4hL0HeQSgUZ9PP2lRDWWLN7KPqIvIusBEjLOc7IqInwTshoYGhREVE0bhqY6Zumsrc3XP1OIPGeTj4nfF6fANkX+klOHXTJv4aNBjT6dPU/XABAWPH2twggNF91Khqo0LLyl2r4EyDzdZ0Hy0GEoGBwCDz+yhbitLYjkDvQD7r8xlDGg3hs/2f8di6x/Q4g8bxyUyDmJ+hyo1wKQlO/tupoHJySHx/Hv+MfRT3G26g/vJl+HbtWi6yclROkTOPwDlXNVtjFKoppWYqpf4yb7MwPKdqnBR3V3de6PgCL3Z8kd/O/MawH4dZ+kU1Gock+v8gKwP6zgFxhaNrAchOSuKfRx/l7Pvv49+vH8GLvsKjbt0SMis7TqWeIj0rvUij4Iyrmq0xChtEZJiIuJi3IcBKWwvT2J5BjQaxsI8xzvCfVf9hTewae0vSaArn4HfgEwSN+kKdcIj5mUuHD/PXoMGkbd1GjekvUGvOq7h4eZWrLEtgnUIGmQH8K/nj4eJR4YzCGGARkGneFgNTRCRFRJJtKU5je8KCwoiKiKJR1UY8sekJ3vn9HT3OoHEsMtMhZg3cfBe4uELIbSRtO0Ls0GGoy5ep97/PqXbvveUyflCQmIv5o60VREScbgGbNbOP/JRSLkopN/PmYt7np5SqXB4iNbYld5xhUKNBfPLnJ9y3+j4OnD1gb1kajcHRn8GUDk3vRmVmcmZVHKe2V8WrQU3qL1+Gd6tWdpMWfSGaG3xvwMfdp8g0NXyca62CNS0FRKSfiLxp3iJsLUpT/ni4evBixxd59ZZXOZV6iuErh/PStpe4cOmCvaVprncOrADvAFStdpx48EEurFhDtebZ3Di8Nm52dntT3CBzLs62qtmaKalzgEnAQfM2ybxPUwGJaBDBD/f8wIibR/BtzLfcteIulhxZoruUNPbBlGEMMt8cwfmoKDJ27abW7NnUuLcH8tcGsON9eTn7MieSTxS6kjkvuauanSXWiTUthTuAXkqpz5RSnwF9zfs0FRQ/Dz+mtZvG0ruWElIlhJnbZzJ85XDtWO86ReXkkHXhApePHiXttx1kxsaWX+FH14Ipjey6vTg7fwE+XbpQZeAACOkFGefhlP3uyeMXj5Otsq1qKWTmZDrN1G9rA/tWAc6b31vjNltTAQipGsJnfT7jp9ifeHPnm9y3+j7639Sfya0n6yA+FQilFGm//MLlo8fIOn+O7LPnyDpnbNnnzpF1/jxk5Vks5u5O8KJFeLVobntxB1aAVzXOrtpHTkoKQVOnGvsb9ADEGG+o08b2Ogohd5C5UZXiWwp51ypU8XT82fzWGIVXgT9EZAOGO+yuwDM2VaVxGESE2+vfTrc63ViwbwFfHPyCdSfW8VirxxjaeChuLtY+V2gckezUVM5Mn07yqtUAiLs7rgEBuFWvjntQEJ5Nb8ategBuAdVxrVYdV39/Tk+fzsnHH6f+N8txrWzDuSamSxD9E5k1+3J+7mKqDByAZ2PzH7BPdbihjdGS6F5kpF+bEnMhBg8XD26sfGOx6fKuVWhcrXF5SCsVxf6ixZjjtRnDrXU4hlGYppQ6Uw7aNA6Et7s3U9pMof9N/Znz2xzm7JjDV4e+okOtDrQKakVYYBh1/OrYZVqg5tq4dPgwJydNJjMujsDHH6fqvcNx8fUt8Tu84a3/cuK++zn9/Avc8M7btvvOj62DzFQSNqcj7u4ETJiQ//hNt8Gm1yD9PHhXs42GYoi+EE3DKg1LfDBytgVsxV6NUkqJyAqlVBvyuL3WXL808G/Ah70+ZP3f61kavZTVf61mafRSAKp5ViMsMMwwEkFh3Fz9Ziq5VrKzYk1BlFJcXLKU+Nmzca1ShXqfR+Ld1vowJt6tWhH0+GQS3niTC4sWUW3ECNsIPfgd6cnVSdm6h4AJ43EPKuBwLqQXbJoDx9ZDi0G20VAMMRdi6Fi7Y4npArwDEMRp1ipY0/bfLiLhSqmdNlejcQpEhJ71etKzXk9yVA7HLh7jj4Q/2Ju4lz0Je1j/z3oA3F3caVq9KWGBYYTXDKdtzbbFzufW2J6ctDROvziD5B9/xKdzZ2q//hpu1a9+fKjaqFGk7dhBwpzX8AoLw6tZs7IVmnUZdXg18X/Wxi3Ik+qjRl2ZpnYr8KpmdCGVs1G4eOkiiRmJJc48AuN3UM2zWsVoKZjpAYw1R0dLw+hCUkqplrYUpnEOXMSFkKohhFQNYUjjIQCczTjL3sS97E3Yy57EPSw6vIjPD36Om7jRMrAlHWp3oGOtjjQLaJYvpq3Gtlw6Es3JyZPJPHGCwEkTqT5mDOJi1VKlKxAXF2rPmcNf/e/h5ONTjPEFX9+yE3tsPckxmVz6J5larzyNi7f3lWlcXKHhrXB0HeTkwDVey7VgWclchHuLgjjTqmZrjMLtNlehqVAEeAXQ88ae9LyxJ2DM596TsIdtp7ax/fR25u+Zzwd7PsDH3YfwmuF0qGUYifr+9fWYhA1QSpH0zTecmTkLFz9fbvzsM3w6tC91vm5VqxrjC/c/wJnp06n93/+W2feXs/dbEvdVoVKTxvjfXUxMrptug/3L4Mw+qB12bYUlRkO1BuBq/aQJS2CdEqaj5lLDuwan0k6VnNABKLIWRMQTGAvcBPwJfGqOpqbRXBWVXCvRvlZ72tcy/oiSLifx2+nf2H56O9tPb2fjPxsB42mqQ60OtK3RljY12lDXr642EqUkJz2dMy+9TNJ33+HdsQM3vPEGbgEBZZa/d5s2BE6aROJbb+Hdrj1Vhw0tfaZZl7nw3TpMaR7cOG0a4upadNqbjAcPjq69NqPwz0749DZoMwruetvq02IuxFClUhUCvKyryxo+NdiT6BzrfIozjZ8DJuBXjNZCU4yVzRpNqfCv5E/v4N70Du4NQFxKnMVA/Br3K98fM+Y0BHkF0aZGG9rWNIxEA/8GtjMSp/6A7fOhegi0fdCY8ujkZOzbx6lnnyXz2HECxo8n4NGxxf/BXiPVH36I9J07iX/lFbzCQvFs0qRU+WXtWcnZfW74tm2KT8cSBnJ9g6BWqGEUuj55dQUpBT9PN97vXghh90LddladWlxgncII8g7i4uWLXMq6hKeb59XpLGeKMwpNlVItAETkU2BH+UjSXG/U8avDIL9BDGo0CKUUx5OOszt+N7vid7H7zG5Wxxpz6KtWqkqbGm0shuKmKjcVOR0wJzOTS/v2kb5zJ1nnL+DbpTPeHTrgUqnAbKjzx2HdTDjwDXj4QmYq/PomhA6HDuMgsOSBxKK4fOwYKT+vReVk4+rvj6t/FeO1SmGm1QMAACAASURBVBVcq/jj6u+Pi59fmRu69N27OTt/AWmbN+NavTo3fvoJPp06lWkZeREXF2q/Zh5fmDSZ4OXLcfW99gkFZ+d9QE6WEPTCLOtOuKkXbJ4LGRfB6yoWh8Wsgb+3Qq+X4bcP4YfJMGYTuBY/zpWjcoi5GMOAkAFWF5W7gC0xPZG6lcsv3sO1UJxRsESaVkpl6Wa8pjwQERpWaUjDKg0Z0ngISiniUuLYFb/LMBLxu1n7txFgxd3FnQb+Dbip6k2EeAfT9LQrNaLP4br3MBl79qAuXzbyrFSJC198gXh749ulC7639sC3bXPc9n0Iuz4DVw/oOhU6TYDk07D9AyMO8O6FENIbOj4G9buBFb8BU3wCyStXkvTjD1w+eKjkC3Z1xbVyZVz9/XG/4QZ8e96K3223XTn9sgSUUqRv387ZD+aTvnMnrtWrE/TkE1QZNrxUf9DW4latGjf8901OPDCSMzNmUPuN16/J2F2OiebC9jiqtKtNpcY3W3fSTbcZhvz4RmjW37pzcrJh7QxjLKHDOKjWEKJGGK3FzhOLPfVkykkysjKsHmSG/KuandkohOaJlyCAl/lz7uwj7TZb8y/ZWUaYxIwLhW+ZqYb748x0MKWZX9Pz7Es3Qi7mZIO7F3j4gIcP4u5NXQ8f6nr4cI+7N3g05kxgU37PSCLxZCrsPEe1I6upF5eJezZkAP/UdOFM+wBMLUPwC29P0xtaUf9YGqnrN5C6fh0pa9aAKLwDTfi2uwW/h17A42azqwRPf+j3LvScbhiMHR/B/+6GGs2NP48Wg8Atf2sjOyWFlDVrSPrhR9J/+w2UwrNpE2o8dj9+bRvg5m4i++I5ss+fJfvCBbKTk8lOSiY7JZXslAyy0y6Rkx7HpYMniN+yhfiZM/FqEEDlDs3x694F94ZNwa82/H975x0eVZX+8c+ZnplJZtJ7hdAJvUgTFFHBhljXuqtrX3tbXV3Xx139WXbVtZd1VVBXBBUr0qQo0nsnIT2QnsykTD2/P+4QWkgmIUAC9/M8w70zc+fOudzM+Z7znrdYoo/wsJFS4ly8mIo336JhwwZ0MTHEPvZn7JdffsILzpiHDSP6T3dR9sqrmEcMJ/zyy9t8jtJn/oJGK4m+47bgP5Q0DIw2xYQUrChs+AxKt8Ll/1VmBr2mQI/z4ednlXPYjx6lvLNaWWQOxh11P10pgE10lcx9+xk6dKhcvXp16weqtIqUEr/TiXffPrylpXhKS/GWluGvr0O63Ei3G+l24Xe5Djyvq8VfW4Z0ViFdDSC9COkDGchWKZRRg/KPbHquMWrQhmjQWXRoLQa0VhPa0BB0YRa0Nitauw2NNRSh1eNz1OItr8ZTUYO32om3qgFPTSNehwePw4+3TuJzBTpHITFFCXQ9YqnK6sHOrHR2hvjYXZPDrqpdVLuqAegb0YfrTclMXPcVvoJqHK4snEUGXNl5ABgzu2OdcBaGtDQ0oVa0oaFoLFa0IXo0hT+j2fgBmsptSvWv4X/E3+18nIsXUfvTzzjX7ER6fejD9di6C8LiKzCaHc3/p2t0ipnKGKo8DFYwBp5LiWtPAbWbS3HsduOqVswYpkg3YckNhKZ4MMTHQmg8UqPDscNJ+fIaXKUe9KEaIoeasPXWodH4FXGVPojsDpe+e8IifqXPR8Efb6F+zRrSPv/8QFqKIKhbuZL8628gepCLqI+2gr4NtvfPr1cWje/f2vqMztMA/x4CoXFw84IDx1fnw+sjIGM8XP3pUT/+1oa3eGP9G/z2u98w65txlW0Gh9vBqE9H8cCQB7ix341BfaajEUKskVK2GqWoJq7pwnirqvBVVSM9+zvwAw9/075HeV5Xh7es7IAAlCkCIBsajjyxRoMwGtEYDAiDHqHxIaQb4a9HI10IrUSj1yHsNsX0otEjhU7p8DQ6ELrAcy2gBaHB43DSWFWFN7cKPC7ABRyaNVIYDAidDn99/RFN0oaHo4uLQ58eS0hcLProaEwJZkLstWjL10H+CpKr1pG1GKWjTRyCTD6PirjeLNy3io/3fMsj2q3Ex1i5ZvQNTBt2HzEGK+6CApwLF+JYuIiK998H39FTMQtdChqDRPO/d/G53sfv0aA1+rCnN2DrZcLUPQZhTwZbMtiSlEdYEpjDwRCqdP46U4udlhGIBqL9flxb1+L44RscP/9K6fpCSteDKSEES2odzl1OXOUeDBF64i9MwJYVidDplJmERqfUMRYCts6B6ZfC9V8rs6DjjNBqSXj+/8iZOpXce+4k/O1/E5va+sKz9Pspfe45dBY/ERePb5sggGJC2vq1MvqPbSWQbuU7UFsEU98+9F7YU5Q8SvOehG3fQu/mS8fsrNpJUmhS0IIAYNVbMevMXSJWQRWFLoY7Lw/H/AU45s+nYf16xYMiSITRiC42Fl1MNCF9+6KbEIsuJgZdTAz6WGWrs+rQ7FsNe5bCniVQvlH5sDEMUkdB2lhIHwux/dsVLCSlxF9Xh6+yEl9VlSJslVWKWaWqEr/LrbQlNg59XCy6uDh0MTFHLhA3R3UBFKxQHvm/IZa+SJT0cwVwWUxvlgy+gg+rN/Hi7s95M+87pmVO49re1xJ/ww1E3HCDIpyVlfidTnwOB36n88C+w4m/LrBfXozwOQmbMBrLmecgIpIVk1dHotFg7DcUY7+hRD0E7sJCHD/NwzF3LhXLN2DM7E7Co7cRdv55LXsU9Z8Ln10D0y+D62YrM5LjjC4qispHbiD0oZeoPHcqu8J1VGfGYhjQn6QzziZz6ET0hkM7/dpvv6Vx6zYSRtagGRD8Am4T3Scq213zWhaFhipY+pKyVpQ+9sj3R94BG/4HPzyszBiMRwbk7fc8agtdqSynaj7q5Egpady6Fcf8+TjnL8C1S4mkNPbpTehZZ2NITVVG2AY9wmAIjO4Pe+j1aMzmo3u6+P2QsxDWfAg7vge/F/QWSBkJ6eOUH0/cgDYF93QKXA4oWqOsWfQ4NzBzgS0VW/hwy4f8lPsTAJNSJ3FD3xvoG9XBqRqOEz6HA43FEnw08rZv4PMblPt5zRdgCH6E2x4Kagu48rsrGVgbzsWVaXg3biFidxl2hx8Alx72pYbh7dONiKFnkDn8HKp+fwc6bR1pEwoRD+9un8i+MUoxk9347dGP+ekJ+PXfcNsyiDtK6u+ClfD+OXDGXXDu3w95q9HbyIhPRnBL1i3cOfDONjXv5rk30+hrZPrk6W36XEehmo+6MNLrpX7NWhzz5+NYMB9vcQloNJiHDCH2sT9jPetsDEmJx/5FtcWwbjqs/Rhq8pU8MiNug94XQeLgVl3zOj3GUGW0dxh9I/vy/LjnuW/wfczYNoNZu2bxQ+4PDIkdQlZUFma9GYveokz5A/uHPHQWrAbrSUsbrg1tfbTv9Xv5ueBnKhsrubzX5Yhp78Ksm+Gzq+Hq/7XdPBMk9Z567l50Nxqh4bHfvU1SaBIAfr+f4uwN5Cz7gdp1qzFtzSXh23Xo5qyjjDcAiJrsRvQ8r/2zrsyJsPwNZTDQ3IyoplBxPR1w1dEFAZRYhSE3Kp5IWVdC/IGMPtk12filv02eR/uJMcewel/nH9CqotBJ8FZVUbdsGc6lS6lbshRfdTXCYMAyejShd96F9awJ6MLDj/2LfF7FP3vth8pW+hV3y3Oegl4XHOFZcyoTb43nwWEPctuA25i1axYzd87kk+2f4PK5Wv2sVW9lSsYULu9xeafKkV/jqmHWrll8tv0zSupKAHB6nPyh3x/A64KvbofPr4MrZ4DO0KHfLaXkyV+fJKcmhzcnvtkkCAAajYakzEEkZQ6CQG67OkclO5b/QPHyhfxc/hvWDD9P9bm4/Q3oPhF+eUUxe/aacuT7i/6hbCc83vq5Jj6lrCt8ex/c9FPTLHNXVSDnUZDpLQ4mxhxDWX0ZfulHI05cnqa2oorCSUL6fDRu2oRzyVKcy5bRuGkTSIk2PBzLuLGEnj0R65jRaCwd5GNelQfrPlZmBo4SsMbC6Hth8HWKr/ZpjNVg5Ya+N3BD3xsA8Pg91HvqqfPUNT3qPfU4PU5l31vP5vLNfLnrS/63439kRWVxWY/LODft3DYtPnYku6t2M2P7DL7N/pZGXyPD4obxyLBHmJs7l3+t+RcJlgTOG/g7RRi+vRe++P0Bd8wO4oMtHzA3dy73D7mfUQmtB8tZQiMYPOkaBk+6huz/XcB7jXkM0/topjsPjuSRipPBrnlHisK+LUrsyRl3gj2IOIGQcDj3H/DlLUq8yrCbAUUUjFojKaEtF9ZpjlhLLF7ppbKxMuj0GCcDVRROIN7ycpzLllG3ZCl1v/yCr6YGNBpCsrKIuutOrOPGYerbt92ZKw/B51Xs6dkLlCySRWuU1zPPgckvKjb2rm4eOk7oNXpsRhs2Y8veOo8Of5Rvsr9h5s6ZPPnrkzy/6vk2zR780s/eur1kV2eTU5NDjauG5NBkUsNSSQlLIdIU2WIAmM/vY2nRUqZvm86KkhUYtUamZEzhd71+1/T9Y5LGsK9+H48ve5wYcwyDh/5eEYYfH4HZt8C095pGwcfCr0W/8sraVzg37Vxu7Htj2z7s83Jn3jbWxEfz9Mrn6BsziDRbWtsboTMos97dCxQHjIP/7xY8rThLjH0g+PNlXQHrZ8D8p6HXhRAay66qXXSzd0Pbjv+zgwPYVFE4jfE5ndR++x3VX86mcYPiyaONjMQ6fjyWcWOxjBrVMWYhUGymuxcoQpDzsxJMJjRK2cIJjyu21GBGSSpBYTPauLbPtVzT+xrWla5j5s6Zzc4ejFojxc5ismuymwRg/7bBe8AlWCM0+KW/6blVbyUlLIXUUEUkUsNSSQ1LJcYcw7y8eXyy7RMKnYXEmGO4Z/A9TMucRrjp0L8lo9bIKxNe4dofruXuRXczY/IMUkfeBj6X4nqpM8LFbxxT2ukCRwEPLXmIbvZuPD3q6TZHMrv3LMNQX87z/Z7ksh3v8tCSh5g+eXr7CjRlToQd30H5TogOCHPuL7DzR8Uk1JZ4DSFgyj/hzTNg7mNw2fvsrNrJmMQxbW8XBwWw1ZXSN7LzOjUcV1EQQpwHvAJogfeklM8d9v79wM2AFygD/iClzDuebToRSClp3LCBqpkzqf3+B2RDA8YePYi+9x4sY8di6t27Y2YDnkbI++WAEJRtV14PTVAWi7ufrYycTkKpwtMJIQSDYwczOHbwEbOHZ1c+i1/6D1mniDHH0M3WjWmZ08iwZ9DN1o0MWwZWg5USZwm5tbnkO/LJq80jrzaPjeUbmZs39xDBABgUM4h7htzD2Slnt1iXwm6y88bZb3Dt99dy+/zbmT55OhGj71FmDIv+rswYL3jl6MLgaYSqPUqeqIpsMIXBoOtBo6HeU889i5Q8ma+Mf6VN5jMpJQtWbyHp+/tJwYQtcyrPRKXyp4V/4oVVL/CXkX8J+lxN7HdN3T1fEYX9Se9CExQnirYS1R3G3A+Ln6Oy38VUNFa02R11P/tnCp09qvm4iYIQQgu8DpwDFAKrhBBzpJRbDzpsHTBUSlkvhLgdeB7ogNy7JwdfdTU1c+ZQPfMLXLt2IcxmwqZMJvzyyzFlZXVc4jOXA1a8pbjWNdaA1qjEEAy6ThGC6F5B5elR6XgOnz18l/MdJp2Jbnal48+wZxBmOHqGmOSw5GZz47h9bgqdheTV5FFcV8zA6IFtcqFNCUvh1bNe5aa5N3H3wrt5b9J7mMY9BN5GxW9fZ4KhN0Fl9oHOvzIbKvcoM1AOc13fNQ95yVv8dcXTZFdn88bZb7Qpp8/2vbW8MXs+9+19lHhNFXe6/8S522u4Yuh4ru9zPR9t/YjhccObMukGjT0Fonoq6wpn3Anb5kDRarjo3+33ahpzH2yaya4FT4C1fYvMAJGmSLRC2+ljFY7nTGE4sFtKmQMghPgMuBhoEgUp5aKDjv8NuPY4tue4IKWkfsVKqr/4AsdPPyHdbkz9+xP3t78RNmVKxyYj8zTAqvdh2T+hvgJ6TlbSPKeOPu6+5ypt4+DZQ0dg0BoUUbG13ylgYMxAnh37LA8ufpDHlz3OC2e+gOasJ5QZw/LXlEjf/YREKA4IqaOUZHERGRCZoWw3fAZzH+O/08/iR30j9w6+l9GJo4NqQ3W9m3/O28n6FYv4wPACViNor/2G/FkuZqzI54qhydw7+F7Wla7jr7/+ld6RvUkObaPJs/tEWPWuMmBa8LQySBrwu7ad42D0JpjyEju/vg6s4e0WBa1GS2RI5Ok7UwASgYKDnhcCLZV7ugn4obk3hBC3ALcApKS0fdX/eCClpPbb7yh/7TXceXloQkOxX3YZ9isuP+Z88kfgdcO6j2DJi4rnUMYEOOsJSBrSsd+jcsozKW0SD9Q9wIurXyRxbSL3D7kfJj0DSUMV54T9nX9IC+tcI2/nV62Pl7e8xTmNfv5gb70yr9fn59OV+bw0bydZrrXMNL2M3hqJ5rovIboHvxuxh799s5XNRTX0S7Tx/LjnueKbK3h48cN8dP5H6NviFJE5EX57Hb68HSp2w1WfHnPgZWl8X2ZExZLoaSBK3/5coHHmuE4/UziezrLN2S+aDZ8WQlwLDAVeaO59KeU7UsqhUsqh0dHRHdjE9uEpKqLg1lspfughNBYL8c89S+aSxcQ9+UTHCoLPC+tmwGtD4LsHwJ4KN34H13+lCoJKu7m+z/Vc2fNKPtj8AZ/v+FwxNfadClmXK39XLQkCSlGkh7M/IyM0hWcadYj/ToFNXxz1+OXZFVzw72U88fUW/mhbxYfGFzBGd0Nz8/ymehWXDkrCpNcwY0U+oNTYeHr002yu2My/1v6rbReYMgr0ZmXBOeUM6HlsFYVrXDXcOu9WqgS8tK8MCle1+1wx5phOP1M4nqJQCBw870sCjihSKoSYCDwOXCSlbD1q6CQifT4qP/qY7Asvon71GmIf+zNpMz/HfsklHZum2O+HzbPgjZHw9R3KVP6aWfCHHyGtfZ4PKir7EULw6PBHGZc0jr+v+DtLCpcE/dl6Tz33LroXv/TzyjlvYb55kTLLmHUTLPy78rcboLCqnjtmrOHqd3/D0eDhx2HruLPqeUTKGfD77yEsvulYm1nPhVkJfL2+CEejUsplYupEru51NR9v/ZhF+YuOaEtzeP1eFu9dwd3JadwWG03JmLuPaX2t3lPPHQvuIL82n3+Pe5G+Hq/i2ddOuoIoHE/z0SogUwiRDhQBVwGHGPaEEIOAt4HzpJSd+n/KtWsXJX95goYNG7CMGUPcU091TKqJg/H7YPu3sPgF2LcJonvDldOVSGN14VilA9FpdLww7gVu/PFGHlz8IB+e9yG9Iw8Utal117KnZg851Tnsqd3DnhrlUegoxC/9vH7266SEBUy5130F390HS56Hsu3IS95k+tpy/vH9diSS+yd25w73B+hWvqnMSKa+3Wzk/DUjU5m5ppCv1xdz7chUAB4Y+gDrS9fzl1/+whcRXxBvjT/icwBFziJm75rNV7u/orS+lEiDDZfVxtXrnufV8CSyols3cR2O2+fm3kX3sqV8C/8c/0+Gp0xQ3LtzfoazgoiKboYYcwxOj5N6T/1JC3RsjeOaEE8IMRl4GcUl9T9Syr8LIZ4GVksp5wgh5gP9gZLAR/KllBe1dM4TnRDP73ZT8dbblL/7LlqLhdjHHyPsggs6toSi1wUb/6eE6FfsVhb2xv8Z+l3aIYFFKipHo7S+lGu+vwa/38/45PHsqVWEoKKxoukYvUZPalgq6bZ00m3pjIgbwfD4w2oZSwnLX0f+9Bdy9d24ynEvPXv05NmLepD48/3KzHfE7UqU8FFcX6WUTHl1GRL4/u4xTb+xvNo8rvjmCnqE9+A/5/2nyf3W7XOzsGAhs3fO5reS3xBCMDphNNMypzEueRz5tfncueBOyurLeGbMM5yfHrwZyef38dCSh5iXN49nRj/Dxd0D6TcWPgNL/wmP7GlXKvJvsr/hsWWPMeeSOaTb0tv8+WMh2IR4apbUFqhfu46SJ57AnZ1N2IUXEvvnR9FFdKDPv8sBa/4Ly19XFpDjByjub70vUsVA5YSxs3InN829BZ/00M2e0dT5Z9iU/QRrQqvJ/6SUzNlQzE9ffcTz8mWEKZSQK95DLHtJyUV0ztMwqnVTzowVeTz+5WZm3zGKwSkH1ja+z/meR5Y+wk39buKibhcxa9csvsn+hipXFfGWeKZmTmVq96nEWeIOOV9lYyX3LbqPtaVruX3A7dw+4PZWB3RSSp5a/hSzd83m4WEPc12f6w68mbsM/jtFWbzuNbnF8zTHypKV3PTTTbw76V1Gxo9s8+ePBTVL6jHgczop++e/qPr0U3TxcSS/8zbWceM67gvqypU4g5XvKG5z6ePgkjcUryLVTKRyAnE0enj9pzoKNjyAXqthRFYil/ZPY2CyPehzVNW5+ctXm/luUwmDU8ZRffYkEn+4ET6+WCn4MzWQmTQILh6YyD++28aM3/IPEYXJGZNZuXcl729+n/c3v49O6JiQMoFpmdMYGT/yqGknIkwRvDvpXf62/G+8ueFNcmtyeXr005h0R88S+681/2L2rtncmnXroYIASulPvVkxIbVDFGItnb8spyoKh9G4YweFt9+Bp6SE8GuvJebeezo2Kd3y15RU1d5GpbLT6PtUTyKVk8K6/Cru/mwdxdWN/OmsHjgavXyxppAv1xUxMNnOjaPSmNw/HoPu6P4oC7fv45FZm6iud/PQuT25dVwGOq0GkhbBgr9B30uh24Sg22Q16rhkUCJfrCnkyQv6YDMfcEV9dPijAKSFpXFhtwuJDIkM6pwGrYFnRj9Dhi2Dl9e+TJGziFfOeqXZ/EPvb3qfD7Z8wFU9r2q+XoIuECi6Z3HQ13QwXSGqWTUfHYRj4SKKHnwQrdVK4ssvYx48qGNOXLoNlv1LcdsTGhhwJYy6p8kdT0UlGKSUbC2p5ecdZfRLtDEuM6pda1t+v+TtJTm89NMOYsNMvHLVQIamKWZRR6OH2WuL+PDXXHLK64iyGrlmRArXjEghJuzA6LrO5eWZ77bx6cp8esaG8s8rB9A3oWPKfW4prmHKq8t48oI+/GFMx9rdF+Qt4M/L/ozNaOO1s147JHHhzJ0zeXr500xOn8yzY589enrrX/8NP/0F7t9+iAdVsIz6dBRT0qfw+Mj2LVa3F3VNoQ1IKan84L+UvvACpj59SHrjDfSxMcd+4sI1SgqBHd8plcyG3KiE3ts62GtJpdPidHkJ0WvRatpvFswpczJnQzHfbCgmu6yu6fXe8WHcdmYGU/rHK6PzICitbeT+zzewbHc5U/rH84+p/Q8Zje/H75cs2VXGh7/msmhHGXqtYHL/eG4clYbXL3ng8w0UVNVzy7gM7j+nB0bdAfONz+OnNN9BRZGT9KwoLPa2J7a75PVfcDR6mH//mR3r1AFsq9jGXQvvwuF28Py45xmfPJ4f9/zIw0seZmzSWF6e8HKLuaTYuwneGtMms9jBTP16KimhKbxy1ivHcBVtRxWFIJFuNyVPP03NF7MIPfdcEp579thiDqRUFtaWvqRMMU12JRHXiFvVxHSnCWUOFz9u2cv3G0tYsacCo05L/0QbWUk2spLtDEiykRJhbrGzK6pu4NsNxczZUMyW4lqEgBHpEVw4IIGJvWNZvLOMd5bksLvUSaI9hJvHpnPlsGTMhqNbhBdtL+WBmRuod3t56sK+XDksOagOd095HR8tz2Xm6kKcLi8AyREhvHT5QIanR9BY52Fvdg0l2TWUZFdTmuvA51XiFcKiTEx9YDDW8LZVepu5uoCHvtjIZ7eMZGRGcGaitlBaX8rdC+9ma8VWLu1+KV/nfE1WVBZvnfMWIbpWfv9+P7yYqaShn/pWm7/71nm3Uuuq5dMLPm1n69uHKgpB4K2qoujue6hftYrI224l+u6725+91O+HnT8o7mpFq5UiNmfcBUN/f0KKpaucXCqcihB8t7GE33Iq8EvoFm3hvH5x1Ll8bCysZktxLa5AZ2k36+mfaGNAkp2sJBsDk+0IIfh+UwnfbChmdV4VAAOS7Vw0IIEp/eOJsx3asfr9koXbS3lrcTar86qwm/Vcf0YaN5yRSqT1wOjc5fXxfz/s4D+/7KFXXCiv/W4Q3WPa/jfpdHmZtbqA6vJGxkeGUZXnoCS7hspiZfai0QiiU0OJ72YjvpsdnVHDj+9sxmIzcsn9g7DYgp8xNLh9jPjHfMb3jOHVqzvIjHsQ1fVuvt2Uy1tb/0GNZg1xpgxmT51OqCHI/5eZv4f85XD/tjY7hzz5y5MsK1rGwisWtqPl7Uf1PmoFV04OBbffjre4hITn/w/bRS2GRxwdnxe2zFbEoGybkorign8pCbiOUx1clc5BhdPF3C37+G5TMcuzFSHIiLZw14TuTMlKoEes9ZCRuMfnZ8deBxsLa9hYWM2GwhreXJyNz3/owKxnbCgPnduTC7LiSY08upODRiOY2CeWiX1iWZ1bydtLcnh1wS7eXpzNFUOT+ePYDDx+P3/6ZB1bS2q5cVQaj57fC5P+UE+dxjoPq3/IxVnRiNfjx+v24XH78XmUrdftw+fx43H78HslOmAZJRhMWuK62cgcGkt8dxsxaWHoDYee+4K7BvDNq+v5+uX1TL1/ECGhwZUADTFouXRwEjNW5FHu7EOU9djLxNbUe/hp616+3VjCL7vL8folyRHXYdAOwGTqE7wggFL7e8vsQ+s2BEmMOYaKxgq8fu9Jq/PdEp2vRSeAul9/pfCeexEGAykffYh5UDtGIjVFsGkmrP4PVOcp0ceXvqt4Wxxj8i2Vzonb62dLcQ2rtpSxdkc58/ZV45OS9CgLd07ozuT+8fSKCz2qSUav1dAv0Ua/RBu/G6FEAze4fWwprmFDYQ11Li/n9o2jZ1zbR/FD0yIYmhbB7lIH7yzJ4bNV+cxYkYdeq8Fs0PLe9UOZ2Cf2iM8VbK9kwX+30VDrxhZrRqfXoDNoMqBqOAAAG9NJREFUMJl16AxGtHoNeoMGnUEbeGgICTUQl2EjIsGCppW1koTudqbckcW3r29kzqvrufjeQZgswSW3u2ZECv/9NZcv1hRy25nd2vx/AlDb6GHeln18t6mEpbvK8PgkSeEh3DQ2nQv6J9AvMYz3lqbz9++3kV3mpFu0NbgTZ4xXtjk/t0sU/NJPeUP5EXEVnYHTznxU9emn7H3m7xgzMkh68822paporFXys2/4TAliQSp1YUffDT3OP6bqVSqdj5oGD2vzq1idW8mW7RV48urIcGmI8yn32W/RkjkqnjMnpQU9Au4IGpxuTGY9ooUOeV9tI//5ZQ8VTjcPTup5hOnJ6/Hx29c5bJhfgD3WzKSb+hKdcvzMnPlbKvjuzY1EJVq56N5BGEOCGzhd8fZy9tY08vOD41sVoINZsqqYbxbn8nVVDW6fnwSbiSlZ8VyQlUBWku0Q4S51NHLGswu5dVwGD5/XhoSWrwyEmN5wddvWBhYXLOauhXcxY/KMdqXfaC+q+egwpNfLvuf+j6rp07GeeSYJL72I1hrEqMDnUSqbbfwMdvygxBdEZMD4R5Uarqd50ftTBY/PT1FVA+sLqlmVW8ma3EqqiuvIdGvp4dEyzK8BdBhjQ+g5JIaoKDPbfikme14huYuK6T4khr7jEonLCOtQbxm/X1JZXEfJ7uqmhVxnpQt7rJlBk1LoOTwOrf7IwUhsmIk/n9+7mTNCRZGTef/ZQkVRHf3OTGTUtO5HmH06mpS+kZx3S39+fGsT3/57AxfePQCDqfXu55oRKdzz2Xp+yS5nbGbrGZKllHywJIeS/+0h1S+4YUIi55+RzKDAmk1zxISaGJcZxZfrinhgUs/gPcUyxitu5j5vm6wDnT1W4bQRhbJ/KoIQcdkUYu76A8JTDg4naA1K1Smd8UBqCSmVQvcbPlPshvUVYI6EwddD1pVKUiw18rhLIaWkos5NfmU9BU2PBuV5VT3F1Q34/ZDo09DHr+Mcrw6T2wQC4jPtZA6OIX1ANNbwA7bt3qPiqShysnlJETtW7GXHir1EJlrpNy6BHsPjMAQ5Gj4Yj9tH6Z7aJgHYm12Du9EHgMVmIL67ncixVrLXlrLo4+2snJPDgLNT6Ds2odXvk37JxkWFLP8yG0OIlil3ZpHW/8QVkE/PimLSzX2Z+94Wvn9jI1PuGtCqGJ3XL44Ii4EZv+W3Kgr1bi9/nr2JquWljPDrEQLG68yHREYfjWlDkrjrk3Usz65gTGaQ/ycZ42HNB1C8FpKHt3Z0E/tFobPWVThtRCFygA7jyCpsunfhrXebP0ijU0pbarTgqlXEoudkRQi6n63UslXpFPg8fnw+/xGvO1xecsvryCmrY0+5kz3ldRSW1VNR2YDGIzH7wSIFIX5BhE5HD52Wwegw+ULRuvxIjx+NTpDcO4KMgdGkD4gixHp001BkopUzr+7JGVO7sWvVPjYvKWLxpzv5dXY2PYbH0ndsIha7EXeDF1e9F1eDB1e998Dzei+uBi/ueg+1FY2U5TnwBxaeIxIsZA6LJb67nfhuNkIjTU2j3SHnpVK4rYo1c/P4dfZuVv+QS/8zE8k6Kxlz2JHtrat2seDDrRRsqyItK4oJ1/Zq9rjjTbfBMUy80c+8D7byw5sbmXxHFjr90YXBqNNy+ZAk3lu2h321jcSGNe+8kVtex23T11BV5OQ6t4leo+LxNPrY9ksxwy9Mb1V8JvaOJcyk44s1BcGLQvo4QCjrCm0QhXBTOHqNXhWFk412+JXYMgYr5h+vC3zuwH5g2/Q88F78ACUxnan9VZZUgkP6JfUON85KF3U1LhrrPLjqvDTWeWis9+ByKttGpxdXvYfGOg9e95GCcDgWoF/gAYd1gAJMej0hIXpCQg2EhBowh+qJ724ntV9km0f5BpOOvmMT6TMmgdJcB5uXFrH9t71sWXpECZFD0GgERosOQ4gOi83IwEkpxHezEZdha3FBVghBcp8IkvtEsC+3lnVz81gzN4/18wvoPSqegeekYItW/O2z15ayaMZ2fB4/46/pSZ8xCR0eENYWegyPw+f1s/Cj7fz4zmbOv7U/2hZSaVw9PIW3l+Twv1UF3H32kaUw52/dx32fr0cnBHeF2BHCz+hp3aksriN7bSk7V+yl79iW1w5Nei0XDEhg9tpCHI0eQk1BDADNEUo/kfMznPlw68cH0AhNp66rcNqIAuFpykPlhOL3SxqdHuqqXTgqG3FWuXBWHbStdFFX7WoaHR+CBqReg1cnaNRAnfRT6/dRI3w0msAXOMyk1xBpNRBpMSpbq5FIiwG7WY9WaNBoBSFh+kDHrwiAyaJDE2QUcFsQQhCbHkZsehijp3UnZ10ZPq8fQ4gOo1mH0azHGNg3mHXo9Jpj7qBj08I479b+VO+rZ91PeWz9tZgtS4voPiQGjU7Djt/2EpMayjl/6Is9tnPk8O89KgGfV7L4kx389N4WJv2xL9qj3I+0KAtjM6P4bGU+d07o3mTz9/klL8/fyb8X7qZvQhgPpsaz9Yd8Jt3cF5NFT3x3G5GJVjb9XBSUEF42JIlPVuTzw6a9XDEsyLrQGeOVLMcuJxiD9FyicxfbOX1EQaXD8Hp8NDg8NDjc1Ne6D9p6mp7vf63R6eFwBzeNTmC1G9FZ9cgoA64oHcVuD7udDRQ2umnUQIOQeAAERFoMxIaZiLeZibWZ6BZmIi7MRFqUhYxoC5EWw0kd+R4Nk0VPnzEJJ+z77LFmJlzXm+EXZrBhQQGblxbhdfkYcn4qwy5IP2qne7LoNy4Rn8fPspm7WPDBVib+vs9RhfqaESncNn0tP+8o5ezesVTXu7nns/Us3lnGZUOSeGRMN2Y/u5q0rCi6D1Fs9kIIsiYksWj6dkp2V5OQ2fLawqBkOxlRFr5YW9g2UfjlZSWQLfOcoK89xhzD9srtQR9/IlFFQQVQFmIbHB5qSuupq3Ef0uEfLgD7Fz4PR2fUYg5VRuRhUSHEZtgwhxowWnTUaiTFLg+7nA1sLnewda8DR7WSMkGrEXSPttI3PYpJcaEk2EOICzMRbzMRE2Y8JK+OSutY7EZGTevOkMlpeBq9bU4xcSIZcHYyPq+f5V9mgxCKMDTj/XN271iiQ43MWJFPbJiJ26avYV9tI3+f2o+rhyXz7WsbEUIw7qoehwwQMofH8uvs3WxcVNiqKAghmDYkiRfm7qCgsp7kiCBmVSkjlXXInJ/bLAqLCxYjpex0AxpVFE4zvG4fNWUNVO2tp3pfPdWlge2+elz13kMPFhBiPWBzj0kNbdr3GzVUer00asEpJDV+H+VuL1X1Hqrq3FTVO6gu8VCZ7aam4cBswaTX0CsujIsGJNA3wUbfhDB6xoUeEWWrcuwYQ3RBxwOcTAafm4rfL1nxdQ4ajeCsG3ofIQx6rYarhiXz2qLd/LK7nHCzgc9vPYNBKeHsWLGXgq2VjL2yB6ERhwqg3qClz+gE1i8owFHZeMT7hzN1UCIv/rSDWWsLuXdiEFmM9SGKMLSxbnOsOZZGXyO17lpsxo7JLttRdP6/GJU2I6WkvtatdPx766jaW0/Vvnqq99bjqGqEg8w51nAj9lgzmUNjsceasceasYYbFbu7VY9GI/D5JTv3OViTV8Xa/CrWbS1mT3ndEd8botcSbtZjNxsIt+hJsIcQbjYQbtaTEW2lb0IY6VGWoDN6qpw+DD0/DaRkxZw9IOCs648UhquGp/Du0hwGJtt57XeDibIaaXC6WTZzF7HpYfQ7s/nF5H5nJrJufj5blhYx8uKWI6MT7CGM6hbJ7LVF3HN2ZnCj+IzxSu0IZylYg8uuHGs+UGxHFQWVDsPv81Nb3khlSR3V++qp2i8Ae+txNxwY9euNWuyxZuK72+gdG9/U+dtjzOiNR47Qq+vdrMivZm2+IgIbCmqasmNGWQ0MSgnniqHJ9EsMI9JiJNyiJ9xsUEf7KsfE0MnpSAkrv9mDEHDWdb0PidpOtIfw66NnYwvRNy02/zJzN+4GLxOu7XXUiOewqBDS+kexdVkxQyentegCCzBtcBL3f76BVblVDE8PIrNxxnhFFPYsgf6XBXWtBwewZYYf6VF1MlFFoRMjpcRV56WmvIHapkdj076j0oU8yGvHYjNgj7PQY3gs4XEWwuPMhMdZsNiPXIjdH8yVU1xDTpmTnPI6csqcZJfVNc0CNELJ2T91UCKDU+0MSYkgOSKk09lAVU4dhk1JR/olq77LRQjBhGt7HSIMEZYDrsX5WyvYsWIvQyenEZnYsudP1oQk5mwsZ/eaUnqNbLkwznn94njiq818saYgOFGIH6CkyM9Z1C5R6GyootBJqK91U5pbS2leLRXFdUrHX9ZwxKJuSKheWcRNt5E51IQ9Vun47XHmZu3HUkqKqhvYXFRDdlkd2WVOcsoUAahtPDCbMOg0pEda6BUXymVDkhiUYmdAkh2LUf0TUTmxDLtAmTGs/j5XiUq+ptcReZ48Lh+LP9mBPdbMkPNTWz1nUq9wwuPMbFpUSM8RcS0ObMwGHZP7x/P9pr387aJ+hLSWAkSjVQLZsn9WsiEEMWjqzFHN6i/+JNDo9FCaX0tpnoPS3FrK8h04q1zKmwLsMWZsMSHEd7MTFmUiLCoEW3QIoZGmVvPF+PyS7XtrWZNXxapcJZlbSU1j0/txYSYyoi1cNDCBjCgrGdEWukVbSbCHHFN1MBWVjkIIwfAL05FSsuaHPNAIxl/d8xBhWPntHmrLG5n6wKBWzUH7z9l/fBJLPtvJvtxa4tJbtuNPG5LEzDWFzN2yl0sGBZE0M2O8kiyzMgciW8/oatAaCDeGq6JwOuJq8FKW76A0r5ayPGVbW36gk1Zs/XZiUkOJSQ0jKtkaVKKw/TS4fawrqGJNbhWr8qpYl1eFI2D/jwszMSw9gqGp4QxKsdMt2qqO/FW6BEIIRlyUgfTD2rl5CCE482rF3bQs38GG+fn0GZvQqpvpwfQcGcfyr7LZtKiwVVEYnhZBUngIs9YWBi8KoJiQghAFgFhLrGo+OtVxN3gpK3BQmuegLK+W0nwHNaUNTe+HRpqISQ2l79hEYlJDiU4JxdhMfdzmaPT42BPI6aOYgJzsLnOyvcSB1y8RQinOcvGgBIamRjA0LZxEu2r/V+m6CCEYeUkGUkrW/ZSPEDDmikwWfryNkDADo6a2rcaCwaSj9xnxbF5SxKhp3VusBKfRCC4dnMS/F+6ipKaBeFsrJTojMsCWorimDrs5qPZ01qhmVRTaifRLygoclOyuoTRPMQVVl9Y3uXtaI4zEpITR64z4JgFoKbEagNfnZ29tI/mV9Qd1/sq2qLrhkMjgRHsIGdEWbhmXwbC0CAanhDdbgF1FpSsjhOCMqd2QEtbPy6cku4aKQifn3dIv6AHVwfQfn8TGRYVsXVbMsCnpLR47bXAiry7YxZfrirhjfPfWGgoZZ8K2b8DvO5BxuQVizDFsLt/cluafEFRRaAO15Q0UbKukYFsVhTsqcdUpZhpruJHolFB6joglOiWM6JTQZjNQNnp8FFc3UFTdQGFVA0VVyv7+7d7axkNKM5oNWjKiLQxOCefyIclN9v/0KEvri18qKqcIQghGXdoNKSUb5heQPiCKjEGt11ZoDnusmZQ+EWxZUsTg81JbTP2RGmlheFoEs9YUcvuZ3VqfdWeMh3UfQ8kGSBzcaltizDFUNlbi9rkxaE98xtqjoYpCCzTWeSjcXkXh9koKtlU2rQVY7EbSs6JI6hVBUs9wLPYjp6FOl1epw1tQw/qCKjYW1hyy4AtKeoe4MBOJ9hBGpEeQGB5Coj2EpHAz3WIsxIWZVPOPigqKMIye1p2kHuHEd7cd0++i/4Qkvnt9IznrysgcemSJ0oOZNiSRR2ZtYn1BNYNaq8uQPk7Z5vwclCgcHMCWFJoUTNNPCKooHITP62dvdg352yop3FZJab4DJOhNWhJ7hDPg7GSSe0dgjzU3W5B9Q2E16/Or2VBYza5SZ5O5Jy3SzPD0CLpHW5s6/sRwJb+PGt2rohIcQgjSso69KFBK30jCokxsWlTYqihM7h/PX+dsYdbawtZFwRoDsf0UURh7f6vtUEWhEyKlpKa0gfytlRRsraBwZzVelw+hEcRlhDFsSjrJvSOISQttmmZW1rlZsaeSXfsc7NznZFtJLZuLa2j0KPn9IywGBibbmdI/gQHJNgYk2Qm3dJ6poYrK6Y5Go7in/vLFbsryHS3Wpg416Tm3bxzfbCjhiQv6tJ6cMWM8rHwXPA1KXqQW6KwBbKedKLgavIo5aGsl+VsrcVQoJp2wKBO9RsSR3EcxCdVLPzv3OVmwr5pdmwvYuc/JrlIH5U5307lCjTp6xoVyzYhUBiTbGZRsJylc9fhRUens9B4Vz4o5OWz6uZCzrm++lvV+pg1O4uv1xSzYVsrk/i1HQyv1FV6D/N+g24QWD+2sAWynjSis/bWIjfMLqCsJeAjpBN4oI3V9rJRbNZTjp6aqnJp5xVTP8VBd72n6rMWgJTM2lLN6xdAjNpTM2FB6xFpVm7+KShfFaNbTc0Qc23/by6hLu2OyHt2TaXT3KOLCTMxaU9i6KKScARq9YkJqRRTCDGGYtKbTa6YghDgPeAXQAu9JKZ877H0j8BEwBKgArpRS5h6Ptvy2s5zifU5yDT5y9X6KtX6Eu4Gwah02lx5biJ6wED3J4SHYQvSkRJjpERtKj7hQEmxq56+icqrRf3wSW5YWs/WXYgafe/RUGVqN4JJBiby7NIcyh4vo0KPHN2C0KvWag0ilLYQgxhxz+swUhBBa4HXgHKAQWCWEmCOl3HrQYTcBVVLK7kKIq4D/A648Hu258KJMCsYmYgvRNz2sRp3a2auonKZEJlpJ7GFn0+JCBp6TctQsqwCXDUnkrcXZfL2+iJvHZrR84ozxsOgfUF+p1HFugc4YwCbk4bUSO+rEQpwBPCWlPDfw/M8AUspnDzpmbuCY5UIIHbAXiJYtNGro0KFy9erVx6XNKioqpxfZ60r58e3N2KJD0Oha9gQsqKzH7fWj07Y8kDRKF3FyH150SFo+tkIL9RrQBdkNh/bYx513tO7Z1BxCiDVSyqGtHXc8zUeJQMFBzwuBEUc7RkrpFULUAJFA+cEHCSFuAW4BSElJOV7tVVFROc1Iz4qi/4Qk6mtcrR4rQ3XkV9YjabkHd0ktXpcfvXS2ek6rxke91ou31SMVLNYgSoQeI8dTFJqTyMP/N4M5BinlO8A7oMwUjr1pKioqKqDRahh3ZRBlN9vMmcfhnCeG4xk5VQgkH/Q8CSg+2jEB85ENqDyObVJRUVFRaYHjKQqrgEwhRLoQwgBcBcw57Jg5wA2B/cuAhS2tJ6ioqKioHF+Om/kosEZwFzAXxSX1P1LKLUKIp4HVUso5wPvAx0KI3SgzhKuOV3tUVFRUVFrnuMYpSCm/B74/7LUnD9pvBC4/nm1QUVFRUQkeNRubioqKikoTqiioqKioqDShioKKioqKShOqKKioqKioNHHc0lwcL4QQZUBeOz8exWHR0qcAp9o1nWrXA6feNZ1q1wOn3jU1dz2pUspW65h2OVE4FoQQq4PJ/dGVONWu6VS7Hjj1rulUux449a7pWK5HNR+pqKioqDShioKKioqKShOnmyi8c7IbcBw41a7pVLseOPWu6VS7Hjj1rqnd13NarSmoqKioqLTM6TZTUFFRUVFpAVUUVFRUVFSaOG1EQQhxnhBihxBitxDi0ZPdnmNFCJErhNgkhFgvhOiS9UmFEP8RQpQKITYf9FqEEGKeEGJXYBt+MtvYFo5yPU8JIYoC92m9EGLyyWxjWxFCJAshFgkhtgkhtggh7gm83iXvUwvX02XvkxDCJIRYKYTYELimvwVeTxdCrAjco/8FShi0fr7TYU1BCKEFdgLnoBT2WQVcLaXcelIbdgwIIXKBoVLKLhtwI4QYBziBj6SU/QKvPQ9USimfC4h3uJTykZPZzmA5yvU8BTillC+ezLa1FyFEPBAvpVwrhAgF1gCXADfSBe9TC9dzBV30PgkhBGCRUjqFEHpgGXAPcD8wW0r5mRDiLWCDlPLN1s53uswUhgO7pZQ5Uko38Blw8Ulu02mPlHIJR1bauxj4MLD/IcoPtktwlOvp0kgpS6SUawP7DmAbSm31LnmfWrieLotU2F8QWh94SOAs4IvA60Hfo9NFFBKBgoOeF9LF/xBQbvpPQog1QohbTnZjOpBYKWUJKD9gIOYkt6cjuEsIsTFgXuoSZpbmEEKkAYOAFZwC9+mw64EufJ+EEFohxHqgFJgHZAPVUkpv4JCg+7zTRRREM691dbvZaCnlYOB84M6A6UKl8/Em0A0YCJQAL53c5rQPIYQVmAXcK6WsPdntOVaauZ4ufZ+klD4p5UAgCcUy0ru5w4I51+kiCoVA8kHPk4Dik9SWDkFKWRzYlgJfovwhnArsC9h999t/S09ye44JKeW+wA/WD7xLF7xPATv1LGCGlHJ24OUue5+au55T4T4BSCmrgZ+BkYBdCLG/umbQfd7pIgqrgMzAarwBpRb0nJPcpnYjhLAEFskQQliAScDmlj/VZZgD3BDYvwH4+iS25ZjZ33EGmEoXu0+BRcz3gW1Syn8e9FaXvE9Hu56ufJ+EENFCCHtgPwSYiLJWsgi4LHBY0PfotPA+Agi4mL0MaIH/SCn/fpKb1G6EEBkoswNQ6mx/0hWvRwjxKTAeJc3vPuCvwFfA50AKkA9cLqXsEou3R7me8SgmCQnkArfut8V3BYQQY4ClwCbAH3j5MRQ7fJe7Ty1cz9V00fskhMhCWUjWogz0P5dSPh3oJz4DIoB1wLVSSler5ztdREFFRUVFpXVOF/ORioqKikoQqKKgoqKiotKEKgoqKioqKk2ooqCioqKi0oQqCioqKioqTaiioKJyGEIIXyBT5pZA5sn7hRDt/q0IIR47aD/t4CyqKiqdDVUUVFSOpEFKOVBK2Rcls+5klJiD9vJY64eoqHQOVFFQUWmBQBqRW1CSpYlA4rEXhBCrAsnTbgUQQowXQiwRQnwphNgqhHhLCKERQjwHhARmHjMCp9UKId4NzER+CkShqqh0ClRRUFFpBSllDspvJQa4CaiRUg4DhgF/FEKkBw4dDjwA9EdJrnaplPJRDsw8rgkclwm8HpiJVAPTTtzVqKi0jCoKKirBsT/T7iTg+kCa4hVAJEonD7AyULPDB3wKjDnKufZIKdcH9tcAacenySoqbUfX+iEqKqc3gRwyPpRMoAL4k5Ry7mHHjOfI1MRHyyFzcP4ZH6Caj1Q6DepMQUWlBYQQ0cBbwGtSSRQ2F7g9kH4ZIUSPQKZagOGBTLwa4EqUsogAnv3Hq6h0dtSZgorKkYQEzEN6wAt8DOxPs/weirlnbSANcxkHyhwuB55DWVNYwoFMtu8AG4UQa4HHT8QFqKi0FzVLqopKBxAwHz0opbzgZLdFReVYUM1HKioqKipNqDMFFRUVFZUm1JmCioqKikoTqiioqKioqDShioKKioqKShOqKKioqKioNKGKgoqKiopKE/8PzpYt6FS4wS8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Analyse sub-dataset\n",
    "lab_distribution = [[] for _ in range(5)]\n",
    "frequencies = [[] for _ in range(5)]\n",
    "number_examples = []\n",
    "for depth in range(max(all_subtrees.keys()) + 1):\n",
    "    if depth not in all_subtrees:\n",
    "        continue\n",
    "    subtrees = all_subtrees[depth]\n",
    "    number_examples.append(len(subtrees))\n",
    "    print(\"=\"*50)\n",
    "    print(\"Depth \" + str(depth))\n",
    "    print(\"=\"*50)\n",
    "    print(\"Number of trees: \" + str(number_examples[depth]))\n",
    "    print(\"Distribution over classes:\")\n",
    "    for lab in range(5):\n",
    "        subtrees_with_label = [t for t in subtrees if t.label==lab]\n",
    "        lab_distribution[lab].append(len(subtrees_with_label)*1.0/len(subtrees))\n",
    "        frequencies[lab].append(1)\n",
    "        print(\" - Label \" + str(lab) + \": %.2f\" % (100.0 * lab_distribution[lab][-1]) + \"%\")\n",
    "    print(\"=\"*50+\"\\n\")\n",
    "    \n",
    "plt.plot(number_examples)\n",
    "plt.title(\"Number of examples for different subtree depths\")\n",
    "plt.ylabel(\"Number of examples\")\n",
    "plt.xlabel(\"Depth\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "for i in range(5):\n",
    "    plt.plot([lab_distribution[i][j] * number_examples[j] for j in range(len(number_examples))], label=\"Class '\" + i2t[i] + \"'\")\n",
    "plt.title(\"Number of examples for different subtree depths splitted into classes\")\n",
    "plt.ylabel(\"Number of examples\")\n",
    "plt.xlabel(\"Depth\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()\n",
    "for i in range(5):\n",
    "    plt.plot(lab_distribution[i], label=\"Class '\" + i2t[i] + \"'\")\n",
    "plt.title(\"Class distribution for different subtree depths\")\n",
    "plt.ylabel(\"Proportion of class\")\n",
    "plt.xlabel(\"Depth\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "DEBUG_TREE_PRINT = True\n",
    "\n",
    "def pred_to_tree(pred, default_tree, pred_vals=None, index=0, loss_factors=None):\n",
    "    if isinstance(default_tree[0], Tree):\n",
    "        for subtree in default_tree:\n",
    "            _, index = pred_to_tree(pred, subtree, pred_vals=pred_vals, index=index, loss_factors=loss_factors)\n",
    "    label = str(pred[index].item())\n",
    "    if pred_vals is not None:\n",
    "        label += \" (%.1f\" % (100.0 * pred_vals[index].item()) + \"%)\"\n",
    "    if loss_factors is not None:\n",
    "        label += \"-%.3f\" % loss_factors[index].item()\n",
    "    default_tree.set_label(label)\n",
    "    index += 1\n",
    "    return default_tree, index\n",
    "\n",
    "def evaluate(model, data, \n",
    "             batch_fn=get_minibatch, prep_fn=prepare_minibatch,\n",
    "             batch_size=16):\n",
    "    \"\"\"Accuracy of a model on given data set (using minibatches)\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()  # disable dropout\n",
    "\n",
    "    for mb in batch_fn(data, batch_size=batch_size, shuffle=False, is_eval=True):\n",
    "        x, targets = prep_fn(mb, model.vocab, is_eval=True)\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)\n",
    "            \n",
    "        input_tokens, input_transitions = x\n",
    "        targets, _, _ = targets\n",
    "        # input_transitions = torch.LongTensor(input_transitions)\n",
    "        # input_transitions = input_transitions.to(device)\n",
    "        predictions = logits.argmax(dim=-1)\n",
    "        # print(input_transitions.size())\n",
    "        # print(predictions.size())\n",
    "        \n",
    "        # Use only valid last prediction to get final prediction\n",
    "        predictions = predictions.transpose(0, 1).contiguous()  # [B, T]\n",
    "        targets = targets.transpose(0, 1).contiguous()  # [B, T]\n",
    "\n",
    "        # to be super-sure we're not accidentally indexing the wrong state\n",
    "        # we zero out positions that are invalid\n",
    "        pad_positions = (targets == -1)\n",
    "\n",
    "        predictions = predictions.contiguous()      \n",
    "        predictions = predictions.masked_fill_(pad_positions, 0.)\n",
    "\n",
    "        mask = (targets != -1)  # true for valid positions [B, T]\n",
    "        lengths = mask.sum(dim=1)                  # [B, 1]\n",
    "\n",
    "        B = targets.size(0)\n",
    "        T = targets.size(1)\n",
    "        \n",
    "        indexes = (lengths - 1) + torch.arange(B, device=targets.device, dtype=targets.dtype) * T\n",
    "        predictions = predictions.view(-1)[indexes]  # [B]\n",
    "        targets = targets.view(-1)[indexes]\n",
    "\n",
    "        # add the number of correct predictions to the total correct\n",
    "        correct += (predictions == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "        \n",
    "        del x\n",
    "        del targets\n",
    "        del predictions\n",
    "        \n",
    "    print(\"Evaluation on \" + str(total) + \" elements. Correct: \" + str(correct))\n",
    "    ex_ind = 3\n",
    "    # single_pred = logits.argmax(dim=-1)[:,0]\n",
    "    softmax = nn.Softmax(dim=-1)\n",
    "    pred_vals, single_pred = torch.exp(logits).max(dim=-1)\n",
    "    pred_vals = pred_vals[:,ex_ind]\n",
    "    single_pred = single_pred[:,ex_ind]\n",
    "    if DEBUG_TREE_PRINT:\n",
    "        print(TreePrettyPrinter(mb[ex_ind].tree))\n",
    "        print(TreePrettyPrinter(pred_to_tree(single_pred, copy.deepcopy(mb[ex_ind].tree), pred_vals=pred_vals)[0]))\n",
    "    del logits\n",
    "    return correct, total, correct / float(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalLoss(nn.Module):\n",
    "    \"\"\"Hierarchical loss function\"\"\"\n",
    "\n",
    "    def __init__(self, decay_factor = 0.25):\n",
    "        super(HierarchicalLoss, self).__init__()\n",
    "        self.loss_func = nn.NLLLoss(reduction='none')\n",
    "        self.decay_factor = decay_factor\n",
    "        self.range_tensor = torch.arange(5).to(device)\n",
    "\n",
    "    def forward(self, pred, targets, depth):\n",
    "        T = pred.size(0)\n",
    "        B = pred.size(1)\n",
    "        \n",
    "        reshaped_targets = targets.view(B*T)\n",
    "        valid_targets = torch.max(reshaped_targets, reshaped_targets.new_zeros(B*T))\n",
    "        reshaped_preds = pred.view([B * T, -1])\n",
    "        \n",
    "        # Introduce weighting of single classes: if the network is far off (very positive instead of very negative) => higher loss\n",
    "        # Or extra ML loss between prediction and target (does that work out?)\n",
    "        \n",
    "        \n",
    "        anti_prob = torch.log(1 - reshaped_preds)\n",
    "        a = torch.pow(valid_targets.unsqueeze(-1) - self.range_tensor.unsqueeze(0), 2)\n",
    "        add_loss = torch.sum(anti_prob * torch.div(a, 16).type_as(anti_prob),dim=-1)\n",
    "        \n",
    "        loss_terms = self.loss_func(reshaped_preds, valid_targets)\n",
    "        # loss_terms += add_loss\n",
    "        loss_terms = loss_terms.view(T, B)\n",
    "        loss_mask = torch.min(targets+1, targets.new_zeros(T,B)+1)\n",
    "        loss_weight = (torch.pow(targets.new_zeros(1)+self.decay_factor, depth) * loss_mask).float()\n",
    "        weighted_loss = loss_terms * loss_weight\n",
    "        loss_norm = torch.sum(weighted_loss, dim=0) / torch.sum(loss_weight, dim=0)\n",
    "        mean_batch_loss = torch.mean(loss_norm)\n",
    "        return mean_batch_loss, loss_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_hardest_words(depth=0):\n",
    "    hard_words = sorted([(ex.tokens, sum(ex.loss)/len(ex.loss)) if len(ex.loss) > 0 else (ex.tokens,-1) for ex in all_subtrees[depth]], key=lambda x:-x[1])\n",
    "    s = \"Hardest words: \"\n",
    "    for tok, loss in hard_words[:5]:\n",
    "        s += \"'\" + \" \".join(tok) + \"' (%.2f)\" % (loss) + \", \"\n",
    "    print(s)\n",
    "    s = \"Easiest words: \"\n",
    "    for tok, loss in hard_words[-5:]:\n",
    "        s += \"'\" + \" \".join(tok) + \"' (%.2f)\" % (loss) + \", \"\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tree_model(model, optimizer, num_iterations=10000, \n",
    "                print_every=1000, eval_every=1000,\n",
    "                batch_fn=get_examples, \n",
    "                prep_fn=prepare_example,\n",
    "                eval_fn=simple_evaluate,\n",
    "                batch_size=1, eval_batch_size=None):\n",
    "    \"\"\"Train a model.\"\"\"  \n",
    "    iter_i = 0\n",
    "    train_loss = 0.\n",
    "    print_num = 0\n",
    "    start = time.time()\n",
    "    criterion = HierarchicalLossV2() # loss function\n",
    "    best_eval = 0.\n",
    "    best_iter = 0\n",
    "\n",
    "    # store train loss and validation accuracy during training\n",
    "    # so we can plot them afterwards\n",
    "    losses = []\n",
    "    accuracies = []  \n",
    "\n",
    "    if eval_batch_size is None:\n",
    "        eval_batch_size = batch_size\n",
    "\n",
    "    while True:  # when we run out of examples, shuffle and continue\n",
    "        for batch in batch_fn(train_data, batch_size=batch_size, is_eval=False, iteration_num=iter_i):\n",
    "\n",
    "            # forward pass\n",
    "            model.train()\n",
    "            x, y = prep_fn(batch, model.vocab)\n",
    "            \n",
    "            targets, label_depth, transition_matrices = y\n",
    "            input_tokens, input_transitions = x\n",
    "            \n",
    "            logits = model(x) # From now on, this must already be with softmax!\n",
    "\n",
    "            T = targets.size(0)\n",
    "            B = targets.size(1)  # later we will use B examples per update\n",
    "\n",
    "            # compute cross-entropy loss (our criterion)\n",
    "            # note that the cross entropy loss function computes the softmax for us\n",
    "            loss, elementwise_loss, loss_factors = criterion(logits, targets, transition_matrices, label_depth)\n",
    "            train_loss += loss.item()\n",
    "            for ex_index, ex in enumerate(batch):\n",
    "                ex.loss.append(elementwise_loss[ex_index].item())\n",
    "                if len(ex.loss) > 5: # Keep record of last losses fixed to 5\n",
    "                    del ex.loss[0]\n",
    "\n",
    "            # backward pass\n",
    "            # Tip: check the Introduction to PyTorch notebook.\n",
    "\n",
    "            # erase previous gradients\n",
    "            model.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            # update weights - take a small step in the opposite dir of the gradient\n",
    "            optimizer.step()\n",
    "\n",
    "            print_num += 1\n",
    "            iter_i += 1\n",
    "\n",
    "            # print info\n",
    "            if iter_i % print_every == 0:\n",
    "                print(\"Iter %r: loss=%.4f, time=%.2fs\" % \n",
    "                (iter_i, train_loss, time.time()-start))\n",
    "                # print_hardest_words()\n",
    "                smallest_transition=100\n",
    "                min_ex_ind=-1\n",
    "                for ex_ind, ex in enumerate(batch):\n",
    "                    if len(ex.transitions) < smallest_transition:\n",
    "                        smallest_transition = len(ex.transitions)\n",
    "                        min_ex_ind = ex_ind\n",
    "                # min_ex_ind = elementwise_loss.argmax(dim=-1).item()\n",
    "                pred_vals, single_pred = torch.exp(logits[:,min_ex_ind,:]).max(dim=-1)\n",
    "                if DEBUG_TREE_PRINT:\n",
    "                    print(TreePrettyPrinter(batch[min_ex_ind].tree))\n",
    "                    print(TreePrettyPrinter(pred_to_tree(single_pred, copy.deepcopy(batch[min_ex_ind].tree), pred_vals=pred_vals, loss_factors=loss_factors[:,min_ex_ind])[0]))\n",
    "                losses.append(train_loss)\n",
    "                print_num = 0        \n",
    "                train_loss = 0.\n",
    "\n",
    "            # evaluate\n",
    "            if iter_i % eval_every == 0:\n",
    "                _, _, accuracy = eval_fn(model, dev_data, batch_size=eval_batch_size,\n",
    "                         batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "                accuracies.append(accuracy)\n",
    "                print(\"iter %r: dev acc=%.4f\" % (iter_i, accuracy)) \n",
    "\n",
    "                # save best model parameters\n",
    "                if accuracy > best_eval:\n",
    "                    print(\"new highscore\")\n",
    "                    best_eval = accuracy\n",
    "                    best_iter = iter_i\n",
    "                    path = \"{}.pt\".format(model.__class__.__name__)\n",
    "                    ckpt = {\n",
    "                    \"state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"best_eval\": best_eval,\n",
    "                    \"best_iter\": best_iter\n",
    "                    }\n",
    "                    torch.save(ckpt, path)\n",
    "                    \n",
    "            del targets\n",
    "            del transition_matrices\n",
    "            del label_depth\n",
    "            del x\n",
    "            del logits\n",
    "            del loss\n",
    "            del elementwise_loss\n",
    "            del loss_factors\n",
    "            torch.cuda.empty_cache()\n",
    "                    \n",
    "            # done training\n",
    "            if iter_i == num_iterations:\n",
    "                print(\"Done training\")\n",
    "\n",
    "                # evaluate on train, dev, and test with best model\n",
    "                print(\"Loading best model\")\n",
    "                path = \"{}.pt\".format(model.__class__.__name__)        \n",
    "                ckpt = torch.load(path)\n",
    "                model.load_state_dict(ckpt[\"state_dict\"])\n",
    "\n",
    "                _, _, train_acc = eval_fn(\n",
    "                model, train_data, batch_size=eval_batch_size, \n",
    "                batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "                _, _, dev_acc = eval_fn(\n",
    "                model, dev_data, batch_size=eval_batch_size,\n",
    "                batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "                _, _, test_acc = eval_fn(\n",
    "                model, test_data, batch_size=eval_batch_size, \n",
    "                batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "\n",
    "                print(\"best model iter {:d}: \"\n",
    "                \"train acc={:.4f}, dev acc={:.4f}, test acc={:.4f}\".format(\n",
    "                best_iter, train_acc, dev_acc, test_acc))\n",
    "\n",
    "                return losses, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionModule(nn.Module):\n",
    "    \"\"\"Realizes a very simple attention module. Takes last state and word into account with two-layer feedforward\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, hidden_dim):\n",
    "        super(AttentionModule, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.state_dim = state_dim\n",
    "        self.output_layer = nn.Sequential(OrderedDict([\n",
    "            ('fc1', nn.Linear(2*state_dim, hidden_dim)),\n",
    "            ('tanh', nn.Tanh()),\n",
    "            ('dropout', nn.Dropout(0.5)),\n",
    "            ('fc2', nn.Linear(hidden_dim, 2)),\n",
    "            # ('fc', nn.Linear(embedding_dim+state_dim, 1)),\n",
    "            ('sigmoid', nn.Sigmoid())\n",
    "        ]))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"This is PyTorch's default initialization method\"\"\"\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_dim)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)  \n",
    "    \n",
    "    def forward(self, child_l, child_r):\n",
    "        \n",
    "        input_tensor = torch.cat([child_l, child_r], dim=1)\n",
    "        \n",
    "        for layer in self.output_layer:\n",
    "            input_tensor = layer(input_tensor)\n",
    "        \n",
    "        return input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeLSTMCell(nn.Module):\n",
    "    \"\"\"A Binary Tree LSTM cell\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        \"\"\"Creates the weights for this LSTM\"\"\"\n",
    "        super(TreeLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "        \n",
    "        self.attent_module = AttentionModule(self.hidden_size, self.hidden_size)\n",
    "\n",
    "        self.reduce_layer = nn.Linear(2 * hidden_size, 5 * hidden_size)\n",
    "        self.dropout_layer = nn.Dropout(p=0.25)\n",
    "        self.tanh_act = nn.Tanh()\n",
    "        self.sigmoid_act = nn.Sigmoid()\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"This is PyTorch's default initialization method\"\"\"\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)  \n",
    "\n",
    "    def forward(self, hx_l, hx_r, mask=None):\n",
    "        \"\"\"\n",
    "        hx_l is ((batch, hidden_size), (batch, hidden_size))\n",
    "        hx_r is ((batch, hidden_size), (batch, hidden_size))    \n",
    "        \"\"\"\n",
    "        prev_h_l, prev_c_l = hx_l  # left child\n",
    "        prev_h_r, prev_c_r = hx_r  # right child\n",
    "\n",
    "        B = prev_h_l.size(0)\n",
    "\n",
    "        # we concatenate the left and right children\n",
    "        # you can also project from them separately and then sum\n",
    "        children = torch.cat([prev_h_l, prev_h_r], dim=1)\n",
    "\n",
    "        # project the combined children into a 5D tensor for i,fl,fr,g,o\n",
    "        # this is done for speed, and you could also do it separately\n",
    "        proj = self.reduce_layer(children)  # shape: B x 5D\n",
    "\n",
    "        # each shape: B x D\n",
    "        i, f_l, f_r, g, o = torch.chunk(proj, 5, dim=-1)\n",
    "\n",
    "        # main Tree LSTM computation\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # You only need to complete the commented lines below.\n",
    "\n",
    "        # The shape of each of these is [batch_size, hidden_size]\n",
    "\n",
    "        i = self.sigmoid_act(i)\n",
    "        f_l = self.sigmoid_act(f_l)    \n",
    "        f_r = self.sigmoid_act(f_r)\n",
    "        g = self.tanh_act(g)\n",
    "        o = self.sigmoid_act(o)\n",
    "\n",
    "        c = i * g + f_l * prev_c_l + f_r * prev_c_r\n",
    "        h = o * self.tanh_act(c)\n",
    "        \n",
    "        # attent_weights = self.attent_module(prev_h_l, prev_h_r)\n",
    "        # h_children = attent_weights[:,0:1] * prev_h_l + (1 - attent_weights[:,0:1]) * prev_h_r\n",
    "        # h = attent_weights[:,1:2] * h + (1 - attent_weights[:,1:2]) * h_children\n",
    "        # c_children = attent_weights[:,0:1] * prev_c_l + (1 - attent_weights[:,0:1]) * prev_c_r\n",
    "        # c = attent_weights[:,1:2] * c + (1 - attent_weights[:,1:2]) * c_children\n",
    "\n",
    "        return h, c\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"{}({:d}, {:d})\".format(\n",
    "            self.__class__.__name__, self.input_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeLSTM(nn.Module):\n",
    "    \"\"\"Encodes a sentence using a TreeLSTMCell\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        \"\"\"Creates the weights for this LSTM\"\"\"\n",
    "        super(TreeLSTM, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "        self.reduce = TreeLSTMCell(input_size, hidden_size)\n",
    "\n",
    "        # project word to initial c\n",
    "        self.proj_x = nn.Linear(input_size, hidden_size)\n",
    "        self.proj_x_gate = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        self.buffers_dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x, transitions):\n",
    "        \"\"\"\n",
    "        WARNING: assuming x is reversed!\n",
    "        :param x: word embeddings [B, T, E]\n",
    "        :param transitions: [2T-1, B]\n",
    "        :return: root states\n",
    "        \"\"\"\n",
    "\n",
    "        B = x.size(0)  # batch size\n",
    "        T = x.size(1)  # time\n",
    "\n",
    "        # compute an initial c and h for each word\n",
    "        # Note: this corresponds to input x in the Tai et al. Tree LSTM paper.\n",
    "        # We do not handle input x in the TreeLSTMCell itself.\n",
    "        buffers_c = self.proj_x(x)\n",
    "        buffers_h = buffers_c.tanh()\n",
    "        buffers_h_gate = self.proj_x_gate(x).sigmoid()\n",
    "        buffers_h = buffers_h_gate * buffers_h\n",
    "\n",
    "        # concatenate h and c for each word\n",
    "        buffers = torch.cat([buffers_h, buffers_c], dim=-1)\n",
    "\n",
    "        D = buffers.size(-1) // 2\n",
    "\n",
    "        # we turn buffers into a list of stacks (1 stack for each sentence)\n",
    "        # first we split buffers so that it is a list of sentences (length B)\n",
    "        # then we split each sentence to be a list of word vectors\n",
    "        buffers = buffers.split(1, dim=0)  # Bx[T, 2D]\n",
    "        buffers = [list(b.squeeze(0).split(1, dim=0)) for b in buffers]  # BxTx[2D]\n",
    "\n",
    "        # create B empty stacks\n",
    "        stacks = [[] for _ in buffers]\n",
    "        \n",
    "        tree_embeddings = []\n",
    "        \n",
    "\n",
    "        # t_batch holds 1 transition for each sentence\n",
    "        for t_batch in transitions:\n",
    "            loc_embedding = list()\n",
    "            child_l = []  # contains the left child for each sentence with reduce action\n",
    "            child_r = []  # contains the corresponding right child\n",
    "\n",
    "            # iterate over sentences in the batch\n",
    "            # each has a transition t, a buffer and a stack\n",
    "            for transition, buffer, stack in zip(t_batch, buffers, stacks):\n",
    "                if transition == SHIFT:\n",
    "                    stack.append(buffer.pop())\n",
    "                elif transition == REDUCE:\n",
    "                    assert len(stack) >= 2, \\\n",
    "                        \"Stack too small! Should not happen with valid transition sequences\"\n",
    "                    child_r.append(stack.pop())  # right child is on top\n",
    "                    child_l.append(stack.pop())\n",
    "\n",
    "            # if there are sentences with reduce transition, perform them batched\n",
    "            if child_l:\n",
    "                reduced = iter(unbatch(self.reduce(batch(child_l), batch(child_r))))\n",
    "                for transition, stack in zip(t_batch, stacks):\n",
    "                    if transition == REDUCE:\n",
    "                        stack.append(next(reduced))\n",
    "            \n",
    "            tree_embeddings.append([stack[-1].chunk(2, -1)[0] for stack in stacks])\n",
    "\n",
    "        # final = [stack.pop().chunk(2, -1)[0] for stack in stacks]\n",
    "        final = torch.stack([torch.cat(emb, dim=0) for emb in tree_embeddings], dim=0)\n",
    "        # final = torch.cat(final, dim=0)  # tensor [T, B, D]\n",
    "        # print(final.shape)\n",
    "        # del tree_embeddings\n",
    "        \n",
    "        return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeLSTMClassifier(nn.Module):\n",
    "    \"\"\"Encodes sentence with a TreeLSTM and projects final hidden state\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, vocab):\n",
    "        super(TreeLSTMClassifier, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim, padding_idx=1)\n",
    "        self.treelstm = TreeLSTM(embedding_dim, hidden_dim)\n",
    "        self.output_dim = output_dim\n",
    "        self.output_layer = nn.Sequential(     \n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.Linear(hidden_dim, output_dim, bias=True),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # x is a pair here of words and transitions; we unpack it here.\n",
    "        # x is batch-major: [B, T], transitions is time major [2T-1, B]\n",
    "        x, transitions = x\n",
    "        emb = self.embed(x)\n",
    "\n",
    "        # we use the root/top state of the Tree LSTM to classify the sentence\n",
    "        root_states = self.treelstm(emb, transitions)\n",
    "\n",
    "        # we use the last hidden state to classify the sentence\n",
    "        T = root_states.size(0)\n",
    "        B = root_states.size(1)\n",
    "        \n",
    "        # print(root_states.view(T * B, self.hidden_dim).shape)\n",
    "        logits = self.output_layer(root_states.view(T * B, self.hidden_dim)).view(T, B, self.output_dim)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalLossV2(nn.Module):\n",
    "    \"\"\"Hierarchical loss function with stopping at wrong predictions\"\"\"\n",
    "\n",
    "    def __init__(self, decay_factor = 0.5):\n",
    "        super(HierarchicalLossV2, self).__init__()\n",
    "        self.loss_func = nn.NLLLoss(reduction='none')\n",
    "        self.decay_factor = decay_factor\n",
    "        self.range_tensor = torch.arange(5).to(device)\n",
    "\n",
    "    def forward(self, pred, targets, transition_matrices, depth):\n",
    "        T = pred.size(0)\n",
    "        B = pred.size(1)\n",
    "        \n",
    "        reshaped_targets = targets.contiguous().view(B*T)\n",
    "        valid_targets = torch.max(reshaped_targets, reshaped_targets.new_zeros(B*T))\n",
    "        reshaped_preds = pred.view([B * T, -1])\n",
    "        \n",
    "        # Introduce weighting of single classes: if the network is far off (very positive instead of very negative) => higher loss\n",
    "        # Or extra ML loss between prediction and target (does that work out?)\n",
    "        \n",
    "        loss_mask = (targets != -1).float()\n",
    "        loss_terms = self.loss_func(reshaped_preds, valid_targets)\n",
    "        loss_terms = loss_terms.view(T, B)\n",
    "        \n",
    "        wrong_prediction_mask = loss_mask * (torch.abs(targets - pred.argmax(dim=-1)) >= 2).float()\n",
    "        loss_weight_matrix = wrong_prediction_mask[:,None,:].float() * transition_matrices\n",
    "        parent_weight_matrix = wrong_prediction_mask[None,:,:].float()*(loss_weight_matrix.transpose(0,1) == 0).float()\n",
    "        valid_parent_weights = (parent_weight_matrix.sum(dim=1) == 0).float()\n",
    "        valid_parent_loss_matrix = loss_weight_matrix * valid_parent_weights[:,None,:]\n",
    "        final_loss_weights = ((valid_parent_loss_matrix != -1).float() * valid_parent_loss_matrix).sum(dim=0)\n",
    "        ignored_nodes = valid_parent_weights * (final_loss_weights == 0).float()\n",
    "        ignored_nodes_weights = ignored_nodes * torch.min(final_loss_weights.sum(dim=0) / (ignored_nodes.sum(dim=0) + 1e-5), ignored_nodes.new_zeros(B) + self.decay_factor**2)[None,:]\n",
    "        # final_loss_weights += ignored_nodes_weights\n",
    "        # loss_weights = loss_mask.new_zeros(T,B)\n",
    "        \"\"\"\n",
    "        print(\"Predictions: \" + str(pred.size()))\n",
    "        print(\"Targets: \" + str(targets.size()))\n",
    "        print(\"Transition matrices: \" + str(transition_matrices.size()))\n",
    "        print(\"Targets: \" + str(targets[:,0]))\n",
    "        print(\"Predictions: \" + str(pred.argmax(dim=-1)[:,0]))\n",
    "        print(\"Wrong predictions: \" + str(wrong_prediction_mask[:,0]))\n",
    "        print(\"Wrong prediction mask: \" + str(wrong_prediction_mask.size()))\n",
    "        print(\"Loss weight matrix: \" + str(loss_weight_matrix[:,:,0]))\n",
    "        print(\"loss_weight_matrix: \" + str(loss_weight_matrix.size()))\n",
    "        print(\"Parent weight matrix: \" + str(parent_weight_matrix[:,:,0]))\n",
    "        print(\"parent_weight_matrix: \" + str(parent_weight_matrix.size()))\n",
    "        print(\"Valid parent weights: \" + str(valid_parent_weights[:,0]))\n",
    "        print(\"valid_parent_weights: \" + str(valid_parent_weights.size()))\n",
    "        print(\"Valid parent loss matrix: \" + str(valid_parent_loss_matrix[:,:,0]))\n",
    "        print(\"valid_parent_loss_matrix: \" + str(valid_parent_loss_matrix.size()))\n",
    "        print(\"final_loss_weights: \" + str(final_loss_weights.size()))\n",
    "        print(\"Final loss weights: \" + str(final_loss_weights[:,0]))\n",
    "        sys.exit(1)\n",
    "        \"\"\"\n",
    "        is_final_loss_zero = (torch.sum(final_loss_weights, dim=0)==0).float()\n",
    "        default_loss_weights = (torch.pow(targets.new_zeros(1).float()+self.decay_factor, depth.float()) * loss_mask).float()\n",
    "        final_loss_weights = final_loss_weights * (1 - is_final_loss_zero) + is_final_loss_zero * default_loss_weights\n",
    "        \"\"\"\n",
    "        print(\"Final loss weights: \" + str(final_loss_weights[:,0]))\n",
    "        print(\"Is final loss zero: \" + str(is_final_loss_zero[0]))\n",
    "        print(\"Default loss weights: \" + str(default_loss_weights[:,0]))\n",
    "        \"\"\"\n",
    "        # loss_weight = (torch.pow(targets.new_zeros(1)+self.decay_factor, depth) * loss_mask).float()\n",
    "        weighted_loss = loss_terms * final_loss_weights\n",
    "        loss_norm = torch.sum(weighted_loss, dim=0) / (torch.sum(final_loss_weights, dim=0)+1e-10)\n",
    "        mean_batch_loss = torch.mean(loss_norm)\n",
    "        \n",
    "        return mean_batch_loss, loss_norm, final_loss_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TreeLSTMClassifier(\n",
      "  (embed): Embedding(20727, 300, padding_idx=1)\n",
      "  (treelstm): TreeLSTM(\n",
      "    (reduce): TreeLSTMCell(300, 150)\n",
      "    (proj_x): Linear(in_features=300, out_features=150, bias=True)\n",
      "    (proj_x_gate): Linear(in_features=300, out_features=150, bias=True)\n",
      "    (buffers_dropout): Dropout(p=0.5)\n",
      "  )\n",
      "  (output_layer): Sequential(\n",
      "    (0): Dropout(p=0.25)\n",
      "    (1): Linear(in_features=150, out_features=5, bias=True)\n",
      "    (2): LogSoftmax()\n",
      "  )\n",
      ")\n",
      "embed.weight             [20727, 300] requires_grad=False\n",
      "treelstm.reduce.attent_module.output_layer.fc1.weight [150, 300]   requires_grad=True\n",
      "treelstm.reduce.attent_module.output_layer.fc1.bias [150]        requires_grad=True\n",
      "treelstm.reduce.attent_module.output_layer.fc2.weight [2, 150]     requires_grad=True\n",
      "treelstm.reduce.attent_module.output_layer.fc2.bias [2]          requires_grad=True\n",
      "treelstm.reduce.reduce_layer.weight [750, 300]   requires_grad=True\n",
      "treelstm.reduce.reduce_layer.bias [750]        requires_grad=True\n",
      "treelstm.proj_x.weight   [150, 300]   requires_grad=True\n",
      "treelstm.proj_x.bias     [150]        requires_grad=True\n",
      "treelstm.proj_x_gate.weight [150, 300]   requires_grad=True\n",
      "treelstm.proj_x_gate.bias [150]        requires_grad=True\n",
      "output_layer.1.weight    [5, 150]     requires_grad=True\n",
      "output_layer.1.bias      [5]          requires_grad=True\n",
      "\n",
      "Total parameters: 6580357\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'zmq.backend.cython.message.Frame.__dealloc__'\n",
      "Traceback (most recent call last):\n",
      "  File \"zmq/backend/cython/checkrc.pxd\", line 12, in zmq.backend.cython.checkrc._check_rc\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 50: loss=75.7851, time=27.37s\n",
      "Iter 100: loss=66.4372, time=53.79s\n",
      "Iter 150: loss=60.0223, time=81.12s\n",
      "Iter 200: loss=57.2240, time=106.77s\n",
      "Evaluation on 1101 elements. Correct: 443\n",
      "iter 200: dev acc=0.4024\n",
      "new highscore\n",
      "Iter 250: loss=55.4200, time=135.18s\n",
      "Iter 300: loss=53.9289, time=162.29s\n",
      "Iter 350: loss=52.7081, time=188.09s\n",
      "Iter 400: loss=52.0940, time=215.18s\n",
      "Evaluation on 1101 elements. Correct: 465\n",
      "iter 400: dev acc=0.4223\n",
      "new highscore\n",
      "Iter 450: loss=51.1777, time=242.93s\n",
      "Iter 500: loss=50.6464, time=270.08s\n",
      "Iter 550: loss=50.5053, time=295.72s\n",
      "Iter 600: loss=49.5055, time=323.36s\n",
      "Evaluation on 1101 elements. Correct: 485\n",
      "iter 600: dev acc=0.4405\n",
      "new highscore\n",
      "Iter 650: loss=48.9985, time=351.41s\n",
      "Iter 700: loss=48.7862, time=376.80s\n",
      "Iter 750: loss=48.2771, time=403.85s\n",
      "Iter 800: loss=48.3157, time=431.01s\n",
      "Evaluation on 1101 elements. Correct: 510\n",
      "iter 800: dev acc=0.4632\n",
      "new highscore\n",
      "Iter 850: loss=47.7250, time=459.01s\n",
      "Iter 900: loss=47.8925, time=484.65s\n",
      "Iter 950: loss=46.8899, time=512.04s\n",
      "Iter 1000: loss=46.9387, time=539.09s\n",
      "Evaluation on 1101 elements. Correct: 528\n",
      "iter 1000: dev acc=0.4796\n",
      "new highscore\n",
      "Iter 1050: loss=46.5868, time=565.38s\n",
      "Iter 1100: loss=46.3244, time=592.41s\n",
      "Iter 1150: loss=46.0230, time=617.49s\n",
      "Iter 1200: loss=45.7990, time=643.29s\n",
      "Evaluation on 1101 elements. Correct: 514\n",
      "iter 1200: dev acc=0.4668\n",
      "Iter 1250: loss=45.9145, time=667.86s\n",
      "Iter 1300: loss=45.2761, time=693.41s\n",
      "Iter 1350: loss=45.4019, time=718.88s\n",
      "Iter 1400: loss=45.0467, time=742.68s\n",
      "Evaluation on 1101 elements. Correct: 534\n",
      "iter 1400: dev acc=0.4850\n",
      "new highscore\n",
      "Iter 1450: loss=45.0806, time=768.72s\n",
      "Iter 1500: loss=45.0990, time=794.44s\n",
      "Iter 1550: loss=44.3591, time=819.85s\n",
      "Iter 1600: loss=44.1378, time=843.93s\n",
      "Evaluation on 1101 elements. Correct: 519\n",
      "iter 1600: dev acc=0.4714\n",
      "Iter 1650: loss=44.2539, time=870.19s\n",
      "Iter 1700: loss=44.2237, time=895.71s\n",
      "Iter 1750: loss=43.9575, time=921.11s\n",
      "Iter 1800: loss=43.7856, time=944.81s\n",
      "Evaluation on 1101 elements. Correct: 525\n",
      "iter 1800: dev acc=0.4768\n",
      "Iter 1850: loss=43.3401, time=971.31s\n",
      "Iter 1900: loss=43.7551, time=996.94s\n",
      "Iter 1950: loss=43.3977, time=1022.40s\n",
      "Iter 2000: loss=43.0419, time=1046.38s\n",
      "Evaluation on 1101 elements. Correct: 534\n",
      "iter 2000: dev acc=0.4850\n",
      "Iter 2050: loss=42.9342, time=1072.88s\n",
      "Iter 2100: loss=42.8791, time=1098.14s\n",
      "Iter 2150: loss=42.6534, time=1122.27s\n",
      "Iter 2200: loss=42.6253, time=1148.00s\n",
      "Evaluation on 1101 elements. Correct: 529\n",
      "iter 2200: dev acc=0.4805\n",
      "Iter 2250: loss=42.2088, time=1174.26s\n",
      "Iter 2300: loss=42.2359, time=1199.95s\n",
      "Iter 2350: loss=41.9663, time=1223.83s\n",
      "Iter 2400: loss=41.8113, time=1249.42s\n",
      "Evaluation on 1101 elements. Correct: 512\n",
      "iter 2400: dev acc=0.4650\n",
      "Iter 2450: loss=41.5584, time=1276.12s\n",
      "Iter 2500: loss=41.6021, time=1299.71s\n",
      "Iter 2550: loss=41.5424, time=1325.36s\n",
      "Iter 2600: loss=41.3734, time=1350.94s\n",
      "Evaluation on 1101 elements. Correct: 510\n",
      "iter 2600: dev acc=0.4632\n",
      "Iter 2650: loss=40.8413, time=1377.46s\n",
      "Iter 2700: loss=40.9486, time=1400.99s\n",
      "Iter 2750: loss=41.1128, time=1426.54s\n",
      "Iter 2800: loss=40.6644, time=1451.87s\n",
      "Evaluation on 1101 elements. Correct: 524\n",
      "iter 2800: dev acc=0.4759\n",
      "Iter 2850: loss=40.6584, time=1479.73s\n",
      "Iter 2900: loss=40.4221, time=1504.80s\n",
      "Iter 2950: loss=40.1489, time=1530.38s\n",
      "Iter 3000: loss=40.2902, time=1556.07s\n",
      "Evaluation on 1101 elements. Correct: 520\n",
      "iter 3000: dev acc=0.4723\n",
      "Iter 3050: loss=39.9559, time=1581.06s\n",
      "Iter 3100: loss=39.7053, time=1608.00s\n",
      "Iter 3150: loss=40.0999, time=1635.01s\n",
      "Iter 3200: loss=39.5372, time=1661.76s\n",
      "Evaluation on 1101 elements. Correct: 500\n",
      "iter 3200: dev acc=0.4541\n",
      "Iter 3250: loss=39.4807, time=1687.83s\n",
      "Iter 3300: loss=39.3298, time=1714.95s\n",
      "Iter 3350: loss=39.4775, time=1742.11s\n",
      "Iter 3400: loss=39.1341, time=1767.80s\n",
      "Evaluation on 1101 elements. Correct: 525\n",
      "iter 3400: dev acc=0.4768\n",
      "Iter 3450: loss=39.0413, time=1796.23s\n",
      "Iter 3500: loss=38.4879, time=1823.74s\n",
      "Iter 3550: loss=38.6867, time=1850.84s\n",
      "Iter 3600: loss=38.5045, time=1876.52s\n",
      "Evaluation on 1101 elements. Correct: 511\n",
      "iter 3600: dev acc=0.4641\n",
      "Iter 3650: loss=37.8186, time=1904.04s\n",
      "Iter 3700: loss=38.3057, time=1930.66s\n",
      "Iter 3750: loss=38.4317, time=1957.63s\n",
      "Iter 3800: loss=37.7595, time=1983.13s\n",
      "Evaluation on 1101 elements. Correct: 503\n",
      "iter 3800: dev acc=0.4569\n",
      "Iter 3850: loss=38.1441, time=2011.10s\n",
      "Iter 3900: loss=37.5459, time=2037.58s\n",
      "Iter 3950: loss=37.8826, time=2064.80s\n",
      "Iter 4000: loss=37.4420, time=2090.08s\n",
      "Evaluation on 1101 elements. Correct: 500\n",
      "iter 4000: dev acc=0.4541\n",
      "Iter 4050: loss=37.0404, time=2118.46s\n",
      "Iter 4100: loss=37.1734, time=2145.88s\n",
      "Iter 4150: loss=37.3250, time=2173.33s\n",
      "Iter 4200: loss=36.7176, time=2198.88s\n",
      "Evaluation on 1101 elements. Correct: 506\n",
      "iter 4200: dev acc=0.4596\n",
      "Iter 4250: loss=37.0050, time=2227.08s\n",
      "Iter 4300: loss=36.7522, time=2254.15s\n",
      "Iter 4350: loss=36.4640, time=2279.92s\n",
      "Iter 4400: loss=35.9268, time=2307.43s\n",
      "Evaluation on 1101 elements. Correct: 500\n",
      "iter 4400: dev acc=0.4541\n",
      "Iter 4450: loss=36.2527, time=2335.18s\n",
      "Iter 4500: loss=36.2499, time=2360.50s\n",
      "Iter 4550: loss=36.0867, time=2387.44s\n",
      "Iter 4600: loss=35.8947, time=2414.28s\n",
      "Evaluation on 1101 elements. Correct: 500\n",
      "iter 4600: dev acc=0.4541\n",
      "Iter 4650: loss=35.8327, time=2441.68s\n",
      "Iter 4700: loss=35.6903, time=2467.67s\n",
      "Iter 4750: loss=35.3558, time=2494.93s\n",
      "Iter 4800: loss=35.4206, time=2522.33s\n",
      "Evaluation on 1101 elements. Correct: 481\n",
      "iter 4800: dev acc=0.4369\n",
      "Iter 4850: loss=35.2145, time=2549.12s\n",
      "Iter 4900: loss=35.3538, time=2576.49s\n",
      "Iter 4950: loss=34.7175, time=2603.89s\n",
      "Iter 5000: loss=34.7915, time=2631.36s\n",
      "Evaluation on 1101 elements. Correct: 500\n",
      "iter 5000: dev acc=0.4541\n",
      "Iter 5050: loss=34.8054, time=2658.23s\n",
      "Iter 5100: loss=34.6545, time=2685.79s\n",
      "Iter 5150: loss=34.7150, time=2713.25s\n",
      "Iter 5200: loss=34.6062, time=2739.44s\n",
      "Evaluation on 1101 elements. Correct: 484\n",
      "iter 5200: dev acc=0.4396\n",
      "Iter 5250: loss=34.3058, time=2767.31s\n",
      "Iter 5300: loss=34.2988, time=2794.36s\n",
      "Iter 5350: loss=34.0559, time=2821.92s\n",
      "Iter 5400: loss=34.2706, time=2847.61s\n",
      "Evaluation on 1101 elements. Correct: 476\n",
      "iter 5400: dev acc=0.4323\n",
      "Iter 5450: loss=33.7083, time=2875.76s\n",
      "Iter 5500: loss=34.0828, time=2902.84s\n",
      "Iter 5550: loss=33.6789, time=2929.96s\n",
      "Iter 5600: loss=33.3504, time=2955.51s\n",
      "Evaluation on 1101 elements. Correct: 484\n",
      "iter 5600: dev acc=0.4396\n",
      "Iter 5650: loss=33.5555, time=2983.08s\n",
      "Iter 5700: loss=33.3122, time=3009.73s\n",
      "Iter 5750: loss=33.3990, time=3035.03s\n",
      "Iter 5800: loss=33.1806, time=3061.75s\n",
      "Evaluation on 1101 elements. Correct: 474\n",
      "iter 5800: dev acc=0.4305\n",
      "Iter 5850: loss=33.0595, time=3090.42s\n",
      "Iter 5900: loss=32.9571, time=3117.41s\n",
      "Iter 5950: loss=32.6317, time=3143.10s\n",
      "Iter 6000: loss=32.4466, time=3170.06s\n",
      "Evaluation on 1101 elements. Correct: 485\n",
      "iter 6000: dev acc=0.4405\n",
      "Iter 6050: loss=32.8238, time=3198.42s\n",
      "Iter 6100: loss=32.9330, time=3225.51s\n",
      "Iter 6150: loss=32.2815, time=3250.86s\n",
      "Iter 6200: loss=32.5923, time=3277.72s\n",
      "Evaluation on 1101 elements. Correct: 476\n",
      "iter 6200: dev acc=0.4323\n",
      "Iter 6250: loss=32.2216, time=3305.61s\n",
      "Iter 6300: loss=32.1733, time=3330.78s\n",
      "Iter 6350: loss=32.3334, time=3357.26s\n",
      "Iter 6400: loss=32.1302, time=3384.43s\n",
      "Evaluation on 1101 elements. Correct: 479\n",
      "iter 6400: dev acc=0.4351\n",
      "Iter 6450: loss=32.0845, time=3413.04s\n",
      "Iter 6500: loss=31.9227, time=3438.40s\n",
      "Iter 6550: loss=31.4972, time=3465.87s\n",
      "Iter 6600: loss=31.6442, time=3491.80s\n",
      "Evaluation on 1101 elements. Correct: 475\n",
      "iter 6600: dev acc=0.4314\n",
      "Iter 6650: loss=31.6450, time=3518.84s\n",
      "Iter 6700: loss=31.5994, time=3543.32s\n",
      "Iter 6750: loss=31.6162, time=3569.23s\n",
      "Iter 6800: loss=31.0388, time=3595.54s\n",
      "Evaluation on 1101 elements. Correct: 475\n",
      "iter 6800: dev acc=0.4314\n",
      "Iter 6850: loss=31.2659, time=3622.11s\n",
      "Iter 6900: loss=30.9213, time=3649.32s\n",
      "Iter 6950: loss=30.8302, time=3676.02s\n",
      "Iter 7000: loss=31.0519, time=3701.68s\n",
      "Evaluation on 1101 elements. Correct: 486\n",
      "iter 7000: dev acc=0.4414\n",
      "Iter 7050: loss=31.1027, time=3729.43s\n",
      "Iter 7100: loss=30.8685, time=3756.70s\n",
      "Iter 7150: loss=30.8738, time=3783.71s\n",
      "Iter 7200: loss=30.1742, time=3808.78s\n",
      "Evaluation on 1101 elements. Correct: 470\n",
      "iter 7200: dev acc=0.4269\n",
      "Iter 7250: loss=30.6199, time=3836.54s\n",
      "Iter 7300: loss=30.4785, time=3863.32s\n",
      "Iter 7350: loss=30.1696, time=3888.32s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 7400: loss=30.9401, time=3915.47s\n",
      "Evaluation on 1101 elements. Correct: 472\n",
      "iter 7400: dev acc=0.4287\n",
      "Iter 7450: loss=30.3337, time=3943.26s\n",
      "Iter 7500: loss=30.1157, time=3970.35s\n",
      "Iter 7550: loss=30.3103, time=3995.82s\n",
      "Iter 7600: loss=30.0004, time=4022.59s\n",
      "Evaluation on 1101 elements. Correct: 466\n",
      "iter 7600: dev acc=0.4233\n",
      "Iter 7650: loss=30.0607, time=4050.34s\n",
      "Iter 7700: loss=29.7915, time=4077.66s\n",
      "Iter 7750: loss=29.7100, time=4102.91s\n",
      "Iter 7800: loss=29.5124, time=4130.30s\n",
      "Evaluation on 1101 elements. Correct: 457\n",
      "iter 7800: dev acc=0.4151\n",
      "Iter 7850: loss=29.0037, time=4158.24s\n",
      "Iter 7900: loss=29.5450, time=4183.64s\n",
      "Iter 7950: loss=29.2786, time=4210.95s\n",
      "Iter 8000: loss=29.0438, time=4237.97s\n",
      "Evaluation on 1101 elements. Correct: 470\n",
      "iter 8000: dev acc=0.4269\n",
      "Iter 8050: loss=29.1320, time=4266.04s\n",
      "Iter 8100: loss=29.2348, time=4291.37s\n",
      "Iter 8150: loss=28.9304, time=4318.17s\n",
      "Iter 8200: loss=29.4332, time=4344.75s\n",
      "Evaluation on 1101 elements. Correct: 458\n",
      "iter 8200: dev acc=0.4160\n",
      "Iter 8250: loss=29.1519, time=4371.27s\n",
      "Iter 8300: loss=28.9928, time=4398.22s\n",
      "Iter 8350: loss=28.8588, time=4424.84s\n",
      "Iter 8400: loss=28.5886, time=4451.79s\n",
      "Evaluation on 1101 elements. Correct: 458\n",
      "iter 8400: dev acc=0.4160\n",
      "Iter 8450: loss=28.6014, time=4477.94s\n",
      "Iter 8500: loss=28.8757, time=4505.10s\n",
      "Iter 8550: loss=28.7717, time=4531.98s\n",
      "Iter 8600: loss=28.6795, time=4556.92s\n",
      "Evaluation on 1101 elements. Correct: 469\n",
      "iter 8600: dev acc=0.4260\n",
      "Iter 8650: loss=28.4527, time=4584.47s\n",
      "Iter 8700: loss=28.2844, time=4611.02s\n",
      "Iter 8750: loss=28.3860, time=4638.14s\n",
      "Iter 8800: loss=27.9247, time=4663.91s\n",
      "Evaluation on 1101 elements. Correct: 460\n",
      "iter 8800: dev acc=0.4178\n",
      "Iter 8850: loss=28.2152, time=4691.97s\n",
      "Iter 8900: loss=27.9158, time=4719.08s\n",
      "Iter 8950: loss=27.9250, time=4746.29s\n",
      "Iter 9000: loss=27.9197, time=4772.16s\n",
      "Evaluation on 1101 elements. Correct: 468\n",
      "iter 9000: dev acc=0.4251\n",
      "Iter 9050: loss=27.7432, time=4800.13s\n",
      "Iter 9100: loss=27.3673, time=4827.13s\n",
      "Iter 9150: loss=27.8695, time=4851.93s\n",
      "Iter 9200: loss=27.2441, time=4878.63s\n",
      "Evaluation on 1101 elements. Correct: 460\n",
      "iter 9200: dev acc=0.4178\n",
      "Iter 9250: loss=27.6751, time=4905.86s\n",
      "Iter 9300: loss=27.4988, time=4932.77s\n",
      "Iter 9350: loss=27.1879, time=4958.22s\n",
      "Iter 9400: loss=27.4513, time=4984.71s\n",
      "Evaluation on 1101 elements. Correct: 456\n",
      "iter 9400: dev acc=0.4142\n",
      "Iter 9450: loss=27.2525, time=5012.53s\n",
      "Iter 9500: loss=26.9519, time=5039.02s\n",
      "Iter 9550: loss=27.4413, time=5064.47s\n",
      "Iter 9600: loss=26.7213, time=5091.11s\n",
      "Evaluation on 1101 elements. Correct: 470\n",
      "iter 9600: dev acc=0.4269\n",
      "Iter 9650: loss=26.9887, time=5118.65s\n",
      "Iter 9700: loss=27.0870, time=5144.15s\n",
      "Iter 9750: loss=26.7804, time=5171.50s\n",
      "Iter 9800: loss=26.6968, time=5198.56s\n",
      "Evaluation on 1101 elements. Correct: 446\n",
      "iter 9800: dev acc=0.4051\n",
      "Iter 9850: loss=26.7298, time=5226.63s\n",
      "Iter 9900: loss=26.5452, time=5252.28s\n",
      "Iter 9950: loss=27.1337, time=5279.78s\n",
      "Iter 10000: loss=26.0799, time=5307.24s\n",
      "Evaluation on 1101 elements. Correct: 453\n",
      "iter 10000: dev acc=0.4114\n",
      "Iter 10050: loss=26.4862, time=5333.82s\n",
      "Iter 10100: loss=26.0604, time=5360.94s\n",
      "Iter 10150: loss=26.3341, time=5388.27s\n",
      "Iter 10200: loss=26.2024, time=5414.73s\n",
      "Evaluation on 1101 elements. Correct: 445\n",
      "iter 10200: dev acc=0.4042\n",
      "Iter 10250: loss=26.1546, time=5441.47s\n",
      "Iter 10300: loss=26.0177, time=5468.81s\n",
      "Iter 10350: loss=26.2635, time=5495.93s\n",
      "Iter 10400: loss=26.0129, time=5521.79s\n",
      "Evaluation on 1101 elements. Correct: 443\n",
      "iter 10400: dev acc=0.4024\n",
      "Iter 10450: loss=25.8268, time=5549.83s\n",
      "Iter 10500: loss=25.9924, time=5577.03s\n",
      "Iter 10550: loss=26.3504, time=5602.40s\n",
      "Iter 10600: loss=25.8013, time=5629.61s\n",
      "Evaluation on 1101 elements. Correct: 450\n",
      "iter 10600: dev acc=0.4087\n",
      "Iter 10650: loss=25.7651, time=5657.79s\n",
      "Iter 10700: loss=25.6017, time=5684.67s\n",
      "Iter 10750: loss=25.3519, time=5710.28s\n",
      "Iter 10800: loss=25.6546, time=5737.45s\n",
      "Evaluation on 1101 elements. Correct: 444\n",
      "iter 10800: dev acc=0.4033\n",
      "Iter 10850: loss=25.4359, time=5765.60s\n",
      "Iter 10900: loss=25.5622, time=5793.09s\n",
      "Iter 10950: loss=25.6674, time=5818.25s\n",
      "Iter 11000: loss=25.7249, time=5845.43s\n",
      "Evaluation on 1101 elements. Correct: 443\n",
      "iter 11000: dev acc=0.4024\n",
      "Iter 11050: loss=25.3466, time=5873.90s\n",
      "Iter 11100: loss=24.9918, time=5899.39s\n",
      "Iter 11150: loss=25.4328, time=5926.63s\n",
      "Iter 11200: loss=24.9304, time=5954.07s\n",
      "Evaluation on 1101 elements. Correct: 450\n",
      "iter 11200: dev acc=0.4087\n",
      "Iter 11250: loss=24.7864, time=5982.55s\n",
      "Iter 11300: loss=24.8158, time=6007.79s\n",
      "Iter 11350: loss=25.1546, time=6034.43s\n",
      "Iter 11400: loss=24.6054, time=6061.64s\n",
      "Evaluation on 1101 elements. Correct: 446\n",
      "iter 11400: dev acc=0.4051\n",
      "Iter 11450: loss=24.6821, time=6090.19s\n",
      "Iter 11500: loss=24.8209, time=6115.94s\n",
      "Iter 11550: loss=25.1836, time=6143.50s\n",
      "Iter 11600: loss=24.9417, time=6170.90s\n",
      "Evaluation on 1101 elements. Correct: 447\n",
      "iter 11600: dev acc=0.4060\n",
      "Iter 11650: loss=24.9336, time=6197.79s\n",
      "Iter 11700: loss=24.2944, time=6225.07s\n",
      "Iter 11750: loss=25.0369, time=6251.95s\n",
      "Iter 11800: loss=24.3925, time=6277.70s\n",
      "Evaluation on 1101 elements. Correct: 433\n",
      "iter 11800: dev acc=0.3933\n",
      "Iter 11850: loss=24.0676, time=6303.81s\n",
      "Iter 11900: loss=24.4780, time=6329.68s\n",
      "Iter 11950: loss=24.4839, time=6355.63s\n",
      "Iter 12000: loss=24.3709, time=6379.90s\n",
      "Evaluation on 1101 elements. Correct: 449\n",
      "iter 12000: dev acc=0.4078\n",
      "Iter 12050: loss=24.3703, time=6407.45s\n",
      "Iter 12100: loss=23.8812, time=6433.23s\n",
      "Iter 12150: loss=24.3573, time=6459.95s\n",
      "Iter 12200: loss=23.7524, time=6485.37s\n",
      "Evaluation on 1101 elements. Correct: 452\n",
      "iter 12200: dev acc=0.4105\n",
      "Iter 12250: loss=23.9840, time=6514.10s\n",
      "Iter 12300: loss=23.5212, time=6541.37s\n",
      "Iter 12350: loss=23.6161, time=6567.23s\n",
      "Iter 12400: loss=24.0451, time=6594.23s\n",
      "Evaluation on 1101 elements. Correct: 437\n",
      "iter 12400: dev acc=0.3969\n",
      "Iter 12450: loss=23.8927, time=6622.66s\n",
      "Iter 12500: loss=23.9768, time=6650.00s\n",
      "Iter 12550: loss=23.6029, time=6675.76s\n",
      "Iter 12600: loss=23.6516, time=6702.91s\n",
      "Evaluation on 1101 elements. Correct: 439\n",
      "iter 12600: dev acc=0.3987\n",
      "Iter 12650: loss=23.5896, time=6731.35s\n",
      "Iter 12700: loss=23.0854, time=6758.45s\n",
      "Iter 12750: loss=23.3763, time=6783.79s\n",
      "Iter 12800: loss=23.2633, time=6811.03s\n",
      "Evaluation on 1101 elements. Correct: 442\n",
      "iter 12800: dev acc=0.4015\n",
      "Iter 12850: loss=23.0318, time=6839.45s\n",
      "Iter 12900: loss=23.3699, time=6865.15s\n",
      "Iter 12950: loss=23.2565, time=6892.24s\n",
      "Iter 13000: loss=22.6637, time=6919.64s\n",
      "Evaluation on 1101 elements. Correct: 456\n",
      "iter 13000: dev acc=0.4142\n",
      "Iter 13050: loss=22.7063, time=6948.06s\n",
      "Iter 13100: loss=23.0857, time=6973.64s\n",
      "Iter 13150: loss=22.7888, time=7001.18s\n",
      "Iter 13200: loss=22.8985, time=7028.36s\n",
      "Evaluation on 1101 elements. Correct: 439\n",
      "iter 13200: dev acc=0.3987\n",
      "Iter 13250: loss=22.7482, time=7055.07s\n",
      "Iter 13300: loss=22.5153, time=7082.47s\n",
      "Iter 13350: loss=22.7487, time=7108.87s\n",
      "Iter 13400: loss=23.0179, time=7136.05s\n",
      "Evaluation on 1101 elements. Correct: 444\n",
      "iter 13400: dev acc=0.4033\n",
      "Iter 13450: loss=22.2322, time=7162.70s\n",
      "Iter 13500: loss=22.8565, time=7190.03s\n",
      "Iter 13550: loss=22.2536, time=7217.10s\n",
      "Iter 13600: loss=23.1346, time=7244.44s\n",
      "Evaluation on 1101 elements. Correct: 440\n",
      "iter 13600: dev acc=0.3996\n",
      "Iter 13650: loss=22.4608, time=7271.13s\n",
      "Iter 13700: loss=22.4231, time=7298.30s\n",
      "Iter 13750: loss=22.7186, time=7325.12s\n",
      "Iter 13800: loss=22.2980, time=7351.13s\n",
      "Evaluation on 1101 elements. Correct: 439\n",
      "iter 13800: dev acc=0.3987\n",
      "Iter 13850: loss=22.0398, time=7379.18s\n",
      "Iter 13900: loss=22.2151, time=7406.23s\n",
      "Iter 13950: loss=22.4918, time=7433.25s\n",
      "Iter 14000: loss=22.1806, time=7459.19s\n",
      "Evaluation on 1101 elements. Correct: 443\n",
      "iter 14000: dev acc=0.4024\n",
      "Iter 14050: loss=21.8577, time=7486.96s\n",
      "Iter 14100: loss=21.9814, time=7514.02s\n",
      "Iter 14150: loss=21.8553, time=7539.21s\n",
      "Iter 14200: loss=22.2312, time=7566.22s\n",
      "Evaluation on 1101 elements. Correct: 452\n",
      "iter 14200: dev acc=0.4105\n",
      "Iter 14250: loss=21.7615, time=7594.68s\n",
      "Iter 14300: loss=21.9056, time=7619.66s\n",
      "Iter 14350: loss=21.9642, time=7646.98s\n",
      "Iter 14400: loss=22.1708, time=7673.15s\n",
      "Evaluation on 1101 elements. Correct: 443\n",
      "iter 14400: dev acc=0.4024\n",
      "Iter 14450: loss=21.7788, time=7699.75s\n",
      "Iter 14500: loss=21.7778, time=7724.39s\n",
      "Iter 14550: loss=21.3362, time=7751.69s\n",
      "Iter 14600: loss=21.3118, time=7778.84s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on 1101 elements. Correct: 445\n",
      "iter 14600: dev acc=0.4042\n",
      "Iter 14650: loss=21.8406, time=7805.07s\n",
      "Iter 14700: loss=21.5942, time=7831.90s\n",
      "Iter 14750: loss=21.7913, time=7859.21s\n",
      "Iter 14800: loss=21.2981, time=7884.81s\n",
      "Evaluation on 1101 elements. Correct: 445\n",
      "iter 14800: dev acc=0.4042\n",
      "Iter 14850: loss=21.3209, time=7912.73s\n",
      "Iter 14900: loss=21.4955, time=7939.62s\n",
      "Iter 14950: loss=21.2525, time=7966.37s\n",
      "Iter 15000: loss=21.3026, time=7991.90s\n",
      "Evaluation on 1101 elements. Correct: 432\n",
      "iter 15000: dev acc=0.3924\n",
      "Iter 15050: loss=21.5083, time=8020.03s\n",
      "Iter 15100: loss=21.0666, time=8046.96s\n",
      "Iter 15150: loss=21.1706, time=8073.96s\n",
      "Iter 15200: loss=20.7274, time=8099.91s\n",
      "Evaluation on 1101 elements. Correct: 438\n",
      "iter 15200: dev acc=0.3978\n",
      "Iter 15250: loss=20.6151, time=8127.81s\n",
      "Iter 15300: loss=21.1618, time=8154.74s\n",
      "Iter 15350: loss=20.8089, time=8180.33s\n",
      "Iter 15400: loss=20.6500, time=8207.24s\n",
      "Evaluation on 1101 elements. Correct: 439\n",
      "iter 15400: dev acc=0.3987\n",
      "Iter 15450: loss=20.9036, time=8234.85s\n",
      "Iter 15500: loss=20.5050, time=8260.70s\n",
      "Iter 15550: loss=20.4397, time=8284.52s\n",
      "Iter 15600: loss=20.6170, time=8310.28s\n",
      "Evaluation on 1101 elements. Correct: 440\n",
      "iter 15600: dev acc=0.3996\n",
      "Iter 15650: loss=20.4805, time=8336.95s\n",
      "Iter 15700: loss=20.7170, time=8362.32s\n",
      "Iter 15750: loss=20.4349, time=8386.16s\n",
      "Iter 15800: loss=20.3202, time=8411.79s\n",
      "Evaluation on 1101 elements. Correct: 446\n",
      "iter 15800: dev acc=0.4051\n",
      "Iter 15850: loss=20.7864, time=8439.08s\n",
      "Iter 15900: loss=20.3614, time=8466.26s\n",
      "Iter 15950: loss=20.3827, time=8492.06s\n",
      "Iter 16000: loss=19.6765, time=8519.68s\n",
      "Evaluation on 1101 elements. Correct: 434\n",
      "iter 16000: dev acc=0.3942\n",
      "Iter 16050: loss=19.9728, time=8547.90s\n",
      "Iter 16100: loss=20.2186, time=8573.35s\n",
      "Iter 16150: loss=20.5452, time=8600.52s\n",
      "Iter 16200: loss=20.3013, time=8627.61s\n",
      "Evaluation on 1101 elements. Correct: 447\n",
      "iter 16200: dev acc=0.4060\n",
      "Iter 16250: loss=19.8916, time=8655.72s\n",
      "Iter 16300: loss=20.1891, time=8681.53s\n",
      "Iter 16350: loss=19.6142, time=8708.96s\n",
      "Iter 16400: loss=20.1408, time=8736.22s\n",
      "Evaluation on 1101 elements. Correct: 442\n",
      "iter 16400: dev acc=0.4015\n",
      "Iter 16450: loss=19.6438, time=8764.43s\n",
      "Iter 16500: loss=20.1264, time=8789.87s\n",
      "Iter 16550: loss=19.5523, time=8816.85s\n",
      "Iter 16600: loss=19.5897, time=8843.90s\n",
      "Evaluation on 1101 elements. Correct: 433\n",
      "iter 16600: dev acc=0.3933\n",
      "Iter 16650: loss=19.4373, time=8870.08s\n",
      "Iter 16700: loss=19.3678, time=8897.14s\n",
      "Iter 16750: loss=19.7319, time=8924.56s\n",
      "Iter 16800: loss=19.9634, time=8952.39s\n",
      "Evaluation on 1101 elements. Correct: 433\n",
      "iter 16800: dev acc=0.3933\n",
      "Iter 16850: loss=19.2690, time=8978.85s\n",
      "Iter 16900: loss=19.4605, time=9006.08s\n",
      "Iter 16950: loss=19.6652, time=9033.15s\n",
      "Iter 17000: loss=19.3300, time=9060.59s\n",
      "Evaluation on 1101 elements. Correct: 429\n",
      "iter 17000: dev acc=0.3896\n",
      "Iter 17050: loss=19.5020, time=9086.81s\n",
      "Iter 17100: loss=19.6433, time=9113.74s\n",
      "Iter 17150: loss=19.0896, time=9140.87s\n",
      "Iter 17200: loss=19.1293, time=9166.63s\n",
      "Evaluation on 1101 elements. Correct: 440\n",
      "iter 17200: dev acc=0.3996\n",
      "Iter 17250: loss=18.9457, time=9194.89s\n",
      "Iter 17300: loss=19.4795, time=9221.76s\n",
      "Iter 17350: loss=19.1953, time=9248.69s\n",
      "Iter 17400: loss=19.0972, time=9274.16s\n",
      "Evaluation on 1101 elements. Correct: 436\n",
      "iter 17400: dev acc=0.3960\n",
      "Iter 17450: loss=18.8266, time=9301.90s\n",
      "Iter 17500: loss=18.8370, time=9329.22s\n",
      "Iter 17550: loss=18.8242, time=9354.56s\n",
      "Iter 17600: loss=19.0775, time=9381.35s\n",
      "Evaluation on 1101 elements. Correct: 430\n",
      "iter 17600: dev acc=0.3906\n",
      "Iter 17650: loss=19.3341, time=9409.44s\n",
      "Iter 17700: loss=18.8364, time=9436.31s\n",
      "Iter 17750: loss=19.2220, time=9462.19s\n",
      "Iter 17800: loss=18.7060, time=9488.99s\n",
      "Evaluation on 1101 elements. Correct: 433\n",
      "iter 17800: dev acc=0.3933\n",
      "Iter 17850: loss=19.0740, time=9517.25s\n",
      "Iter 17900: loss=18.4729, time=9542.53s\n",
      "Iter 17950: loss=19.2212, time=9569.82s\n",
      "Iter 18000: loss=18.9100, time=9597.03s\n",
      "Evaluation on 1101 elements. Correct: 446\n",
      "iter 18000: dev acc=0.4051\n",
      "Iter 18050: loss=18.8142, time=9625.14s\n",
      "Iter 18100: loss=18.4928, time=9650.57s\n",
      "Iter 18150: loss=18.2659, time=9677.28s\n",
      "Iter 18200: loss=18.6856, time=9704.07s\n",
      "Evaluation on 1101 elements. Correct: 439\n",
      "iter 18200: dev acc=0.3987\n",
      "Iter 18250: loss=18.2888, time=9729.95s\n",
      "Iter 18300: loss=18.4333, time=9756.70s\n",
      "Iter 18350: loss=18.3653, time=9783.61s\n",
      "Iter 18400: loss=18.3872, time=9810.21s\n",
      "Evaluation on 1101 elements. Correct: 433\n",
      "iter 18400: dev acc=0.3933\n",
      "Iter 18450: loss=18.0164, time=9836.18s\n",
      "Iter 18500: loss=18.0917, time=9863.03s\n",
      "Iter 18550: loss=18.3793, time=9889.72s\n",
      "Iter 18600: loss=17.9986, time=9914.47s\n",
      "Evaluation on 1101 elements. Correct: 435\n",
      "iter 18600: dev acc=0.3951\n",
      "Iter 18650: loss=18.5870, time=9941.93s\n",
      "Iter 18700: loss=18.1069, time=9968.25s\n",
      "Iter 18750: loss=18.2515, time=9994.99s\n",
      "Iter 18800: loss=17.9972, time=10019.90s\n",
      "Evaluation on 1101 elements. Correct: 445\n",
      "iter 18800: dev acc=0.4042\n",
      "Iter 18850: loss=18.1722, time=10047.67s\n",
      "Iter 18900: loss=18.0239, time=10074.69s\n",
      "Iter 18950: loss=17.5285, time=10101.72s\n",
      "Iter 19000: loss=17.9415, time=10126.84s\n",
      "Evaluation on 1101 elements. Correct: 430\n",
      "iter 19000: dev acc=0.3906\n",
      "Iter 19050: loss=17.5283, time=10155.15s\n",
      "Iter 19100: loss=18.0666, time=10182.31s\n",
      "Iter 19150: loss=17.6459, time=10207.80s\n",
      "Iter 19200: loss=17.9162, time=10234.82s\n",
      "Evaluation on 1101 elements. Correct: 443\n",
      "iter 19200: dev acc=0.4024\n",
      "Iter 19250: loss=17.8488, time=10262.89s\n",
      "Iter 19300: loss=17.3225, time=10289.84s\n",
      "Iter 19350: loss=17.3869, time=10315.33s\n",
      "Iter 19400: loss=17.7272, time=10342.93s\n",
      "Evaluation on 1101 elements. Correct: 428\n",
      "iter 19400: dev acc=0.3887\n",
      "Iter 19450: loss=17.3348, time=10371.30s\n",
      "Iter 19500: loss=17.4269, time=10396.96s\n",
      "Iter 19550: loss=17.3839, time=10424.25s\n",
      "Iter 19600: loss=17.4565, time=10451.19s\n",
      "Evaluation on 1101 elements. Correct: 433\n",
      "iter 19600: dev acc=0.3933\n",
      "Iter 19650: loss=17.2183, time=10478.78s\n",
      "Iter 19700: loss=17.3813, time=10504.46s\n",
      "Iter 19750: loss=16.9907, time=10531.82s\n",
      "Iter 19800: loss=17.3940, time=10559.05s\n",
      "Evaluation on 1101 elements. Correct: 427\n",
      "iter 19800: dev acc=0.3878\n",
      "Iter 19850: loss=17.1698, time=10586.94s\n",
      "Iter 19900: loss=17.0955, time=10612.42s\n",
      "Iter 19950: loss=17.1017, time=10639.80s\n",
      "Iter 20000: loss=17.1077, time=10666.93s\n",
      "Evaluation on 1101 elements. Correct: 429\n",
      "iter 20000: dev acc=0.3896\n",
      "Iter 20050: loss=16.7520, time=10693.38s\n",
      "Iter 20100: loss=17.0548, time=10720.49s\n",
      "Iter 20150: loss=17.2211, time=10747.60s\n",
      "Iter 20200: loss=16.7770, time=10774.36s\n",
      "Evaluation on 1101 elements. Correct: 421\n",
      "iter 20200: dev acc=0.3824\n",
      "Iter 20250: loss=17.0626, time=10800.98s\n",
      "Iter 20300: loss=17.2857, time=10828.58s\n",
      "Iter 20350: loss=17.2342, time=10855.81s\n",
      "Iter 20400: loss=16.7337, time=10881.46s\n",
      "Evaluation on 1101 elements. Correct: 433\n",
      "iter 20400: dev acc=0.3933\n",
      "Iter 20450: loss=16.9230, time=10909.39s\n",
      "Iter 20500: loss=16.6203, time=10936.44s\n",
      "Iter 20550: loss=16.9432, time=10963.21s\n",
      "Iter 20600: loss=16.7236, time=10988.63s\n",
      "Evaluation on 1101 elements. Correct: 438\n",
      "iter 20600: dev acc=0.3978\n",
      "Iter 20650: loss=16.4295, time=11016.33s\n",
      "Iter 20700: loss=16.7508, time=11043.20s\n",
      "Iter 20750: loss=16.5616, time=11069.26s\n",
      "Iter 20800: loss=16.4546, time=11096.19s\n",
      "Evaluation on 1101 elements. Correct: 436\n",
      "iter 20800: dev acc=0.3960\n",
      "Iter 20850: loss=16.2789, time=11124.20s\n",
      "Iter 20900: loss=16.2813, time=11151.89s\n",
      "Iter 20950: loss=16.7194, time=11177.37s\n",
      "Iter 21000: loss=16.4862, time=11204.92s\n",
      "Evaluation on 1101 elements. Correct: 424\n",
      "iter 21000: dev acc=0.3851\n",
      "Iter 21050: loss=16.5208, time=11233.53s\n",
      "Iter 21100: loss=16.4795, time=11259.31s\n",
      "Iter 21150: loss=16.6732, time=11286.26s\n",
      "Iter 21200: loss=15.9875, time=11313.24s\n",
      "Evaluation on 1101 elements. Correct: 428\n",
      "iter 21200: dev acc=0.3887\n",
      "Iter 21250: loss=16.5882, time=11341.01s\n",
      "Iter 21300: loss=16.3392, time=11366.59s\n",
      "Iter 21350: loss=16.2203, time=11393.89s\n",
      "Iter 21400: loss=16.2277, time=11421.17s\n",
      "Evaluation on 1101 elements. Correct: 432\n",
      "iter 21400: dev acc=0.3924\n",
      "Iter 21450: loss=16.0834, time=11449.62s\n",
      "Iter 21500: loss=16.4519, time=11475.01s\n",
      "Iter 21550: loss=16.1403, time=11502.20s\n",
      "Iter 21600: loss=16.1312, time=11529.51s\n",
      "Evaluation on 1101 elements. Correct: 427\n",
      "iter 21600: dev acc=0.3878\n",
      "Iter 21650: loss=16.2433, time=11556.15s\n",
      "Iter 21700: loss=15.7481, time=11583.56s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 21750: loss=16.1804, time=11610.67s\n",
      "Iter 21800: loss=15.9442, time=11637.81s\n",
      "Evaluation on 1101 elements. Correct: 427\n",
      "iter 21800: dev acc=0.3878\n",
      "Iter 21850: loss=15.5602, time=11664.04s\n",
      "Iter 21900: loss=16.3819, time=11691.63s\n",
      "Iter 21950: loss=16.1367, time=11719.13s\n",
      "Iter 22000: loss=15.7421, time=11746.31s\n",
      "Evaluation on 1101 elements. Correct: 428\n",
      "iter 22000: dev acc=0.3887\n",
      "Iter 22050: loss=15.7516, time=11773.17s\n",
      "Iter 22100: loss=15.6617, time=11800.41s\n",
      "Iter 22150: loss=15.7471, time=11827.65s\n",
      "Iter 22200: loss=15.6384, time=11854.86s\n",
      "Evaluation on 1101 elements. Correct: 427\n",
      "iter 22200: dev acc=0.3878\n",
      "Iter 22250: loss=15.5806, time=11881.20s\n",
      "Iter 22300: loss=15.3890, time=11908.87s\n",
      "Iter 22350: loss=15.5005, time=11935.81s\n",
      "Iter 22400: loss=15.3127, time=11962.81s\n",
      "Evaluation on 1101 elements. Correct: 429\n",
      "iter 22400: dev acc=0.3896\n",
      "Iter 22450: loss=15.4660, time=11989.45s\n",
      "Iter 22500: loss=14.9454, time=12017.02s\n",
      "Iter 22550: loss=15.2401, time=12044.17s\n",
      "Iter 22600: loss=15.2268, time=12069.65s\n",
      "Evaluation on 1101 elements. Correct: 438\n",
      "iter 22600: dev acc=0.3978\n",
      "Iter 22650: loss=14.9376, time=12097.86s\n",
      "Iter 22700: loss=15.4360, time=12125.01s\n",
      "Iter 22750: loss=15.2111, time=12152.20s\n",
      "Iter 22800: loss=15.3721, time=12177.52s\n",
      "Evaluation on 1101 elements. Correct: 420\n",
      "iter 22800: dev acc=0.3815\n",
      "Iter 22850: loss=15.2238, time=12205.17s\n",
      "Iter 22900: loss=15.1927, time=12232.23s\n",
      "Iter 22950: loss=14.9735, time=12257.87s\n",
      "Iter 23000: loss=15.1126, time=12284.21s\n",
      "Evaluation on 1101 elements. Correct: 439\n",
      "iter 23000: dev acc=0.3987\n",
      "Iter 23050: loss=15.3945, time=12312.19s\n",
      "Iter 23100: loss=14.9590, time=12339.20s\n",
      "Iter 23150: loss=15.1916, time=12364.59s\n",
      "Iter 23200: loss=14.9743, time=12391.53s\n",
      "Evaluation on 1101 elements. Correct: 426\n",
      "iter 23200: dev acc=0.3869\n",
      "Iter 23250: loss=14.6774, time=12419.46s\n",
      "Iter 23300: loss=14.8566, time=12446.28s\n",
      "Iter 23350: loss=15.0236, time=12470.75s\n",
      "Iter 23400: loss=14.6077, time=12497.42s\n",
      "Evaluation on 1101 elements. Correct: 436\n",
      "iter 23400: dev acc=0.3960\n",
      "Iter 23450: loss=14.9663, time=12524.36s\n",
      "Iter 23500: loss=14.7581, time=12548.16s\n",
      "Iter 23550: loss=14.5529, time=12572.92s\n",
      "Iter 23600: loss=14.8125, time=12598.37s\n",
      "Evaluation on 1101 elements. Correct: 432\n",
      "iter 23600: dev acc=0.3924\n",
      "Iter 23650: loss=14.6708, time=12624.71s\n",
      "Iter 23700: loss=14.4197, time=12648.89s\n",
      "Iter 23750: loss=14.5144, time=12674.13s\n",
      "Iter 23800: loss=14.3946, time=12699.38s\n",
      "Evaluation on 1101 elements. Correct: 427\n",
      "iter 23800: dev acc=0.3878\n",
      "Iter 23850: loss=14.6235, time=12724.22s\n",
      "Iter 23900: loss=14.2299, time=12749.78s\n",
      "Iter 23950: loss=14.2163, time=12775.30s\n",
      "Iter 24000: loss=14.3373, time=12801.02s\n",
      "Evaluation on 1101 elements. Correct: 429\n",
      "iter 24000: dev acc=0.3896\n",
      "Iter 24050: loss=14.6223, time=12825.83s\n",
      "Iter 24100: loss=14.5488, time=12851.31s\n",
      "Iter 24150: loss=14.1659, time=12876.55s\n",
      "Iter 24200: loss=14.3200, time=12902.33s\n",
      "Evaluation on 1101 elements. Correct: 433\n",
      "iter 24200: dev acc=0.3933\n",
      "Iter 24250: loss=14.4036, time=12927.39s\n",
      "Iter 24300: loss=14.6885, time=12952.77s\n",
      "Iter 24350: loss=14.3606, time=12978.17s\n",
      "Iter 24400: loss=14.4120, time=13002.60s\n",
      "Evaluation on 1101 elements. Correct: 435\n",
      "iter 24400: dev acc=0.3951\n",
      "Iter 24450: loss=14.3235, time=13029.09s\n",
      "Iter 24500: loss=14.1828, time=13054.74s\n",
      "Iter 24550: loss=14.0702, time=13080.53s\n",
      "Iter 24600: loss=14.2045, time=13104.34s\n",
      "Evaluation on 1101 elements. Correct: 439\n",
      "iter 24600: dev acc=0.3987\n",
      "Iter 24650: loss=13.9499, time=13131.07s\n",
      "Iter 24700: loss=14.1149, time=13156.31s\n",
      "Iter 24750: loss=13.9704, time=13182.06s\n",
      "Iter 24800: loss=13.6654, time=13206.00s\n",
      "Evaluation on 1101 elements. Correct: 425\n",
      "iter 24800: dev acc=0.3860\n",
      "Iter 24850: loss=13.6948, time=13232.51s\n",
      "Iter 24900: loss=13.9273, time=13257.84s\n",
      "Iter 24950: loss=13.8504, time=13281.56s\n",
      "Iter 25000: loss=13.9914, time=13307.01s\n",
      "Evaluation on 1101 elements. Correct: 425\n",
      "iter 25000: dev acc=0.3860\n",
      "Iter 25050: loss=14.0331, time=13333.30s\n",
      "Iter 25100: loss=13.8417, time=13358.69s\n",
      "Iter 25150: loss=14.0702, time=13382.98s\n",
      "Iter 25200: loss=13.6023, time=13408.35s\n",
      "Evaluation on 1101 elements. Correct: 434\n",
      "iter 25200: dev acc=0.3942\n",
      "Iter 25250: loss=13.4959, time=13435.03s\n",
      "Iter 25300: loss=13.7300, time=13458.84s\n",
      "Iter 25350: loss=13.3699, time=13484.84s\n",
      "Iter 25400: loss=13.6425, time=13510.16s\n",
      "Evaluation on 1101 elements. Correct: 426\n",
      "iter 25400: dev acc=0.3869\n",
      "Iter 25450: loss=13.5349, time=13537.02s\n",
      "Iter 25500: loss=13.3535, time=13560.78s\n",
      "Iter 25550: loss=13.7476, time=13586.40s\n",
      "Iter 25600: loss=13.9602, time=13611.99s\n",
      "Evaluation on 1101 elements. Correct: 423\n",
      "iter 25600: dev acc=0.3842\n",
      "Iter 25650: loss=13.8232, time=13638.39s\n",
      "Iter 25700: loss=13.4116, time=13662.65s\n",
      "Iter 25750: loss=13.2597, time=13687.82s\n",
      "Iter 25800: loss=13.5167, time=13713.51s\n",
      "Evaluation on 1101 elements. Correct: 439\n",
      "iter 25800: dev acc=0.3987\n",
      "Iter 25850: loss=13.0227, time=13738.55s\n",
      "Iter 25900: loss=13.3466, time=13763.86s\n",
      "Iter 25950: loss=13.1858, time=13789.09s\n",
      "Iter 26000: loss=13.5816, time=13814.76s\n",
      "Evaluation on 1101 elements. Correct: 448\n",
      "iter 26000: dev acc=0.4069\n",
      "Iter 26050: loss=13.2382, time=13839.58s\n",
      "Iter 26100: loss=13.2958, time=13865.27s\n",
      "Iter 26150: loss=12.9000, time=13890.54s\n",
      "Iter 26200: loss=13.6356, time=13914.53s\n",
      "Evaluation on 1101 elements. Correct: 427\n",
      "iter 26200: dev acc=0.3878\n",
      "Iter 26250: loss=12.8994, time=13941.18s\n",
      "Iter 26300: loss=13.1975, time=13966.55s\n",
      "Iter 26350: loss=12.9475, time=13992.18s\n",
      "Iter 26400: loss=12.6399, time=14015.75s\n",
      "Evaluation on 1101 elements. Correct: 433\n",
      "iter 26400: dev acc=0.3933\n",
      "Iter 26450: loss=12.6856, time=14042.24s\n",
      "Iter 26500: loss=12.8353, time=14067.39s\n",
      "Iter 26550: loss=12.9291, time=14091.34s\n",
      "Iter 26600: loss=13.0985, time=14116.95s\n",
      "Evaluation on 1101 elements. Correct: 440\n",
      "iter 26600: dev acc=0.3996\n",
      "Iter 26650: loss=12.7837, time=14144.91s\n",
      "Iter 26700: loss=12.2432, time=14171.85s\n",
      "Iter 26750: loss=12.8920, time=14196.97s\n",
      "Iter 26800: loss=13.3712, time=14223.71s\n",
      "Evaluation on 1101 elements. Correct: 440\n",
      "iter 26800: dev acc=0.3996\n",
      "Iter 26850: loss=12.8776, time=14251.80s\n",
      "Iter 26900: loss=12.6204, time=14277.48s\n",
      "Iter 26950: loss=12.8829, time=14305.05s\n",
      "Iter 27000: loss=12.7248, time=14331.82s\n",
      "Evaluation on 1101 elements. Correct: 435\n",
      "iter 27000: dev acc=0.3951\n",
      "Iter 27050: loss=12.6720, time=14359.59s\n",
      "Iter 27100: loss=12.4015, time=14385.26s\n",
      "Iter 27150: loss=12.8163, time=14412.36s\n",
      "Iter 27200: loss=12.8804, time=14439.39s\n",
      "Evaluation on 1101 elements. Correct: 436\n",
      "iter 27200: dev acc=0.3960\n",
      "Iter 27250: loss=12.4460, time=14465.94s\n",
      "Iter 27300: loss=12.3232, time=14493.21s\n",
      "Iter 27350: loss=12.6751, time=14520.45s\n",
      "Iter 27400: loss=12.5276, time=14547.77s\n",
      "Evaluation on 1101 elements. Correct: 426\n",
      "iter 27400: dev acc=0.3869\n",
      "Iter 27450: loss=12.6355, time=14575.53s\n",
      "Iter 27500: loss=12.5017, time=14601.06s\n",
      "Iter 27550: loss=12.4954, time=14628.24s\n",
      "Iter 27600: loss=12.5030, time=14655.19s\n",
      "Evaluation on 1101 elements. Correct: 445\n",
      "iter 27600: dev acc=0.4042\n",
      "Iter 27650: loss=12.1149, time=14681.86s\n",
      "Iter 27700: loss=12.3904, time=14708.98s\n",
      "Iter 27750: loss=12.2022, time=14735.94s\n",
      "Iter 27800: loss=12.2025, time=14763.54s\n",
      "Evaluation on 1101 elements. Correct: 436\n",
      "iter 27800: dev acc=0.3960\n",
      "Iter 27850: loss=12.1711, time=14790.19s\n",
      "Iter 27900: loss=12.2289, time=14817.20s\n",
      "Iter 27950: loss=12.3635, time=14844.69s\n",
      "Iter 28000: loss=11.9953, time=14870.73s\n",
      "Evaluation on 1101 elements. Correct: 430\n",
      "iter 28000: dev acc=0.3906\n",
      "Iter 28050: loss=12.4611, time=14898.93s\n",
      "Iter 28100: loss=12.0654, time=14926.06s\n",
      "Iter 28150: loss=12.0886, time=14953.32s\n",
      "Iter 28200: loss=11.9205, time=14978.88s\n",
      "Evaluation on 1101 elements. Correct: 436\n",
      "iter 28200: dev acc=0.3960\n",
      "Iter 28250: loss=12.1707, time=15007.29s\n",
      "Iter 28300: loss=12.2455, time=15034.37s\n",
      "Iter 28350: loss=11.4777, time=15061.00s\n",
      "Iter 28400: loss=11.7592, time=15086.29s\n",
      "Evaluation on 1101 elements. Correct: 440\n",
      "iter 28400: dev acc=0.3996\n",
      "Iter 28450: loss=11.9317, time=15114.46s\n",
      "Iter 28500: loss=11.8490, time=15141.65s\n",
      "Iter 28550: loss=12.2816, time=15167.47s\n",
      "Iter 28600: loss=12.3750, time=15195.05s\n",
      "Evaluation on 1101 elements. Correct: 440\n",
      "iter 28600: dev acc=0.3996\n",
      "Iter 28650: loss=12.8468, time=15222.98s\n",
      "Iter 28700: loss=11.8212, time=15250.47s\n",
      "Iter 28750: loss=11.7850, time=15276.08s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 28800: loss=11.9378, time=15303.28s\n",
      "Evaluation on 1101 elements. Correct: 446\n",
      "iter 28800: dev acc=0.4051\n",
      "Iter 28850: loss=11.4522, time=15331.38s\n",
      "Iter 28900: loss=11.5281, time=15356.60s\n",
      "Iter 28950: loss=11.9789, time=15383.60s\n",
      "Iter 29000: loss=11.8513, time=15411.17s\n",
      "Evaluation on 1101 elements. Correct: 438\n",
      "iter 29000: dev acc=0.3978\n",
      "Iter 29050: loss=11.5521, time=15439.38s\n",
      "Iter 29100: loss=11.5876, time=15465.04s\n",
      "Iter 29150: loss=11.4907, time=15491.91s\n",
      "Iter 29200: loss=11.3559, time=15519.14s\n",
      "Evaluation on 1101 elements. Correct: 449\n",
      "iter 29200: dev acc=0.4078\n",
      "Iter 29250: loss=11.3506, time=15547.09s\n",
      "Iter 29300: loss=11.3081, time=15572.56s\n",
      "Iter 29350: loss=10.9777, time=15599.97s\n",
      "Iter 29400: loss=11.2032, time=15627.18s\n",
      "Evaluation on 1101 elements. Correct: 447\n",
      "iter 29400: dev acc=0.4060\n",
      "Iter 29450: loss=11.3467, time=15653.64s\n",
      "Iter 29500: loss=11.5618, time=15681.16s\n",
      "Iter 29550: loss=11.3538, time=15707.88s\n",
      "Iter 29600: loss=11.4307, time=15734.96s\n",
      "Evaluation on 1101 elements. Correct: 425\n",
      "iter 29600: dev acc=0.3860\n",
      "Iter 29650: loss=12.0154, time=15761.30s\n",
      "Iter 29700: loss=11.3622, time=15788.43s\n",
      "Iter 29750: loss=11.5451, time=15815.25s\n",
      "Iter 29800: loss=11.3672, time=15840.56s\n",
      "Evaluation on 1101 elements. Correct: 433\n",
      "iter 29800: dev acc=0.3933\n",
      "Iter 29850: loss=11.3132, time=15868.41s\n",
      "Iter 29900: loss=11.5029, time=15895.57s\n",
      "Iter 29950: loss=11.4299, time=15920.91s\n",
      "Iter 30000: loss=11.4006, time=15948.01s\n",
      "Evaluation on 1101 elements. Correct: 445\n",
      "iter 30000: dev acc=0.4042\n",
      "Done training\n",
      "Loading best model\n",
      "Evaluation on 8544 elements. Correct: 4470\n",
      "Evaluation on 1101 elements. Correct: 534\n",
      "Evaluation on 2210 elements. Correct: 1038\n",
      "best model iter 1400: train acc=0.5232, dev acc=0.4850, test acc=0.4697\n"
     ]
    }
   ],
   "source": [
    "for _, depth_dict in all_subtrees.items():\n",
    "    for subtree in depth_dict:\n",
    "        subtree.loss.clear()\n",
    "\n",
    "tree_model = TreeLSTMClassifier(\n",
    "    len(v.w2i), 300, 150, len(t2i), v)\n",
    "\n",
    "with torch.no_grad():\n",
    "    tree_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
    "    tree_model.embed.weight.requires_grad = False\n",
    "  \n",
    "def do_train(model):\n",
    "\n",
    "    print(model)\n",
    "    print_parameters(model)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
    "\n",
    "    return train_tree_model(\n",
    "        model, optimizer, num_iterations=30000, \n",
    "        print_every=50, eval_every=200,\n",
    "        prep_fn=prepare_treelstm_minibatch,\n",
    "        eval_fn=evaluate,\n",
    "        batch_fn=get_minibatch,\n",
    "        batch_size=128, eval_batch_size=64)\n",
    "\n",
    "DEBUG_TREE_PRINT = False\n",
    "results = do_train(tree_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TreeLSTMClassifier(\n",
      "  (embed): Embedding(20727, 300, padding_idx=1)\n",
      "  (treelstm): TreeLSTM(\n",
      "    (reduce): TreeLSTMCell(300, 150)\n",
      "    (proj_x): Linear(in_features=300, out_features=150, bias=True)\n",
      "    (proj_x_gate): Linear(in_features=300, out_features=150, bias=True)\n",
      "    (buffers_dropout): Dropout(p=0.5)\n",
      "  )\n",
      "  (output_layer): Sequential(\n",
      "    (0): Dropout(p=0.25)\n",
      "    (1): Linear(in_features=150, out_features=5, bias=True)\n",
      "    (2): LogSoftmax()\n",
      "  )\n",
      ")\n",
      "embed.weight             [20727, 300] requires_grad=False\n",
      "treelstm.reduce.attent_module.output_layer.fc1.weight [150, 300]   requires_grad=True\n",
      "treelstm.reduce.attent_module.output_layer.fc1.bias [150]        requires_grad=True\n",
      "treelstm.reduce.attent_module.output_layer.fc2.weight [2, 150]     requires_grad=True\n",
      "treelstm.reduce.attent_module.output_layer.fc2.bias [2]          requires_grad=True\n",
      "treelstm.reduce.reduce_layer.weight [750, 300]   requires_grad=True\n",
      "treelstm.reduce.reduce_layer.bias [750]        requires_grad=True\n",
      "treelstm.proj_x.weight   [150, 300]   requires_grad=True\n",
      "treelstm.proj_x.bias     [150]        requires_grad=True\n",
      "treelstm.proj_x_gate.weight [150, 300]   requires_grad=True\n",
      "treelstm.proj_x_gate.bias [150]        requires_grad=True\n",
      "output_layer.1.weight    [5, 150]     requires_grad=True\n",
      "output_layer.1.bias      [5]          requires_grad=True\n",
      "\n",
      "Total parameters: 6580357\n",
      "\n",
      "Iter 50: loss=79.9884, time=27.19s\n",
      "Iter 100: loss=78.4510, time=53.93s\n",
      "Iter 150: loss=77.1578, time=80.93s\n",
      "Iter 200: loss=75.7156, time=106.77s\n",
      "Evaluation on 1101 elements. Correct: 230\n",
      "iter 200: dev acc=0.2089\n",
      "new highscore\n",
      "Iter 250: loss=73.9582, time=135.74s\n",
      "Iter 300: loss=71.9478, time=163.63s\n",
      "Iter 350: loss=69.8283, time=189.85s\n",
      "Iter 400: loss=67.4390, time=217.85s\n",
      "Evaluation on 1101 elements. Correct: 385\n",
      "iter 400: dev acc=0.3497\n",
      "new highscore\n",
      "Iter 450: loss=65.0399, time=246.27s\n",
      "Iter 500: loss=63.5225, time=274.10s\n",
      "Iter 550: loss=62.2904, time=300.28s\n",
      "Iter 600: loss=61.1920, time=327.99s\n",
      "Evaluation on 1101 elements. Correct: 430\n",
      "iter 600: dev acc=0.3906\n",
      "new highscore\n",
      "Iter 650: loss=60.1542, time=356.67s\n",
      "Iter 700: loss=59.5584, time=382.42s\n",
      "Iter 750: loss=58.8697, time=410.26s\n",
      "Iter 800: loss=58.1096, time=437.90s\n",
      "Evaluation on 1101 elements. Correct: 449\n",
      "iter 800: dev acc=0.4078\n",
      "new highscore\n",
      "Iter 850: loss=57.4343, time=465.75s\n",
      "Iter 900: loss=57.1854, time=490.12s\n",
      "Iter 950: loss=56.5716, time=516.32s\n",
      "Iter 1000: loss=56.4772, time=543.73s\n",
      "Evaluation on 1101 elements. Correct: 452\n",
      "iter 1000: dev acc=0.4105\n",
      "new highscore\n",
      "Iter 1050: loss=55.9693, time=572.31s\n",
      "Iter 1100: loss=55.7183, time=597.87s\n",
      "Iter 1150: loss=55.3359, time=625.53s\n",
      "Iter 1200: loss=55.0367, time=653.48s\n",
      "Evaluation on 1101 elements. Correct: 453\n",
      "iter 1200: dev acc=0.4114\n",
      "new highscore\n",
      "Iter 1250: loss=54.6738, time=681.90s\n",
      "Iter 1300: loss=54.5298, time=707.96s\n",
      "Iter 1350: loss=54.0528, time=735.72s\n",
      "Iter 1400: loss=54.0702, time=763.55s\n",
      "Evaluation on 1101 elements. Correct: 460\n",
      "iter 1400: dev acc=0.4178\n",
      "new highscore\n",
      "Iter 1450: loss=53.8198, time=792.19s\n",
      "Iter 1500: loss=53.2544, time=818.35s\n",
      "Iter 1550: loss=53.5792, time=845.90s\n",
      "Iter 1600: loss=53.2284, time=873.67s\n",
      "Evaluation on 1101 elements. Correct: 473\n",
      "iter 1600: dev acc=0.4296\n",
      "new highscore\n",
      "Iter 1650: loss=52.8318, time=902.16s\n",
      "Iter 1700: loss=52.8329, time=928.57s\n",
      "Iter 1750: loss=52.4132, time=956.08s\n",
      "Iter 1800: loss=52.4184, time=983.25s\n",
      "Evaluation on 1101 elements. Correct: 478\n",
      "iter 1800: dev acc=0.4342\n",
      "new highscore\n",
      "Iter 1850: loss=52.2425, time=1011.10s\n",
      "Iter 1900: loss=52.1110, time=1036.84s\n",
      "Iter 1950: loss=51.7430, time=1064.44s\n",
      "Iter 2000: loss=51.5949, time=1091.69s\n",
      "Evaluation on 1101 elements. Correct: 487\n",
      "iter 2000: dev acc=0.4423\n",
      "new highscore\n",
      "Iter 2050: loss=51.4254, time=1118.50s\n",
      "Iter 2100: loss=51.4341, time=1145.89s\n",
      "Iter 2150: loss=51.2604, time=1173.36s\n",
      "Iter 2200: loss=51.0281, time=1201.13s\n",
      "Evaluation on 1101 elements. Correct: 485\n",
      "iter 2200: dev acc=0.4405\n",
      "Iter 2250: loss=51.1880, time=1227.68s\n",
      "Iter 2300: loss=50.6185, time=1255.28s\n",
      "Iter 2350: loss=50.5979, time=1282.74s\n",
      "Iter 2400: loss=50.8399, time=1309.15s\n",
      "Evaluation on 1101 elements. Correct: 482\n",
      "iter 2400: dev acc=0.4378\n",
      "Iter 2450: loss=50.3779, time=1338.01s\n",
      "Iter 2500: loss=50.4019, time=1365.21s\n",
      "Iter 2550: loss=50.3702, time=1392.24s\n",
      "Iter 2600: loss=50.1476, time=1417.95s\n",
      "Evaluation on 1101 elements. Correct: 491\n",
      "iter 2600: dev acc=0.4460\n",
      "new highscore\n",
      "Iter 2650: loss=50.0164, time=1446.81s\n",
      "Iter 2700: loss=50.1014, time=1474.51s\n",
      "Iter 2750: loss=49.7280, time=1500.89s\n",
      "Iter 2800: loss=49.7983, time=1528.55s\n",
      "Evaluation on 1101 elements. Correct: 498\n",
      "iter 2800: dev acc=0.4523\n",
      "new highscore\n",
      "Iter 2850: loss=49.5091, time=1557.21s\n",
      "Iter 2900: loss=49.4893, time=1584.74s\n",
      "Iter 2950: loss=49.5573, time=1610.56s\n",
      "Iter 3000: loss=49.4828, time=1638.01s\n",
      "Evaluation on 1101 elements. Correct: 503\n",
      "iter 3000: dev acc=0.4569\n",
      "new highscore\n",
      "Iter 3050: loss=49.3309, time=1666.53s\n",
      "Iter 3100: loss=49.0011, time=1691.88s\n",
      "Iter 3150: loss=48.9643, time=1719.31s\n",
      "Iter 3200: loss=48.9768, time=1746.77s\n",
      "Evaluation on 1101 elements. Correct: 498\n",
      "iter 3200: dev acc=0.4523\n",
      "Iter 3250: loss=48.8309, time=1775.25s\n",
      "Iter 3300: loss=48.6301, time=1800.80s\n",
      "Iter 3350: loss=48.7418, time=1828.58s\n",
      "Iter 3400: loss=48.6784, time=1855.92s\n",
      "Evaluation on 1101 elements. Correct: 509\n",
      "iter 3400: dev acc=0.4623\n",
      "new highscore\n",
      "Iter 3450: loss=48.8098, time=1882.64s\n",
      "Iter 3500: loss=48.3374, time=1910.58s\n",
      "Iter 3550: loss=48.1251, time=1938.43s\n",
      "Iter 3600: loss=48.3106, time=1965.87s\n",
      "Evaluation on 1101 elements. Correct: 513\n",
      "iter 3600: dev acc=0.4659\n",
      "new highscore\n",
      "Iter 3650: loss=48.2642, time=1993.03s\n",
      "Iter 3700: loss=48.1568, time=2020.35s\n",
      "Iter 3750: loss=47.9887, time=2047.84s\n",
      "Iter 3800: loss=47.9526, time=2073.37s\n",
      "Evaluation on 1101 elements. Correct: 516\n",
      "iter 3800: dev acc=0.4687\n",
      "new highscore\n",
      "Iter 3850: loss=47.8456, time=2101.60s\n",
      "Iter 3900: loss=48.0037, time=2128.87s\n",
      "Iter 3950: loss=47.6512, time=2156.75s\n",
      "Iter 4000: loss=47.8554, time=2182.95s\n",
      "Evaluation on 1101 elements. Correct: 509\n",
      "iter 4000: dev acc=0.4623\n",
      "Iter 4050: loss=47.5865, time=2211.25s\n",
      "Iter 4100: loss=47.7928, time=2238.81s\n",
      "Iter 4150: loss=47.5515, time=2266.63s\n",
      "Iter 4200: loss=47.2108, time=2292.51s\n",
      "Evaluation on 1101 elements. Correct: 520\n",
      "iter 4200: dev acc=0.4723\n",
      "new highscore\n",
      "Iter 4250: loss=47.3863, time=2321.09s\n",
      "Iter 4300: loss=47.4574, time=2348.25s\n",
      "Iter 4350: loss=47.0720, time=2374.48s\n",
      "Iter 4400: loss=47.5366, time=2400.92s\n",
      "Evaluation on 1101 elements. Correct: 521\n",
      "iter 4400: dev acc=0.4732\n",
      "new highscore\n",
      "Iter 4450: loss=47.1116, time=2427.71s\n",
      "Iter 4500: loss=46.9543, time=2452.63s\n",
      "Iter 4550: loss=47.0604, time=2479.93s\n",
      "Iter 4600: loss=47.2749, time=2507.48s\n",
      "Evaluation on 1101 elements. Correct: 524\n",
      "iter 4600: dev acc=0.4759\n",
      "new highscore\n",
      "Iter 4650: loss=46.8669, time=2535.49s\n",
      "Iter 4700: loss=46.8376, time=2560.77s\n",
      "Iter 4750: loss=46.7139, time=2588.15s\n",
      "Iter 4800: loss=46.7175, time=2615.46s\n",
      "Evaluation on 1101 elements. Correct: 513\n",
      "iter 4800: dev acc=0.4659\n",
      "Iter 4850: loss=46.5588, time=2641.76s\n",
      "Iter 4900: loss=46.7038, time=2669.09s\n",
      "Iter 4950: loss=46.5956, time=2696.40s\n",
      "Iter 5000: loss=46.5102, time=2723.74s\n",
      "Evaluation on 1101 elements. Correct: 525\n",
      "iter 5000: dev acc=0.4768\n",
      "new highscore\n",
      "Iter 5050: loss=46.5771, time=2750.52s\n",
      "Iter 5100: loss=46.2689, time=2777.56s\n",
      "Iter 5150: loss=46.2150, time=2805.07s\n",
      "Iter 5200: loss=46.4224, time=2830.20s\n",
      "Evaluation on 1101 elements. Correct: 522\n",
      "iter 5200: dev acc=0.4741\n",
      "Iter 5250: loss=45.9800, time=2858.57s\n",
      "Iter 5300: loss=46.5071, time=2885.66s\n",
      "Iter 5350: loss=46.3355, time=2913.01s\n",
      "Iter 5400: loss=46.2621, time=2938.67s\n",
      "Evaluation on 1101 elements. Correct: 524\n",
      "iter 5400: dev acc=0.4759\n",
      "Iter 5450: loss=45.8028, time=2967.02s\n",
      "Iter 5500: loss=46.0622, time=2994.55s\n",
      "Iter 5550: loss=45.8087, time=3020.57s\n",
      "Iter 5600: loss=46.0096, time=3048.24s\n",
      "Evaluation on 1101 elements. Correct: 516\n",
      "iter 5600: dev acc=0.4687\n",
      "Iter 5650: loss=45.8198, time=3076.51s\n",
      "Iter 5700: loss=45.6744, time=3104.14s\n",
      "Iter 5750: loss=45.7349, time=3130.37s\n",
      "Iter 5800: loss=45.6891, time=3157.92s\n",
      "Evaluation on 1101 elements. Correct: 523\n",
      "iter 5800: dev acc=0.4750\n",
      "Iter 5850: loss=45.7653, time=3186.23s\n",
      "Iter 5900: loss=45.5374, time=3212.21s\n",
      "Iter 5950: loss=45.7351, time=3240.28s\n",
      "Iter 6000: loss=45.5339, time=3268.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on 1101 elements. Correct: 522\n",
      "iter 6000: dev acc=0.4741\n",
      "Iter 6050: loss=45.9544, time=3296.54s\n",
      "Iter 6100: loss=45.3930, time=3322.55s\n",
      "Iter 6150: loss=45.4750, time=3349.67s\n",
      "Iter 6200: loss=45.4295, time=3377.04s\n",
      "Evaluation on 1101 elements. Correct: 523\n",
      "iter 6200: dev acc=0.4750\n",
      "Iter 6250: loss=45.4579, time=3405.48s\n",
      "Iter 6300: loss=45.0719, time=3430.90s\n",
      "Iter 6350: loss=45.2733, time=3458.46s\n",
      "Iter 6400: loss=45.4950, time=3486.07s\n",
      "Evaluation on 1101 elements. Correct: 521\n",
      "iter 6400: dev acc=0.4732\n",
      "Iter 6450: loss=45.0359, time=3514.57s\n",
      "Iter 6500: loss=45.0783, time=3540.18s\n",
      "Iter 6550: loss=45.0754, time=3567.61s\n",
      "Iter 6600: loss=45.1414, time=3595.34s\n",
      "Evaluation on 1101 elements. Correct: 527\n",
      "iter 6600: dev acc=0.4787\n",
      "new highscore\n",
      "Iter 6650: loss=45.1851, time=3622.51s\n",
      "Iter 6700: loss=44.9929, time=3650.12s\n",
      "Iter 6750: loss=44.9126, time=3678.04s\n",
      "Iter 6800: loss=44.7129, time=3703.99s\n",
      "Evaluation on 1101 elements. Correct: 528\n",
      "iter 6800: dev acc=0.4796\n",
      "new highscore\n",
      "Iter 6850: loss=45.0036, time=3732.72s\n",
      "Iter 6900: loss=44.8145, time=3760.32s\n",
      "Iter 6950: loss=44.7541, time=3788.05s\n",
      "Iter 7000: loss=44.6749, time=3814.11s\n",
      "Evaluation on 1101 elements. Correct: 525\n",
      "iter 7000: dev acc=0.4768\n",
      "Iter 7050: loss=44.7239, time=3841.67s\n",
      "Iter 7100: loss=44.9353, time=3868.45s\n",
      "Iter 7150: loss=44.7393, time=3893.58s\n",
      "Iter 7200: loss=44.3438, time=3920.42s\n",
      "Evaluation on 1101 elements. Correct: 528\n",
      "iter 7200: dev acc=0.4796\n",
      "Iter 7250: loss=44.8529, time=3947.75s\n",
      "Iter 7300: loss=44.5964, time=3974.77s\n",
      "Iter 7350: loss=44.3706, time=3999.72s\n",
      "Iter 7400: loss=44.3804, time=4026.55s\n",
      "Evaluation on 1101 elements. Correct: 525\n",
      "iter 7400: dev acc=0.4768\n",
      "Iter 7450: loss=44.3345, time=4053.90s\n",
      "Iter 7500: loss=44.5382, time=4080.72s\n",
      "Iter 7550: loss=44.2570, time=4105.78s\n",
      "Iter 7600: loss=44.4428, time=4132.34s\n",
      "Evaluation on 1101 elements. Correct: 523\n",
      "iter 7600: dev acc=0.4750\n",
      "Iter 7650: loss=44.4393, time=4159.99s\n",
      "Iter 7700: loss=44.2610, time=4185.26s\n",
      "Iter 7750: loss=44.0170, time=4211.73s\n",
      "Iter 7800: loss=44.2440, time=4239.36s\n",
      "Evaluation on 1101 elements. Correct: 518\n",
      "iter 7800: dev acc=0.4705\n",
      "Iter 7850: loss=44.0253, time=4268.22s\n",
      "Iter 7900: loss=43.9528, time=4294.49s\n",
      "Iter 7950: loss=43.9143, time=4322.05s\n",
      "Iter 8000: loss=44.2113, time=4349.52s\n",
      "Evaluation on 1101 elements. Correct: 521\n",
      "iter 8000: dev acc=0.4732\n",
      "Iter 8050: loss=43.9682, time=4376.84s\n",
      "Iter 8100: loss=43.7947, time=4404.32s\n",
      "Iter 8150: loss=43.8649, time=4431.60s\n",
      "Iter 8200: loss=43.6319, time=4457.70s\n",
      "Evaluation on 1101 elements. Correct: 524\n",
      "iter 8200: dev acc=0.4759\n",
      "Iter 8250: loss=44.0156, time=4486.03s\n",
      "Iter 8300: loss=43.6110, time=4512.65s\n",
      "Iter 8350: loss=43.9874, time=4538.84s\n",
      "Iter 8400: loss=43.6676, time=4563.98s\n",
      "Evaluation on 1101 elements. Correct: 514\n",
      "iter 8400: dev acc=0.4668\n",
      "Iter 8450: loss=43.7215, time=4592.02s\n",
      "Iter 8500: loss=43.8603, time=4620.32s\n",
      "Iter 8550: loss=43.8038, time=4648.42s\n",
      "Iter 8600: loss=43.7953, time=4674.63s\n",
      "Evaluation on 1101 elements. Correct: 516\n",
      "iter 8600: dev acc=0.4687\n",
      "Iter 8650: loss=43.2516, time=4703.51s\n",
      "Iter 8700: loss=43.5085, time=4731.02s\n",
      "Iter 8750: loss=43.4920, time=4756.73s\n",
      "Iter 8800: loss=43.4403, time=4784.18s\n",
      "Evaluation on 1101 elements. Correct: 513\n",
      "iter 8800: dev acc=0.4659\n",
      "Iter 8850: loss=43.5237, time=4812.60s\n",
      "Iter 8900: loss=43.2570, time=4840.23s\n",
      "Iter 8950: loss=43.3081, time=4866.03s\n",
      "Iter 9000: loss=43.2723, time=4893.91s\n",
      "Evaluation on 1101 elements. Correct: 518\n",
      "iter 9000: dev acc=0.4705\n",
      "Iter 9050: loss=43.0966, time=4922.58s\n",
      "Iter 9100: loss=43.3565, time=4950.50s\n",
      "Iter 9150: loss=42.7600, time=4976.72s\n",
      "Iter 9200: loss=43.2876, time=5004.35s\n",
      "Evaluation on 1101 elements. Correct: 514\n",
      "iter 9200: dev acc=0.4668\n",
      "Iter 9250: loss=43.1099, time=5033.13s\n",
      "Iter 9300: loss=43.0449, time=5060.81s\n",
      "Iter 9350: loss=43.2613, time=5087.17s\n",
      "Iter 9400: loss=43.2360, time=5114.70s\n",
      "Evaluation on 1101 elements. Correct: 514\n",
      "iter 9400: dev acc=0.4668\n",
      "Iter 9450: loss=43.0825, time=5143.91s\n",
      "Iter 9500: loss=43.1306, time=5171.73s\n",
      "Iter 9550: loss=42.8325, time=5197.64s\n",
      "Iter 9600: loss=43.1313, time=5225.35s\n",
      "Evaluation on 1101 elements. Correct: 515\n",
      "iter 9600: dev acc=0.4678\n",
      "Iter 9650: loss=42.9292, time=5254.01s\n",
      "Iter 9700: loss=43.0342, time=5279.52s\n",
      "Iter 9750: loss=42.8322, time=5305.98s\n",
      "Iter 9800: loss=42.5593, time=5332.58s\n",
      "Evaluation on 1101 elements. Correct: 509\n",
      "iter 9800: dev acc=0.4623\n",
      "Iter 9850: loss=42.9256, time=5360.40s\n",
      "Iter 9900: loss=42.3778, time=5385.39s\n",
      "Iter 9950: loss=42.8614, time=5411.94s\n",
      "Iter 10000: loss=42.6298, time=5438.65s\n",
      "Evaluation on 1101 elements. Correct: 504\n",
      "iter 10000: dev acc=0.4578\n",
      "Iter 10050: loss=42.5200, time=5464.52s\n",
      "Iter 10100: loss=42.3469, time=5491.12s\n",
      "Iter 10150: loss=42.7785, time=5517.73s\n",
      "Iter 10200: loss=42.3400, time=5544.04s\n",
      "Evaluation on 1101 elements. Correct: 511\n",
      "iter 10200: dev acc=0.4641\n",
      "Iter 10250: loss=42.3359, time=5569.83s\n",
      "Iter 10300: loss=42.3648, time=5597.24s\n",
      "Iter 10350: loss=42.5890, time=5625.09s\n",
      "Iter 10400: loss=42.3237, time=5651.29s\n",
      "Evaluation on 1101 elements. Correct: 510\n",
      "iter 10400: dev acc=0.4632\n",
      "Iter 10450: loss=42.3843, time=5680.27s\n",
      "Iter 10500: loss=42.4362, time=5707.65s\n",
      "Iter 10550: loss=42.2832, time=5735.57s\n",
      "Iter 10600: loss=42.1260, time=5761.75s\n",
      "Evaluation on 1101 elements. Correct: 509\n",
      "iter 10600: dev acc=0.4623\n",
      "Iter 10650: loss=42.4514, time=5790.65s\n",
      "Iter 10700: loss=42.0828, time=5818.21s\n",
      "Iter 10750: loss=42.1792, time=5844.19s\n",
      "Iter 10800: loss=42.1036, time=5872.06s\n",
      "Evaluation on 1101 elements. Correct: 506\n",
      "iter 10800: dev acc=0.4596\n",
      "Iter 10850: loss=42.1597, time=5901.05s\n",
      "Iter 10900: loss=41.9333, time=5928.52s\n",
      "Iter 10950: loss=42.0268, time=5955.09s\n",
      "Iter 11000: loss=41.8197, time=5982.84s\n",
      "Evaluation on 1101 elements. Correct: 506\n",
      "iter 11000: dev acc=0.4596\n",
      "Iter 11050: loss=41.8769, time=6011.57s\n",
      "Iter 11100: loss=41.6048, time=6039.30s\n",
      "Iter 11150: loss=42.3190, time=6065.80s\n",
      "Iter 11200: loss=41.7376, time=6092.44s\n",
      "Evaluation on 1101 elements. Correct: 515\n",
      "iter 11200: dev acc=0.4678\n",
      "Iter 11250: loss=41.9283, time=6121.03s\n",
      "Iter 11300: loss=41.7291, time=6146.70s\n",
      "Iter 11350: loss=41.7343, time=6174.06s\n",
      "Iter 11400: loss=42.0389, time=6201.88s\n",
      "Evaluation on 1101 elements. Correct: 511\n",
      "iter 11400: dev acc=0.4641\n",
      "Iter 11450: loss=41.6321, time=6230.75s\n",
      "Iter 11500: loss=41.6460, time=6256.58s\n",
      "Iter 11550: loss=41.4946, time=6284.26s\n",
      "Iter 11600: loss=41.6408, time=6311.72s\n",
      "Evaluation on 1101 elements. Correct: 509\n",
      "iter 11600: dev acc=0.4623\n",
      "Iter 11650: loss=41.5097, time=6338.53s\n",
      "Iter 11700: loss=41.4268, time=6365.91s\n",
      "Iter 11750: loss=41.6839, time=6393.41s\n",
      "Iter 11800: loss=41.3538, time=6420.87s\n",
      "Evaluation on 1101 elements. Correct: 503\n",
      "iter 11800: dev acc=0.4569\n",
      "Iter 11850: loss=41.2920, time=6448.10s\n",
      "Iter 11900: loss=41.3486, time=6475.93s\n",
      "Iter 11950: loss=41.2270, time=6503.71s\n",
      "Iter 12000: loss=41.5853, time=6529.65s\n",
      "Evaluation on 1101 elements. Correct: 503\n",
      "iter 12000: dev acc=0.4569\n",
      "Iter 12050: loss=40.9957, time=6558.29s\n",
      "Iter 12100: loss=41.5375, time=6586.29s\n",
      "Iter 12150: loss=41.4325, time=6613.96s\n",
      "Iter 12200: loss=41.2617, time=6639.45s\n",
      "Evaluation on 1101 elements. Correct: 513\n",
      "iter 12200: dev acc=0.4659\n",
      "Iter 12250: loss=41.1297, time=6668.08s\n",
      "Iter 12300: loss=41.4636, time=6695.86s\n",
      "Iter 12350: loss=41.0745, time=6721.80s\n",
      "Iter 12400: loss=41.2044, time=6749.16s\n",
      "Evaluation on 1101 elements. Correct: 508\n",
      "iter 12400: dev acc=0.4614\n",
      "Iter 12450: loss=41.2530, time=6778.22s\n",
      "Iter 12500: loss=41.1632, time=6805.48s\n",
      "Iter 12550: loss=41.1512, time=6831.10s\n",
      "Iter 12600: loss=40.8373, time=6858.32s\n",
      "Evaluation on 1101 elements. Correct: 501\n",
      "iter 12600: dev acc=0.4550\n",
      "Iter 12650: loss=41.0408, time=6886.42s\n",
      "Iter 12700: loss=40.8437, time=6913.72s\n",
      "Iter 12750: loss=40.9189, time=6939.44s\n",
      "Iter 12800: loss=40.8549, time=6966.35s\n",
      "Evaluation on 1101 elements. Correct: 508\n",
      "iter 12800: dev acc=0.4614\n",
      "Iter 12850: loss=40.9468, time=6994.51s\n",
      "Iter 12900: loss=40.6641, time=7020.26s\n",
      "Iter 12950: loss=41.0961, time=7048.20s\n",
      "Iter 13000: loss=40.6490, time=7076.15s\n",
      "Evaluation on 1101 elements. Correct: 509\n",
      "iter 13000: dev acc=0.4623\n",
      "Iter 13050: loss=40.6425, time=7104.49s\n",
      "Iter 13100: loss=40.6337, time=7130.45s\n",
      "Iter 13150: loss=40.9491, time=7158.08s\n",
      "Iter 13200: loss=40.5297, time=7185.11s\n",
      "Evaluation on 1101 elements. Correct: 509\n",
      "iter 13200: dev acc=0.4623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 13250: loss=40.4971, time=7213.82s\n",
      "Iter 13300: loss=40.7145, time=7240.05s\n",
      "Iter 13350: loss=40.7944, time=7268.18s\n",
      "Iter 13400: loss=40.5662, time=7295.83s\n",
      "Evaluation on 1101 elements. Correct: 499\n",
      "iter 13400: dev acc=0.4532\n",
      "Iter 13450: loss=40.7434, time=7324.35s\n",
      "Iter 13500: loss=40.2840, time=7350.03s\n",
      "Iter 13550: loss=40.2890, time=7377.19s\n",
      "Iter 13600: loss=40.4288, time=7404.93s\n",
      "Evaluation on 1101 elements. Correct: 497\n",
      "iter 13600: dev acc=0.4514\n",
      "Iter 13650: loss=40.4834, time=7431.32s\n",
      "Iter 13700: loss=40.4986, time=7459.09s\n",
      "Iter 13750: loss=40.5008, time=7486.79s\n",
      "Iter 13800: loss=40.3087, time=7514.51s\n",
      "Evaluation on 1101 elements. Correct: 503\n",
      "iter 13800: dev acc=0.4569\n",
      "Iter 13850: loss=40.3102, time=7541.89s\n",
      "Iter 13900: loss=40.2126, time=7569.08s\n",
      "Iter 13950: loss=40.2467, time=7596.47s\n",
      "Iter 14000: loss=40.3067, time=7622.92s\n",
      "Evaluation on 1101 elements. Correct: 509\n",
      "iter 14000: dev acc=0.4623\n",
      "Iter 14050: loss=40.2404, time=7651.42s\n",
      "Iter 14100: loss=39.9157, time=7679.23s\n",
      "Iter 14150: loss=40.0930, time=7706.92s\n",
      "Iter 14200: loss=40.5176, time=7732.44s\n",
      "Evaluation on 1101 elements. Correct: 502\n",
      "iter 14200: dev acc=0.4559\n",
      "Iter 14250: loss=40.3210, time=7759.81s\n",
      "Iter 14300: loss=40.2352, time=7786.36s\n",
      "Iter 14350: loss=39.7443, time=7812.75s\n",
      "Iter 14400: loss=40.2491, time=7837.94s\n",
      "Evaluation on 1101 elements. Correct: 498\n",
      "iter 14400: dev acc=0.4523\n",
      "Iter 14450: loss=39.8132, time=7865.44s\n",
      "Iter 14500: loss=40.2026, time=7892.07s\n",
      "Iter 14550: loss=39.8012, time=7918.69s\n",
      "Iter 14600: loss=39.8854, time=7944.15s\n",
      "Evaluation on 1101 elements. Correct: 502\n",
      "iter 14600: dev acc=0.4559\n",
      "Iter 14650: loss=39.6668, time=7971.66s\n",
      "Iter 14700: loss=40.0443, time=7998.66s\n",
      "Iter 14750: loss=39.9478, time=8026.09s\n",
      "Iter 14800: loss=39.7692, time=8052.10s\n",
      "Evaluation on 1101 elements. Correct: 497\n",
      "iter 14800: dev acc=0.4514\n",
      "Iter 14850: loss=39.5169, time=8080.74s\n",
      "Iter 14900: loss=40.0091, time=8108.34s\n",
      "Iter 14950: loss=39.3704, time=8135.86s\n",
      "Iter 15000: loss=39.7307, time=8161.91s\n",
      "Evaluation on 1101 elements. Correct: 499\n",
      "iter 15000: dev acc=0.4532\n",
      "Iter 15050: loss=39.5387, time=8190.49s\n",
      "Iter 15100: loss=39.5839, time=8218.27s\n",
      "Iter 15150: loss=39.9521, time=8243.98s\n",
      "Iter 15200: loss=39.2495, time=8271.73s\n",
      "Evaluation on 1101 elements. Correct: 500\n",
      "iter 15200: dev acc=0.4541\n",
      "Iter 15250: loss=39.3696, time=8300.32s\n",
      "Iter 15300: loss=39.4070, time=8328.04s\n",
      "Iter 15350: loss=40.0086, time=8354.14s\n",
      "Iter 15400: loss=39.6169, time=8381.69s\n",
      "Evaluation on 1101 elements. Correct: 510\n",
      "iter 15400: dev acc=0.4632\n",
      "Iter 15450: loss=39.3097, time=8410.70s\n",
      "Iter 15500: loss=39.2125, time=8436.94s\n",
      "Iter 15550: loss=39.8030, time=8464.66s\n",
      "Iter 15600: loss=39.3203, time=8492.75s\n",
      "Evaluation on 1101 elements. Correct: 500\n",
      "iter 15600: dev acc=0.4541\n",
      "Iter 15650: loss=39.4531, time=8520.96s\n",
      "Iter 15700: loss=39.0215, time=8546.96s\n",
      "Iter 15750: loss=39.5360, time=8574.35s\n",
      "Iter 15800: loss=39.3917, time=8602.14s\n",
      "Evaluation on 1101 elements. Correct: 511\n",
      "iter 15800: dev acc=0.4641\n",
      "Iter 15850: loss=39.1649, time=8629.33s\n",
      "Iter 15900: loss=39.0825, time=8656.63s\n",
      "Iter 15950: loss=39.0799, time=8684.01s\n",
      "Iter 16000: loss=39.1704, time=8711.33s\n",
      "Evaluation on 1101 elements. Correct: 499\n",
      "iter 16000: dev acc=0.4532\n",
      "Iter 16050: loss=39.5213, time=8738.04s\n",
      "Iter 16100: loss=38.9922, time=8765.39s\n",
      "Iter 16150: loss=38.7156, time=8792.58s\n",
      "Iter 16200: loss=39.1325, time=8818.54s\n",
      "Evaluation on 1101 elements. Correct: 504\n",
      "iter 16200: dev acc=0.4578\n",
      "Iter 16250: loss=38.7344, time=8846.48s\n",
      "Iter 16300: loss=38.8520, time=8874.18s\n",
      "Iter 16350: loss=38.9325, time=8901.77s\n",
      "Iter 16400: loss=39.2756, time=8927.77s\n",
      "Evaluation on 1101 elements. Correct: 503\n",
      "iter 16400: dev acc=0.4569\n",
      "Iter 16450: loss=38.7655, time=8956.38s\n",
      "Iter 16500: loss=38.9270, time=8983.68s\n",
      "Iter 16550: loss=39.1528, time=9009.34s\n",
      "Iter 16600: loss=38.8260, time=9036.75s\n",
      "Evaluation on 1101 elements. Correct: 500\n",
      "iter 16600: dev acc=0.4541\n",
      "Iter 16650: loss=38.5374, time=9065.08s\n",
      "Iter 16700: loss=38.8618, time=9092.68s\n",
      "Iter 16750: loss=38.8207, time=9118.55s\n",
      "Iter 16800: loss=38.7037, time=9146.15s\n",
      "Evaluation on 1101 elements. Correct: 492\n",
      "iter 16800: dev acc=0.4469\n",
      "Iter 16850: loss=38.6604, time=9174.79s\n",
      "Iter 16900: loss=39.0941, time=9202.69s\n",
      "Iter 16950: loss=38.6335, time=9228.68s\n",
      "Iter 17000: loss=38.5077, time=9256.29s\n",
      "Evaluation on 1101 elements. Correct: 508\n",
      "iter 17000: dev acc=0.4614\n",
      "Iter 17050: loss=38.4477, time=9285.01s\n",
      "Iter 17100: loss=38.6244, time=9311.23s\n",
      "Iter 17150: loss=38.8853, time=9339.19s\n",
      "Iter 17200: loss=38.3351, time=9367.01s\n",
      "Evaluation on 1101 elements. Correct: 509\n",
      "iter 17200: dev acc=0.4623\n",
      "Iter 17250: loss=38.8163, time=9395.12s\n",
      "Iter 17300: loss=38.6118, time=9421.37s\n",
      "Iter 17350: loss=38.3065, time=9448.14s\n",
      "Iter 17400: loss=38.7322, time=9476.24s\n",
      "Evaluation on 1101 elements. Correct: 507\n",
      "iter 17400: dev acc=0.4605\n",
      "Iter 17450: loss=38.1884, time=9503.25s\n",
      "Iter 17500: loss=38.3486, time=9531.16s\n",
      "Iter 17550: loss=38.1945, time=9558.81s\n",
      "Iter 17600: loss=37.9843, time=9586.44s\n",
      "Evaluation on 1101 elements. Correct: 498\n",
      "iter 17600: dev acc=0.4523\n",
      "Iter 17650: loss=38.6057, time=9613.60s\n",
      "Iter 17700: loss=38.2645, time=9641.19s\n",
      "Iter 17750: loss=38.0515, time=9668.86s\n",
      "Iter 17800: loss=38.3211, time=9696.62s\n",
      "Evaluation on 1101 elements. Correct: 496\n",
      "iter 17800: dev acc=0.4505\n",
      "Iter 17850: loss=38.2704, time=9723.35s\n",
      "Iter 17900: loss=37.8715, time=9750.50s\n",
      "Iter 17950: loss=38.2746, time=9777.69s\n",
      "Iter 18000: loss=38.4579, time=9802.96s\n",
      "Evaluation on 1101 elements. Correct: 495\n",
      "iter 18000: dev acc=0.4496\n",
      "Iter 18050: loss=38.0972, time=9831.85s\n",
      "Iter 18100: loss=38.0649, time=9859.85s\n",
      "Iter 18150: loss=38.0561, time=9887.55s\n",
      "Iter 18200: loss=38.2519, time=9913.60s\n",
      "Evaluation on 1101 elements. Correct: 490\n",
      "iter 18200: dev acc=0.4450\n",
      "Iter 18250: loss=37.9654, time=9941.91s\n",
      "Iter 18300: loss=38.2022, time=9969.43s\n",
      "Iter 18350: loss=37.8580, time=9996.75s\n",
      "Iter 18400: loss=37.9107, time=10022.14s\n",
      "Evaluation on 1101 elements. Correct: 494\n",
      "iter 18400: dev acc=0.4487\n",
      "Iter 18450: loss=37.7383, time=10050.87s\n",
      "Iter 18500: loss=38.1240, time=10078.79s\n",
      "Iter 18550: loss=37.6566, time=10104.95s\n",
      "Iter 18600: loss=38.0531, time=10132.73s\n",
      "Evaluation on 1101 elements. Correct: 495\n",
      "iter 18600: dev acc=0.4496\n",
      "Iter 18650: loss=38.0016, time=10161.80s\n",
      "Iter 18700: loss=37.8037, time=10188.71s\n",
      "Iter 18750: loss=37.8423, time=10214.88s\n",
      "Iter 18800: loss=37.6803, time=10242.59s\n",
      "Evaluation on 1101 elements. Correct: 498\n",
      "iter 18800: dev acc=0.4523\n",
      "Iter 18850: loss=37.8059, time=10271.47s\n",
      "Iter 18900: loss=37.8096, time=10299.18s\n",
      "Iter 18950: loss=37.5510, time=10325.25s\n",
      "Iter 19000: loss=37.9069, time=10352.95s\n",
      "Evaluation on 1101 elements. Correct: 496\n",
      "iter 19000: dev acc=0.4505\n",
      "Iter 19050: loss=37.6731, time=10381.66s\n",
      "Iter 19100: loss=37.2622, time=10407.82s\n",
      "Iter 19150: loss=37.4491, time=10435.57s\n",
      "Iter 19200: loss=37.5905, time=10462.98s\n",
      "Evaluation on 1101 elements. Correct: 487\n",
      "iter 19200: dev acc=0.4423\n",
      "Iter 19250: loss=38.0030, time=10490.99s\n",
      "Iter 19300: loss=37.5934, time=10516.62s\n",
      "Iter 19350: loss=37.5499, time=10543.93s\n",
      "Iter 19400: loss=37.5250, time=10570.99s\n",
      "Evaluation on 1101 elements. Correct: 493\n",
      "iter 19400: dev acc=0.4478\n",
      "Iter 19450: loss=38.0006, time=10597.48s\n",
      "Iter 19500: loss=37.6029, time=10621.99s\n",
      "Iter 19550: loss=37.3148, time=10647.89s\n",
      "Iter 19600: loss=37.5852, time=10673.62s\n",
      "Evaluation on 1101 elements. Correct: 489\n",
      "iter 19600: dev acc=0.4441\n",
      "Iter 19650: loss=37.2152, time=10699.25s\n",
      "Iter 19700: loss=37.0351, time=10725.47s\n",
      "Iter 19750: loss=37.3473, time=10751.37s\n",
      "Iter 19800: loss=37.3789, time=10777.57s\n",
      "Evaluation on 1101 elements. Correct: 490\n",
      "iter 19800: dev acc=0.4450\n",
      "Iter 19850: loss=37.1877, time=10802.78s\n",
      "Iter 19900: loss=37.2511, time=10829.03s\n",
      "Iter 19950: loss=37.2483, time=10855.43s\n",
      "Iter 20000: loss=37.2281, time=10881.27s\n",
      "Evaluation on 1101 elements. Correct: 483\n",
      "iter 20000: dev acc=0.4387\n",
      "Iter 20050: loss=37.2088, time=10909.90s\n",
      "Iter 20100: loss=37.2729, time=10937.75s\n",
      "Iter 20150: loss=36.9239, time=10965.19s\n",
      "Iter 20200: loss=37.0835, time=10991.61s\n",
      "Evaluation on 1101 elements. Correct: 485\n",
      "iter 20200: dev acc=0.4405\n",
      "Iter 20250: loss=37.4504, time=11019.99s\n",
      "Iter 20300: loss=37.3225, time=11048.15s\n",
      "Iter 20350: loss=36.8870, time=11074.04s\n",
      "Iter 20400: loss=37.4514, time=11101.16s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on 1101 elements. Correct: 479\n",
      "iter 20400: dev acc=0.4351\n",
      "Iter 20450: loss=37.0577, time=11129.41s\n",
      "Iter 20500: loss=36.9153, time=11156.82s\n",
      "Iter 20550: loss=37.0448, time=11182.51s\n",
      "Iter 20600: loss=36.8913, time=11210.19s\n",
      "Evaluation on 1101 elements. Correct: 483\n",
      "iter 20600: dev acc=0.4387\n",
      "Iter 20650: loss=37.1535, time=11238.12s\n",
      "Iter 20700: loss=36.7354, time=11264.14s\n",
      "Iter 20750: loss=36.8875, time=11291.99s\n",
      "Iter 20800: loss=36.8950, time=11319.83s\n",
      "Evaluation on 1101 elements. Correct: 479\n",
      "iter 20800: dev acc=0.4351\n",
      "Iter 20850: loss=36.6708, time=11348.55s\n",
      "Iter 20900: loss=36.8337, time=11374.63s\n",
      "Iter 20950: loss=36.8752, time=11402.32s\n",
      "Iter 21000: loss=36.9965, time=11429.76s\n",
      "Evaluation on 1101 elements. Correct: 486\n",
      "iter 21000: dev acc=0.4414\n",
      "Iter 21050: loss=36.9337, time=11456.59s\n",
      "Iter 21100: loss=36.6517, time=11484.43s\n",
      "Iter 21150: loss=36.6330, time=11511.53s\n",
      "Iter 21200: loss=36.9531, time=11537.70s\n",
      "Evaluation on 1101 elements. Correct: 480\n",
      "iter 21200: dev acc=0.4360\n",
      "Iter 21250: loss=36.6628, time=11565.98s\n",
      "Iter 21300: loss=36.5514, time=11593.28s\n",
      "Iter 21350: loss=36.6322, time=11620.42s\n",
      "Iter 21400: loss=36.3529, time=11646.33s\n",
      "Evaluation on 1101 elements. Correct: 484\n",
      "iter 21400: dev acc=0.4396\n",
      "Iter 21450: loss=36.9777, time=11675.07s\n",
      "Iter 21500: loss=36.3995, time=11702.33s\n",
      "Iter 21550: loss=36.6708, time=11728.30s\n",
      "Iter 21600: loss=36.3196, time=11755.49s\n",
      "Evaluation on 1101 elements. Correct: 482\n",
      "iter 21600: dev acc=0.4378\n",
      "Iter 21650: loss=36.8405, time=11784.04s\n",
      "Iter 21700: loss=36.6495, time=11811.33s\n",
      "Iter 21750: loss=36.2081, time=11837.14s\n",
      "Iter 21800: loss=36.6596, time=11864.33s\n",
      "Evaluation on 1101 elements. Correct: 480\n",
      "iter 21800: dev acc=0.4360\n",
      "Iter 21850: loss=36.5506, time=11892.65s\n",
      "Iter 21900: loss=36.5696, time=11918.69s\n",
      "Iter 21950: loss=36.1577, time=11946.47s\n",
      "Iter 22000: loss=36.4338, time=11974.39s\n",
      "Evaluation on 1101 elements. Correct: 486\n",
      "iter 22000: dev acc=0.4414\n",
      "Iter 22050: loss=36.6324, time=12003.48s\n",
      "Iter 22100: loss=35.8653, time=12029.99s\n",
      "Iter 22150: loss=36.3923, time=12057.21s\n",
      "Iter 22200: loss=36.1602, time=12084.95s\n",
      "Evaluation on 1101 elements. Correct: 480\n",
      "iter 22200: dev acc=0.4360\n",
      "Iter 22250: loss=36.2770, time=12113.84s\n",
      "Iter 22300: loss=36.0129, time=12140.23s\n",
      "Iter 22350: loss=36.0432, time=12167.87s\n",
      "Iter 22400: loss=36.3231, time=12195.64s\n",
      "Evaluation on 1101 elements. Correct: 477\n",
      "iter 22400: dev acc=0.4332\n",
      "Iter 22450: loss=36.2585, time=12224.55s\n",
      "Iter 22500: loss=36.3597, time=12251.04s\n",
      "Iter 22550: loss=36.0587, time=12278.41s\n",
      "Iter 22600: loss=35.7977, time=12306.14s\n",
      "Evaluation on 1101 elements. Correct: 475\n",
      "iter 22600: dev acc=0.4314\n",
      "Iter 22650: loss=36.3212, time=12333.15s\n",
      "Iter 22700: loss=36.0129, time=12360.88s\n",
      "Iter 22750: loss=35.7295, time=12388.05s\n",
      "Iter 22800: loss=36.2106, time=12413.31s\n",
      "Evaluation on 1101 elements. Correct: 480\n",
      "iter 22800: dev acc=0.4360\n",
      "Iter 22850: loss=36.2869, time=12440.91s\n",
      "Iter 22900: loss=36.0386, time=12467.35s\n",
      "Iter 22950: loss=35.8672, time=12493.93s\n",
      "Iter 23000: loss=36.1962, time=12519.20s\n",
      "Evaluation on 1101 elements. Correct: 476\n",
      "iter 23000: dev acc=0.4323\n",
      "Iter 23050: loss=36.2958, time=12547.90s\n",
      "Iter 23100: loss=35.6083, time=12575.49s\n",
      "Iter 23150: loss=35.9702, time=12602.98s\n",
      "Iter 23200: loss=35.9127, time=12629.14s\n",
      "Evaluation on 1101 elements. Correct: 484\n",
      "iter 23200: dev acc=0.4396\n",
      "Iter 23250: loss=35.9342, time=12657.78s\n",
      "Iter 23300: loss=35.6804, time=12685.56s\n",
      "Iter 23350: loss=35.8005, time=12712.00s\n",
      "Iter 23400: loss=35.8337, time=12739.57s\n",
      "Evaluation on 1101 elements. Correct: 480\n",
      "iter 23400: dev acc=0.4360\n",
      "Iter 23450: loss=35.9216, time=12768.15s\n",
      "Iter 23500: loss=35.9592, time=12795.57s\n",
      "Iter 23550: loss=35.9968, time=12821.70s\n",
      "Iter 23600: loss=35.7634, time=12848.78s\n",
      "Evaluation on 1101 elements. Correct: 479\n",
      "iter 23600: dev acc=0.4351\n",
      "Iter 23650: loss=35.5877, time=12877.57s\n",
      "Iter 23700: loss=35.7164, time=12905.15s\n",
      "Iter 23750: loss=35.7556, time=12931.38s\n",
      "Iter 23800: loss=35.6509, time=12958.94s\n",
      "Evaluation on 1101 elements. Correct: 479\n",
      "iter 23800: dev acc=0.4351\n",
      "Iter 23850: loss=35.5322, time=12987.71s\n",
      "Iter 23900: loss=35.2603, time=13013.93s\n",
      "Iter 23950: loss=35.7158, time=13041.47s\n",
      "Iter 24000: loss=35.8355, time=13068.96s\n",
      "Evaluation on 1101 elements. Correct: 480\n",
      "iter 24000: dev acc=0.4360\n",
      "Iter 24050: loss=35.4787, time=13097.47s\n",
      "Iter 24100: loss=35.3503, time=13123.88s\n",
      "Iter 24150: loss=35.4750, time=13151.35s\n",
      "Iter 24200: loss=35.4932, time=13178.91s\n",
      "Evaluation on 1101 elements. Correct: 475\n",
      "iter 24200: dev acc=0.4314\n",
      "Iter 24250: loss=35.3891, time=13207.58s\n",
      "Iter 24300: loss=35.4579, time=13233.64s\n",
      "Iter 24350: loss=35.4769, time=13261.23s\n",
      "Iter 24400: loss=35.1874, time=13288.33s\n",
      "Evaluation on 1101 elements. Correct: 472\n",
      "iter 24400: dev acc=0.4287\n",
      "Iter 24450: loss=35.5990, time=13314.39s\n",
      "Iter 24500: loss=35.2928, time=13341.09s\n",
      "Iter 24550: loss=35.7865, time=13368.92s\n",
      "Iter 24600: loss=35.0342, time=13396.33s\n",
      "Evaluation on 1101 elements. Correct: 481\n",
      "iter 24600: dev acc=0.4369\n",
      "Iter 24650: loss=35.3605, time=13423.59s\n",
      "Iter 24700: loss=35.1817, time=13451.30s\n",
      "Iter 24750: loss=35.2860, time=13478.98s\n",
      "Iter 24800: loss=35.4424, time=13505.45s\n",
      "Evaluation on 1101 elements. Correct: 476\n",
      "iter 24800: dev acc=0.4323\n",
      "Iter 24850: loss=35.0624, time=13534.30s\n",
      "Iter 24900: loss=35.5282, time=13562.43s\n",
      "Iter 24950: loss=34.9849, time=13588.28s\n",
      "Iter 25000: loss=35.3185, time=13615.57s\n",
      "Evaluation on 1101 elements. Correct: 471\n",
      "iter 25000: dev acc=0.4278\n",
      "Iter 25050: loss=34.9546, time=13643.48s\n",
      "Iter 25100: loss=35.3768, time=13669.85s\n",
      "Iter 25150: loss=34.9766, time=13695.23s\n",
      "Iter 25200: loss=35.3653, time=13721.77s\n",
      "Evaluation on 1101 elements. Correct: 477\n",
      "iter 25200: dev acc=0.4332\n",
      "Iter 25250: loss=35.1188, time=13749.28s\n",
      "Iter 25300: loss=35.1335, time=13775.79s\n",
      "Iter 25350: loss=35.1956, time=13800.94s\n",
      "Iter 25400: loss=35.2493, time=13827.60s\n",
      "Evaluation on 1101 elements. Correct: 474\n",
      "iter 25400: dev acc=0.4305\n",
      "Iter 25450: loss=35.0553, time=13855.19s\n",
      "Iter 25500: loss=34.9485, time=13881.81s\n",
      "Iter 25550: loss=35.2769, time=13906.99s\n",
      "Iter 25600: loss=34.8651, time=13933.32s\n",
      "Evaluation on 1101 elements. Correct: 481\n",
      "iter 25600: dev acc=0.4369\n",
      "Iter 25650: loss=34.6716, time=13961.00s\n",
      "Iter 25700: loss=35.0839, time=13986.01s\n",
      "Iter 25750: loss=34.8927, time=14012.57s\n",
      "Iter 25800: loss=34.9197, time=14039.16s\n",
      "Evaluation on 1101 elements. Correct: 486\n",
      "iter 25800: dev acc=0.4414\n",
      "Iter 25850: loss=35.0706, time=14066.99s\n",
      "Iter 25900: loss=34.9774, time=14092.04s\n",
      "Iter 25950: loss=34.9659, time=14118.67s\n",
      "Iter 26000: loss=35.0985, time=14145.28s\n",
      "Evaluation on 1101 elements. Correct: 470\n",
      "iter 26000: dev acc=0.4269\n",
      "Iter 26050: loss=34.7291, time=14171.23s\n",
      "Iter 26100: loss=34.8475, time=14197.60s\n",
      "Iter 26150: loss=34.3508, time=14225.51s\n",
      "Iter 26200: loss=34.6477, time=14251.85s\n",
      "Evaluation on 1101 elements. Correct: 477\n",
      "iter 26200: dev acc=0.4332\n",
      "Iter 26250: loss=34.9058, time=14280.49s\n",
      "Iter 26300: loss=34.5270, time=14308.09s\n",
      "Iter 26350: loss=34.7496, time=14335.42s\n",
      "Iter 26400: loss=34.8196, time=14361.09s\n",
      "Evaluation on 1101 elements. Correct: 471\n",
      "iter 26400: dev acc=0.4278\n",
      "Iter 26450: loss=34.7205, time=14389.55s\n",
      "Iter 26500: loss=34.7583, time=14416.93s\n",
      "Iter 26550: loss=34.7066, time=14444.48s\n",
      "Iter 26600: loss=34.4488, time=14469.97s\n",
      "Evaluation on 1101 elements. Correct: 471\n",
      "iter 26600: dev acc=0.4278\n",
      "Iter 26650: loss=34.2991, time=14498.32s\n",
      "Iter 26700: loss=34.4983, time=14525.93s\n",
      "Iter 26750: loss=34.6607, time=14551.50s\n",
      "Iter 26800: loss=34.6692, time=14579.07s\n",
      "Evaluation on 1101 elements. Correct: 459\n",
      "iter 26800: dev acc=0.4169\n",
      "Iter 26850: loss=34.5556, time=14608.07s\n",
      "Iter 26900: loss=34.9941, time=14636.01s\n",
      "Iter 26950: loss=34.3299, time=14662.17s\n",
      "Iter 27000: loss=34.2458, time=14690.05s\n",
      "Evaluation on 1101 elements. Correct: 469\n",
      "iter 27000: dev acc=0.4260\n",
      "Iter 27050: loss=34.3411, time=14718.72s\n",
      "Iter 27100: loss=34.3926, time=14746.63s\n",
      "Iter 27150: loss=34.4959, time=14773.17s\n",
      "Iter 27200: loss=34.2996, time=14802.57s\n",
      "Evaluation on 1101 elements. Correct: 476\n",
      "iter 27200: dev acc=0.4323\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-177-0544e68a2fcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mDEBUG_TREE_PRINT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mresults_small_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-177-0544e68a2fcf>\u001b[0m in \u001b[0;36mdo_train\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0meval_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mbatch_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_minibatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         batch_size=128, eval_batch_size=64)\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mDEBUG_TREE_PRINT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-168-5e0f8f1b7968>\u001b[0m in \u001b[0;36mtrain_tree_model\u001b[0;34m(model, optimizer, num_iterations, print_every, eval_every, batch_fn, prep_fn, eval_fn, batch_size, eval_batch_size)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;31m# update weights - take a small step in the opposite dir of the gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for _, depth_dict in all_subtrees.items():\n",
    "    for subtree in depth_dict:\n",
    "        subtree.loss.clear()\n",
    "\n",
    "tree_model = TreeLSTMClassifier(\n",
    "    len(v.w2i), 300, 150, len(t2i), v)\n",
    "\n",
    "with torch.no_grad():\n",
    "    tree_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
    "    tree_model.embed.weight.requires_grad = False\n",
    "  \n",
    "def do_train(model):\n",
    "\n",
    "    print(model)\n",
    "    print_parameters(model)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=4e-5)\n",
    "\n",
    "    return train_tree_model(\n",
    "        model, optimizer, num_iterations=30000, \n",
    "        print_every=50, eval_every=200,\n",
    "        prep_fn=prepare_treelstm_minibatch,\n",
    "        eval_fn=evaluate,\n",
    "        batch_fn=get_minibatch,\n",
    "        batch_size=128, eval_batch_size=64)\n",
    "\n",
    "DEBUG_TREE_PRINT = False\n",
    "results_small_lr = do_train(tree_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TreeLSTMClassifier(\n",
      "  (embed): Embedding(20727, 300, padding_idx=1)\n",
      "  (treelstm): TreeLSTM(\n",
      "    (reduce): TreeLSTMCell(300, 150)\n",
      "    (proj_x): Linear(in_features=300, out_features=150, bias=True)\n",
      "    (proj_x_gate): Linear(in_features=300, out_features=150, bias=True)\n",
      "    (buffers_dropout): Dropout(p=0.5)\n",
      "  )\n",
      "  (output_layer): Sequential(\n",
      "    (0): Dropout(p=0.25)\n",
      "    (1): Linear(in_features=150, out_features=5, bias=True)\n",
      "    (2): LogSoftmax()\n",
      "  )\n",
      ")\n",
      "embed.weight             [20727, 300] requires_grad=False\n",
      "treelstm.reduce.attent_module.output_layer.fc1.weight [150, 300]   requires_grad=True\n",
      "treelstm.reduce.attent_module.output_layer.fc1.bias [150]        requires_grad=True\n",
      "treelstm.reduce.attent_module.output_layer.fc2.weight [2, 150]     requires_grad=True\n",
      "treelstm.reduce.attent_module.output_layer.fc2.bias [2]          requires_grad=True\n",
      "treelstm.reduce.reduce_layer.weight [750, 300]   requires_grad=True\n",
      "treelstm.reduce.reduce_layer.bias [750]        requires_grad=True\n",
      "treelstm.proj_x.weight   [150, 300]   requires_grad=True\n",
      "treelstm.proj_x.bias     [150]        requires_grad=True\n",
      "treelstm.proj_x_gate.weight [150, 300]   requires_grad=True\n",
      "treelstm.proj_x_gate.bias [150]        requires_grad=True\n",
      "output_layer.1.weight    [5, 150]     requires_grad=True\n",
      "output_layer.1.bias      [5]          requires_grad=True\n",
      "\n",
      "Total parameters: 6580357\n",
      "\n",
      "Iter 50: loss=79.7637, time=7.76s\n",
      "Evaluation on 1101 elements. Correct: 232\n",
      "iter 50: dev acc=0.2107\n",
      "new highscore\n",
      "Iter 100: loss=76.8928, time=16.36s\n",
      "Evaluation on 1101 elements. Correct: 229\n",
      "iter 100: dev acc=0.2080\n",
      "Iter 150: loss=73.8792, time=26.69s\n",
      "Evaluation on 1101 elements. Correct: 231\n",
      "iter 150: dev acc=0.2098\n",
      "Iter 200: loss=70.4357, time=35.36s\n",
      "Evaluation on 1101 elements. Correct: 281\n",
      "iter 200: dev acc=0.2552\n",
      "new highscore\n",
      "Iter 250: loss=66.5850, time=44.05s\n",
      "Evaluation on 1101 elements. Correct: 405\n",
      "iter 250: dev acc=0.3678\n",
      "new highscore\n",
      "Iter 300: loss=63.4106, time=52.52s\n",
      "Evaluation on 1101 elements. Correct: 416\n",
      "iter 300: dev acc=0.3778\n",
      "new highscore\n",
      "Iter 350: loss=61.7841, time=61.20s\n",
      "Evaluation on 1101 elements. Correct: 438\n",
      "iter 350: dev acc=0.3978\n",
      "new highscore\n",
      "Iter 400: loss=60.0341, time=69.81s\n",
      "Evaluation on 1101 elements. Correct: 414\n",
      "iter 400: dev acc=0.3760\n",
      "Iter 450: loss=58.9327, time=78.50s\n",
      "Evaluation on 1101 elements. Correct: 429\n",
      "iter 450: dev acc=0.3896\n",
      "Iter 500: loss=58.0995, time=87.12s\n",
      "Evaluation on 1101 elements. Correct: 440\n",
      "iter 500: dev acc=0.3996\n",
      "new highscore\n",
      "Iter 550: loss=57.2638, time=95.71s\n",
      "Evaluation on 1101 elements. Correct: 440\n",
      "iter 550: dev acc=0.3996\n",
      "Iter 600: loss=56.1238, time=104.51s\n",
      "Evaluation on 1101 elements. Correct: 436\n",
      "iter 600: dev acc=0.3960\n",
      "Iter 650: loss=56.5310, time=114.50s\n",
      "Evaluation on 1101 elements. Correct: 438\n",
      "iter 650: dev acc=0.3978\n",
      "Iter 700: loss=55.3733, time=123.03s\n",
      "Evaluation on 1101 elements. Correct: 444\n",
      "iter 700: dev acc=0.4033\n",
      "new highscore\n",
      "Iter 750: loss=54.7451, time=131.53s\n",
      "Evaluation on 1101 elements. Correct: 442\n",
      "iter 750: dev acc=0.4015\n",
      "Iter 800: loss=55.0579, time=140.17s\n",
      "Evaluation on 1101 elements. Correct: 455\n",
      "iter 800: dev acc=0.4133\n",
      "new highscore\n",
      "Iter 850: loss=53.8177, time=149.10s\n",
      "Evaluation on 1101 elements. Correct: 450\n",
      "iter 850: dev acc=0.4087\n",
      "Iter 900: loss=53.8527, time=157.65s\n",
      "Evaluation on 1101 elements. Correct: 453\n",
      "iter 900: dev acc=0.4114\n",
      "Iter 950: loss=53.2500, time=166.51s\n",
      "Evaluation on 1101 elements. Correct: 465\n",
      "iter 950: dev acc=0.4223\n",
      "new highscore\n",
      "Iter 1000: loss=52.6644, time=175.10s\n",
      "Evaluation on 1101 elements. Correct: 464\n",
      "iter 1000: dev acc=0.4214\n",
      "Iter 1050: loss=53.7081, time=183.19s\n",
      "Evaluation on 1101 elements. Correct: 461\n",
      "iter 1050: dev acc=0.4187\n",
      "Iter 1100: loss=51.9880, time=191.45s\n",
      "Evaluation on 1101 elements. Correct: 470\n",
      "iter 1100: dev acc=0.4269\n",
      "new highscore\n",
      "Iter 1150: loss=52.5602, time=201.83s\n",
      "Evaluation on 1101 elements. Correct: 465\n",
      "iter 1150: dev acc=0.4223\n",
      "Iter 1200: loss=51.9190, time=209.46s\n",
      "Evaluation on 1101 elements. Correct: 466\n",
      "iter 1200: dev acc=0.4233\n",
      "Iter 1250: loss=51.9949, time=216.92s\n",
      "Evaluation on 1101 elements. Correct: 458\n",
      "iter 1250: dev acc=0.4160\n",
      "Iter 1300: loss=52.3068, time=224.58s\n",
      "Evaluation on 1101 elements. Correct: 464\n",
      "iter 1300: dev acc=0.4214\n",
      "Iter 1350: loss=51.4545, time=232.19s\n",
      "Evaluation on 1101 elements. Correct: 473\n",
      "iter 1350: dev acc=0.4296\n",
      "new highscore\n",
      "Iter 1400: loss=50.8213, time=239.79s\n",
      "Evaluation on 1101 elements. Correct: 470\n",
      "iter 1400: dev acc=0.4269\n",
      "Iter 1450: loss=50.7964, time=248.12s\n",
      "Evaluation on 1101 elements. Correct: 489\n",
      "iter 1450: dev acc=0.4441\n",
      "new highscore\n",
      "Iter 1500: loss=50.7589, time=256.34s\n",
      "Evaluation on 1101 elements. Correct: 478\n",
      "iter 1500: dev acc=0.4342\n",
      "Iter 1550: loss=51.3672, time=264.79s\n",
      "Evaluation on 1101 elements. Correct: 480\n",
      "iter 1550: dev acc=0.4360\n",
      "Iter 1600: loss=51.0006, time=273.11s\n",
      "Evaluation on 1101 elements. Correct: 471\n",
      "iter 1600: dev acc=0.4278\n",
      "Iter 1650: loss=50.2244, time=282.77s\n",
      "Evaluation on 1101 elements. Correct: 478\n",
      "iter 1650: dev acc=0.4342\n",
      "Iter 1700: loss=49.9848, time=290.73s\n",
      "Evaluation on 1101 elements. Correct: 477\n",
      "iter 1700: dev acc=0.4332\n",
      "Iter 1750: loss=49.7846, time=299.12s\n",
      "Evaluation on 1101 elements. Correct: 480\n",
      "iter 1750: dev acc=0.4360\n",
      "Iter 1800: loss=49.5447, time=307.35s\n",
      "Evaluation on 1101 elements. Correct: 468\n",
      "iter 1800: dev acc=0.4251\n",
      "Iter 1850: loss=50.2791, time=316.02s\n",
      "Evaluation on 1101 elements. Correct: 469\n",
      "iter 1850: dev acc=0.4260\n",
      "Iter 1900: loss=49.8124, time=324.48s\n",
      "Evaluation on 1101 elements. Correct: 495\n",
      "iter 1900: dev acc=0.4496\n",
      "new highscore\n",
      "Iter 1950: loss=49.7771, time=332.94s\n",
      "Evaluation on 1101 elements. Correct: 480\n",
      "iter 1950: dev acc=0.4360\n",
      "Iter 2000: loss=49.4328, time=341.40s\n",
      "Evaluation on 1101 elements. Correct: 484\n",
      "iter 2000: dev acc=0.4396\n",
      "Iter 2050: loss=49.5508, time=349.83s\n",
      "Evaluation on 1101 elements. Correct: 488\n",
      "iter 2050: dev acc=0.4432\n",
      "Iter 2100: loss=48.6331, time=358.56s\n",
      "Evaluation on 1101 elements. Correct: 500\n",
      "iter 2100: dev acc=0.4541\n",
      "new highscore\n",
      "Iter 2150: loss=48.9567, time=368.50s\n",
      "Evaluation on 1101 elements. Correct: 491\n",
      "iter 2150: dev acc=0.4460\n",
      "Iter 2200: loss=48.5558, time=377.37s\n",
      "Evaluation on 1101 elements. Correct: 496\n",
      "iter 2200: dev acc=0.4505\n",
      "Iter 2250: loss=49.0873, time=386.03s\n",
      "Evaluation on 1101 elements. Correct: 488\n",
      "iter 2250: dev acc=0.4432\n",
      "Iter 2300: loss=48.8982, time=394.40s\n",
      "Evaluation on 1101 elements. Correct: 488\n",
      "iter 2300: dev acc=0.4432\n",
      "Iter 2350: loss=48.4784, time=402.92s\n",
      "Evaluation on 1101 elements. Correct: 495\n",
      "iter 2350: dev acc=0.4496\n",
      "Iter 2400: loss=48.0081, time=411.60s\n",
      "Evaluation on 1101 elements. Correct: 500\n",
      "iter 2400: dev acc=0.4541\n",
      "Iter 2450: loss=48.7171, time=420.35s\n",
      "Evaluation on 1101 elements. Correct: 498\n",
      "iter 2450: dev acc=0.4523\n",
      "Iter 2500: loss=47.9053, time=428.74s\n",
      "Evaluation on 1101 elements. Correct: 495\n",
      "iter 2500: dev acc=0.4496\n",
      "Iter 2550: loss=47.6376, time=436.91s\n",
      "Evaluation on 1101 elements. Correct: 501\n",
      "iter 2550: dev acc=0.4550\n",
      "new highscore\n",
      "Iter 2600: loss=48.4795, time=445.25s\n",
      "Evaluation on 1101 elements. Correct: 504\n",
      "iter 2600: dev acc=0.4578\n",
      "new highscore\n",
      "Iter 2650: loss=48.1982, time=455.09s\n",
      "Evaluation on 1101 elements. Correct: 507\n",
      "iter 2650: dev acc=0.4605\n",
      "new highscore\n",
      "Iter 2700: loss=47.5351, time=463.39s\n",
      "Evaluation on 1101 elements. Correct: 501\n",
      "iter 2700: dev acc=0.4550\n",
      "Iter 2750: loss=47.0491, time=471.53s\n",
      "Evaluation on 1101 elements. Correct: 505\n",
      "iter 2750: dev acc=0.4587\n",
      "Iter 2800: loss=48.3335, time=479.74s\n",
      "Evaluation on 1101 elements. Correct: 507\n",
      "iter 2800: dev acc=0.4605\n",
      "Iter 2850: loss=47.6092, time=488.10s\n",
      "Evaluation on 1101 elements. Correct: 499\n",
      "iter 2850: dev acc=0.4532\n",
      "Iter 2900: loss=47.6289, time=496.54s\n",
      "Evaluation on 1101 elements. Correct: 498\n",
      "iter 2900: dev acc=0.4523\n",
      "Iter 2950: loss=47.4134, time=504.96s\n",
      "Evaluation on 1101 elements. Correct: 511\n",
      "iter 2950: dev acc=0.4641\n",
      "new highscore\n",
      "Iter 3000: loss=46.9265, time=513.48s\n",
      "Evaluation on 1101 elements. Correct: 506\n",
      "iter 3000: dev acc=0.4596\n",
      "Iter 3050: loss=47.4651, time=522.07s\n",
      "Evaluation on 1101 elements. Correct: 505\n",
      "iter 3050: dev acc=0.4587\n",
      "Iter 3100: loss=47.0984, time=530.70s\n",
      "Evaluation on 1101 elements. Correct: 506\n",
      "iter 3100: dev acc=0.4596\n",
      "Iter 3150: loss=46.9692, time=540.93s\n",
      "Evaluation on 1101 elements. Correct: 519\n",
      "iter 3150: dev acc=0.4714\n",
      "new highscore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 3200: loss=48.0883, time=549.39s\n",
      "Evaluation on 1101 elements. Correct: 505\n",
      "iter 3200: dev acc=0.4587\n",
      "Iter 3250: loss=47.3595, time=557.90s\n",
      "Evaluation on 1101 elements. Correct: 519\n",
      "iter 3250: dev acc=0.4714\n",
      "Iter 3300: loss=46.9559, time=566.41s\n",
      "Evaluation on 1101 elements. Correct: 510\n",
      "iter 3300: dev acc=0.4632\n",
      "Iter 3350: loss=46.2107, time=574.91s\n",
      "Evaluation on 1101 elements. Correct: 503\n",
      "iter 3350: dev acc=0.4569\n",
      "Iter 3400: loss=47.3569, time=583.16s\n",
      "Evaluation on 1101 elements. Correct: 518\n",
      "iter 3400: dev acc=0.4705\n",
      "Iter 3450: loss=46.8774, time=591.60s\n",
      "Evaluation on 1101 elements. Correct: 516\n",
      "iter 3450: dev acc=0.4687\n",
      "Iter 3500: loss=46.1747, time=600.14s\n",
      "Evaluation on 1101 elements. Correct: 526\n",
      "iter 3500: dev acc=0.4777\n",
      "new highscore\n",
      "Iter 3550: loss=46.1213, time=608.76s\n",
      "Evaluation on 1101 elements. Correct: 515\n",
      "iter 3550: dev acc=0.4678\n",
      "Iter 3600: loss=46.7490, time=617.31s\n",
      "Evaluation on 1101 elements. Correct: 517\n",
      "iter 3600: dev acc=0.4696\n",
      "Iter 3650: loss=46.4997, time=627.78s\n",
      "Evaluation on 1101 elements. Correct: 513\n",
      "iter 3650: dev acc=0.4659\n",
      "Iter 3700: loss=46.4504, time=636.71s\n",
      "Evaluation on 1101 elements. Correct: 522\n",
      "iter 3700: dev acc=0.4741\n",
      "Iter 3750: loss=46.9188, time=645.05s\n",
      "Evaluation on 1101 elements. Correct: 525\n",
      "iter 3750: dev acc=0.4768\n",
      "Iter 3800: loss=46.3072, time=653.65s\n",
      "Evaluation on 1101 elements. Correct: 527\n",
      "iter 3800: dev acc=0.4787\n",
      "new highscore\n",
      "Iter 3850: loss=46.3934, time=662.31s\n",
      "Evaluation on 1101 elements. Correct: 521\n",
      "iter 3850: dev acc=0.4732\n",
      "Iter 3900: loss=46.4369, time=671.04s\n",
      "Evaluation on 1101 elements. Correct: 522\n",
      "iter 3900: dev acc=0.4741\n",
      "Iter 3950: loss=45.8833, time=679.69s\n",
      "Evaluation on 1101 elements. Correct: 529\n",
      "iter 3950: dev acc=0.4805\n",
      "new highscore\n",
      "Iter 4000: loss=45.2644, time=688.50s\n",
      "Evaluation on 1101 elements. Correct: 517\n",
      "iter 4000: dev acc=0.4696\n",
      "Iter 4050: loss=46.3755, time=697.11s\n",
      "Evaluation on 1101 elements. Correct: 524\n",
      "iter 4050: dev acc=0.4759\n",
      "Iter 4100: loss=46.1464, time=705.72s\n",
      "Evaluation on 1101 elements. Correct: 531\n",
      "iter 4100: dev acc=0.4823\n",
      "new highscore\n",
      "Iter 4150: loss=45.0305, time=715.69s\n",
      "Evaluation on 1101 elements. Correct: 520\n",
      "iter 4150: dev acc=0.4723\n",
      "Iter 4200: loss=46.1796, time=724.38s\n",
      "Evaluation on 1101 elements. Correct: 509\n",
      "iter 4200: dev acc=0.4623\n",
      "Iter 4250: loss=45.9425, time=733.18s\n",
      "Evaluation on 1101 elements. Correct: 506\n",
      "iter 4250: dev acc=0.4596\n",
      "Iter 4300: loss=45.2799, time=741.71s\n",
      "Evaluation on 1101 elements. Correct: 512\n",
      "iter 4300: dev acc=0.4650\n",
      "Iter 4350: loss=45.8177, time=750.29s\n",
      "Evaluation on 1101 elements. Correct: 528\n",
      "iter 4350: dev acc=0.4796\n",
      "Iter 4400: loss=45.4968, time=758.87s\n",
      "Evaluation on 1101 elements. Correct: 518\n",
      "iter 4400: dev acc=0.4705\n",
      "Iter 4450: loss=45.6257, time=767.55s\n",
      "Evaluation on 1101 elements. Correct: 524\n",
      "iter 4450: dev acc=0.4759\n",
      "Iter 4500: loss=45.5894, time=776.31s\n",
      "Evaluation on 1101 elements. Correct: 508\n",
      "iter 4500: dev acc=0.4614\n",
      "Iter 4550: loss=45.6797, time=784.79s\n",
      "Evaluation on 1101 elements. Correct: 526\n",
      "iter 4550: dev acc=0.4777\n",
      "Iter 4600: loss=45.3196, time=793.74s\n",
      "Evaluation on 1101 elements. Correct: 521\n",
      "iter 4600: dev acc=0.4732\n",
      "Iter 4650: loss=44.8394, time=804.15s\n",
      "Evaluation on 1101 elements. Correct: 509\n",
      "iter 4650: dev acc=0.4623\n",
      "Iter 4700: loss=45.0893, time=812.50s\n",
      "Evaluation on 1101 elements. Correct: 525\n",
      "iter 4700: dev acc=0.4768\n",
      "Iter 4750: loss=45.8461, time=821.14s\n",
      "Evaluation on 1101 elements. Correct: 527\n",
      "iter 4750: dev acc=0.4787\n",
      "Iter 4800: loss=45.6621, time=829.68s\n",
      "Evaluation on 1101 elements. Correct: 533\n",
      "iter 4800: dev acc=0.4841\n",
      "new highscore\n",
      "Iter 4850: loss=45.2303, time=838.52s\n",
      "Evaluation on 1101 elements. Correct: 506\n",
      "iter 4850: dev acc=0.4596\n",
      "Iter 4900: loss=43.9208, time=847.16s\n",
      "Evaluation on 1101 elements. Correct: 519\n",
      "iter 4900: dev acc=0.4714\n",
      "Iter 4950: loss=45.5735, time=855.89s\n",
      "Evaluation on 1101 elements. Correct: 513\n",
      "iter 4950: dev acc=0.4659\n",
      "Iter 5000: loss=45.5094, time=864.39s\n",
      "Evaluation on 1101 elements. Correct: 526\n",
      "iter 5000: dev acc=0.4777\n",
      "Iter 5050: loss=44.3841, time=872.92s\n",
      "Evaluation on 1101 elements. Correct: 531\n",
      "iter 5050: dev acc=0.4823\n",
      "Iter 5100: loss=45.5026, time=881.13s\n",
      "Evaluation on 1101 elements. Correct: 527\n",
      "iter 5100: dev acc=0.4787\n",
      "Iter 5150: loss=44.2456, time=890.99s\n",
      "Evaluation on 1101 elements. Correct: 525\n",
      "iter 5150: dev acc=0.4768\n",
      "Iter 5200: loss=44.8076, time=899.52s\n",
      "Evaluation on 1101 elements. Correct: 529\n",
      "iter 5200: dev acc=0.4805\n",
      "Iter 5250: loss=44.6638, time=908.14s\n",
      "Evaluation on 1101 elements. Correct: 530\n",
      "iter 5250: dev acc=0.4814\n",
      "Iter 5300: loss=45.7608, time=916.47s\n",
      "Evaluation on 1101 elements. Correct: 521\n",
      "iter 5300: dev acc=0.4732\n",
      "Iter 5350: loss=45.2287, time=924.59s\n",
      "Evaluation on 1101 elements. Correct: 535\n",
      "iter 5350: dev acc=0.4859\n",
      "new highscore\n",
      "Iter 5400: loss=43.8584, time=933.04s\n",
      "Evaluation on 1101 elements. Correct: 525\n",
      "iter 5400: dev acc=0.4768\n",
      "Iter 5450: loss=44.5164, time=941.24s\n",
      "Evaluation on 1101 elements. Correct: 516\n",
      "iter 5450: dev acc=0.4687\n",
      "Iter 5500: loss=45.4173, time=949.71s\n",
      "Evaluation on 1101 elements. Correct: 537\n",
      "iter 5500: dev acc=0.4877\n",
      "new highscore\n",
      "Iter 5550: loss=44.2789, time=957.98s\n",
      "Evaluation on 1101 elements. Correct: 530\n",
      "iter 5550: dev acc=0.4814\n",
      "Iter 5600: loss=44.2948, time=966.62s\n",
      "Evaluation on 1101 elements. Correct: 524\n",
      "iter 5600: dev acc=0.4759\n",
      "Iter 5650: loss=44.3145, time=976.69s\n",
      "Evaluation on 1101 elements. Correct: 533\n",
      "iter 5650: dev acc=0.4841\n",
      "Iter 5700: loss=44.5281, time=984.92s\n",
      "Evaluation on 1101 elements. Correct: 532\n",
      "iter 5700: dev acc=0.4832\n",
      "Iter 5750: loss=43.4711, time=993.20s\n",
      "Evaluation on 1101 elements. Correct: 527\n",
      "iter 5750: dev acc=0.4787\n",
      "Iter 5800: loss=44.8475, time=1001.65s\n",
      "Evaluation on 1101 elements. Correct: 528\n",
      "iter 5800: dev acc=0.4796\n",
      "Iter 5850: loss=44.8233, time=1010.30s\n",
      "Evaluation on 1101 elements. Correct: 516\n",
      "iter 5850: dev acc=0.4687\n",
      "Iter 5900: loss=43.5243, time=1018.40s\n",
      "Evaluation on 1101 elements. Correct: 529\n",
      "iter 5900: dev acc=0.4805\n",
      "Iter 5950: loss=44.4456, time=1026.92s\n",
      "Evaluation on 1101 elements. Correct: 538\n",
      "iter 5950: dev acc=0.4886\n",
      "new highscore\n",
      "Iter 6000: loss=43.0673, time=1035.41s\n",
      "Evaluation on 1101 elements. Correct: 509\n",
      "iter 6000: dev acc=0.4623\n",
      "Iter 6050: loss=44.1022, time=1044.00s\n",
      "Evaluation on 1101 elements. Correct: 521\n",
      "iter 6050: dev acc=0.4732\n",
      "Iter 6100: loss=44.8878, time=1052.79s\n",
      "Evaluation on 1101 elements. Correct: 525\n",
      "iter 6100: dev acc=0.4768\n",
      "Iter 6150: loss=44.0504, time=1063.03s\n",
      "Evaluation on 1101 elements. Correct: 505\n",
      "iter 6150: dev acc=0.4587\n",
      "Iter 6200: loss=43.6933, time=1071.63s\n",
      "Evaluation on 1101 elements. Correct: 531\n",
      "iter 6200: dev acc=0.4823\n",
      "Iter 6250: loss=44.1644, time=1080.02s\n",
      "Evaluation on 1101 elements. Correct: 529\n",
      "iter 6250: dev acc=0.4805\n",
      "Iter 6300: loss=43.8067, time=1088.65s\n",
      "Evaluation on 1101 elements. Correct: 544\n",
      "iter 6300: dev acc=0.4941\n",
      "new highscore\n",
      "Iter 6350: loss=44.1288, time=1097.39s\n",
      "Evaluation on 1101 elements. Correct: 532\n",
      "iter 6350: dev acc=0.4832\n",
      "Iter 6400: loss=43.7542, time=1105.98s\n",
      "Evaluation on 1101 elements. Correct: 527\n",
      "iter 6400: dev acc=0.4787\n",
      "Iter 6450: loss=43.8087, time=1114.73s\n",
      "Evaluation on 1101 elements. Correct: 524\n",
      "iter 6450: dev acc=0.4759\n",
      "Iter 6500: loss=43.4723, time=1123.66s\n",
      "Evaluation on 1101 elements. Correct: 516\n",
      "iter 6500: dev acc=0.4687\n",
      "Iter 6550: loss=44.0209, time=1132.11s\n",
      "Evaluation on 1101 elements. Correct: 522\n",
      "iter 6550: dev acc=0.4741\n",
      "Iter 6600: loss=43.6617, time=1140.57s\n",
      "Evaluation on 1101 elements. Correct: 516\n",
      "iter 6600: dev acc=0.4687\n",
      "Iter 6650: loss=43.3135, time=1150.48s\n",
      "Evaluation on 1101 elements. Correct: 537\n",
      "iter 6650: dev acc=0.4877\n",
      "Iter 6700: loss=43.3598, time=1159.13s\n",
      "Evaluation on 1101 elements. Correct: 536\n",
      "iter 6700: dev acc=0.4868\n",
      "Iter 6750: loss=43.2721, time=1167.61s\n",
      "Evaluation on 1101 elements. Correct: 542\n",
      "iter 6750: dev acc=0.4923\n",
      "Iter 6800: loss=43.0531, time=1175.98s\n",
      "Evaluation on 1101 elements. Correct: 533\n",
      "iter 6800: dev acc=0.4841\n",
      "Iter 6850: loss=43.6074, time=1184.75s\n",
      "Evaluation on 1101 elements. Correct: 539\n",
      "iter 6850: dev acc=0.4896\n",
      "Iter 6900: loss=43.8146, time=1193.64s\n",
      "Evaluation on 1101 elements. Correct: 524\n",
      "iter 6900: dev acc=0.4759\n",
      "Iter 6950: loss=43.8463, time=1202.33s\n",
      "Evaluation on 1101 elements. Correct: 529\n",
      "iter 6950: dev acc=0.4805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 7000: loss=43.2199, time=1211.00s\n",
      "Evaluation on 1101 elements. Correct: 527\n",
      "iter 7000: dev acc=0.4787\n",
      "Iter 7050: loss=43.4246, time=1219.59s\n",
      "Evaluation on 1101 elements. Correct: 518\n",
      "iter 7050: dev acc=0.4705\n",
      "Iter 7100: loss=43.6372, time=1228.12s\n",
      "Evaluation on 1101 elements. Correct: 522\n",
      "iter 7100: dev acc=0.4741\n",
      "Iter 7150: loss=43.3646, time=1238.12s\n",
      "Evaluation on 1101 elements. Correct: 540\n",
      "iter 7150: dev acc=0.4905\n",
      "Iter 7200: loss=43.4652, time=1246.64s\n",
      "Evaluation on 1101 elements. Correct: 525\n",
      "iter 7200: dev acc=0.4768\n",
      "Iter 7250: loss=42.6779, time=1255.29s\n",
      "Evaluation on 1101 elements. Correct: 529\n",
      "iter 7250: dev acc=0.4805\n",
      "Iter 7300: loss=43.4319, time=1264.07s\n",
      "Evaluation on 1101 elements. Correct: 534\n",
      "iter 7300: dev acc=0.4850\n",
      "Iter 7350: loss=42.8867, time=1272.74s\n",
      "Evaluation on 1101 elements. Correct: 542\n",
      "iter 7350: dev acc=0.4923\n",
      "Iter 7400: loss=42.0885, time=1281.26s\n",
      "Evaluation on 1101 elements. Correct: 516\n",
      "iter 7400: dev acc=0.4687\n",
      "Iter 7450: loss=43.4591, time=1289.86s\n",
      "Evaluation on 1101 elements. Correct: 524\n",
      "iter 7450: dev acc=0.4759\n",
      "Iter 7500: loss=43.2947, time=1298.49s\n",
      "Evaluation on 1101 elements. Correct: 512\n",
      "iter 7500: dev acc=0.4650\n",
      "Iter 7550: loss=43.4976, time=1307.26s\n",
      "Evaluation on 1101 elements. Correct: 526\n",
      "iter 7550: dev acc=0.4777\n",
      "Iter 7600: loss=42.9218, time=1315.97s\n",
      "Evaluation on 1101 elements. Correct: 525\n",
      "iter 7600: dev acc=0.4768\n",
      "Iter 7650: loss=43.1269, time=1326.49s\n",
      "Evaluation on 1101 elements. Correct: 528\n",
      "iter 7650: dev acc=0.4796\n",
      "Iter 7700: loss=42.2653, time=1335.41s\n",
      "Evaluation on 1101 elements. Correct: 527\n",
      "iter 7700: dev acc=0.4787\n",
      "Iter 7750: loss=42.2881, time=1344.04s\n",
      "Evaluation on 1101 elements. Correct: 523\n",
      "iter 7750: dev acc=0.4750\n",
      "Iter 7800: loss=42.6116, time=1352.83s\n",
      "Evaluation on 1101 elements. Correct: 516\n",
      "iter 7800: dev acc=0.4687\n",
      "Iter 7850: loss=42.9649, time=1361.48s\n",
      "Evaluation on 1101 elements. Correct: 518\n",
      "iter 7850: dev acc=0.4705\n",
      "Iter 7900: loss=42.6765, time=1370.09s\n",
      "Evaluation on 1101 elements. Correct: 516\n",
      "iter 7900: dev acc=0.4687\n",
      "Iter 7950: loss=41.8768, time=1378.84s\n",
      "Evaluation on 1101 elements. Correct: 503\n",
      "iter 7950: dev acc=0.4569\n",
      "Iter 8000: loss=43.1195, time=1387.46s\n",
      "Evaluation on 1101 elements. Correct: 524\n",
      "iter 8000: dev acc=0.4759\n",
      "Iter 8050: loss=42.5386, time=1396.35s\n",
      "Evaluation on 1101 elements. Correct: 523\n",
      "iter 8050: dev acc=0.4750\n",
      "Iter 8100: loss=42.7748, time=1405.07s\n",
      "Evaluation on 1101 elements. Correct: 525\n",
      "iter 8100: dev acc=0.4768\n",
      "Iter 8150: loss=42.4275, time=1415.11s\n",
      "Evaluation on 1101 elements. Correct: 495\n",
      "iter 8150: dev acc=0.4496\n",
      "Iter 8200: loss=42.4701, time=1423.74s\n",
      "Evaluation on 1101 elements. Correct: 503\n",
      "iter 8200: dev acc=0.4569\n",
      "Iter 8250: loss=43.2592, time=1432.43s\n",
      "Evaluation on 1101 elements. Correct: 520\n",
      "iter 8250: dev acc=0.4723\n",
      "Iter 8300: loss=42.4754, time=1441.32s\n",
      "Evaluation on 1101 elements. Correct: 506\n",
      "iter 8300: dev acc=0.4596\n",
      "Iter 8350: loss=42.4025, time=1449.93s\n",
      "Evaluation on 1101 elements. Correct: 491\n",
      "iter 8350: dev acc=0.4460\n",
      "Iter 8400: loss=42.7300, time=1458.44s\n",
      "Evaluation on 1101 elements. Correct: 528\n",
      "iter 8400: dev acc=0.4796\n",
      "Iter 8450: loss=42.4412, time=1466.95s\n",
      "Evaluation on 1101 elements. Correct: 513\n",
      "iter 8450: dev acc=0.4659\n",
      "Iter 8500: loss=42.4005, time=1475.70s\n",
      "Evaluation on 1101 elements. Correct: 513\n",
      "iter 8500: dev acc=0.4659\n",
      "Iter 8550: loss=42.4885, time=1484.19s\n",
      "Evaluation on 1101 elements. Correct: 512\n",
      "iter 8550: dev acc=0.4650\n",
      "Iter 8600: loss=41.9412, time=1493.00s\n",
      "Evaluation on 1101 elements. Correct: 509\n",
      "iter 8600: dev acc=0.4623\n",
      "Iter 8650: loss=41.7548, time=1503.27s\n",
      "Evaluation on 1101 elements. Correct: 511\n",
      "iter 8650: dev acc=0.4641\n",
      "Iter 8700: loss=42.3501, time=1511.79s\n",
      "Evaluation on 1101 elements. Correct: 506\n",
      "iter 8700: dev acc=0.4596\n",
      "Iter 8750: loss=42.3701, time=1520.11s\n",
      "Evaluation on 1101 elements. Correct: 525\n",
      "iter 8750: dev acc=0.4768\n",
      "Iter 8800: loss=42.3230, time=1528.57s\n",
      "Evaluation on 1101 elements. Correct: 503\n",
      "iter 8800: dev acc=0.4569\n",
      "Iter 8850: loss=42.0599, time=1536.98s\n",
      "Evaluation on 1101 elements. Correct: 506\n",
      "iter 8850: dev acc=0.4596\n",
      "Iter 8900: loss=41.8495, time=1545.59s\n",
      "Evaluation on 1101 elements. Correct: 503\n",
      "iter 8900: dev acc=0.4569\n",
      "Iter 8950: loss=41.9105, time=1554.37s\n",
      "Evaluation on 1101 elements. Correct: 497\n",
      "iter 8950: dev acc=0.4514\n",
      "Iter 9000: loss=42.5366, time=1563.16s\n",
      "Evaluation on 1101 elements. Correct: 516\n",
      "iter 9000: dev acc=0.4687\n",
      "Iter 9050: loss=42.1183, time=1571.60s\n",
      "Evaluation on 1101 elements. Correct: 520\n",
      "iter 9050: dev acc=0.4723\n",
      "Iter 9100: loss=42.4526, time=1580.34s\n",
      "Evaluation on 1101 elements. Correct: 526\n",
      "iter 9100: dev acc=0.4777\n",
      "Iter 9150: loss=41.5774, time=1590.69s\n",
      "Evaluation on 1101 elements. Correct: 496\n",
      "iter 9150: dev acc=0.4505\n",
      "Iter 9200: loss=41.6232, time=1599.10s\n",
      "Evaluation on 1101 elements. Correct: 499\n",
      "iter 9200: dev acc=0.4532\n",
      "Iter 9250: loss=41.7148, time=1607.78s\n",
      "Evaluation on 1101 elements. Correct: 508\n",
      "iter 9250: dev acc=0.4614\n",
      "Iter 9300: loss=41.6269, time=1616.22s\n",
      "Evaluation on 1101 elements. Correct: 499\n",
      "iter 9300: dev acc=0.4532\n",
      "Iter 9350: loss=41.5192, time=1624.94s\n",
      "Evaluation on 1101 elements. Correct: 503\n",
      "iter 9350: dev acc=0.4569\n",
      "Iter 9400: loss=41.3595, time=1633.50s\n",
      "Evaluation on 1101 elements. Correct: 496\n",
      "iter 9400: dev acc=0.4505\n",
      "Iter 9450: loss=40.6696, time=1642.08s\n",
      "Evaluation on 1101 elements. Correct: 506\n",
      "iter 9450: dev acc=0.4596\n",
      "Iter 9500: loss=41.8568, time=1650.54s\n",
      "Evaluation on 1101 elements. Correct: 503\n",
      "iter 9500: dev acc=0.4569\n",
      "Iter 9550: loss=41.9262, time=1659.16s\n",
      "Evaluation on 1101 elements. Correct: 519\n",
      "iter 9550: dev acc=0.4714\n",
      "Iter 9600: loss=41.7824, time=1667.82s\n",
      "Evaluation on 1101 elements. Correct: 505\n",
      "iter 9600: dev acc=0.4587\n",
      "Iter 9650: loss=41.6811, time=1676.40s\n",
      "Evaluation on 1101 elements. Correct: 512\n",
      "iter 9650: dev acc=0.4650\n",
      "Iter 9700: loss=40.5423, time=1686.41s\n",
      "Evaluation on 1101 elements. Correct: 520\n",
      "iter 9700: dev acc=0.4723\n",
      "Iter 9750: loss=42.5258, time=1694.83s\n",
      "Evaluation on 1101 elements. Correct: 512\n",
      "iter 9750: dev acc=0.4650\n",
      "Iter 9800: loss=41.8020, time=1702.99s\n",
      "Evaluation on 1101 elements. Correct: 501\n",
      "iter 9800: dev acc=0.4550\n",
      "Iter 9850: loss=41.5922, time=1711.43s\n",
      "Evaluation on 1101 elements. Correct: 510\n",
      "iter 9850: dev acc=0.4632\n",
      "Iter 9900: loss=40.5375, time=1719.97s\n",
      "Evaluation on 1101 elements. Correct: 506\n",
      "iter 9900: dev acc=0.4596\n",
      "Iter 9950: loss=41.5474, time=1728.54s\n",
      "Evaluation on 1101 elements. Correct: 506\n",
      "iter 9950: dev acc=0.4596\n",
      "Iter 10000: loss=40.7188, time=1736.94s\n",
      "Evaluation on 1101 elements. Correct: 518\n",
      "iter 10000: dev acc=0.4705\n",
      "Iter 10050: loss=41.4813, time=1745.33s\n",
      "Evaluation on 1101 elements. Correct: 506\n",
      "iter 10050: dev acc=0.4596\n",
      "Iter 10100: loss=41.1342, time=1753.88s\n",
      "Evaluation on 1101 elements. Correct: 507\n",
      "iter 10100: dev acc=0.4605\n",
      "Iter 10150: loss=41.0341, time=1762.60s\n",
      "Evaluation on 1101 elements. Correct: 497\n",
      "iter 10150: dev acc=0.4514\n",
      "Iter 10200: loss=40.2887, time=1772.64s\n",
      "Evaluation on 1101 elements. Correct: 506\n",
      "iter 10200: dev acc=0.4596\n",
      "Iter 10250: loss=41.4628, time=1781.16s\n",
      "Evaluation on 1101 elements. Correct: 502\n",
      "iter 10250: dev acc=0.4559\n",
      "Iter 10300: loss=41.2938, time=1789.71s\n",
      "Evaluation on 1101 elements. Correct: 490\n",
      "iter 10300: dev acc=0.4450\n",
      "Iter 10350: loss=40.2875, time=1798.19s\n",
      "Evaluation on 1101 elements. Correct: 507\n",
      "iter 10350: dev acc=0.4605\n",
      "Iter 10400: loss=40.8366, time=1806.86s\n",
      "Evaluation on 1101 elements. Correct: 515\n",
      "iter 10400: dev acc=0.4678\n",
      "Iter 10450: loss=41.3164, time=1815.47s\n",
      "Evaluation on 1101 elements. Correct: 505\n",
      "iter 10450: dev acc=0.4587\n",
      "Iter 10500: loss=39.8169, time=1824.34s\n",
      "Evaluation on 1101 elements. Correct: 518\n",
      "iter 10500: dev acc=0.4705\n",
      "Iter 10550: loss=41.3213, time=1832.84s\n",
      "Evaluation on 1101 elements. Correct: 503\n",
      "iter 10550: dev acc=0.4569\n",
      "Iter 10600: loss=41.3233, time=1840.63s\n",
      "Evaluation on 1101 elements. Correct: 496\n",
      "iter 10600: dev acc=0.4505\n",
      "Iter 10650: loss=40.0092, time=1847.89s\n",
      "Evaluation on 1101 elements. Correct: 480\n",
      "iter 10650: dev acc=0.4360\n",
      "Iter 10700: loss=40.5510, time=1856.70s\n",
      "Evaluation on 1101 elements. Correct: 484\n",
      "iter 10700: dev acc=0.4396\n",
      "Iter 10750: loss=40.9721, time=1863.95s\n",
      "Evaluation on 1101 elements. Correct: 511\n",
      "iter 10750: dev acc=0.4641\n",
      "Iter 10800: loss=39.9317, time=1871.36s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on 1101 elements. Correct: 512\n",
      "iter 10800: dev acc=0.4650\n",
      "Iter 10850: loss=40.6297, time=1878.78s\n",
      "Evaluation on 1101 elements. Correct: 504\n",
      "iter 10850: dev acc=0.4578\n",
      "Iter 10900: loss=40.9498, time=1885.98s\n",
      "Evaluation on 1101 elements. Correct: 495\n",
      "iter 10900: dev acc=0.4496\n",
      "Iter 10950: loss=40.7165, time=1893.08s\n",
      "Evaluation on 1101 elements. Correct: 497\n",
      "iter 10950: dev acc=0.4514\n",
      "Iter 11000: loss=40.0643, time=1900.19s\n",
      "Evaluation on 1101 elements. Correct: 512\n",
      "iter 11000: dev acc=0.4650\n",
      "Iter 11050: loss=40.4650, time=1907.37s\n",
      "Evaluation on 1101 elements. Correct: 496\n",
      "iter 11050: dev acc=0.4505\n",
      "Iter 11100: loss=40.8936, time=1914.70s\n",
      "Evaluation on 1101 elements. Correct: 503\n",
      "iter 11100: dev acc=0.4569\n",
      "Iter 11150: loss=40.3683, time=1921.87s\n",
      "Evaluation on 1101 elements. Correct: 486\n",
      "iter 11150: dev acc=0.4414\n",
      "Iter 11200: loss=41.1039, time=1930.79s\n",
      "Evaluation on 1101 elements. Correct: 493\n",
      "iter 11200: dev acc=0.4478\n",
      "Iter 11250: loss=40.3040, time=1939.37s\n",
      "Evaluation on 1101 elements. Correct: 483\n",
      "iter 11250: dev acc=0.4387\n",
      "Iter 11300: loss=40.1675, time=1947.99s\n",
      "Evaluation on 1101 elements. Correct: 517\n",
      "iter 11300: dev acc=0.4696\n",
      "Iter 11350: loss=40.1281, time=1956.57s\n",
      "Evaluation on 1101 elements. Correct: 506\n",
      "iter 11350: dev acc=0.4596\n",
      "Iter 11400: loss=39.9214, time=1964.93s\n",
      "Evaluation on 1101 elements. Correct: 498\n",
      "iter 11400: dev acc=0.4523\n",
      "Iter 11450: loss=40.2091, time=1973.42s\n",
      "Evaluation on 1101 elements. Correct: 510\n",
      "iter 11450: dev acc=0.4632\n",
      "Iter 11500: loss=40.2169, time=1982.02s\n",
      "Evaluation on 1101 elements. Correct: 499\n",
      "iter 11500: dev acc=0.4532\n",
      "Iter 11550: loss=40.3310, time=1990.69s\n",
      "Evaluation on 1101 elements. Correct: 512\n",
      "iter 11550: dev acc=0.4650\n",
      "Iter 11600: loss=40.9800, time=1999.29s\n",
      "Evaluation on 1101 elements. Correct: 495\n",
      "iter 11600: dev acc=0.4496\n",
      "Iter 11650: loss=39.3922, time=2007.66s\n",
      "Evaluation on 1101 elements. Correct: 502\n",
      "iter 11650: dev acc=0.4559\n",
      "Iter 11700: loss=40.3902, time=2017.88s\n",
      "Evaluation on 1101 elements. Correct: 505\n",
      "iter 11700: dev acc=0.4587\n",
      "Iter 11750: loss=39.6483, time=2026.53s\n",
      "Evaluation on 1101 elements. Correct: 502\n",
      "iter 11750: dev acc=0.4559\n",
      "Iter 11800: loss=40.1433, time=2035.07s\n",
      "Evaluation on 1101 elements. Correct: 482\n",
      "iter 11800: dev acc=0.4378\n",
      "Iter 11850: loss=39.7650, time=2043.79s\n",
      "Evaluation on 1101 elements. Correct: 493\n",
      "iter 11850: dev acc=0.4478\n",
      "Iter 11900: loss=40.1585, time=2052.46s\n",
      "Evaluation on 1101 elements. Correct: 506\n",
      "iter 11900: dev acc=0.4596\n",
      "Iter 11950: loss=39.8196, time=2061.17s\n",
      "Evaluation on 1101 elements. Correct: 506\n",
      "iter 11950: dev acc=0.4596\n",
      "Iter 12000: loss=40.2706, time=2069.85s\n",
      "Evaluation on 1101 elements. Correct: 481\n",
      "iter 12000: dev acc=0.4369\n",
      "Iter 12050: loss=39.4042, time=2078.61s\n",
      "Evaluation on 1101 elements. Correct: 493\n",
      "iter 12050: dev acc=0.4478\n",
      "Iter 12100: loss=39.5147, time=2086.93s\n",
      "Evaluation on 1101 elements. Correct: 488\n",
      "iter 12100: dev acc=0.4432\n",
      "Iter 12150: loss=40.1025, time=2095.44s\n",
      "Evaluation on 1101 elements. Correct: 488\n",
      "iter 12150: dev acc=0.4432\n",
      "Iter 12200: loss=40.8478, time=2105.45s\n",
      "Evaluation on 1101 elements. Correct: 490\n",
      "iter 12200: dev acc=0.4450\n",
      "Iter 12250: loss=39.9275, time=2113.93s\n",
      "Evaluation on 1101 elements. Correct: 504\n",
      "iter 12250: dev acc=0.4578\n",
      "Iter 12300: loss=39.6141, time=2122.56s\n",
      "Evaluation on 1101 elements. Correct: 491\n",
      "iter 12300: dev acc=0.4460\n",
      "Iter 12350: loss=39.4985, time=2131.10s\n",
      "Evaluation on 1101 elements. Correct: 491\n",
      "iter 12350: dev acc=0.4460\n",
      "Iter 12400: loss=39.6649, time=2139.94s\n",
      "Evaluation on 1101 elements. Correct: 485\n",
      "iter 12400: dev acc=0.4405\n",
      "Iter 12450: loss=39.0848, time=2148.53s\n",
      "Evaluation on 1101 elements. Correct: 490\n",
      "iter 12450: dev acc=0.4450\n",
      "Iter 12500: loss=40.3125, time=2156.99s\n",
      "Evaluation on 1101 elements. Correct: 487\n",
      "iter 12500: dev acc=0.4423\n",
      "Iter 12550: loss=39.3140, time=2165.74s\n",
      "Evaluation on 1101 elements. Correct: 489\n",
      "iter 12550: dev acc=0.4441\n",
      "Iter 12600: loss=38.5202, time=2174.12s\n",
      "Evaluation on 1101 elements. Correct: 494\n",
      "iter 12600: dev acc=0.4487\n",
      "Iter 12650: loss=39.3728, time=2183.14s\n",
      "Evaluation on 1101 elements. Correct: 514\n",
      "iter 12650: dev acc=0.4668\n",
      "Iter 12700: loss=39.7514, time=2193.34s\n",
      "Evaluation on 1101 elements. Correct: 514\n",
      "iter 12700: dev acc=0.4668\n",
      "Iter 12750: loss=40.0875, time=2201.73s\n",
      "Evaluation on 1101 elements. Correct: 503\n",
      "iter 12750: dev acc=0.4569\n",
      "Iter 12800: loss=39.4873, time=2210.21s\n",
      "Evaluation on 1101 elements. Correct: 483\n",
      "iter 12800: dev acc=0.4387\n",
      "Iter 12850: loss=39.0746, time=2218.59s\n",
      "Evaluation on 1101 elements. Correct: 505\n",
      "iter 12850: dev acc=0.4587\n",
      "Iter 12900: loss=38.2620, time=2227.08s\n",
      "Evaluation on 1101 elements. Correct: 507\n",
      "iter 12900: dev acc=0.4605\n",
      "Iter 12950: loss=39.4618, time=2235.22s\n",
      "Evaluation on 1101 elements. Correct: 496\n",
      "iter 12950: dev acc=0.4505\n",
      "Iter 13000: loss=39.5496, time=2243.67s\n",
      "Evaluation on 1101 elements. Correct: 509\n",
      "iter 13000: dev acc=0.4623\n",
      "Iter 13050: loss=39.4983, time=2252.01s\n",
      "Evaluation on 1101 elements. Correct: 496\n",
      "iter 13050: dev acc=0.4505\n",
      "Iter 13100: loss=39.3209, time=2260.63s\n",
      "Evaluation on 1101 elements. Correct: 489\n",
      "iter 13100: dev acc=0.4441\n",
      "Iter 13150: loss=39.5914, time=2269.16s\n",
      "Evaluation on 1101 elements. Correct: 492\n",
      "iter 13150: dev acc=0.4469\n",
      "Iter 13200: loss=38.8678, time=2279.17s\n",
      "Evaluation on 1101 elements. Correct: 476\n",
      "iter 13200: dev acc=0.4323\n",
      "Iter 13250: loss=38.1134, time=2287.48s\n",
      "Evaluation on 1101 elements. Correct: 514\n",
      "iter 13250: dev acc=0.4668\n",
      "Iter 13300: loss=39.3367, time=2295.74s\n",
      "Evaluation on 1101 elements. Correct: 489\n",
      "iter 13300: dev acc=0.4441\n",
      "Iter 13350: loss=39.6980, time=2304.54s\n",
      "Evaluation on 1101 elements. Correct: 490\n",
      "iter 13350: dev acc=0.4450\n",
      "Iter 13400: loss=38.9414, time=2312.98s\n",
      "Evaluation on 1101 elements. Correct: 504\n",
      "iter 13400: dev acc=0.4578\n",
      "Iter 13450: loss=38.6984, time=2321.54s\n",
      "Evaluation on 1101 elements. Correct: 501\n",
      "iter 13450: dev acc=0.4550\n",
      "Iter 13500: loss=38.2187, time=2329.83s\n",
      "Evaluation on 1101 elements. Correct: 496\n",
      "iter 13500: dev acc=0.4505\n",
      "Iter 13550: loss=38.8804, time=2338.48s\n",
      "Evaluation on 1101 elements. Correct: 498\n",
      "iter 13550: dev acc=0.4523\n",
      "Iter 13600: loss=38.4910, time=2347.17s\n",
      "Evaluation on 1101 elements. Correct: 497\n",
      "iter 13600: dev acc=0.4514\n",
      "Iter 13650: loss=38.5701, time=2355.69s\n",
      "Evaluation on 1101 elements. Correct: 495\n",
      "iter 13650: dev acc=0.4496\n",
      "Iter 13700: loss=39.3133, time=2365.66s\n",
      "Evaluation on 1101 elements. Correct: 496\n",
      "iter 13700: dev acc=0.4505\n",
      "Iter 13750: loss=38.5883, time=2374.03s\n",
      "Evaluation on 1101 elements. Correct: 501\n",
      "iter 13750: dev acc=0.4550\n",
      "Iter 13800: loss=38.6386, time=2382.61s\n",
      "Evaluation on 1101 elements. Correct: 514\n",
      "iter 13800: dev acc=0.4668\n",
      "Iter 13850: loss=39.1844, time=2391.29s\n",
      "Evaluation on 1101 elements. Correct: 490\n",
      "iter 13850: dev acc=0.4450\n",
      "Iter 13900: loss=38.5049, time=2399.55s\n",
      "Evaluation on 1101 elements. Correct: 508\n",
      "iter 13900: dev acc=0.4614\n",
      "Iter 13950: loss=37.9958, time=2408.04s\n",
      "Evaluation on 1101 elements. Correct: 508\n",
      "iter 13950: dev acc=0.4614\n",
      "Iter 14000: loss=39.4103, time=2416.78s\n",
      "Evaluation on 1101 elements. Correct: 501\n",
      "iter 14000: dev acc=0.4550\n",
      "Iter 14050: loss=38.3219, time=2425.70s\n",
      "Evaluation on 1101 elements. Correct: 498\n",
      "iter 14050: dev acc=0.4523\n",
      "Iter 14100: loss=39.0762, time=2434.41s\n",
      "Evaluation on 1101 elements. Correct: 517\n",
      "iter 14100: dev acc=0.4696\n",
      "Iter 14150: loss=38.1885, time=2444.98s\n",
      "Evaluation on 1101 elements. Correct: 490\n",
      "iter 14150: dev acc=0.4450\n",
      "Iter 14200: loss=38.7419, time=2453.64s\n",
      "Evaluation on 1101 elements. Correct: 487\n",
      "iter 14200: dev acc=0.4423\n",
      "Iter 14250: loss=38.2074, time=2462.20s\n",
      "Evaluation on 1101 elements. Correct: 482\n",
      "iter 14250: dev acc=0.4378\n",
      "Iter 14300: loss=38.0308, time=2470.50s\n",
      "Evaluation on 1101 elements. Correct: 485\n",
      "iter 14300: dev acc=0.4405\n",
      "Iter 14350: loss=38.5070, time=2479.09s\n",
      "Evaluation on 1101 elements. Correct: 481\n",
      "iter 14350: dev acc=0.4369\n",
      "Iter 14400: loss=38.1017, time=2487.30s\n",
      "Evaluation on 1101 elements. Correct: 488\n",
      "iter 14400: dev acc=0.4432\n",
      "Iter 14450: loss=37.5336, time=2495.75s\n",
      "Evaluation on 1101 elements. Correct: 485\n",
      "iter 14450: dev acc=0.4405\n",
      "Iter 14500: loss=37.3958, time=2504.51s\n",
      "Evaluation on 1101 elements. Correct: 497\n",
      "iter 14500: dev acc=0.4514\n",
      "Iter 14550: loss=38.2656, time=2513.19s\n",
      "Evaluation on 1101 elements. Correct: 480\n",
      "iter 14550: dev acc=0.4360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 14600: loss=37.8829, time=2521.81s\n",
      "Evaluation on 1101 elements. Correct: 477\n",
      "iter 14600: dev acc=0.4332\n",
      "Iter 14650: loss=38.8131, time=2532.13s\n",
      "Evaluation on 1101 elements. Correct: 484\n",
      "iter 14650: dev acc=0.4396\n",
      "Iter 14700: loss=38.6205, time=2540.89s\n",
      "Evaluation on 1101 elements. Correct: 489\n",
      "iter 14700: dev acc=0.4441\n",
      "Iter 14750: loss=38.2581, time=2549.36s\n",
      "Evaluation on 1101 elements. Correct: 492\n",
      "iter 14750: dev acc=0.4469\n",
      "Iter 14800: loss=37.8211, time=2557.82s\n",
      "Evaluation on 1101 elements. Correct: 486\n",
      "iter 14800: dev acc=0.4414\n",
      "Iter 14850: loss=38.2852, time=2566.19s\n",
      "Evaluation on 1101 elements. Correct: 504\n",
      "iter 14850: dev acc=0.4578\n",
      "Iter 14900: loss=38.7197, time=2574.86s\n",
      "Evaluation on 1101 elements. Correct: 503\n",
      "iter 14900: dev acc=0.4569\n",
      "Iter 14950: loss=37.5728, time=2583.54s\n",
      "Evaluation on 1101 elements. Correct: 489\n",
      "iter 14950: dev acc=0.4441\n",
      "Iter 15000: loss=38.0871, time=2592.02s\n",
      "Evaluation on 1101 elements. Correct: 499\n",
      "iter 15000: dev acc=0.4532\n",
      "Iter 15050: loss=38.1014, time=2600.46s\n",
      "Evaluation on 1101 elements. Correct: 491\n",
      "iter 15050: dev acc=0.4460\n",
      "Iter 15100: loss=38.1377, time=2609.23s\n",
      "Evaluation on 1101 elements. Correct: 487\n",
      "iter 15100: dev acc=0.4423\n",
      "Iter 15150: loss=37.7136, time=2619.40s\n",
      "Evaluation on 1101 elements. Correct: 495\n",
      "iter 15150: dev acc=0.4496\n",
      "Iter 15200: loss=37.9484, time=2627.96s\n",
      "Evaluation on 1101 elements. Correct: 479\n",
      "iter 15200: dev acc=0.4351\n",
      "Iter 15250: loss=38.1479, time=2636.53s\n",
      "Evaluation on 1101 elements. Correct: 487\n",
      "iter 15250: dev acc=0.4423\n",
      "Iter 15300: loss=37.2470, time=2644.89s\n",
      "Evaluation on 1101 elements. Correct: 481\n",
      "iter 15300: dev acc=0.4369\n",
      "Iter 15350: loss=37.6280, time=2653.42s\n",
      "Evaluation on 1101 elements. Correct: 493\n",
      "iter 15350: dev acc=0.4478\n",
      "Iter 15400: loss=38.2968, time=2661.99s\n",
      "Evaluation on 1101 elements. Correct: 488\n",
      "iter 15400: dev acc=0.4432\n",
      "Iter 15450: loss=37.3646, time=2670.39s\n",
      "Evaluation on 1101 elements. Correct: 491\n",
      "iter 15450: dev acc=0.4460\n",
      "Iter 15500: loss=37.4580, time=2679.10s\n",
      "Evaluation on 1101 elements. Correct: 491\n",
      "iter 15500: dev acc=0.4460\n",
      "Iter 15550: loss=37.3325, time=2687.38s\n",
      "Evaluation on 1101 elements. Correct: 496\n",
      "iter 15550: dev acc=0.4505\n",
      "Iter 15600: loss=37.9965, time=2695.54s\n",
      "Evaluation on 1101 elements. Correct: 483\n",
      "iter 15600: dev acc=0.4387\n",
      "Iter 15650: loss=37.6748, time=2705.43s\n",
      "Evaluation on 1101 elements. Correct: 485\n",
      "iter 15650: dev acc=0.4405\n",
      "Iter 15700: loss=37.7178, time=2713.90s\n",
      "Evaluation on 1101 elements. Correct: 491\n",
      "iter 15700: dev acc=0.4460\n",
      "Iter 15750: loss=38.0609, time=2722.40s\n",
      "Evaluation on 1101 elements. Correct: 477\n",
      "iter 15750: dev acc=0.4332\n",
      "Iter 15800: loss=37.4376, time=2730.53s\n",
      "Evaluation on 1101 elements. Correct: 485\n",
      "iter 15800: dev acc=0.4405\n",
      "Iter 15850: loss=37.2110, time=2738.85s\n",
      "Evaluation on 1101 elements. Correct: 481\n",
      "iter 15850: dev acc=0.4369\n",
      "Iter 15900: loss=37.1152, time=2747.19s\n",
      "Evaluation on 1101 elements. Correct: 500\n",
      "iter 15900: dev acc=0.4541\n",
      "Iter 15950: loss=37.6234, time=2755.62s\n",
      "Evaluation on 1101 elements. Correct: 487\n",
      "iter 15950: dev acc=0.4423\n",
      "Iter 16000: loss=37.3878, time=2764.01s\n",
      "Evaluation on 1101 elements. Correct: 482\n",
      "iter 16000: dev acc=0.4378\n",
      "Iter 16050: loss=36.9685, time=2772.30s\n",
      "Evaluation on 1101 elements. Correct: 487\n",
      "iter 16050: dev acc=0.4423\n",
      "Iter 16100: loss=36.5245, time=2781.04s\n",
      "Evaluation on 1101 elements. Correct: 483\n",
      "iter 16100: dev acc=0.4387\n",
      "Iter 16150: loss=36.9282, time=2790.87s\n",
      "Evaluation on 1101 elements. Correct: 484\n",
      "iter 16150: dev acc=0.4396\n",
      "Iter 16200: loss=37.7714, time=2799.19s\n",
      "Evaluation on 1101 elements. Correct: 483\n",
      "iter 16200: dev acc=0.4387\n",
      "Iter 16250: loss=37.2291, time=2807.70s\n",
      "Evaluation on 1101 elements. Correct: 486\n",
      "iter 16250: dev acc=0.4414\n",
      "Iter 16300: loss=36.7627, time=2816.18s\n",
      "Evaluation on 1101 elements. Correct: 493\n",
      "iter 16300: dev acc=0.4478\n",
      "Iter 16350: loss=37.1450, time=2824.53s\n",
      "Evaluation on 1101 elements. Correct: 475\n",
      "iter 16350: dev acc=0.4314\n",
      "Iter 16400: loss=37.5446, time=2832.85s\n",
      "Evaluation on 1101 elements. Correct: 472\n",
      "iter 16400: dev acc=0.4287\n",
      "Iter 16450: loss=37.7273, time=2841.06s\n",
      "Evaluation on 1101 elements. Correct: 479\n",
      "iter 16450: dev acc=0.4351\n",
      "Iter 16500: loss=36.8749, time=2849.60s\n",
      "Evaluation on 1101 elements. Correct: 469\n",
      "iter 16500: dev acc=0.4260\n",
      "Iter 16550: loss=36.7158, time=2857.88s\n",
      "Evaluation on 1101 elements. Correct: 467\n",
      "iter 16550: dev acc=0.4242\n",
      "Iter 16600: loss=36.3608, time=2866.12s\n",
      "Evaluation on 1101 elements. Correct: 469\n",
      "iter 16600: dev acc=0.4260\n",
      "Iter 16650: loss=36.9326, time=2876.21s\n",
      "Evaluation on 1101 elements. Correct: 492\n",
      "iter 16650: dev acc=0.4469\n",
      "Iter 16700: loss=37.4289, time=2884.49s\n",
      "Evaluation on 1101 elements. Correct: 480\n",
      "iter 16700: dev acc=0.4360\n",
      "Iter 16750: loss=37.0010, time=2892.69s\n",
      "Evaluation on 1101 elements. Correct: 500\n",
      "iter 16750: dev acc=0.4541\n",
      "Iter 16800: loss=36.8234, time=2901.08s\n",
      "Evaluation on 1101 elements. Correct: 490\n",
      "iter 16800: dev acc=0.4450\n",
      "Iter 16850: loss=37.0554, time=2909.23s\n",
      "Evaluation on 1101 elements. Correct: 482\n",
      "iter 16850: dev acc=0.4378\n",
      "Iter 16900: loss=36.4518, time=2917.64s\n",
      "Evaluation on 1101 elements. Correct: 456\n",
      "iter 16900: dev acc=0.4142\n",
      "Iter 16950: loss=37.1615, time=2926.61s\n",
      "Evaluation on 1101 elements. Correct: 479\n",
      "iter 16950: dev acc=0.4351\n",
      "Iter 17000: loss=36.2250, time=2935.54s\n",
      "Evaluation on 1101 elements. Correct: 485\n",
      "iter 17000: dev acc=0.4405\n",
      "Iter 17050: loss=37.1648, time=2944.35s\n",
      "Evaluation on 1101 elements. Correct: 496\n",
      "iter 17050: dev acc=0.4505\n",
      "Iter 17100: loss=37.2864, time=2953.30s\n",
      "Evaluation on 1101 elements. Correct: 481\n",
      "iter 17100: dev acc=0.4369\n",
      "Iter 17150: loss=36.2353, time=2963.21s\n",
      "Evaluation on 1101 elements. Correct: 482\n",
      "iter 17150: dev acc=0.4378\n",
      "Iter 17200: loss=36.7047, time=2972.09s\n",
      "Evaluation on 1101 elements. Correct: 488\n",
      "iter 17200: dev acc=0.4432\n",
      "Iter 17250: loss=36.2886, time=2980.88s\n",
      "Evaluation on 1101 elements. Correct: 467\n",
      "iter 17250: dev acc=0.4242\n",
      "Iter 17300: loss=36.5962, time=2989.68s\n",
      "Evaluation on 1101 elements. Correct: 485\n",
      "iter 17300: dev acc=0.4405\n",
      "Iter 17350: loss=36.2912, time=2998.40s\n",
      "Evaluation on 1101 elements. Correct: 474\n",
      "iter 17350: dev acc=0.4305\n",
      "Iter 17400: loss=36.1926, time=3007.26s\n",
      "Evaluation on 1101 elements. Correct: 486\n",
      "iter 17400: dev acc=0.4414\n",
      "Iter 17450: loss=36.4410, time=3015.71s\n",
      "Evaluation on 1101 elements. Correct: 483\n",
      "iter 17450: dev acc=0.4387\n",
      "Iter 17500: loss=36.7403, time=3024.20s\n",
      "Evaluation on 1101 elements. Correct: 489\n",
      "iter 17500: dev acc=0.4441\n",
      "Iter 17550: loss=35.9037, time=3032.56s\n",
      "Evaluation on 1101 elements. Correct: 486\n",
      "iter 17550: dev acc=0.4414\n",
      "Iter 17600: loss=36.1191, time=3041.35s\n",
      "Evaluation on 1101 elements. Correct: 488\n",
      "iter 17600: dev acc=0.4432\n",
      "Iter 17650: loss=36.4243, time=3051.51s\n",
      "Evaluation on 1101 elements. Correct: 475\n",
      "iter 17650: dev acc=0.4314\n",
      "Iter 17700: loss=36.6898, time=3059.96s\n",
      "Evaluation on 1101 elements. Correct: 487\n",
      "iter 17700: dev acc=0.4423\n",
      "Iter 17750: loss=36.5059, time=3068.44s\n",
      "Evaluation on 1101 elements. Correct: 506\n",
      "iter 17750: dev acc=0.4596\n",
      "Iter 17800: loss=35.8729, time=3077.05s\n",
      "Evaluation on 1101 elements. Correct: 495\n",
      "iter 17800: dev acc=0.4496\n",
      "Iter 17850: loss=36.1609, time=3085.38s\n",
      "Evaluation on 1101 elements. Correct: 495\n",
      "iter 17850: dev acc=0.4496\n",
      "Iter 17900: loss=36.6381, time=3093.75s\n",
      "Evaluation on 1101 elements. Correct: 479\n",
      "iter 17900: dev acc=0.4351\n",
      "Iter 17950: loss=35.7122, time=3102.40s\n",
      "Evaluation on 1101 elements. Correct: 475\n",
      "iter 17950: dev acc=0.4314\n",
      "Iter 18000: loss=35.2286, time=3110.76s\n",
      "Evaluation on 1101 elements. Correct: 492\n",
      "iter 18000: dev acc=0.4469\n",
      "Iter 18050: loss=36.9520, time=3119.50s\n",
      "Evaluation on 1101 elements. Correct: 500\n",
      "iter 18050: dev acc=0.4541\n",
      "Iter 18100: loss=35.6066, time=3128.31s\n",
      "Evaluation on 1101 elements. Correct: 492\n",
      "iter 18100: dev acc=0.4469\n",
      "Iter 18150: loss=36.7949, time=3137.00s\n",
      "Evaluation on 1101 elements. Correct: 488\n",
      "iter 18150: dev acc=0.4432\n",
      "Iter 18200: loss=35.5168, time=3146.96s\n",
      "Evaluation on 1101 elements. Correct: 478\n",
      "iter 18200: dev acc=0.4342\n",
      "Iter 18250: loss=35.9149, time=3155.58s\n",
      "Evaluation on 1101 elements. Correct: 488\n",
      "iter 18250: dev acc=0.4432\n",
      "Iter 18300: loss=35.6119, time=3164.21s\n",
      "Evaluation on 1101 elements. Correct: 478\n",
      "iter 18300: dev acc=0.4342\n",
      "Iter 18350: loss=36.2624, time=3172.34s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on 1101 elements. Correct: 486\n",
      "iter 18350: dev acc=0.4414\n",
      "Iter 18400: loss=35.4310, time=3180.53s\n",
      "Evaluation on 1101 elements. Correct: 476\n",
      "iter 18400: dev acc=0.4323\n",
      "Iter 18450: loss=35.6047, time=3188.77s\n",
      "Evaluation on 1101 elements. Correct: 499\n",
      "iter 18450: dev acc=0.4532\n",
      "Iter 18500: loss=36.3711, time=3196.95s\n",
      "Evaluation on 1101 elements. Correct: 488\n",
      "iter 18500: dev acc=0.4432\n",
      "Iter 18550: loss=35.4424, time=3205.49s\n",
      "Evaluation on 1101 elements. Correct: 465\n",
      "iter 18550: dev acc=0.4223\n",
      "Iter 18600: loss=36.1381, time=3213.81s\n",
      "Evaluation on 1101 elements. Correct: 477\n",
      "iter 18600: dev acc=0.4332\n",
      "Iter 18650: loss=36.5103, time=3222.40s\n",
      "Evaluation on 1101 elements. Correct: 485\n",
      "iter 18650: dev acc=0.4405\n",
      "Iter 18700: loss=35.7036, time=3232.56s\n",
      "Evaluation on 1101 elements. Correct: 474\n",
      "iter 18700: dev acc=0.4305\n",
      "Iter 18750: loss=34.3560, time=3241.36s\n",
      "Evaluation on 1101 elements. Correct: 468\n",
      "iter 18750: dev acc=0.4251\n",
      "Iter 18800: loss=35.4897, time=3249.98s\n",
      "Evaluation on 1101 elements. Correct: 469\n",
      "iter 18800: dev acc=0.4260\n",
      "Iter 18850: loss=35.9294, time=3258.75s\n",
      "Evaluation on 1101 elements. Correct: 495\n",
      "iter 18850: dev acc=0.4496\n",
      "Iter 18900: loss=36.0524, time=3267.29s\n",
      "Evaluation on 1101 elements. Correct: 495\n",
      "iter 18900: dev acc=0.4496\n",
      "Iter 18950: loss=36.2694, time=3275.95s\n",
      "Evaluation on 1101 elements. Correct: 472\n",
      "iter 18950: dev acc=0.4287\n",
      "Iter 19000: loss=35.1514, time=3284.35s\n",
      "Evaluation on 1101 elements. Correct: 476\n",
      "iter 19000: dev acc=0.4323\n",
      "Iter 19050: loss=35.9960, time=3292.77s\n",
      "Evaluation on 1101 elements. Correct: 488\n",
      "iter 19050: dev acc=0.4432\n",
      "Iter 19100: loss=35.8633, time=3301.35s\n",
      "Evaluation on 1101 elements. Correct: 484\n",
      "iter 19100: dev acc=0.4396\n",
      "Iter 19150: loss=35.4434, time=3309.81s\n",
      "Evaluation on 1101 elements. Correct: 468\n",
      "iter 19150: dev acc=0.4251\n",
      "Iter 19200: loss=35.6234, time=3319.76s\n",
      "Evaluation on 1101 elements. Correct: 492\n",
      "iter 19200: dev acc=0.4469\n",
      "Iter 19250: loss=35.4058, time=3328.01s\n",
      "Evaluation on 1101 elements. Correct: 487\n",
      "iter 19250: dev acc=0.4423\n",
      "Iter 19300: loss=35.3765, time=3336.50s\n",
      "Evaluation on 1101 elements. Correct: 489\n",
      "iter 19300: dev acc=0.4441\n",
      "Iter 19350: loss=35.2405, time=3344.96s\n",
      "Evaluation on 1101 elements. Correct: 484\n",
      "iter 19350: dev acc=0.4396\n",
      "Iter 19400: loss=35.2178, time=3353.30s\n",
      "Evaluation on 1101 elements. Correct: 488\n",
      "iter 19400: dev acc=0.4432\n",
      "Iter 19450: loss=36.3793, time=3361.82s\n",
      "Evaluation on 1101 elements. Correct: 490\n",
      "iter 19450: dev acc=0.4450\n",
      "Iter 19500: loss=35.8787, time=3370.51s\n",
      "Evaluation on 1101 elements. Correct: 456\n",
      "iter 19500: dev acc=0.4142\n",
      "Iter 19550: loss=35.1441, time=3379.01s\n",
      "Evaluation on 1101 elements. Correct: 484\n",
      "iter 19550: dev acc=0.4396\n",
      "Iter 19600: loss=36.0444, time=3387.43s\n",
      "Evaluation on 1101 elements. Correct: 481\n",
      "iter 19600: dev acc=0.4369\n",
      "Iter 19650: loss=35.3839, time=3396.02s\n",
      "Evaluation on 1101 elements. Correct: 483\n",
      "iter 19650: dev acc=0.4387\n",
      "Iter 19700: loss=35.7669, time=3406.36s\n",
      "Evaluation on 1101 elements. Correct: 470\n",
      "iter 19700: dev acc=0.4269\n",
      "Iter 19750: loss=34.9625, time=3414.87s\n",
      "Evaluation on 1101 elements. Correct: 482\n",
      "iter 19750: dev acc=0.4378\n",
      "Iter 19800: loss=35.0582, time=3423.54s\n",
      "Evaluation on 1101 elements. Correct: 474\n",
      "iter 19800: dev acc=0.4305\n",
      "Iter 19850: loss=34.8639, time=3432.27s\n",
      "Evaluation on 1101 elements. Correct: 476\n",
      "iter 19850: dev acc=0.4323\n",
      "Iter 19900: loss=35.4216, time=3440.72s\n",
      "Evaluation on 1101 elements. Correct: 483\n",
      "iter 19900: dev acc=0.4387\n",
      "Iter 19950: loss=34.6854, time=3448.97s\n",
      "Evaluation on 1101 elements. Correct: 481\n",
      "iter 19950: dev acc=0.4369\n",
      "Iter 20000: loss=34.9955, time=3457.45s\n",
      "Evaluation on 1101 elements. Correct: 480\n",
      "iter 20000: dev acc=0.4360\n",
      "Iter 20050: loss=34.8005, time=3465.79s\n",
      "Evaluation on 1101 elements. Correct: 479\n",
      "iter 20050: dev acc=0.4351\n",
      "Iter 20100: loss=34.9893, time=3474.19s\n",
      "Evaluation on 1101 elements. Correct: 469\n",
      "iter 20100: dev acc=0.4260\n",
      "Iter 20150: loss=34.3428, time=3484.26s\n",
      "Evaluation on 1101 elements. Correct: 487\n",
      "iter 20150: dev acc=0.4423\n",
      "Iter 20200: loss=34.9187, time=3492.57s\n",
      "Evaluation on 1101 elements. Correct: 483\n",
      "iter 20200: dev acc=0.4387\n",
      "Iter 20250: loss=35.2136, time=3500.94s\n",
      "Evaluation on 1101 elements. Correct: 499\n",
      "iter 20250: dev acc=0.4532\n",
      "Iter 20300: loss=35.6508, time=3509.31s\n",
      "Evaluation on 1101 elements. Correct: 468\n",
      "iter 20300: dev acc=0.4251\n",
      "Iter 20350: loss=35.6121, time=3517.85s\n",
      "Evaluation on 1101 elements. Correct: 492\n",
      "iter 20350: dev acc=0.4469\n",
      "Iter 20400: loss=34.9938, time=3526.60s\n",
      "Evaluation on 1101 elements. Correct: 482\n",
      "iter 20400: dev acc=0.4378\n",
      "Iter 20450: loss=34.4314, time=3535.16s\n",
      "Evaluation on 1101 elements. Correct: 470\n",
      "iter 20450: dev acc=0.4269\n",
      "Iter 20500: loss=34.6388, time=3543.72s\n",
      "Evaluation on 1101 elements. Correct: 466\n",
      "iter 20500: dev acc=0.4233\n",
      "Iter 20550: loss=35.2442, time=3552.64s\n",
      "Evaluation on 1101 elements. Correct: 475\n",
      "iter 20550: dev acc=0.4314\n",
      "Iter 20600: loss=34.6076, time=3561.19s\n",
      "Evaluation on 1101 elements. Correct: 476\n",
      "iter 20600: dev acc=0.4323\n",
      "Iter 20650: loss=34.8161, time=3571.39s\n",
      "Evaluation on 1101 elements. Correct: 473\n",
      "iter 20650: dev acc=0.4296\n",
      "Iter 20700: loss=35.2682, time=3580.00s\n",
      "Evaluation on 1101 elements. Correct: 472\n",
      "iter 20700: dev acc=0.4287\n",
      "Iter 20750: loss=34.9364, time=3588.47s\n",
      "Evaluation on 1101 elements. Correct: 485\n",
      "iter 20750: dev acc=0.4405\n",
      "Iter 20800: loss=34.8394, time=3597.19s\n",
      "Evaluation on 1101 elements. Correct: 485\n",
      "iter 20800: dev acc=0.4405\n",
      "Iter 20850: loss=34.6376, time=3605.62s\n",
      "Evaluation on 1101 elements. Correct: 479\n",
      "iter 20850: dev acc=0.4351\n",
      "Iter 20900: loss=34.1306, time=3614.18s\n",
      "Evaluation on 1101 elements. Correct: 493\n",
      "iter 20900: dev acc=0.4478\n",
      "Iter 20950: loss=33.9672, time=3623.03s\n",
      "Evaluation on 1101 elements. Correct: 493\n",
      "iter 20950: dev acc=0.4478\n",
      "Iter 21000: loss=34.5016, time=3632.07s\n",
      "Evaluation on 1101 elements. Correct: 491\n",
      "iter 21000: dev acc=0.4460\n",
      "Iter 21050: loss=34.5028, time=3640.48s\n",
      "Evaluation on 1101 elements. Correct: 482\n",
      "iter 21050: dev acc=0.4378\n",
      "Iter 21100: loss=34.9468, time=3649.37s\n",
      "Evaluation on 1101 elements. Correct: 468\n",
      "iter 21100: dev acc=0.4251\n",
      "Iter 21150: loss=34.3418, time=3659.58s\n",
      "Evaluation on 1101 elements. Correct: 476\n",
      "iter 21150: dev acc=0.4323\n",
      "Iter 21200: loss=34.3596, time=3667.31s\n",
      "Evaluation on 1101 elements. Correct: 474\n",
      "iter 21200: dev acc=0.4305\n",
      "Iter 21250: loss=34.8541, time=3674.75s\n",
      "Evaluation on 1101 elements. Correct: 476\n",
      "iter 21250: dev acc=0.4323\n",
      "Iter 21300: loss=33.9368, time=3682.09s\n",
      "Evaluation on 1101 elements. Correct: 472\n",
      "iter 21300: dev acc=0.4287\n",
      "Iter 21350: loss=34.2830, time=3689.49s\n",
      "Evaluation on 1101 elements. Correct: 488\n",
      "iter 21350: dev acc=0.4432\n",
      "Iter 21400: loss=33.9757, time=3697.06s\n",
      "Evaluation on 1101 elements. Correct: 483\n",
      "iter 21400: dev acc=0.4387\n",
      "Iter 21450: loss=33.6724, time=3704.68s\n",
      "Evaluation on 1101 elements. Correct: 484\n",
      "iter 21450: dev acc=0.4396\n",
      "Iter 21500: loss=34.6511, time=3712.09s\n",
      "Evaluation on 1101 elements. Correct: 481\n",
      "iter 21500: dev acc=0.4369\n",
      "Iter 21550: loss=33.7910, time=3719.49s\n",
      "Evaluation on 1101 elements. Correct: 495\n",
      "iter 21550: dev acc=0.4496\n",
      "Iter 21600: loss=34.6759, time=3726.95s\n",
      "Evaluation on 1101 elements. Correct: 490\n",
      "iter 21600: dev acc=0.4450\n",
      "Iter 21650: loss=34.5001, time=3734.40s\n",
      "Evaluation on 1101 elements. Correct: 493\n",
      "iter 21650: dev acc=0.4478\n",
      "Iter 21700: loss=34.0405, time=3743.45s\n",
      "Evaluation on 1101 elements. Correct: 488\n",
      "iter 21700: dev acc=0.4432\n",
      "Iter 21750: loss=34.7609, time=3750.99s\n",
      "Evaluation on 1101 elements. Correct: 482\n",
      "iter 21750: dev acc=0.4378\n",
      "Iter 21800: loss=34.4625, time=3758.42s\n",
      "Evaluation on 1101 elements. Correct: 481\n",
      "iter 21800: dev acc=0.4369\n",
      "Iter 21850: loss=33.6962, time=3766.00s\n",
      "Evaluation on 1101 elements. Correct: 478\n",
      "iter 21850: dev acc=0.4342\n",
      "Iter 21900: loss=35.1597, time=3773.52s\n",
      "Evaluation on 1101 elements. Correct: 470\n",
      "iter 21900: dev acc=0.4269\n",
      "Iter 21950: loss=34.3277, time=3780.83s\n",
      "Evaluation on 1101 elements. Correct: 477\n",
      "iter 21950: dev acc=0.4332\n",
      "Iter 22000: loss=33.5664, time=3788.50s\n",
      "Evaluation on 1101 elements. Correct: 459\n",
      "iter 22000: dev acc=0.4169\n",
      "Iter 22050: loss=34.1894, time=3796.10s\n",
      "Evaluation on 1101 elements. Correct: 478\n",
      "iter 22050: dev acc=0.4342\n",
      "Iter 22100: loss=33.7824, time=3803.62s\n",
      "Evaluation on 1101 elements. Correct: 465\n",
      "iter 22100: dev acc=0.4223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 22150: loss=34.4485, time=3810.95s\n",
      "Evaluation on 1101 elements. Correct: 477\n",
      "iter 22150: dev acc=0.4332\n",
      "Iter 22200: loss=33.1698, time=3819.94s\n",
      "Evaluation on 1101 elements. Correct: 478\n",
      "iter 22200: dev acc=0.4342\n",
      "Iter 22250: loss=33.6401, time=3827.20s\n",
      "Evaluation on 1101 elements. Correct: 475\n",
      "iter 22250: dev acc=0.4314\n",
      "Iter 22300: loss=33.7717, time=3834.62s\n",
      "Evaluation on 1101 elements. Correct: 480\n",
      "iter 22300: dev acc=0.4360\n",
      "Iter 22350: loss=33.2753, time=3842.23s\n",
      "Evaluation on 1101 elements. Correct: 475\n",
      "iter 22350: dev acc=0.4314\n",
      "Iter 22400: loss=34.7099, time=3849.69s\n",
      "Evaluation on 1101 elements. Correct: 474\n",
      "iter 22400: dev acc=0.4305\n",
      "Iter 22450: loss=34.2456, time=3857.41s\n",
      "Evaluation on 1101 elements. Correct: 477\n",
      "iter 22450: dev acc=0.4332\n",
      "Iter 22500: loss=33.4026, time=3865.88s\n",
      "Evaluation on 1101 elements. Correct: 482\n",
      "iter 22500: dev acc=0.4378\n",
      "Iter 22550: loss=34.1761, time=3874.53s\n",
      "Evaluation on 1101 elements. Correct: 479\n",
      "iter 22550: dev acc=0.4351\n",
      "Iter 22600: loss=34.6544, time=3883.19s\n",
      "Evaluation on 1101 elements. Correct: 488\n",
      "iter 22600: dev acc=0.4432\n",
      "Iter 22650: loss=33.0252, time=3891.55s\n",
      "Evaluation on 1101 elements. Correct: 480\n",
      "iter 22650: dev acc=0.4360\n",
      "Iter 22700: loss=32.8644, time=3901.76s\n",
      "Evaluation on 1101 elements. Correct: 471\n",
      "iter 22700: dev acc=0.4278\n",
      "Iter 22750: loss=33.5859, time=3910.42s\n",
      "Evaluation on 1101 elements. Correct: 465\n",
      "iter 22750: dev acc=0.4223\n",
      "Iter 22800: loss=34.1099, time=3919.25s\n",
      "Evaluation on 1101 elements. Correct: 467\n",
      "iter 22800: dev acc=0.4242\n",
      "Iter 22850: loss=33.5258, time=3927.87s\n",
      "Evaluation on 1101 elements. Correct: 466\n",
      "iter 22850: dev acc=0.4233\n",
      "Iter 22900: loss=32.7804, time=3936.68s\n",
      "Evaluation on 1101 elements. Correct: 475\n",
      "iter 22900: dev acc=0.4314\n",
      "Iter 22950: loss=34.1359, time=3945.34s\n",
      "Evaluation on 1101 elements. Correct: 494\n",
      "iter 22950: dev acc=0.4487\n",
      "Iter 23000: loss=33.1435, time=3953.88s\n",
      "Evaluation on 1101 elements. Correct: 473\n",
      "iter 23000: dev acc=0.4296\n",
      "Iter 23050: loss=33.0071, time=3962.52s\n",
      "Evaluation on 1101 elements. Correct: 487\n",
      "iter 23050: dev acc=0.4423\n",
      "Iter 23100: loss=33.4052, time=3971.13s\n",
      "Evaluation on 1101 elements. Correct: 479\n",
      "iter 23100: dev acc=0.4351\n",
      "Iter 23150: loss=33.2125, time=3979.66s\n",
      "Evaluation on 1101 elements. Correct: 470\n",
      "iter 23150: dev acc=0.4269\n",
      "Iter 23200: loss=33.4844, time=3989.67s\n",
      "Evaluation on 1101 elements. Correct: 471\n",
      "iter 23200: dev acc=0.4278\n",
      "Iter 23250: loss=33.0507, time=3998.31s\n",
      "Evaluation on 1101 elements. Correct: 470\n",
      "iter 23250: dev acc=0.4269\n",
      "Iter 23300: loss=33.3440, time=4006.87s\n",
      "Evaluation on 1101 elements. Correct: 482\n",
      "iter 23300: dev acc=0.4378\n",
      "Iter 23350: loss=33.1264, time=4015.88s\n",
      "Evaluation on 1101 elements. Correct: 477\n",
      "iter 23350: dev acc=0.4332\n",
      "Iter 23400: loss=33.5658, time=4024.60s\n",
      "Evaluation on 1101 elements. Correct: 481\n",
      "iter 23400: dev acc=0.4369\n",
      "Iter 23450: loss=34.0578, time=4032.95s\n",
      "Evaluation on 1101 elements. Correct: 483\n",
      "iter 23450: dev acc=0.4387\n",
      "Iter 23500: loss=33.2252, time=4041.61s\n",
      "Evaluation on 1101 elements. Correct: 481\n",
      "iter 23500: dev acc=0.4369\n",
      "Iter 23550: loss=33.1329, time=4049.99s\n",
      "Evaluation on 1101 elements. Correct: 475\n",
      "iter 23550: dev acc=0.4314\n",
      "Iter 23600: loss=33.3393, time=4058.57s\n",
      "Evaluation on 1101 elements. Correct: 471\n",
      "iter 23600: dev acc=0.4278\n",
      "Iter 23650: loss=33.2276, time=4067.04s\n",
      "Evaluation on 1101 elements. Correct: 481\n",
      "iter 23650: dev acc=0.4369\n",
      "Iter 23700: loss=33.4489, time=4077.18s\n",
      "Evaluation on 1101 elements. Correct: 484\n",
      "iter 23700: dev acc=0.4396\n",
      "Iter 23750: loss=34.0224, time=4085.41s\n",
      "Evaluation on 1101 elements. Correct: 484\n",
      "iter 23750: dev acc=0.4396\n",
      "Iter 23800: loss=32.7856, time=4093.65s\n",
      "Evaluation on 1101 elements. Correct: 478\n",
      "iter 23800: dev acc=0.4342\n",
      "Iter 23850: loss=33.0756, time=4102.06s\n",
      "Evaluation on 1101 elements. Correct: 471\n",
      "iter 23850: dev acc=0.4278\n",
      "Iter 23900: loss=32.2396, time=4109.78s\n",
      "Evaluation on 1101 elements. Correct: 473\n",
      "iter 23900: dev acc=0.4296\n",
      "Iter 23950: loss=34.0090, time=4117.22s\n",
      "Evaluation on 1101 elements. Correct: 474\n",
      "iter 23950: dev acc=0.4305\n",
      "Iter 24000: loss=33.3589, time=4124.59s\n",
      "Evaluation on 1101 elements. Correct: 488\n",
      "iter 24000: dev acc=0.4432\n",
      "Iter 24050: loss=33.2318, time=4131.88s\n",
      "Evaluation on 1101 elements. Correct: 478\n",
      "iter 24050: dev acc=0.4342\n",
      "Iter 24100: loss=32.7094, time=4139.22s\n",
      "Evaluation on 1101 elements. Correct: 483\n",
      "iter 24100: dev acc=0.4387\n",
      "Iter 24150: loss=32.9702, time=4146.69s\n",
      "Evaluation on 1101 elements. Correct: 456\n",
      "iter 24150: dev acc=0.4142\n",
      "Iter 24200: loss=33.1117, time=4155.68s\n",
      "Evaluation on 1101 elements. Correct: 486\n",
      "iter 24200: dev acc=0.4414\n",
      "Iter 24250: loss=34.0239, time=4162.97s\n",
      "Evaluation on 1101 elements. Correct: 465\n",
      "iter 24250: dev acc=0.4223\n",
      "Iter 24300: loss=32.8084, time=4170.45s\n",
      "Evaluation on 1101 elements. Correct: 475\n",
      "iter 24300: dev acc=0.4314\n",
      "Iter 24350: loss=32.7452, time=4177.76s\n",
      "Evaluation on 1101 elements. Correct: 477\n",
      "iter 24350: dev acc=0.4332\n",
      "Iter 24400: loss=32.5874, time=4185.19s\n",
      "Evaluation on 1101 elements. Correct: 463\n",
      "iter 24400: dev acc=0.4205\n",
      "Iter 24450: loss=32.7583, time=4192.67s\n",
      "Evaluation on 1101 elements. Correct: 470\n",
      "iter 24450: dev acc=0.4269\n",
      "Iter 24500: loss=32.9482, time=4199.90s\n",
      "Evaluation on 1101 elements. Correct: 463\n",
      "iter 24500: dev acc=0.4205\n",
      "Iter 24550: loss=32.8875, time=4207.29s\n",
      "Evaluation on 1101 elements. Correct: 476\n",
      "iter 24550: dev acc=0.4323\n",
      "Iter 24600: loss=32.7314, time=4214.60s\n",
      "Evaluation on 1101 elements. Correct: 462\n",
      "iter 24600: dev acc=0.4196\n",
      "Iter 24650: loss=32.4572, time=4222.08s\n",
      "Evaluation on 1101 elements. Correct: 476\n",
      "iter 24650: dev acc=0.4323\n",
      "Iter 24700: loss=32.5994, time=4230.94s\n",
      "Evaluation on 1101 elements. Correct: 465\n",
      "iter 24700: dev acc=0.4223\n",
      "Iter 24750: loss=32.6567, time=4238.43s\n",
      "Evaluation on 1101 elements. Correct: 471\n",
      "iter 24750: dev acc=0.4278\n",
      "Iter 24800: loss=31.7030, time=4245.57s\n",
      "Evaluation on 1101 elements. Correct: 485\n",
      "iter 24800: dev acc=0.4405\n",
      "Iter 24850: loss=32.9110, time=4253.12s\n",
      "Evaluation on 1101 elements. Correct: 464\n",
      "iter 24850: dev acc=0.4214\n",
      "Iter 24900: loss=32.7779, time=4260.54s\n",
      "Evaluation on 1101 elements. Correct: 476\n",
      "iter 24900: dev acc=0.4323\n",
      "Iter 24950: loss=31.7938, time=4268.14s\n",
      "Evaluation on 1101 elements. Correct: 470\n",
      "iter 24950: dev acc=0.4269\n",
      "Iter 25000: loss=31.9097, time=4275.86s\n",
      "Evaluation on 1101 elements. Correct: 480\n",
      "iter 25000: dev acc=0.4360\n",
      "Iter 25050: loss=33.0691, time=4284.49s\n",
      "Evaluation on 1101 elements. Correct: 475\n",
      "iter 25050: dev acc=0.4314\n",
      "Iter 25100: loss=32.8293, time=4292.92s\n",
      "Evaluation on 1101 elements. Correct: 484\n",
      "iter 25100: dev acc=0.4396\n",
      "Iter 25150: loss=31.5108, time=4301.67s\n",
      "Evaluation on 1101 elements. Correct: 454\n",
      "iter 25150: dev acc=0.4124\n",
      "Iter 25200: loss=32.0568, time=4311.72s\n",
      "Evaluation on 1101 elements. Correct: 479\n",
      "iter 25200: dev acc=0.4351\n",
      "Iter 25250: loss=32.3124, time=4320.47s\n",
      "Evaluation on 1101 elements. Correct: 463\n",
      "iter 25250: dev acc=0.4205\n",
      "Iter 25300: loss=32.7321, time=4329.15s\n",
      "Evaluation on 1101 elements. Correct: 468\n",
      "iter 25300: dev acc=0.4251\n",
      "Iter 25350: loss=32.5259, time=4337.96s\n",
      "Evaluation on 1101 elements. Correct: 477\n",
      "iter 25350: dev acc=0.4332\n",
      "Iter 25400: loss=32.5025, time=4346.46s\n",
      "Evaluation on 1101 elements. Correct: 474\n",
      "iter 25400: dev acc=0.4305\n",
      "Iter 25450: loss=32.6071, time=4355.10s\n",
      "Evaluation on 1101 elements. Correct: 472\n",
      "iter 25450: dev acc=0.4287\n",
      "Iter 25500: loss=32.0597, time=4363.60s\n",
      "Evaluation on 1101 elements. Correct: 479\n",
      "iter 25500: dev acc=0.4351\n",
      "Iter 25550: loss=33.2165, time=4371.97s\n",
      "Evaluation on 1101 elements. Correct: 480\n",
      "iter 25550: dev acc=0.4360\n",
      "Iter 25600: loss=32.2435, time=4380.29s\n",
      "Evaluation on 1101 elements. Correct: 481\n",
      "iter 25600: dev acc=0.4369\n",
      "Iter 25650: loss=31.6606, time=4388.82s\n",
      "Evaluation on 1101 elements. Correct: 466\n",
      "iter 25650: dev acc=0.4233\n",
      "Iter 25700: loss=32.8765, time=4398.97s\n",
      "Evaluation on 1101 elements. Correct: 470\n",
      "iter 25700: dev acc=0.4269\n",
      "Iter 25750: loss=31.7307, time=4407.37s\n",
      "Evaluation on 1101 elements. Correct: 469\n",
      "iter 25750: dev acc=0.4260\n",
      "Iter 25800: loss=31.6506, time=4415.76s\n",
      "Evaluation on 1101 elements. Correct: 459\n",
      "iter 25800: dev acc=0.4169\n",
      "Iter 25850: loss=32.4200, time=4423.98s\n",
      "Evaluation on 1101 elements. Correct: 475\n",
      "iter 25850: dev acc=0.4314\n",
      "Iter 25900: loss=30.9915, time=4432.54s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on 1101 elements. Correct: 452\n",
      "iter 25900: dev acc=0.4105\n",
      "Iter 25950: loss=31.7960, time=4441.06s\n",
      "Evaluation on 1101 elements. Correct: 467\n",
      "iter 25950: dev acc=0.4242\n",
      "Iter 26000: loss=32.1268, time=4449.65s\n",
      "Evaluation on 1101 elements. Correct: 482\n",
      "iter 26000: dev acc=0.4378\n",
      "Iter 26050: loss=33.1413, time=4458.22s\n",
      "Evaluation on 1101 elements. Correct: 466\n",
      "iter 26050: dev acc=0.4233\n",
      "Iter 26100: loss=31.7784, time=4466.76s\n",
      "Evaluation on 1101 elements. Correct: 479\n",
      "iter 26100: dev acc=0.4351\n",
      "Iter 26150: loss=31.5037, time=4475.62s\n",
      "Evaluation on 1101 elements. Correct: 471\n",
      "iter 26150: dev acc=0.4278\n",
      "Iter 26200: loss=31.8379, time=4485.79s\n",
      "Evaluation on 1101 elements. Correct: 470\n",
      "iter 26200: dev acc=0.4269\n",
      "Iter 26250: loss=31.1240, time=4494.29s\n",
      "Evaluation on 1101 elements. Correct: 468\n",
      "iter 26250: dev acc=0.4251\n",
      "Iter 26300: loss=32.0618, time=4502.91s\n",
      "Evaluation on 1101 elements. Correct: 464\n",
      "iter 26300: dev acc=0.4214\n",
      "Iter 26350: loss=32.2313, time=4511.43s\n",
      "Evaluation on 1101 elements. Correct: 460\n",
      "iter 26350: dev acc=0.4178\n",
      "Iter 26400: loss=31.9735, time=4520.15s\n",
      "Evaluation on 1101 elements. Correct: 462\n",
      "iter 26400: dev acc=0.4196\n",
      "Iter 26450: loss=32.1379, time=4528.97s\n",
      "Evaluation on 1101 elements. Correct: 466\n",
      "iter 26450: dev acc=0.4233\n",
      "Iter 26500: loss=31.3807, time=4537.59s\n",
      "Evaluation on 1101 elements. Correct: 472\n",
      "iter 26500: dev acc=0.4287\n",
      "Iter 26550: loss=32.0355, time=4546.39s\n",
      "Evaluation on 1101 elements. Correct: 472\n",
      "iter 26550: dev acc=0.4287\n",
      "Iter 26600: loss=31.3141, time=4555.06s\n",
      "Evaluation on 1101 elements. Correct: 473\n",
      "iter 26600: dev acc=0.4296\n",
      "Iter 26650: loss=32.2195, time=4563.65s\n",
      "Evaluation on 1101 elements. Correct: 479\n",
      "iter 26650: dev acc=0.4351\n",
      "Iter 26700: loss=32.7390, time=4572.31s\n",
      "Evaluation on 1101 elements. Correct: 485\n",
      "iter 26700: dev acc=0.4405\n",
      "Iter 26750: loss=31.1193, time=4582.73s\n",
      "Evaluation on 1101 elements. Correct: 474\n",
      "iter 26750: dev acc=0.4305\n",
      "Iter 26800: loss=31.4939, time=4591.18s\n",
      "Evaluation on 1101 elements. Correct: 480\n",
      "iter 26800: dev acc=0.4360\n",
      "Iter 26850: loss=30.9014, time=4599.76s\n",
      "Evaluation on 1101 elements. Correct: 479\n",
      "iter 26850: dev acc=0.4351\n",
      "Iter 26900: loss=32.6097, time=4607.99s\n",
      "Evaluation on 1101 elements. Correct: 482\n",
      "iter 26900: dev acc=0.4378\n",
      "Iter 26950: loss=32.4446, time=4616.36s\n",
      "Evaluation on 1101 elements. Correct: 469\n",
      "iter 26950: dev acc=0.4260\n",
      "Iter 27000: loss=32.1228, time=4624.70s\n",
      "Evaluation on 1101 elements. Correct: 495\n",
      "iter 27000: dev acc=0.4496\n",
      "Iter 27050: loss=31.1110, time=4633.12s\n",
      "Evaluation on 1101 elements. Correct: 488\n",
      "iter 27050: dev acc=0.4432\n",
      "Iter 27100: loss=31.6236, time=4641.66s\n",
      "Evaluation on 1101 elements. Correct: 471\n",
      "iter 27100: dev acc=0.4278\n",
      "Iter 27150: loss=31.5805, time=4650.37s\n",
      "Evaluation on 1101 elements. Correct: 482\n",
      "iter 27150: dev acc=0.4378\n",
      "Iter 27200: loss=31.2205, time=4659.05s\n",
      "Evaluation on 1101 elements. Correct: 484\n",
      "iter 27200: dev acc=0.4396\n",
      "Iter 27250: loss=31.3293, time=4668.87s\n",
      "Evaluation on 1101 elements. Correct: 470\n",
      "iter 27250: dev acc=0.4269\n",
      "Iter 27300: loss=31.4149, time=4677.42s\n",
      "Evaluation on 1101 elements. Correct: 465\n",
      "iter 27300: dev acc=0.4223\n",
      "Iter 27350: loss=31.0301, time=4686.29s\n",
      "Evaluation on 1101 elements. Correct: 461\n",
      "iter 27350: dev acc=0.4187\n",
      "Iter 27400: loss=32.2761, time=4694.72s\n",
      "Evaluation on 1101 elements. Correct: 470\n",
      "iter 27400: dev acc=0.4269\n",
      "Iter 27450: loss=31.3969, time=4703.27s\n",
      "Evaluation on 1101 elements. Correct: 469\n",
      "iter 27450: dev acc=0.4260\n",
      "Iter 27500: loss=31.8559, time=4711.47s\n",
      "Evaluation on 1101 elements. Correct: 483\n",
      "iter 27500: dev acc=0.4387\n",
      "Iter 27550: loss=31.6753, time=4720.03s\n",
      "Evaluation on 1101 elements. Correct: 488\n",
      "iter 27550: dev acc=0.4432\n",
      "Iter 27600: loss=30.8997, time=4728.72s\n",
      "Evaluation on 1101 elements. Correct: 468\n",
      "iter 27600: dev acc=0.4251\n",
      "Iter 27650: loss=31.3833, time=4737.36s\n",
      "Evaluation on 1101 elements. Correct: 474\n",
      "iter 27650: dev acc=0.4305\n",
      "Iter 27700: loss=31.0374, time=4746.20s\n",
      "Evaluation on 1101 elements. Correct: 469\n",
      "iter 27700: dev acc=0.4260\n",
      "Iter 27750: loss=31.4629, time=4755.46s\n",
      "Evaluation on 1101 elements. Correct: 456\n",
      "iter 27750: dev acc=0.4142\n",
      "Iter 27800: loss=31.7908, time=4763.71s\n",
      "Evaluation on 1101 elements. Correct: 468\n",
      "iter 27800: dev acc=0.4251\n",
      "Iter 27850: loss=31.4587, time=4772.34s\n",
      "Evaluation on 1101 elements. Correct: 473\n",
      "iter 27850: dev acc=0.4296\n",
      "Iter 27900: loss=30.5727, time=4781.04s\n",
      "Evaluation on 1101 elements. Correct: 471\n",
      "iter 27900: dev acc=0.4278\n",
      "Iter 27950: loss=30.9857, time=4789.68s\n",
      "Evaluation on 1101 elements. Correct: 478\n",
      "iter 27950: dev acc=0.4342\n",
      "Iter 28000: loss=31.0521, time=4798.35s\n",
      "Evaluation on 1101 elements. Correct: 484\n",
      "iter 28000: dev acc=0.4396\n",
      "Iter 28050: loss=31.9056, time=4806.85s\n",
      "Evaluation on 1101 elements. Correct: 478\n",
      "iter 28050: dev acc=0.4342\n",
      "Iter 28100: loss=31.2642, time=4815.47s\n",
      "Evaluation on 1101 elements. Correct: 477\n",
      "iter 28100: dev acc=0.4332\n",
      "Iter 28150: loss=31.2454, time=4824.01s\n",
      "Evaluation on 1101 elements. Correct: 463\n",
      "iter 28150: dev acc=0.4205\n",
      "Iter 28200: loss=30.7092, time=4832.37s\n",
      "Evaluation on 1101 elements. Correct: 471\n",
      "iter 28200: dev acc=0.4278\n",
      "Iter 28250: loss=30.7214, time=4842.34s\n",
      "Evaluation on 1101 elements. Correct: 464\n",
      "iter 28250: dev acc=0.4214\n",
      "Iter 28300: loss=30.6784, time=4851.00s\n",
      "Evaluation on 1101 elements. Correct: 468\n",
      "iter 28300: dev acc=0.4251\n",
      "Iter 28350: loss=29.9926, time=4859.41s\n",
      "Evaluation on 1101 elements. Correct: 476\n",
      "iter 28350: dev acc=0.4323\n",
      "Iter 28400: loss=30.3457, time=4867.77s\n",
      "Evaluation on 1101 elements. Correct: 472\n",
      "iter 28400: dev acc=0.4287\n",
      "Iter 28450: loss=31.4489, time=4876.56s\n",
      "Evaluation on 1101 elements. Correct: 470\n",
      "iter 28450: dev acc=0.4269\n",
      "Iter 28500: loss=31.4996, time=4885.31s\n",
      "Evaluation on 1101 elements. Correct: 474\n",
      "iter 28500: dev acc=0.4305\n",
      "Iter 28550: loss=31.4918, time=4894.19s\n",
      "Evaluation on 1101 elements. Correct: 461\n",
      "iter 28550: dev acc=0.4187\n",
      "Iter 28600: loss=30.9468, time=4903.09s\n",
      "Evaluation on 1101 elements. Correct: 471\n",
      "iter 28600: dev acc=0.4278\n",
      "Iter 28650: loss=30.3629, time=4911.70s\n",
      "Evaluation on 1101 elements. Correct: 460\n",
      "iter 28650: dev acc=0.4178\n",
      "Iter 28700: loss=31.5004, time=4920.46s\n",
      "Evaluation on 1101 elements. Correct: 459\n",
      "iter 28700: dev acc=0.4169\n",
      "Iter 28750: loss=30.4324, time=4930.69s\n",
      "Evaluation on 1101 elements. Correct: 481\n",
      "iter 28750: dev acc=0.4369\n",
      "Iter 28800: loss=30.9590, time=4939.31s\n",
      "Evaluation on 1101 elements. Correct: 477\n",
      "iter 28800: dev acc=0.4332\n",
      "Iter 28850: loss=31.5857, time=4947.68s\n",
      "Evaluation on 1101 elements. Correct: 483\n",
      "iter 28850: dev acc=0.4387\n",
      "Iter 28900: loss=30.5010, time=4955.88s\n",
      "Evaluation on 1101 elements. Correct: 461\n",
      "iter 28900: dev acc=0.4187\n",
      "Iter 28950: loss=29.4605, time=4964.31s\n",
      "Evaluation on 1101 elements. Correct: 472\n",
      "iter 28950: dev acc=0.4287\n",
      "Iter 29000: loss=31.4151, time=4972.55s\n",
      "Evaluation on 1101 elements. Correct: 467\n",
      "iter 29000: dev acc=0.4242\n",
      "Iter 29050: loss=30.2329, time=4981.03s\n",
      "Evaluation on 1101 elements. Correct: 474\n",
      "iter 29050: dev acc=0.4305\n",
      "Iter 29100: loss=31.7703, time=4989.40s\n",
      "Evaluation on 1101 elements. Correct: 481\n",
      "iter 29100: dev acc=0.4369\n",
      "Iter 29150: loss=30.7300, time=4998.10s\n",
      "Evaluation on 1101 elements. Correct: 468\n",
      "iter 29150: dev acc=0.4251\n",
      "Iter 29200: loss=30.9390, time=5006.35s\n",
      "Evaluation on 1101 elements. Correct: 458\n",
      "iter 29200: dev acc=0.4160\n",
      "Iter 29250: loss=31.0742, time=5016.32s\n",
      "Evaluation on 1101 elements. Correct: 452\n",
      "iter 29250: dev acc=0.4105\n",
      "Iter 29300: loss=30.5178, time=5024.81s\n",
      "Evaluation on 1101 elements. Correct: 468\n",
      "iter 29300: dev acc=0.4251\n",
      "Iter 29350: loss=30.1664, time=5033.17s\n",
      "Evaluation on 1101 elements. Correct: 468\n",
      "iter 29350: dev acc=0.4251\n",
      "Iter 29400: loss=30.5206, time=5041.61s\n",
      "Evaluation on 1101 elements. Correct: 481\n",
      "iter 29400: dev acc=0.4369\n",
      "Iter 29450: loss=30.1794, time=5050.22s\n",
      "Evaluation on 1101 elements. Correct: 476\n",
      "iter 29450: dev acc=0.4323\n",
      "Iter 29500: loss=29.8822, time=5058.97s\n",
      "Evaluation on 1101 elements. Correct: 484\n",
      "iter 29500: dev acc=0.4396\n",
      "Iter 29550: loss=30.5532, time=5067.81s\n",
      "Evaluation on 1101 elements. Correct: 471\n",
      "iter 29550: dev acc=0.4278\n",
      "Iter 29600: loss=30.7226, time=5076.56s\n",
      "Evaluation on 1101 elements. Correct: 476\n",
      "iter 29600: dev acc=0.4323\n",
      "Iter 29650: loss=30.0412, time=5084.99s\n",
      "Evaluation on 1101 elements. Correct: 462\n",
      "iter 29650: dev acc=0.4196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 29700: loss=30.0932, time=5093.67s\n",
      "Evaluation on 1101 elements. Correct: 482\n",
      "iter 29700: dev acc=0.4378\n",
      "Iter 29750: loss=30.3739, time=5103.90s\n",
      "Evaluation on 1101 elements. Correct: 467\n",
      "iter 29750: dev acc=0.4242\n",
      "Iter 29800: loss=30.1632, time=5112.77s\n",
      "Evaluation on 1101 elements. Correct: 476\n",
      "iter 29800: dev acc=0.4323\n",
      "Iter 29850: loss=30.2032, time=5121.40s\n",
      "Evaluation on 1101 elements. Correct: 482\n",
      "iter 29850: dev acc=0.4378\n",
      "Iter 29900: loss=30.3255, time=5130.23s\n",
      "Evaluation on 1101 elements. Correct: 463\n",
      "iter 29900: dev acc=0.4205\n",
      "Iter 29950: loss=29.3973, time=5138.81s\n",
      "Evaluation on 1101 elements. Correct: 477\n",
      "iter 29950: dev acc=0.4332\n",
      "Iter 30000: loss=30.0759, time=5147.72s\n",
      "Evaluation on 1101 elements. Correct: 473\n",
      "iter 30000: dev acc=0.4296\n",
      "Done training\n",
      "Loading best model\n",
      "Evaluation on 8544 elements. Correct: 4578\n",
      "Evaluation on 1101 elements. Correct: 544\n",
      "Evaluation on 2210 elements. Correct: 1048\n",
      "best model iter 6300: train acc=0.5358, dev acc=0.4941, test acc=0.4742\n"
     ]
    }
   ],
   "source": [
    "for _, depth_dict in all_subtrees.items():\n",
    "    for subtree in depth_dict:\n",
    "        subtree.loss.clear()\n",
    "\n",
    "tree_model = TreeLSTMClassifier(\n",
    "    len(v.w2i), 300, 150, len(t2i), v)\n",
    "\n",
    "with torch.no_grad():\n",
    "    tree_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
    "    tree_model.embed.weight.requires_grad = False\n",
    "  \n",
    "def do_train(model):\n",
    "\n",
    "    print(model)\n",
    "    print_parameters(model)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    return train_tree_model(\n",
    "        model, optimizer, num_iterations=30000, \n",
    "        print_every=50, eval_every=50,\n",
    "        prep_fn=prepare_treelstm_minibatch,\n",
    "        eval_fn=evaluate,\n",
    "        batch_fn=get_minibatch,\n",
    "        batch_size=32, eval_batch_size=64)\n",
    "\n",
    "DEBUG_TREE_PRINT = False\n",
    "results_small_batch = do_train(tree_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TreeLSTMClassifier(\n",
      "  (embed): Embedding(20727, 300, padding_idx=1)\n",
      "  (treelstm): TreeLSTM(\n",
      "    (reduce): TreeLSTMCell(300, 300)\n",
      "    (proj_x): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (proj_x_gate): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (buffers_dropout): Dropout(p=0.5)\n",
      "  )\n",
      "  (output_layer): Sequential(\n",
      "    (0): Dropout(p=0.25)\n",
      "    (1): Linear(in_features=300, out_features=5, bias=True)\n",
      "    (2): LogSoftmax()\n",
      "  )\n",
      ")\n",
      "embed.weight             [20727, 300] requires_grad=False\n",
      "treelstm.reduce.attent_module.output_layer.fc1.weight [300, 600]   requires_grad=True\n",
      "treelstm.reduce.attent_module.output_layer.fc1.bias [300]        requires_grad=True\n",
      "treelstm.reduce.attent_module.output_layer.fc2.weight [2, 300]     requires_grad=True\n",
      "treelstm.reduce.attent_module.output_layer.fc2.bias [2]          requires_grad=True\n",
      "treelstm.reduce.reduce_layer.weight [1500, 600]  requires_grad=True\n",
      "treelstm.reduce.reduce_layer.bias [1500]       requires_grad=True\n",
      "treelstm.proj_x.weight   [300, 300]   requires_grad=True\n",
      "treelstm.proj_x.bias     [300]        requires_grad=True\n",
      "treelstm.proj_x_gate.weight [300, 300]   requires_grad=True\n",
      "treelstm.proj_x_gate.bias [300]        requires_grad=True\n",
      "output_layer.1.weight    [5, 300]     requires_grad=True\n",
      "output_layer.1.bias      [5]          requires_grad=True\n",
      "\n",
      "Total parameters: 7482607\n",
      "\n",
      "Iter 50: loss=80.3210, time=6.90s\n",
      "Iter 100: loss=76.3220, time=13.90s\n",
      "Iter 150: loss=72.1583, time=20.85s\n",
      "Iter 200: loss=67.1129, time=27.84s\n",
      "Evaluation on 1101 elements. Correct: 386\n",
      "iter 200: dev acc=0.3506\n",
      "new highscore\n",
      "Iter 250: loss=63.4993, time=35.91s\n",
      "Iter 300: loss=61.4329, time=42.94s\n",
      "Iter 350: loss=59.2196, time=49.92s\n",
      "Iter 400: loss=58.1125, time=57.18s\n",
      "Evaluation on 1101 elements. Correct: 446\n",
      "iter 400: dev acc=0.4051\n",
      "new highscore\n",
      "Iter 450: loss=56.9601, time=65.28s\n",
      "Iter 500: loss=56.7836, time=73.07s\n",
      "Iter 550: loss=54.7204, time=81.40s\n",
      "Iter 600: loss=55.5320, time=91.15s\n",
      "Evaluation on 1101 elements. Correct: 458\n",
      "iter 600: dev acc=0.4160\n",
      "new highscore\n",
      "Iter 650: loss=54.2628, time=100.50s\n",
      "Iter 700: loss=53.5870, time=108.41s\n",
      "Iter 750: loss=53.5498, time=116.73s\n",
      "Iter 800: loss=52.8655, time=124.93s\n",
      "Evaluation on 1101 elements. Correct: 469\n",
      "iter 800: dev acc=0.4260\n",
      "new highscore\n",
      "Iter 850: loss=52.6461, time=134.12s\n",
      "Iter 900: loss=52.6180, time=142.14s\n",
      "Iter 950: loss=52.3196, time=150.46s\n",
      "Iter 1000: loss=52.0876, time=158.85s\n",
      "Evaluation on 1101 elements. Correct: 479\n",
      "iter 1000: dev acc=0.4351\n",
      "new highscore\n",
      "Iter 1050: loss=51.3523, time=168.08s\n",
      "Iter 1100: loss=50.9460, time=176.26s\n",
      "Iter 1150: loss=50.6533, time=184.56s\n",
      "Iter 1200: loss=50.4650, time=192.64s\n",
      "Evaluation on 1101 elements. Correct: 491\n",
      "iter 1200: dev acc=0.4460\n",
      "new highscore\n",
      "Iter 1250: loss=49.8576, time=201.78s\n",
      "Iter 1300: loss=50.7351, time=209.80s\n",
      "Iter 1350: loss=50.0181, time=218.13s\n",
      "Iter 1400: loss=49.5359, time=226.12s\n",
      "Evaluation on 1101 elements. Correct: 492\n",
      "iter 1400: dev acc=0.4469\n",
      "new highscore\n",
      "Iter 1450: loss=50.2421, time=235.31s\n",
      "Iter 1500: loss=48.6075, time=244.89s\n",
      "Iter 1550: loss=49.2528, time=252.99s\n",
      "Iter 1600: loss=48.9607, time=260.69s\n",
      "Evaluation on 1101 elements. Correct: 503\n",
      "iter 1600: dev acc=0.4569\n",
      "new highscore\n",
      "Iter 1650: loss=48.4168, time=269.87s\n",
      "Iter 1700: loss=48.0772, time=278.39s\n",
      "Iter 1750: loss=49.0816, time=286.64s\n",
      "Iter 1800: loss=48.1158, time=294.98s\n",
      "Evaluation on 1101 elements. Correct: 502\n",
      "iter 1800: dev acc=0.4559\n",
      "Iter 1850: loss=48.7339, time=304.19s\n",
      "Iter 1900: loss=47.6845, time=312.30s\n",
      "Iter 1950: loss=47.7800, time=320.31s\n",
      "Iter 2000: loss=48.1033, time=328.50s\n",
      "Evaluation on 1101 elements. Correct: 506\n",
      "iter 2000: dev acc=0.4596\n",
      "new highscore\n",
      "Iter 2050: loss=47.5638, time=337.38s\n",
      "Iter 2100: loss=47.6743, time=345.60s\n",
      "Iter 2150: loss=47.3205, time=353.62s\n",
      "Iter 2200: loss=47.4444, time=361.89s\n",
      "Evaluation on 1101 elements. Correct: 510\n",
      "iter 2200: dev acc=0.4632\n",
      "new highscore\n",
      "Iter 2250: loss=46.9960, time=370.94s\n",
      "Iter 2300: loss=46.1567, time=378.91s\n",
      "Iter 2350: loss=47.5652, time=386.83s\n",
      "Iter 2400: loss=47.4887, time=394.95s\n",
      "Evaluation on 1101 elements. Correct: 522\n",
      "iter 2400: dev acc=0.4741\n",
      "new highscore\n",
      "Iter 2450: loss=46.9395, time=405.69s\n",
      "Iter 2500: loss=46.1080, time=413.87s\n",
      "Iter 2550: loss=46.6764, time=422.11s\n",
      "Iter 2600: loss=46.7819, time=429.62s\n",
      "Evaluation on 1101 elements. Correct: 534\n",
      "iter 2600: dev acc=0.4850\n",
      "new highscore\n",
      "Iter 2650: loss=47.1356, time=438.88s\n",
      "Iter 2700: loss=45.6253, time=447.01s\n",
      "Iter 2750: loss=46.1739, time=455.32s\n",
      "Iter 2800: loss=46.1100, time=463.54s\n",
      "Evaluation on 1101 elements. Correct: 523\n",
      "iter 2800: dev acc=0.4750\n",
      "Iter 2850: loss=45.4140, time=472.58s\n",
      "Iter 2900: loss=46.1306, time=480.63s\n",
      "Iter 2950: loss=45.5401, time=488.98s\n",
      "Iter 3000: loss=45.5130, time=497.24s\n",
      "Evaluation on 1101 elements. Correct: 525\n",
      "iter 3000: dev acc=0.4768\n",
      "Iter 3050: loss=46.0328, time=506.38s\n",
      "Iter 3100: loss=45.0976, time=514.57s\n",
      "Iter 3150: loss=45.8504, time=522.61s\n",
      "Iter 3200: loss=45.6310, time=530.88s\n",
      "Evaluation on 1101 elements. Correct: 518\n",
      "iter 3200: dev acc=0.4705\n",
      "Iter 3250: loss=44.4276, time=541.61s\n",
      "Iter 3300: loss=45.2225, time=549.61s\n",
      "Iter 3350: loss=45.6234, time=557.65s\n",
      "Iter 3400: loss=44.5448, time=565.76s\n",
      "Evaluation on 1101 elements. Correct: 525\n",
      "iter 3400: dev acc=0.4768\n",
      "Iter 3450: loss=45.5609, time=574.99s\n",
      "Iter 3500: loss=44.3878, time=583.27s\n",
      "Iter 3550: loss=45.4080, time=591.70s\n",
      "Iter 3600: loss=44.0529, time=600.09s\n",
      "Evaluation on 1101 elements. Correct: 495\n",
      "iter 3600: dev acc=0.4496\n",
      "Iter 3650: loss=44.7110, time=609.39s\n",
      "Iter 3700: loss=44.9073, time=617.78s\n",
      "Iter 3750: loss=45.0109, time=626.03s\n",
      "Iter 3800: loss=44.3909, time=634.35s\n",
      "Evaluation on 1101 elements. Correct: 520\n",
      "iter 3800: dev acc=0.4723\n",
      "Iter 3850: loss=44.1508, time=643.62s\n",
      "Iter 3900: loss=44.5473, time=652.00s\n",
      "Iter 3950: loss=44.2726, time=660.23s\n",
      "Iter 4000: loss=44.4519, time=668.39s\n",
      "Evaluation on 1101 elements. Correct: 504\n",
      "iter 4000: dev acc=0.4578\n",
      "Iter 4050: loss=43.6453, time=677.25s\n",
      "Iter 4100: loss=44.0125, time=685.32s\n",
      "Iter 4150: loss=43.4674, time=695.07s\n",
      "Iter 4200: loss=43.8528, time=703.21s\n",
      "Evaluation on 1101 elements. Correct: 528\n",
      "iter 4200: dev acc=0.4796\n",
      "Iter 4250: loss=44.7308, time=712.95s\n",
      "Iter 4300: loss=43.6788, time=721.29s\n",
      "Iter 4350: loss=43.4331, time=729.52s\n",
      "Iter 4400: loss=43.5499, time=737.71s\n",
      "Evaluation on 1101 elements. Correct: 530\n",
      "iter 4400: dev acc=0.4814\n",
      "Iter 4450: loss=43.3580, time=747.15s\n",
      "Iter 4500: loss=43.7589, time=755.62s\n",
      "Iter 4550: loss=43.6502, time=763.97s\n",
      "Iter 4600: loss=43.1091, time=772.45s\n",
      "Evaluation on 1101 elements. Correct: 526\n",
      "iter 4600: dev acc=0.4777\n",
      "Iter 4650: loss=43.1917, time=781.78s\n",
      "Iter 4700: loss=43.9488, time=789.88s\n",
      "Iter 4750: loss=43.2546, time=798.07s\n",
      "Iter 4800: loss=43.7121, time=806.04s\n",
      "Evaluation on 1101 elements. Correct: 520\n",
      "iter 4800: dev acc=0.4723\n",
      "Iter 4850: loss=43.7027, time=815.07s\n",
      "Iter 4900: loss=41.9577, time=823.06s\n",
      "Iter 4950: loss=42.9772, time=830.99s\n",
      "Iter 5000: loss=43.4154, time=838.92s\n",
      "Evaluation on 1101 elements. Correct: 528\n",
      "iter 5000: dev acc=0.4796\n",
      "Iter 5050: loss=42.6290, time=849.30s\n",
      "Iter 5100: loss=42.7422, time=857.42s\n",
      "Iter 5150: loss=42.1057, time=865.78s\n",
      "Iter 5200: loss=42.9778, time=873.73s\n",
      "Evaluation on 1101 elements. Correct: 528\n",
      "iter 5200: dev acc=0.4796\n",
      "Iter 5250: loss=42.8807, time=882.51s\n",
      "Iter 5300: loss=42.9193, time=890.55s\n",
      "Iter 5350: loss=42.1898, time=898.93s\n",
      "Iter 5400: loss=42.1935, time=907.00s\n",
      "Evaluation on 1101 elements. Correct: 525\n",
      "iter 5400: dev acc=0.4768\n",
      "Iter 5450: loss=42.8961, time=916.23s\n",
      "Iter 5500: loss=41.6538, time=924.22s\n",
      "Iter 5550: loss=42.2593, time=932.28s\n",
      "Iter 5600: loss=41.9371, time=940.16s\n",
      "Evaluation on 1101 elements. Correct: 515\n",
      "iter 5600: dev acc=0.4678\n",
      "Iter 5650: loss=41.7446, time=949.26s\n",
      "Iter 5700: loss=41.6941, time=957.24s\n",
      "Iter 5750: loss=41.8077, time=965.41s\n",
      "Iter 5800: loss=42.4169, time=973.94s\n",
      "Evaluation on 1101 elements. Correct: 528\n",
      "iter 5800: dev acc=0.4796\n",
      "Iter 5850: loss=42.3213, time=984.66s\n",
      "Iter 5900: loss=41.6230, time=992.54s\n",
      "Iter 5950: loss=41.2832, time=1000.57s\n",
      "Iter 6000: loss=41.2334, time=1008.57s\n",
      "Evaluation on 1101 elements. Correct: 534\n",
      "iter 6000: dev acc=0.4850\n",
      "Iter 6050: loss=41.6746, time=1017.50s\n",
      "Iter 6100: loss=42.2956, time=1025.61s\n",
      "Iter 6150: loss=41.0906, time=1033.40s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 6200: loss=41.1850, time=1041.21s\n",
      "Evaluation on 1101 elements. Correct: 525\n",
      "iter 6200: dev acc=0.4768\n",
      "Iter 6250: loss=40.8530, time=1050.09s\n",
      "Iter 6300: loss=41.1734, time=1057.89s\n",
      "Iter 6350: loss=41.2068, time=1065.75s\n",
      "Iter 6400: loss=41.8055, time=1073.56s\n",
      "Evaluation on 1101 elements. Correct: 511\n",
      "iter 6400: dev acc=0.4641\n",
      "Iter 6450: loss=40.1320, time=1082.59s\n",
      "Iter 6500: loss=40.2084, time=1090.61s\n",
      "Iter 6550: loss=40.3822, time=1098.40s\n",
      "Iter 6600: loss=41.3501, time=1106.46s\n",
      "Evaluation on 1101 elements. Correct: 527\n",
      "iter 6600: dev acc=0.4787\n",
      "Iter 6650: loss=41.0208, time=1115.30s\n",
      "Iter 6700: loss=41.0006, time=1123.30s\n",
      "Iter 6750: loss=40.5320, time=1132.78s\n",
      "Iter 6800: loss=40.7101, time=1140.57s\n",
      "Evaluation on 1101 elements. Correct: 518\n",
      "iter 6800: dev acc=0.4705\n",
      "Iter 6850: loss=40.7387, time=1149.66s\n",
      "Iter 6900: loss=40.4932, time=1157.36s\n",
      "Iter 6950: loss=40.3967, time=1165.41s\n",
      "Iter 7000: loss=39.7000, time=1173.51s\n",
      "Evaluation on 1101 elements. Correct: 531\n",
      "iter 7000: dev acc=0.4823\n",
      "Iter 7050: loss=39.3034, time=1182.87s\n",
      "Iter 7100: loss=40.7352, time=1191.48s\n",
      "Iter 7150: loss=40.2693, time=1199.40s\n",
      "Iter 7200: loss=40.3281, time=1207.54s\n",
      "Evaluation on 1101 elements. Correct: 510\n",
      "iter 7200: dev acc=0.4632\n",
      "Iter 7250: loss=39.8185, time=1216.71s\n",
      "Iter 7300: loss=39.3686, time=1224.85s\n",
      "Iter 7350: loss=40.0197, time=1233.05s\n",
      "Iter 7400: loss=39.2535, time=1241.33s\n",
      "Evaluation on 1101 elements. Correct: 520\n",
      "iter 7400: dev acc=0.4723\n",
      "Iter 7450: loss=39.4598, time=1250.09s\n",
      "Iter 7500: loss=39.8175, time=1257.92s\n",
      "Iter 7550: loss=39.0789, time=1266.08s\n",
      "Iter 7600: loss=39.2694, time=1274.05s\n",
      "Evaluation on 1101 elements. Correct: 485\n",
      "iter 7600: dev acc=0.4405\n",
      "Iter 7650: loss=39.8524, time=1284.45s\n",
      "Iter 7700: loss=39.6829, time=1292.27s\n",
      "Iter 7750: loss=40.3514, time=1300.19s\n",
      "Iter 7800: loss=40.0414, time=1308.01s\n",
      "Evaluation on 1101 elements. Correct: 501\n",
      "iter 7800: dev acc=0.4550\n",
      "Iter 7850: loss=39.4283, time=1316.83s\n",
      "Iter 7900: loss=39.0349, time=1324.56s\n",
      "Iter 7950: loss=39.5483, time=1332.48s\n",
      "Iter 8000: loss=38.8121, time=1340.68s\n",
      "Evaluation on 1101 elements. Correct: 523\n",
      "iter 8000: dev acc=0.4750\n",
      "Iter 8050: loss=39.0618, time=1349.55s\n",
      "Iter 8100: loss=38.3850, time=1357.53s\n",
      "Iter 8150: loss=39.1217, time=1365.18s\n",
      "Iter 8200: loss=39.4420, time=1373.16s\n",
      "Evaluation on 1101 elements. Correct: 512\n",
      "iter 8200: dev acc=0.4650\n",
      "Iter 8250: loss=38.4596, time=1382.41s\n",
      "Iter 8300: loss=38.9587, time=1390.40s\n",
      "Iter 8350: loss=38.5380, time=1398.43s\n",
      "Iter 8400: loss=38.3539, time=1406.54s\n",
      "Evaluation on 1101 elements. Correct: 514\n",
      "iter 8400: dev acc=0.4668\n",
      "Iter 8450: loss=38.7765, time=1415.74s\n",
      "Iter 8500: loss=38.9686, time=1425.51s\n",
      "Iter 8550: loss=38.6493, time=1433.95s\n",
      "Iter 8600: loss=37.6617, time=1442.50s\n",
      "Evaluation on 1101 elements. Correct: 519\n",
      "iter 8600: dev acc=0.4714\n",
      "Iter 8650: loss=37.9581, time=1451.40s\n",
      "Iter 8700: loss=37.8095, time=1459.44s\n",
      "Iter 8750: loss=39.0670, time=1467.80s\n",
      "Iter 8800: loss=38.0850, time=1475.94s\n",
      "Evaluation on 1101 elements. Correct: 471\n",
      "iter 8800: dev acc=0.4278\n",
      "Iter 8850: loss=37.1305, time=1484.92s\n",
      "Iter 8900: loss=37.4250, time=1493.11s\n",
      "Iter 8950: loss=38.7963, time=1501.29s\n",
      "Iter 9000: loss=37.2912, time=1509.35s\n",
      "Evaluation on 1101 elements. Correct: 498\n",
      "iter 9000: dev acc=0.4523\n",
      "Iter 9050: loss=38.2203, time=1518.25s\n",
      "Iter 9100: loss=38.3494, time=1526.28s\n",
      "Iter 9150: loss=37.6224, time=1534.32s\n",
      "Iter 9200: loss=37.3767, time=1542.33s\n",
      "Evaluation on 1101 elements. Correct: 509\n",
      "iter 9200: dev acc=0.4623\n",
      "Iter 9250: loss=37.4387, time=1551.25s\n",
      "Iter 9300: loss=37.7111, time=1559.49s\n",
      "Iter 9350: loss=37.5690, time=1569.34s\n",
      "Iter 9400: loss=37.0622, time=1577.38s\n",
      "Evaluation on 1101 elements. Correct: 501\n",
      "iter 9400: dev acc=0.4550\n",
      "Iter 9450: loss=36.8391, time=1586.43s\n",
      "Iter 9500: loss=37.9802, time=1594.38s\n",
      "Iter 9550: loss=38.0135, time=1602.62s\n",
      "Iter 9600: loss=37.4889, time=1610.95s\n",
      "Evaluation on 1101 elements. Correct: 514\n",
      "iter 9600: dev acc=0.4668\n",
      "Iter 9650: loss=36.6267, time=1619.36s\n",
      "Iter 9700: loss=36.8514, time=1626.76s\n",
      "Iter 9750: loss=36.8048, time=1633.75s\n",
      "Iter 9800: loss=37.7089, time=1640.87s\n",
      "Evaluation on 1101 elements. Correct: 513\n",
      "iter 9800: dev acc=0.4659\n",
      "Iter 9850: loss=37.0019, time=1648.91s\n",
      "Iter 9900: loss=36.4187, time=1656.24s\n",
      "Iter 9950: loss=36.8804, time=1664.89s\n",
      "Iter 10000: loss=36.0214, time=1673.31s\n",
      "Evaluation on 1101 elements. Correct: 518\n",
      "iter 10000: dev acc=0.4705\n",
      "Iter 10050: loss=36.2523, time=1682.55s\n",
      "Iter 10100: loss=36.6496, time=1690.71s\n",
      "Iter 10150: loss=36.9223, time=1698.60s\n",
      "Iter 10200: loss=35.7712, time=1706.65s\n",
      "Evaluation on 1101 elements. Correct: 499\n",
      "iter 10200: dev acc=0.4532\n",
      "Iter 10250: loss=36.1214, time=1717.24s\n",
      "Iter 10300: loss=36.1167, time=1725.43s\n",
      "Iter 10350: loss=36.7062, time=1733.59s\n",
      "Iter 10400: loss=36.9883, time=1741.66s\n",
      "Evaluation on 1101 elements. Correct: 484\n",
      "iter 10400: dev acc=0.4396\n",
      "Iter 10450: loss=36.2481, time=1750.93s\n",
      "Iter 10500: loss=35.0283, time=1759.16s\n",
      "Iter 10550: loss=36.1841, time=1767.42s\n",
      "Iter 10600: loss=36.3488, time=1775.54s\n",
      "Evaluation on 1101 elements. Correct: 509\n",
      "iter 10600: dev acc=0.4623\n",
      "Iter 10650: loss=36.2661, time=1784.76s\n",
      "Iter 10700: loss=35.2207, time=1793.04s\n",
      "Iter 10750: loss=35.4338, time=1801.37s\n",
      "Iter 10800: loss=35.7107, time=1809.66s\n",
      "Evaluation on 1101 elements. Correct: 507\n",
      "iter 10800: dev acc=0.4605\n",
      "Iter 10850: loss=35.8892, time=1818.83s\n",
      "Iter 10900: loss=35.9649, time=1826.87s\n",
      "Iter 10950: loss=36.1659, time=1834.96s\n",
      "Iter 11000: loss=34.7992, time=1842.84s\n",
      "Evaluation on 1101 elements. Correct: 477\n",
      "iter 11000: dev acc=0.4332\n",
      "Iter 11050: loss=36.0513, time=1852.14s\n",
      "Iter 11100: loss=34.3741, time=1861.72s\n",
      "Iter 11150: loss=35.1749, time=1870.16s\n",
      "Iter 11200: loss=35.9505, time=1878.32s\n",
      "Evaluation on 1101 elements. Correct: 479\n",
      "iter 11200: dev acc=0.4351\n",
      "Iter 11250: loss=35.7405, time=1887.31s\n",
      "Iter 11300: loss=35.0061, time=1895.14s\n",
      "Iter 11350: loss=34.3942, time=1903.11s\n",
      "Iter 11400: loss=34.3871, time=1911.24s\n",
      "Evaluation on 1101 elements. Correct: 502\n",
      "iter 11400: dev acc=0.4559\n",
      "Iter 11450: loss=35.1006, time=1920.17s\n",
      "Iter 11500: loss=35.1903, time=1928.31s\n",
      "Iter 11550: loss=34.9854, time=1936.63s\n",
      "Iter 11600: loss=34.5268, time=1944.76s\n",
      "Evaluation on 1101 elements. Correct: 499\n",
      "iter 11600: dev acc=0.4532\n",
      "Iter 11650: loss=34.6117, time=1954.00s\n",
      "Iter 11700: loss=35.0416, time=1961.97s\n",
      "Iter 11750: loss=34.5308, time=1970.49s\n",
      "Iter 11800: loss=34.4439, time=1978.70s\n",
      "Evaluation on 1101 elements. Correct: 494\n",
      "iter 11800: dev acc=0.4487\n",
      "Iter 11850: loss=34.6614, time=1988.13s\n",
      "Iter 11900: loss=34.9007, time=1996.36s\n",
      "Iter 11950: loss=34.6602, time=2004.69s\n",
      "Iter 12000: loss=34.4021, time=2012.88s\n",
      "Evaluation on 1101 elements. Correct: 498\n",
      "iter 12000: dev acc=0.4523\n",
      "Iter 12050: loss=34.0462, time=2023.48s\n",
      "Iter 12100: loss=33.0174, time=2031.88s\n",
      "Iter 12150: loss=33.0775, time=2040.37s\n",
      "Iter 12200: loss=34.3325, time=2048.66s\n",
      "Evaluation on 1101 elements. Correct: 494\n",
      "iter 12200: dev acc=0.4487\n",
      "Iter 12250: loss=35.1333, time=2058.25s\n",
      "Iter 12300: loss=34.1810, time=2066.61s\n",
      "Iter 12350: loss=33.6001, time=2075.08s\n",
      "Iter 12400: loss=34.0168, time=2083.28s\n",
      "Evaluation on 1101 elements. Correct: 494\n",
      "iter 12400: dev acc=0.4487\n",
      "Iter 12450: loss=33.9225, time=2092.54s\n",
      "Iter 12500: loss=33.4881, time=2101.12s\n",
      "Iter 12550: loss=34.3387, time=2109.18s\n",
      "Iter 12600: loss=32.9917, time=2117.51s\n",
      "Evaluation on 1101 elements. Correct: 476\n",
      "iter 12600: dev acc=0.4323\n",
      "Iter 12650: loss=34.1518, time=2126.40s\n",
      "Iter 12700: loss=33.9666, time=2134.55s\n",
      "Iter 12750: loss=33.1873, time=2142.55s\n",
      "Iter 12800: loss=33.1645, time=2150.37s\n",
      "Evaluation on 1101 elements. Correct: 507\n",
      "iter 12800: dev acc=0.4605\n",
      "Iter 12850: loss=33.0922, time=2160.55s\n",
      "Iter 12900: loss=32.6129, time=2168.63s\n",
      "Iter 12950: loss=33.9022, time=2176.48s\n",
      "Iter 13000: loss=32.0119, time=2184.37s\n",
      "Evaluation on 1101 elements. Correct: 485\n",
      "iter 13000: dev acc=0.4405\n",
      "Iter 13050: loss=32.9589, time=2193.10s\n",
      "Iter 13100: loss=32.4373, time=2201.07s\n",
      "Iter 13150: loss=32.9390, time=2209.34s\n",
      "Iter 13200: loss=33.0767, time=2217.24s\n",
      "Evaluation on 1101 elements. Correct: 498\n",
      "iter 13200: dev acc=0.4523\n",
      "Iter 13250: loss=32.5142, time=2226.29s\n",
      "Iter 13300: loss=33.2825, time=2234.32s\n",
      "Iter 13350: loss=32.5117, time=2242.24s\n",
      "Iter 13400: loss=33.5592, time=2250.05s\n",
      "Evaluation on 1101 elements. Correct: 472\n",
      "iter 13400: dev acc=0.4287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 13450: loss=32.0617, time=2259.01s\n",
      "Iter 13500: loss=32.1103, time=2267.27s\n",
      "Iter 13550: loss=33.6447, time=2275.88s\n",
      "Iter 13600: loss=32.0039, time=2284.05s\n",
      "Evaluation on 1101 elements. Correct: 477\n",
      "iter 13600: dev acc=0.4332\n",
      "Iter 13650: loss=31.3992, time=2293.11s\n",
      "Iter 13700: loss=31.7345, time=2301.37s\n",
      "Iter 13750: loss=32.4438, time=2309.61s\n",
      "Iter 13800: loss=32.7415, time=2319.48s\n",
      "Evaluation on 1101 elements. Correct: 482\n",
      "iter 13800: dev acc=0.4378\n",
      "Iter 13850: loss=31.8157, time=2328.86s\n",
      "Iter 13900: loss=32.6478, time=2336.98s\n",
      "Iter 13950: loss=31.1712, time=2345.21s\n",
      "Iter 14000: loss=31.8264, time=2353.41s\n",
      "Evaluation on 1101 elements. Correct: 486\n",
      "iter 14000: dev acc=0.4414\n",
      "Iter 14050: loss=32.2529, time=2362.63s\n",
      "Iter 14100: loss=33.1040, time=2370.83s\n",
      "Iter 14150: loss=32.4578, time=2379.11s\n",
      "Iter 14200: loss=31.8502, time=2386.94s\n",
      "Evaluation on 1101 elements. Correct: 476\n",
      "iter 14200: dev acc=0.4323\n",
      "Iter 14250: loss=31.5507, time=2396.01s\n",
      "Iter 14300: loss=31.3495, time=2404.16s\n",
      "Iter 14350: loss=30.6451, time=2412.68s\n",
      "Iter 14400: loss=32.6192, time=2420.74s\n",
      "Evaluation on 1101 elements. Correct: 478\n",
      "iter 14400: dev acc=0.4342\n",
      "Iter 14450: loss=30.4916, time=2430.10s\n",
      "Iter 14500: loss=31.3001, time=2438.26s\n",
      "Iter 14550: loss=31.4808, time=2446.22s\n",
      "Iter 14600: loss=31.7345, time=2454.16s\n",
      "Evaluation on 1101 elements. Correct: 473\n",
      "iter 14600: dev acc=0.4296\n",
      "Iter 14650: loss=30.8406, time=2464.75s\n",
      "Iter 14700: loss=31.3801, time=2472.89s\n",
      "Iter 14750: loss=31.7192, time=2480.86s\n",
      "Iter 14800: loss=30.6952, time=2489.38s\n",
      "Evaluation on 1101 elements. Correct: 484\n",
      "iter 14800: dev acc=0.4396\n",
      "Iter 14850: loss=30.7399, time=2498.81s\n",
      "Iter 14900: loss=31.2452, time=2506.90s\n",
      "Iter 14950: loss=31.6694, time=2514.97s\n",
      "Iter 15000: loss=30.5993, time=2523.32s\n",
      "Evaluation on 1101 elements. Correct: 475\n",
      "iter 15000: dev acc=0.4314\n",
      "Iter 15050: loss=30.4223, time=2532.73s\n",
      "Iter 15100: loss=31.8758, time=2541.07s\n",
      "Iter 15150: loss=31.0444, time=2549.15s\n",
      "Iter 15200: loss=31.3610, time=2557.67s\n",
      "Evaluation on 1101 elements. Correct: 485\n",
      "iter 15200: dev acc=0.4405\n",
      "Iter 15250: loss=29.7868, time=2566.75s\n",
      "Iter 15300: loss=30.4088, time=2574.95s\n",
      "Iter 15350: loss=30.4700, time=2583.10s\n",
      "Iter 15400: loss=30.6267, time=2591.26s\n",
      "Evaluation on 1101 elements. Correct: 503\n",
      "iter 15400: dev acc=0.4569\n",
      "Iter 15450: loss=30.4442, time=2600.39s\n",
      "Iter 15500: loss=31.1264, time=2608.77s\n",
      "Iter 15550: loss=30.1625, time=2617.08s\n",
      "Iter 15600: loss=30.1851, time=2626.59s\n",
      "Evaluation on 1101 elements. Correct: 484\n",
      "iter 15600: dev acc=0.4396\n",
      "Iter 15650: loss=30.0600, time=2635.69s\n",
      "Iter 15700: loss=29.9056, time=2643.87s\n",
      "Iter 15750: loss=30.5086, time=2651.92s\n",
      "Iter 15800: loss=29.7291, time=2659.48s\n",
      "Evaluation on 1101 elements. Correct: 488\n",
      "iter 15800: dev acc=0.4432\n",
      "Iter 15850: loss=29.4375, time=2668.43s\n",
      "Iter 15900: loss=30.7554, time=2676.44s\n",
      "Iter 15950: loss=30.2702, time=2684.39s\n",
      "Iter 16000: loss=30.1865, time=2692.62s\n",
      "Evaluation on 1101 elements. Correct: 480\n",
      "iter 16000: dev acc=0.4360\n",
      "Iter 16050: loss=29.7716, time=2701.53s\n",
      "Iter 16100: loss=30.2079, time=2709.72s\n",
      "Iter 16150: loss=29.1820, time=2717.71s\n",
      "Iter 16200: loss=29.4215, time=2725.90s\n",
      "Evaluation on 1101 elements. Correct: 457\n",
      "iter 16200: dev acc=0.4151\n",
      "Iter 16250: loss=29.3965, time=2734.95s\n",
      "Iter 16300: loss=29.4433, time=2742.83s\n",
      "Iter 16350: loss=29.3519, time=2750.97s\n",
      "Iter 16400: loss=29.4763, time=2758.96s\n",
      "Evaluation on 1101 elements. Correct: 468\n",
      "iter 16400: dev acc=0.4251\n",
      "Iter 16450: loss=29.1254, time=2769.34s\n",
      "Iter 16500: loss=29.3707, time=2777.08s\n",
      "Iter 16550: loss=28.9891, time=2785.09s\n",
      "Iter 16600: loss=29.7061, time=2793.13s\n",
      "Evaluation on 1101 elements. Correct: 470\n",
      "iter 16600: dev acc=0.4269\n",
      "Iter 16650: loss=29.0210, time=2801.80s\n",
      "Iter 16700: loss=29.1296, time=2809.64s\n",
      "Iter 16750: loss=29.3018, time=2817.87s\n",
      "Iter 16800: loss=28.7725, time=2826.26s\n",
      "Evaluation on 1101 elements. Correct: 465\n",
      "iter 16800: dev acc=0.4223\n",
      "Iter 16850: loss=29.2675, time=2836.23s\n",
      "Iter 16900: loss=28.9908, time=2844.82s\n",
      "Iter 16950: loss=29.0591, time=2853.26s\n",
      "Iter 17000: loss=28.9331, time=2861.37s\n",
      "Evaluation on 1101 elements. Correct: 484\n",
      "iter 17000: dev acc=0.4396\n",
      "Iter 17050: loss=29.4778, time=2870.69s\n",
      "Iter 17100: loss=28.4190, time=2878.77s\n",
      "Iter 17150: loss=28.7801, time=2887.22s\n",
      "Iter 17200: loss=28.5925, time=2895.44s\n",
      "Evaluation on 1101 elements. Correct: 484\n",
      "iter 17200: dev acc=0.4396\n",
      "Iter 17250: loss=29.2906, time=2904.56s\n",
      "Iter 17300: loss=28.5949, time=2914.52s\n",
      "Iter 17350: loss=29.2541, time=2922.67s\n",
      "Iter 17400: loss=28.3552, time=2930.80s\n",
      "Evaluation on 1101 elements. Correct: 479\n",
      "iter 17400: dev acc=0.4351\n",
      "Iter 17450: loss=27.8695, time=2940.08s\n",
      "Iter 17500: loss=28.7654, time=2948.03s\n",
      "Iter 17550: loss=29.0182, time=2955.96s\n",
      "Iter 17600: loss=28.8329, time=2963.84s\n",
      "Evaluation on 1101 elements. Correct: 485\n",
      "iter 17600: dev acc=0.4405\n",
      "Iter 17650: loss=28.0938, time=2972.97s\n",
      "Iter 17700: loss=27.8977, time=2981.12s\n",
      "Iter 17750: loss=28.1563, time=2989.07s\n",
      "Iter 17800: loss=28.0239, time=2997.11s\n",
      "Evaluation on 1101 elements. Correct: 475\n",
      "iter 17800: dev acc=0.4314\n",
      "Iter 17850: loss=28.0415, time=3006.15s\n",
      "Iter 17900: loss=28.2389, time=3014.16s\n",
      "Iter 17950: loss=28.0810, time=3022.14s\n",
      "Iter 18000: loss=28.1111, time=3030.36s\n",
      "Evaluation on 1101 elements. Correct: 488\n",
      "iter 18000: dev acc=0.4432\n",
      "Iter 18050: loss=27.6647, time=3039.37s\n",
      "Iter 18100: loss=28.7273, time=3047.20s\n",
      "Iter 18150: loss=28.4411, time=3055.70s\n",
      "Iter 18200: loss=27.9399, time=3065.83s\n",
      "Evaluation on 1101 elements. Correct: 470\n",
      "iter 18200: dev acc=0.4269\n",
      "Iter 18250: loss=27.7802, time=3075.02s\n",
      "Iter 18300: loss=27.2364, time=3083.09s\n",
      "Iter 18350: loss=27.3694, time=3091.25s\n",
      "Iter 18400: loss=27.0144, time=3099.22s\n",
      "Evaluation on 1101 elements. Correct: 481\n",
      "iter 18400: dev acc=0.4369\n",
      "Iter 18450: loss=27.5497, time=3108.38s\n",
      "Iter 18500: loss=26.7621, time=3116.40s\n",
      "Iter 18550: loss=27.2712, time=3124.55s\n",
      "Iter 18600: loss=27.8538, time=3132.65s\n",
      "Evaluation on 1101 elements. Correct: 472\n",
      "iter 18600: dev acc=0.4287\n",
      "Iter 18650: loss=27.5172, time=3141.63s\n",
      "Iter 18700: loss=26.8325, time=3149.80s\n",
      "Iter 18750: loss=27.2611, time=3157.88s\n",
      "Iter 18800: loss=27.8108, time=3166.06s\n",
      "Evaluation on 1101 elements. Correct: 463\n",
      "iter 18800: dev acc=0.4205\n",
      "Iter 18850: loss=27.0818, time=3174.97s\n",
      "Iter 18900: loss=27.4792, time=3182.84s\n",
      "Iter 18950: loss=26.9046, time=3189.80s\n",
      "Iter 19000: loss=27.4926, time=3196.77s\n",
      "Evaluation on 1101 elements. Correct: 455\n",
      "iter 19000: dev acc=0.4133\n",
      "Iter 19050: loss=27.2277, time=3206.36s\n",
      "Iter 19100: loss=25.8528, time=3213.47s\n",
      "Iter 19150: loss=26.9027, time=3220.54s\n",
      "Iter 19200: loss=26.8519, time=3227.60s\n",
      "Evaluation on 1101 elements. Correct: 469\n",
      "iter 19200: dev acc=0.4260\n",
      "Iter 19250: loss=26.8204, time=3236.28s\n",
      "Iter 19300: loss=26.2317, time=3244.68s\n",
      "Iter 19350: loss=26.5343, time=3253.14s\n",
      "Iter 19400: loss=27.5694, time=3261.74s\n",
      "Evaluation on 1101 elements. Correct: 471\n",
      "iter 19400: dev acc=0.4278\n",
      "Iter 19450: loss=26.2578, time=3270.85s\n",
      "Iter 19500: loss=26.3767, time=3279.20s\n",
      "Iter 19550: loss=26.4092, time=3287.76s\n",
      "Iter 19600: loss=26.0022, time=3296.18s\n",
      "Evaluation on 1101 elements. Correct: 475\n",
      "iter 19600: dev acc=0.4314\n",
      "Iter 19650: loss=26.4402, time=3305.39s\n",
      "Iter 19700: loss=26.4524, time=3313.65s\n",
      "Iter 19750: loss=26.4144, time=3321.54s\n",
      "Iter 19800: loss=26.7524, time=3329.77s\n",
      "Evaluation on 1101 elements. Correct: 470\n",
      "iter 19800: dev acc=0.4269\n",
      "Iter 19850: loss=26.5673, time=3338.51s\n",
      "Iter 19900: loss=26.2805, time=3346.63s\n",
      "Iter 19950: loss=26.1497, time=3356.12s\n",
      "Iter 20000: loss=26.6169, time=3364.05s\n",
      "Evaluation on 1101 elements. Correct: 477\n",
      "iter 20000: dev acc=0.4332\n",
      "Done training\n",
      "Loading best model\n",
      "Evaluation on 8544 elements. Correct: 4199\n",
      "Evaluation on 1101 elements. Correct: 534\n",
      "Evaluation on 2210 elements. Correct: 1006\n",
      "best model iter 2600: train acc=0.4915, dev acc=0.4850, test acc=0.4552\n"
     ]
    }
   ],
   "source": [
    "for _, depth_dict in all_subtrees.items():\n",
    "    for subtree in depth_dict:\n",
    "        subtree.loss.clear()\n",
    "\n",
    "tree_model = TreeLSTMClassifier(\n",
    "    len(v.w2i), 300, 300, len(t2i), v)\n",
    "\n",
    "with torch.no_grad():\n",
    "    tree_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
    "    tree_model.embed.weight.requires_grad = False\n",
    "  \n",
    "def do_train(model):\n",
    "\n",
    "    print(model)\n",
    "    print_parameters(model)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    return train_tree_model(\n",
    "        model, optimizer, num_iterations=20000, \n",
    "        print_every=50, eval_every=200,\n",
    "        prep_fn=prepare_treelstm_minibatch,\n",
    "        eval_fn=evaluate,\n",
    "        batch_fn=get_minibatch,\n",
    "        batch_size=32, eval_batch_size=64)\n",
    "\n",
    "DEBUG_TREE_PRINT = False\n",
    "results_larger_features = do_train(tree_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Large features')"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, axarr = plt.subplots(2, 2, figsize=(20,10))\n",
    "axarr[0, 0].plot(results[1])\n",
    "axarr[0, 0].set_title('Standard')\n",
    "# axarr[0, 1].plot(results_small_lr)\n",
    "axarr[0, 1].set_title('Small learning rate')\n",
    "axarr[1, 0].plot(results_small_batch[1])\n",
    "axarr[1, 0].set_title('Small batches')\n",
    "axarr[1, 1].plot(results_larger_features[1])\n",
    "axarr[1, 1].set_title('Large features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f7QZZH86eHqu"
   },
   "source": [
    "# Further experiments and report\n",
    "\n",
    "For your report, you are expected to answer research questions by doing further experiments.\n",
    "\n",
    "## Research Questions\n",
    "\n",
    "Make sure you cover at least the following:\n",
    "\n",
    "- How important is word order for this task?\n",
    "- Does the tree structure help to get a better accuracy?\n",
    "- How does performance depend on the sentence length? Compare the various models. Is there a model that does better on longer sentences? If so, why?\n",
    "- Do you get better performance if you supervise the sentiment **at each node in the tree**? You can extract more training examples by treating every node in each tree as a separate tree. You will need to write a function that extracts all subtrees given a treestring. \n",
    "    - Warning: NLTK's Tree function seems to result in invalid trees in some cases, so be careful if you want to parse the string to a tree structure before extraction the phrases.\n",
    "\n",
    "Optionally, you can also investigate the following:\n",
    "\n",
    "- When making a wrong prediction, can you figure out at what point in the tree (sentence) the model fails? You can make a prediction at each node to investigate.\n",
    "- How does N-ary Tree LSTM compare to the Child-Sum Tree LSTM? \n",
    "- How do the Tai et al. Tree LSTMs compare to Le & Zuidema's formulation?\n",
    "- Or.. your own research question!\n",
    "\n",
    "In general:\n",
    "\n",
    "- ***When you report numbers, please report the mean accuracy across 3 (or more) runs with different random seed, together with the standard deviation.*** This is because the final performance may vary per random seed.\n",
    "\n",
    "## Report instructions\n",
    "\n",
    "Your report needs to be written in LaTeX. You are required to use the ACL 2018 template [(zip)](https://acl2018.org/downloads/acl18-latex.zip) which you can edit directly on [Overleaf](https://www.overleaf.com/latex/templates/instructions-for-acl-2018-proceedings/xzmhqgnmkppc). Make sure your names and student numbers are visible at the top. (Tip: you need to uncomment `/aclfinalcopy`).\n",
    "You can find some general tips about writing a research paper [here](https://www.microsoft.com/en-us/research/academic-program/write-great-research-paper/), but note that you need to make your own judgment about what is appropriate for this project. \n",
    "\n",
    "We expect you to use the following structure:\n",
    "1. Introduction (~1 page) - describe the problem, your research questions and goals, a summary of your findings and contributions. Please cite related work (models, data set) as part of your introduction here, since this is a short paper.\n",
    "    - Introduce the task and the main goal\n",
    "    - Clear research questions\n",
    "    - Motivating the importance of the questions and explaining the expectations\n",
    "    - How are these addressed or not addressed in the literature\n",
    "    - What is your approach\n",
    "    - Short summary of your findings\n",
    "2. Background (~1/2-1 page) -\n",
    "cover the main techniques (\"building blocks\") used in your project (e.g. word embeddings, LSTM, Tree LSTM) and intuitions behind them. Be accurate and concise.\n",
    "    - How each technique that you use works (don't just copy the formulas)\n",
    "    - The relation between the techniques\n",
    "3. Models (~1/2 page) - Cover the models that you used.\n",
    "    - The architecture of the final models (how do you use LSTM or Tree LSTM for the sentiment classification task. what layers you have, how do you do classification? What is your loss function?)\n",
    "4. Experiments (~1/2 page) - Describe your experimental setup. The information here should allow someone else to re-create your experiments. Describe how you evaluate the models.\n",
    "    - Explain the task and the data\n",
    "    - Training the models (model, data, parameters and hyper parameters if the models, training algorithms, what supervision signals you use, etc.)\n",
    "    - Evaluation (e.g. metrics)\n",
    "5. Results and Analysis (~1 page). Go over the results and analyse your findings.\n",
    "    - Answer each of the research questions you raised in the introduction.\n",
    "    - Plots and figures highlighting interesting patterns\n",
    "    - What are the factors that makes model A  better than model B in task C? investigate to prove their effect!\n",
    "6. Conclusion (~1/4 page). The main conclusions of your experiments.\n",
    "    - What you learned from you experiments? how does it relate to what is already known in the literature?\n",
    "    - Where the results as expected ? any surprising results? why?\n",
    "    - Based on what you learned what would you suggest to do next?\n",
    "\n",
    "\n",
    "General Tips:\n",
    "\n",
    "- Math notation  define each variable (either in running text, or in a pseudo-legenda after or before the equation)\n",
    "- Define technical terminology you need\n",
    "- Avoid colloquial language  everything can be said in a scientific-sounding way\n",
    "- Avoid lengthy sentences, stay to the point!\n",
    "- Do not spend space on \"obvious\" things!\n",
    "\n",
    "\n",
    "An ideal report:\n",
    "- Precise, scientific-sounding, technical, to the point \n",
    "  - Little general waffle/chit-chat\n",
    "- Not boring  because you dont explain obvious things too much\n",
    "- Efficient delivery of (only) the facts that we need to know to understand/reimplement\n",
    "- Results visually well-presented and described with the correct priority of importance of sub-results\n",
    "- Insightful analysis  speculation should connect to something interesting and not be too much; the reader learns something new\n",
    "- No typos, no colloquialisms  well-considered language\n",
    "- This normally means several re-draftings (re-orderings of information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uCINIXV1q1oe"
   },
   "source": [
    "# Iterative attention LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_example(ex):\n",
    "    tokens = ex.tokens\n",
    "    rand_num = random.uniform(0,1)\n",
    "    if rand_num < 0.0:\n",
    "        random.shuffle(tokens)\n",
    "    elif rand_num < 0.0:\n",
    "        # Two words switching their place\n",
    "        index_1 = random.randint(0,len(tokens)-1)\n",
    "        index_2 = random.randint(0,len(tokens)-1)\n",
    "        tmp = tokens[index_1]\n",
    "        tokens[index_1] = tokens[index_2]\n",
    "        tokens[index_2] = tmp\n",
    "    \n",
    "    # for i in range(len(tokens)):\n",
    "    #     if random.uniform(0,1) < 1.0/(len(tokens)):\n",
    "    #         tokens[i] = \"<unk>\"\n",
    "    return tokens\n",
    "\n",
    "def prepare_minibatch(mb, vocab, is_eval=False):\n",
    "    \"\"\"\n",
    "    Minibatch is a list of examples.\n",
    "    This function converts words to IDs and returns\n",
    "    torch tensors to be used as input/targets.\n",
    "    \"\"\"\n",
    "    batch_size = len(mb)\n",
    "    maxlen = max([len(ex.tokens) for ex in mb])\n",
    "    if not is_eval:\n",
    "        aug_ex = [augment_example(ex) for ex in mb]\n",
    "    else:\n",
    "        aug_ex = [ex.tokens for ex in mb]\n",
    "\n",
    "    # vocab returns 0 if the word is not there\n",
    "    x = [pad([vocab.w2i.get(t, 0) for t in ex_tokens], maxlen) for ex_tokens in aug_ex]\n",
    "\n",
    "    x = torch.LongTensor(x)\n",
    "    x = x.to(device)\n",
    "\n",
    "    y = [ex.label for ex in mb]\n",
    "    y = torch.LongTensor(y)\n",
    "    y = y.to(device)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, \n",
    "             batch_fn=get_minibatch, prep_fn=prepare_minibatch,\n",
    "             batch_size=16):\n",
    "    \"\"\"Accuracy of a model on given data set (using minibatches)\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()  # disable dropout\n",
    "\n",
    "    for mb in batch_fn(data, batch_size=batch_size, shuffle=False):\n",
    "        x, targets = prep_fn(mb, model.vocab, is_eval=True)\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)\n",
    "\n",
    "        predictions = logits.argmax(dim=-1).view(-1)\n",
    "\n",
    "        # add the number of correct predictions to the total correct\n",
    "        correct += (predictions == targets.view(-1)).sum().item()\n",
    "        total += targets.size(0)\n",
    "    print(\"Evaluation on \" + str(total) + \" elements. Correct: \" + str(correct)) \n",
    "\n",
    "    return correct, total, correct / float(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, num_iterations=10000, \n",
    "                print_every=1000, eval_every=1000,\n",
    "                batch_fn=get_examples, \n",
    "                prep_fn=prepare_example,\n",
    "                eval_fn=simple_evaluate,\n",
    "                batch_size=1, eval_batch_size=None):\n",
    "    \"\"\"Train a model.\"\"\"  \n",
    "    iter_i = 0\n",
    "    train_loss = 0.\n",
    "    print_num = 0\n",
    "    start = time.time()\n",
    "    criterion = nn.CrossEntropyLoss() # loss function\n",
    "    best_eval = 0.\n",
    "    best_iter = 0\n",
    "\n",
    "    # store train loss and validation accuracy during training\n",
    "    # so we can plot them afterwards\n",
    "    losses = []\n",
    "    accuracies = []  \n",
    "\n",
    "    if eval_batch_size is None:\n",
    "        eval_batch_size = batch_size\n",
    "\n",
    "    while True:  # when we run out of examples, shuffle and continue\n",
    "        for batch in batch_fn(train_data, batch_size=batch_size):\n",
    "\n",
    "            # forward pass\n",
    "            model.train()\n",
    "            x, targets = prep_fn(batch, model.vocab)\n",
    "            logits = model(x)\n",
    "\n",
    "            B = targets.size(0)  # later we will use B examples per update\n",
    "\n",
    "            # compute cross-entropy loss (our criterion)\n",
    "            # note that the cross entropy loss function computes the softmax for us\n",
    "            loss = criterion(logits.view([B, -1]), targets.view(-1))\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # backward pass\n",
    "            # Tip: check the Introduction to PyTorch notebook.\n",
    "\n",
    "            # erase previous gradients\n",
    "            model.zero_grad()\n",
    "            \n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            # update weights - take a small step in the opposite dir of the gradient\n",
    "            optimizer.step()\n",
    "\n",
    "            print_num += 1\n",
    "            iter_i += 1\n",
    "\n",
    "            # print info\n",
    "            if iter_i % print_every == 0:\n",
    "                print(\"Iter %r: loss=%.4f, time=%.2fs\" % \n",
    "                (iter_i, train_loss, time.time()-start))\n",
    "                losses.append(train_loss)\n",
    "                print_num = 0        \n",
    "                train_loss = 0.\n",
    "\n",
    "            # evaluate\n",
    "            if iter_i % eval_every == 0:\n",
    "                _, _, accuracy = eval_fn(model, dev_data, batch_size=eval_batch_size,\n",
    "                         batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "                accuracies.append(accuracy)\n",
    "                print(\"iter %r: dev acc=%.4f\" % (iter_i, accuracy))       \n",
    "\n",
    "                # save best model parameters\n",
    "                if accuracy > best_eval:\n",
    "                    print(\"new highscore\")\n",
    "                    best_eval = accuracy\n",
    "                    best_iter = iter_i\n",
    "                    path = \"{}.pt\".format(model.__class__.__name__)\n",
    "                    ckpt = {\n",
    "                    \"state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"best_eval\": best_eval,\n",
    "                    \"best_iter\": best_iter\n",
    "                    }\n",
    "                    torch.save(ckpt, path)\n",
    "\n",
    "            # done training\n",
    "            if iter_i == num_iterations:\n",
    "                print(\"Done training\")\n",
    "\n",
    "                # evaluate on train, dev, and test with best model\n",
    "                print(\"Loading best model\")\n",
    "                path = \"{}.pt\".format(model.__class__.__name__)        \n",
    "                ckpt = torch.load(path)\n",
    "                model.load_state_dict(ckpt[\"state_dict\"])\n",
    "\n",
    "                _, _, train_acc = eval_fn(\n",
    "                model, train_data, batch_size=eval_batch_size, \n",
    "                batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "                _, _, dev_acc = eval_fn(\n",
    "                model, dev_data, batch_size=eval_batch_size,\n",
    "                batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "                _, _, test_acc = eval_fn(\n",
    "                model, test_data, batch_size=eval_batch_size, \n",
    "                batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "\n",
    "                print(\"best model iter {:d}: \"\n",
    "                \"train acc={:.4f}, dev acc={:.4f}, test acc={:.4f}\".format(\n",
    "                best_iter, train_acc, dev_acc, test_acc))\n",
    "\n",
    "                return losses, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionModule(nn.Module):\n",
    "    \"\"\"Realizes a very simple attention module. Takes last state and word into account with two-layer feedforward\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim, state_dim, hidden_dim):\n",
    "        super(AttentionModule, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.state_dim = state_dim\n",
    "        self.output_layer = nn.Sequential(OrderedDict([\n",
    "            ('fc1', nn.Linear(embedding_dim+state_dim, hidden_dim)),\n",
    "            ('tanh', nn.Tanh()),\n",
    "            ('dropout', nn.Dropout(0.5)),\n",
    "            ('fc2', nn.Linear(hidden_dim, 1)),\n",
    "            # ('fc', nn.Linear(embedding_dim+state_dim, 1)),\n",
    "            ('sigmoid', nn.Sigmoid())\n",
    "        ]))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"This is PyTorch's default initialization method\"\"\"\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_dim)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "    \n",
    "    def forward(self, word_embed, last_state):\n",
    "        \n",
    "        input_tensor = torch.cat([word_embed, last_state], dim=1)\n",
    "        \n",
    "        for layer in self.output_layer:\n",
    "            input_tensor = layer(input_tensor)\n",
    "        \n",
    "        return input_tensor\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterativeLSTMClassifier(nn.Module):\n",
    "    \"\"\"Encodes sentence with an LSTM and projects final hidden state\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, vocab, iterations):\n",
    "        super(IterativeLSTMClassifier, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_size = embedding_dim\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim, padding_idx=1)\n",
    "        self.rnn = MyLSTMCell(embedding_dim, hidden_dim)\n",
    "        self.attention_module = AttentionModule(embedding_dim, hidden_dim, embedding_dim)\n",
    "        self.iterations = iterations\n",
    "\n",
    "        self.output_layer = nn.Sequential(     \n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        self.state_dropout = nn.Dropout(p=0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        B = x.size(0)  # batch size (this is 1 for now, i.e. 1 single example)\n",
    "        T = x.size(1)  # time (the number of words in the sentence)\n",
    "        \n",
    "        input_ = self.embed(x)\n",
    "\n",
    "        # here we create initial hidden states containing zeros\n",
    "        # we use a trick here so that, if input is on the GPU, then so are hx and cx\n",
    "        hx = input_.new_zeros(B, self.rnn.hidden_size)\n",
    "        cx = input_.new_zeros(B, self.rnn.hidden_size)\n",
    "\n",
    "        # process input sentences one word/timestep at a time\n",
    "        # input is batch-major, so the first word(s) is/are input_[:, 0]\n",
    "        for iter_ind in range(self.iterations):\n",
    "            if iter_ind > -1:\n",
    "                # hx has dimension B,hidden_size -> need to be B,T,hidden_size\n",
    "                exp_hx = hx.unsqueeze(dim=1).repeat([1,T,1])\n",
    "                attent_hx_input = exp_hx.view((B * T, self.rnn.hidden_size))\n",
    "                attent_word_input = input_.view((B * T, self.embedding_size))\n",
    "                attention_scores = self.attention_module(attent_word_input, attent_hx_input)\n",
    "                attention_scores = attention_scores.reshape(B,T,1)\n",
    "            \n",
    "            outputs = []   \n",
    "            for i in range(T):\n",
    "                hx = self.state_dropout(hx)\n",
    "                cx = self.state_dropout(cx)\n",
    "                hx_new, cx_new = self.rnn(input_[:, i], (hx, cx))\n",
    "                if iter_ind == -1:\n",
    "                    hx = hx_new\n",
    "                    cx = cx_new\n",
    "                else:\n",
    "                    hx = attention_scores[:, i] * hx_new + (1 - attention_scores[:, i]) * hx\n",
    "                    cx = attention_scores[:, i] * cx_new + (1 - attention_scores[:, i]) * cx\n",
    "                    # print(hx.shape)\n",
    "                    # print(cx.shape)\n",
    "                hx\n",
    "                outputs.append(hx)\n",
    "\n",
    "            # if we have a single example, our final LSTM state is the last hx\n",
    "            if B == 1:\n",
    "                last_final_states = hx\n",
    "            else:\n",
    "                #\n",
    "                # This part is explained in next section, ignore this else-block for now.\n",
    "                #\n",
    "                # we processed sentences with different lengths, so some of the sentences\n",
    "                # had already finished and we have been adding padding inputs to hx\n",
    "                # we select the final state based on the length of each sentence\n",
    "\n",
    "                # two lines below not needed if using LSTM form pytorch\n",
    "                outputs = torch.stack(outputs, dim=0)          # [T, B, D]\n",
    "                outputs = outputs.transpose(0, 1).contiguous()  # [B, T, D]\n",
    "\n",
    "                # to be super-sure we're not accidentally indexing the wrong state\n",
    "                # we zero out positions that are invalid\n",
    "                pad_positions = (x == 1).unsqueeze(-1)\n",
    "\n",
    "                outputs = outputs.contiguous()      \n",
    "                outputs = outputs.masked_fill_(pad_positions, 0.)\n",
    "\n",
    "                mask = (x != 1)  # true for valid positions [B, T]\n",
    "                lengths = mask.sum(dim=1)                  # [B, 1]\n",
    "\n",
    "                indexes = (lengths - 1) + torch.arange(B, device=x.device, dtype=x.dtype) * T\n",
    "                last_final_states = outputs.view(-1, self.hidden_dim)[indexes]  # [B, D]\n",
    "            \n",
    "            hx = last_final_states \n",
    "            cx = input_.new_zeros(B, self.rnn.hidden_size)\n",
    "            \n",
    "            if iter_ind == 0:\n",
    "                # we use the last hidden state to classify the sentence\n",
    "                logits = self.output_layer(last_final_states) / self.iterations\n",
    "            else:\n",
    "                logits += self.output_layer(last_final_states) / self.iterations\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = IterativeLSTMClassifier(\n",
    "    len(v.w2i), 300, 168, len(t2i), v, 1) # 168\n",
    "\n",
    "# copy pre-trained vectors into embeddings table\n",
    "with torch.no_grad():\n",
    "    lstm_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
    "    lstm_model.embed.weight.requires_grad = False\n",
    "\n",
    "print(lstm_model)\n",
    "print_parameters(lstm_model)  \n",
    "  \n",
    "lstm_model = lstm_model.to(device)\n",
    "\n",
    "batch_size = 4096\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=2e-4)\n",
    "\n",
    "lstm_losses, lstm_accuracies = train_model(\n",
    "    lstm_model, optimizer, num_iterations=30000, \n",
    "    print_every=250, eval_every=50,\n",
    "    batch_size=batch_size,\n",
    "    batch_fn=get_minibatch, \n",
    "    prep_fn=prepare_minibatch,\n",
    "    eval_fn=evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lstm_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lstm_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLP1 Practical II (student version)",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
